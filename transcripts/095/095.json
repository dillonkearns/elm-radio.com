{"text": " Hello, you're in. Hello, Dillon. Well, typically, when we have a guest on, it's nice to give them a bit of a break and let them rest a little. But recently, we had Wolfgang Schuster on, and I thought, what better way to let him rest than having him back on the podcast to talk about rest endpoints and auto-generating APIs with Elm Open API? Wolfgang Schuster, welcome back to the podcast. Thank you for having me back. Great to have you. That was a good one. Yeah, this was, again, one of those episodes where I'm like, hello, you're in. Hello, Dillon. And I'm like, oh, no, he's going to make a pun now. Prepare for this. And this was a good one. This was a good one. You actually surprised me with it. Surprised you with it actually being a good one or with me doing it? Yeah. No, no. Let's say both. I didn't even see it coming. I was like, rest. Yeah, yeah. Okay. Oh, oh, rest endpoints. It does feel like we're giving a little balance to the universe, getting non-Martin guests a chance as well. So thank you for representing the non-Martins. Happy to. Happy to. So, so Wolfgang, you recently had a big release of Elm Open API. And why don't you give us a quick intro to what it is? Sure, sure. Elm Open API is a combination Elm package and Node CLI tool for generating, as you pointed out earlier, rest endpoints for Elm. The idea being kind of, I want to talk to some third party service or maybe an internal one and I need an SDK and no one has made an Elm one. So hey, now I have an Elm one. What is an SDK? Software development kit, technically, I think. I don't know. Yeah. Basically a way to talk to series of functions, endpoints, whatever to talk to some other service or, yeah, yeah. In this case, some other service. So yeah, let's, maybe let's give a definition of what Open API is. So it's a specification. What does that specification tell you? It tells you which URI endpoints service provides, usually slash API slash products or slash catalog or something along those lines. Incontain version information, whether it's a get post, put, delete, et cetera. What body needs to be passed if you're creating something, if you're creating something in your catalog, what do you need to pass in? If you're retrieving a list of catalog information, what does that catalog information look like? How is it structured? What optional things you need to send in? What security requirements, headers, bearer tokens, et cetera. Yeah. It's a way, it is in some ways kind of like GraphQL schemas, if people are familiar with those. It is a similar type of description, but for traditional rest endpoints. Right. Right. And so because it lives separate the API itself, right, or at least it can. So you could take an existing popular API and then create a JSON file that describes that API using the open API specification. Is that correct? Correct. Or you can go the other way too. There are tools to generate backend services, backend servers. So you have a specification and you're like, I need this to talk to my database. You can go that way. There are other things that are like PostgresQL can generate one of these schemas for you in JSON or even YAML as well. And then you can then generate an SDK from your Postgres schema and have Elm directly talk to a Postgres database. Yeah. There's a whole bunch of different directions you can go with it. There's even a, there's a cool tool company called Akita. I think they got, I think they're now part of Postman. Yeah. But they, they provided a way to watch your traffic from over your network and generate one of these schemas from that. So you could eat, you don't even have to hand write it. You could just watch network traffic with the, the Postman tool, generate a schema, generate an SDK, which is kind of a funky, cool little thing. Yeah. And then you hope that everyone was using your API the correct way and that they were using everything. You always help a little bit. You never know for certain. There's always, there's always education. Always. Yeah. I'm, I'm sure there are things that are neatly described by an open API specification, but then it suddenly returns HTML instead of JSON in the response and definitely doesn't conform. We've all been there. There's also companies like GitHub who, who define their own schemas and instead of an octet stream, define an octocat stream, which is not an actual thing, but hey, they have it returns octocat for you. It's fun unless you actually need it. Maybe. Yeah. Well, it is pretty cool. The, for, for anyone who hasn't seen open API specifications or, or use them in any way, it's more than just saying that this is a string. Like you can specify that something is an enum, you know, even if you're not defining types for your schema, you can just say, well, these are the four possible string values. This response can return or this is an integer in this range and things like that. So you can actually put constraints on both input and output values. And that's us. So it kind of sits on top of or sits next to JSON schema. And so JSON schema is used to define all of the, the, the bodies essentially. So those limits, like you mentioned, like if you're requesting price information and that price has, you don't want negative prices, you could set a minimum, minimum of zero and that's using JSON schema definitions, which is kind of fun too, that it's building on top of these prior art and existing things, existing tools. Right. And then there are also JSON schema validator tools in Node.js or other ecosystems where you can actually verify before you consume the, the data, the request data, you can verify that it conforms to that specification. Which is also what made my job easy is I used Elm JSON schema packages that already existed. I got to build on others work, just like open APIs building on others work. Oh, cool. What did, what did those tools help you do? Instead of having to write a parser for everything, I only had to write a parser for or decoder parser for just the open API portion of the schema. So all like the body decoders and encoders and everything that's all handled by existing Elm packages. So it probably saved me, I would guess a few, few good months at least. That makes sense. So what are some popular APIs out there that, that use open API? Well, I don't know how popular it is. My start was actually with Square. When I was working at Square some years ago, they produced an open API schema through a series. There's, there's was not handwritten. There's was actually generated. I believe it was proto buffs. They used to generate the open API schema and then that was both available to customers to use as from a development standpoint, as well as through various other things. So that's where I got my start. They generated an open API specification from proto buff. I believe that's what it was. If I remember right, yes, various teams write their own proto buffs. Those are merged into one giant proto buff essentially, which then generates an open API schema. Is it like a proto buff specification? Okay. Okay. Cause I have never used proto buff. The only reason I know about proto buff is about, is because Evan talks about proto buff in relation to Jason and all that. And was like, Oh, okay. Proto buff is a, is like Jason with different semantics. It's like, so how do you make a specification from Jason? Which, okay. You get, you have a Jason specification of the, sorry, as a open API specification. But okay. You get what I mean. Yeah. Yeah. It's just a series of specification transformers. Essentially you're going from one specification to another format to another format and eventually you get SDKs and can write code and yeah, send data. But yeah. So that was, that was my first experience. I had seen it before them, but that was my first like real hands on Leonardo. Most people in the, in the Elm community know him as mini bill. He actually helped me a ton with the package and both the Elm package and the CLI tool ton to ton. He was using it for Spotify. So they, they have their own open API spec. He was using it. I have never actually seen what he built with it. I know he was building some Spotify tool for himself though. So and it works for him, which is awesome. That's the whole reason I built this for, for people to be able to use it. Stripe I know from my time at square stripe being a competitive squares, like they have their own open API spec that people can use. I know there are lots of others. Most, I feel like most companies these days have some type of open API spec. I remember at my, at one of my previous companies, we, we had an open API spec or swagger spec because that's what it was called back then, right? And yeah, it was only for internal use, but that helped us make sure that the back end and the front end were in sync and having the spec also allowed for automated tests. So that was quite nice. Did you build any back end with those specific, with those specifications as well? Or I have not. I have been dreaming that somebody will use it on something like Lambda era for their back end, because a lot of these times these SDKs are more designed for back end because you're holding API keys and other secret keys that you don't want to expose to the client, especially payments. Payments is a great example. If you were connecting up to Stripe, you're given an API, a secret API key that is just for your company. You don't want to expose that to your customers to the front end. Yep. So you put that, you know, in your Lambda era back end and can do your communication with Stripe from Lambda era back end and present some type of interface to the front end. And yeah, there you go. Yes. So I don't have the business ideas yet to implement that and use that, but I know it's, I know there's something there. Yeah. That's very cool. I've heard too that the ChatGPT plugins are built around open API specifications, which is really confusing because the company is open AI, but they use open API, but they're not related. And also there's nothing open about open AI anymore. All that aside, it's... I've also been confused about reading open AI and... Yeah. Like in my mind it's like open API because I was researching this episode or the other way around. Gets me every time. And yet they actually are related because ChatGPT uses open API specifications to basically build a plugin. You give it a specification that tells it how to interact with your API and then use plain English to describe how to use those endpoints. So can you generate an open API spec using open AI? You totally can and it's really good at that too. Oh. Yeah. Full circle? Short circle, but... See, could use open AI to talk to Lambda era back end. Yeah. Yes. Exactly. I like it. Wait, does Lambda have endpoints, back end endpoints? I believe... I don't know if they're still in the labs testing area or if they're released fully or not, but it does have a way to communicate through REST to Lambda era back end. Okay. I missed that. Yeah. Interesting. Makes me wonder now, could you use open AI or co-pilot or something to look at your Lambda era back end code to generate an open API spec that you could then feed back into open AI to communicate to Lambda era in production? Snake eating his tail. We are in the inception part of the episode. Yes. It's fun though. I don't know. I like the code gen side of things. With open API makes for a lot of, I won't even say code, just generation of information, open API. Makes it really, already messing it up. Makes it really easy. Just go to Swagger. Swagger. Swagger, yeah. Swagger is... So yeah, those who aren't familiar with open API or only familiar with Swagger or confused by it all. Swagger is open API version two or lower. I don't know the history there, but they changed names between version two and three. They didn't have enough swag. They lost all their Swagger when they became open. But they were open about changing the name. So you mentioned, yeah, that it was for V2 and below. And when you open the Elm package you wrote, it says here are all the features we support from V3 and V3.1. And V2 is in the works, and maybe V3 is in the works as well. I don't remember. Kind of. When I have time, I work on it. Yeah. So 3.0 points. Why? I don't even know why there's a why there. Is not supported yet. 3.1 is mostly. What is hard about supporting that or is it just like it needs to put in some work or is it like a very different system? Not very different. No, just time to go back through their docs and see what changed between the two versions. They are mostly compatible versions. I haven't heard of anyone with like 3.0 not being able to use it. I mean if you go back to Swagger, any of the two 2.x versions, you might have to do a few tiny changes, but they're mostly compatible. So there's the CLI and there's the Elm package. The CLI is there to generate Elm code for people to use the SDK. Or I guess it is the SDK then. Or yeah, to use the rest, the rest endpoints. What about the Elm package? Is that meant to be used by people or is it just like you built the parser for the open API spec and you thought it might be useful for other people as well? Should I ever look at this package is what I meant? Yes. I do think there are benefits to it. So going back to Square, all this came to me as something I wanted to work on when I was at Square because of how we used it. So I mentioned earlier that we would generate this open API spec. But what happens from it there? We didn't just give it to users. One of the things we would do with it was send it off to another company called API Matic to generate SDKs. So that's kind of where I was like, oh, that's really cool, but they don't make an Elm SDK. I want an Elm SDK. I'm not going to pay them to make one. I'll just do it myself. So it's like, all right, well, in order to do that, I need to be able to parse the spec. So that's kind of where the package started. And then in addition to that, we would also take the spec and we would generate documentation. So if you go to Square's developer docs, a significant portion of that is run through the spec. We take the spec and they would add markdown and other documentation into the spec written by tech writers. And you have a website, basically, then from that. So if you wanted to, if your company exposed an open API spec that other people could use, you could, in addition to that, generate all of your documentation from that same spec, which is quite handy, then your documentation always matches your SDK, always matches your backend. Yeah. It seems like a, I mean, if you're using REST endpoints, it seems like a great way to go to get a lot of the tooling benefits that GraphQL would provide. It seems like a great alternative to GraphQL. And of course, GraphQL has its share of tradeoffs as well. The whole overfetching, GraphQL is trying to solve overfetching and N plus one database, you know, N plus one queries from the front end, but it makes it really hard to avoid N plus one queries in the database because you have to tack on all these additional queries and use creative ways to like aggregate them into one sort of query runner, which is very difficult is a difficult problem. So, so some people like using REST APIs for their back end. And if you do, why not use open API and if you get a nice specification, it's way nicer to use from, from Elm. Also not everything goes through a get request with, with GraphQL, everything goes through get request, right? Or post or post request. Yeah. Oh, post request. Was that it? Yeah. Yeah, I think it's two either. But yeah, usually people do post request with a body. Wait, you can do either. Can you also do it with a delete request? Maybe. If you're a psychopath, then yes. Yeah. Yeah. No, I think the comparisons to the GraphQL, I think are, are very warranted. It is very, very similar in many ways to, to working with GraphQL. You get a lot of similar tooling, a lot of similar typed benefits in a way, a lot of similar guarantees. I think I haven't worked with GraphQL long enough. I've worked with REST for over a decade now. I've worked with GraphQL for over a year. So I feel like I still have a lot of learning on how to best use GraphQL. I think with REST, it's a little bit more fixed in terms of what to expect, which is kind of nice. There's also, so other, other ways, since we were talking about, would, would you rune use, use the Elm package? Another thing I've been wanting to explore and bring it back to beginning with Martins. I know there is a Martin already exploring this. Which one? Which one? Oh, I'm trying to, I'm blanking on his last name. Stuart? No, no, no. Wait, another one? Yes. We were talking in Elm, the Elm online meetup a few weeks ago about OpenAPI because the company works for is using it. They're actually using the package, not the CLI tool. So you can too. But they're looking at doing form generation. So because it's using JSON schema under the hood for the bodies of the posts and gets and deletes, you could just use whatever else exists for JSON schema and you could do form generation. I think he is actually using Dillon, your library as well, for the form side. So he's kind of combining the Elm Open API and Elm forms to do form generation. All of these schemas definitely, you squint your eyes and you're like, hmm, these do look similar. Yeah, in the blog post you wrote about effortless SDKs, which introduces Elm Open API. You do talk about forms. So is this something that people can do already or is this something that you want to work on later? Something I want to work on later. When I was at Strange Loop, I was it was chatting with someone who they know they're Martin. No, no. I want to say if my memory is not failing me, I believe his name was Kevin. He was doing some biomedical stuff prior and doing some form generation and had been using ClosureScript for the form generation and was just trying to explore what else was out there. His back end was in Rust. That's how the machines were communicating with the web interface. And so he was exploring what else is out there. I was like, you know, I think there is something Elm could do there. I don't know for certain, but I think there is something. We got to chatting about JSON schemas, which does most of the data portion. And then there's also UI schema, which is UI definitions for rows, columns, and a bit of other stuff to use in conjunction with JSON schema for defining forms. So there is there is an existing schema for defining forms on top of JSON schema. So I think I think I could probably my my idea is to try out something in that area. I don't know what it looks like yet. I haven't haven't had the time to get around to it. But I think there could be something there. So a little bit confused. Can you define all the necessary validations on data through the open API spec? Or is it too limited? I'm guessing you can do relationships between two objects or whatever. But relationships are not certain. You might be able to. I'm not 100% certain offhand. You can define so it's not on the open API that's on. So the validation is done on JSON schema, which is confusing. But yeah, but open API spec is JSON schema also. It uses JSON schema for defining request bodies and response bodies. OK, because you did mention like, yeah, you can say that a price has to be a minimum zero. So there are some validations. So I'm curious about like, if I'm using Elm open API, so I've I've run the CLI tool. I've generated some code for the Spotify API or some open API spec. And now I've got a folder full of generated Elm code that I can use for all of the rest endpoints in my specification. Tell us a little bit about that code. Is it like what are the what are the functions that it's giving giving you to consume these APIs? What do those look like? Yeah, it is it is giving you your standard HTTP command requests. Say you had a create product definition endpoint that so you would give you a create product definition, it would take whatever data is necessary for actually creating a product name price, something along maybe a description, optional description. It would also provide you all the types for what a product is encoders decoders if you need to use it somewhere else. It will auto wire up all the encoding decoding for you in the request itself. But if you need to use it to store in local storage or some something else outside of the API, all that is provided for you are exposed for you. That is a pretty cool detail because we just talked about concurrent task in last episode. And it defines its own tasks, meaning that you could probably not reuse the HTTP request if you wanted to be able to use them concurrently. But if you have the building blocks to do that manually, you could still do it, although you probably still would like to have some kind of cogeneration because it's a lot of code otherwise. Don't recall from the episode or from reading the docs. If you can mix Elm tasks with Elm concurrent tasks. No, no, not really. I mean, it's its own thing. You definitely can't create an Elm concurrent task from an Elm task. Okay, okay. Yeah, so there would be a limitation there because I do I do also expose a task version. So there is like a there would be a create product, which we're going to turn a command. And then there's also, I don't remember the naming scheme I use, but it is like create product task that would create a task as well. Just in case you don't I don't know what what you're going to want to consume. Maybe you need both. Right. So you have the create article, create article task, or you have the decoder article encoder, you have everything is generated and exposed. That's nice. The one thing that is not not released yet, I am working on it. I think I'm getting close to something that I like is don't make promises on this podcast. Don't. No, no, I am actually decently close, but is is is air handling instead of returning just an HTTP error. The the spec or open API specs can also define what type of error to return what the error messages can look like, which I think is very useful. That's that is one thing that is. Yeah, that's really nice to have. So I'm trying to come up with a better air type that is specific to your API so that you get this custom air type. And then if it can't if for whatever reason you're back and returns something invalid or some other network traffic error occurs that you can fall back to to a regular error of some kind. So that's that is the my experiments with that are a little weak. I've I've been using the real world app for a lot of my local testing and local experimentation. And that only defines two custom errors, which are there's a 401 on authorized error. And there is another one called generic error in the spec I'm looking at, which is the error for everything else. Yeah, so and I've I've also learned from this that there are actually multiple implementations of the real world backend in terms of what that spec looks like, which means sometimes my test work or sometimes my experiments work with the backend that I'm hitting and sometimes they don't. So that's that's my latest or my current current work on the open API stuff. Yeah, it it's very cool how if a if an endpoint requires an authorization token or a particular query param with a given name, the code that elm open API generates shows reflects that so it gives you the record argument to call the function has the authorization token and the query programs that you need to pass in of the correct types. One thing I can't help but think about. And if you're playing Elm Radio bingo, now's the time to get out your bingo cards. What if you wanted to create some sort of opaque type, let's say, Ding, ding, ding, ding, ding, please. I think that was going to be my next question as well. What a surprise. Yeah, so like with with Done Kerns on GraphQL, for example, you know, I'm I'm a big fan of using opaque types as much as possible, both decoding into and using, you know, custom input types and that sort of thing, like for an authorization token, for example, I might want to have a wrapped opaque type that is actually an authorization token and maybe that I can't accidentally pass a string oops, that I mixed up the names of the authorization token and the username and now that's getting logged to, you know, the console or something like that, right? And same with like getting back data, things like enums or things like a non negative integer, like a price, I might want to have an opaque type to make sure I don't cross wires for these different result types that I'm getting back and have certain guarantees that I that I know invariance about those values. Has that been on your radar at all? Have you thought about that? Is that compatible with this approach? Yes, yes, yes. A thousand times yes. I'm glad you didn't put a no in there because I didn't know to which question it would have been an answer to. I honestly don't know which one I would have lined up with. So the the the tokens that had not specifically crossed my mind, like that specifically, I do really like that, though. I don't know if I could make a special exception just for that or not. I'm tempted to. I do. So my thinking has been that in this kind of goes with the form generation as well, because I think it kind of benefits both sides is the schemas. All these schemas have a way to define your own properties on the schema. You usually form like X dash custom property name and that can do whatever you want. Essentially, it can mean whatever you want. And I think there might be a way to use that to define custom ways to handle encoding and decoding and maybe form, form handling as well. So like an example, there would be units. No one's familiar. There there is a great package from Ian McKenzie called Elm units. That is really fantastic for handling anything with any type of unit value, like feet to meters, you don't want to mess that up. So why not use a package that just does it for you? Similarly, if you're dealing with endpoints that are to have that kind of data similar to a token, like I don't want to pass in the wrong string for a token, I don't want to pass in feet when I want to use meters in my endpoint. That would that could be catastrophic in some cases. There are spaceships that have blown up because of that. So I haven't had the chance to experiment yet in code itself, but I've been thinking for months now that there is probably a way to handle automatically pulling in a package. So a way to find I want to use this package for this type of data and have it auto encode, decode that type of data, wrap that type of data in whatever wrapper it needs. Yeah, I really, I think there's a lot of potential in that area, a lot of potential. That would be, that would be amazing. Yeah. So like in the, in the case of Dillon Kern's Elm GraphQL, what I did is I introduced, you know, this custom scalar codecs file where you, for each custom scalar in your schema, you define a codec encoder and decoder pair. And now the nice thing about the GraphQL spec in that regard is you do have this concept of a nominal type, a type which is not just a set of fields of particular types. It's like, this is a, you know, a unit of, of feet or of meters, like you can specify custom scalars and give it a name and say, well, the underlying representation might be exactly the same as this other thing, but it's not like a, if it quacks like a duck, it's like this is units and meters, not to be confused with some other unit that we use, unit and millimeters or something, you know. So the GraphQL schema definition language has this notion of users being able to define, to define their own custom scalars. And then Dillon Kern's Elm GraphQL just provides a way to define encoder, decoder pairs for each of those custom scalars in the schema. Now with OpenAPI and JSON schema, I wonder like, is it more duck typed by its nature in the sense of it's just saying, well, this is an integer and its minimum value is this and it doesn't have a maximum value. And you're not really naming things in one central location where you give something a name and then you reference that named thing you defined at the top. I will say it's limited by JSON a little bit. So the defaults are things like a generic number or a string, an array, they're very limited in that scope initially. There is a portion of the schema that is just for defining, called objects for, since it kind of fits with JSON a little bit. So you, when you define your product, your product could be a ruler. So a ruler measures things, but doesn't tell you just by the name if it's in imperial or metric, you have to actually see it or have something else. And that might actually be defined more in that object schema portion of the full schema, the full spec. So there you could, you might have a minimum of like, a ruler is never going to be less than zero, so a min of zero and a max of maybe 50 or something, I don't know. You, on top of that, if you wanted to define units, so if you wanted to know if it was imperial or metric, you would most likely, I think, use a custom property that was unique to your API. Those, those are quite common using custom properties. It's one thing that is probably the one thing I'm not sure entirely how to support yet in, in my package or my CLI tools, because they, they can be anything almost that basically any JSON schema definition can be used there. Although I do like that you mentioned having a separate config specifically for some of this. Think I hadn't considered that entirely, but I kind of like that because if I'm using, if I'm using Spotify's API, I can't modify their, their API spec, their open API spec. I can't modify it's not my, I mean, I could download it and edit it. But then when they release a new version, I have to go and manually edit it again. And that's, that's not fun for somebody. So having the option to either include your own custom properties or for a third party spec, being able to define your own local config to add on to it, essentially that that could be a very nice route to go. Yeah, if there was some way to hook into that, that would, because I guess what often happens is like things, you know, tools around GraphQL and JSON schemas, you know, a lot of people are using them with TypeScript maybe, and they're just like, okay, let's just add TypeScript types. And they use sort of the lowest common denominator, primitive types to describe it. And that, and that's pretty nice. And then with, you know, with TypeScript, you have an enum and it can be five different strings. And then in TypeScript, you can just describe that. You can say, this is one of these five strings, which is really nice. But at the same time, it is, you know, we also like opaque types and being able to have certain guarantees around those things. So we also like custom types. And so if you want to do more nominally typed things, TypeScript isn't, doesn't give you the same guarantees with that. And it's not the path of least resistance. And a lot of tools end up just catering to that sort of, hey, let's nicely describe what primitives we're using everywhere. But to me, the really interesting thing is when you go beyond that and you say like, okay, well, it's giving us all these primitives as a starting point, but then we can actually describe, give more semantic meaning to that set of primitives and use opaque types around them. Yeah, I do. There absolutely is a reason for, like you pointed out, being a being very minimal in your definitions. We ran into that a lot at Square with our open API spec in that we were having to support, we support TypeScript, which was usually fine. Ruby was usually fine. Java and C sharp were always a bit frustrating, more, more due to like the generated definitions would, you know, the, if everyone's familiar with working with, especially with Java, you would end up with function calls that have like six nullable arguments. And so sometimes you just have to pass in null like four, five, six times. That was always fun. But yeah, so you don't know, like you're having to potentially target many different languages with very different semantics. And it's hard to define something that fits all of them equally. Everyone's going to be slightly disappointed in their, in their generated SDK. So if you, yeah, if you have your own little config, you say, cool, I like this, but I'm going to tweak it to fit my language or even to fit my app, like just to be able to fit your specific project. Very handy. Very nice to have too, too many ideas now. Yeah. To throw another idea out there. It, uh, like, I, well, I'm not sure if you do this at all. I think maybe you don't, but for enums in a JSON schema, you can describe like an enum like these are the five possible string types for this value. Right. Do, do those get generated as string values or custom types? Believe they get generated as custom types right now. They do. It was, they do. I'm fairly certain they do. I went back and forth on that a lot actually. Intuitively, you think that's the right way to go. However, there are, there are weird edge cases, um, which I only know from dealing with them at square in that people like to sit on one version of an SDK for a very long time, sometimes. And so if you, you keep updating your backend to define new, new variations on that enum, the SDK might not be able to handle that anymore. Uh, and so we would run into issues with customers on very old versions of the SDK, not having the time, maybe even to upgrade. Maybe they're just swamped with other feature work. They can't, you know, spend a day or a week even upgrading the SDK, but you don't want to break them either. So we ended up, at some point, we ended up switching most of our SDKs back to using strings for enums for that specific reason, which is a bit disappointing, but it also kind of makes sense from that standpoint. I do think this did inspire me though. So maybe that's where opaque types come in. You could probably with Elm instead of exposing the strings directly. Right. Yes. I wonder if there's some way to do something opaque or just expose them through. Like if you want to create them, you just expose a function name that behind the scenes is still generating a string, but hidden within some opaque type. Yes. So you could totally do that. And, and the benefit to that would be that, uh, from an API point of view, of course, if you're not actually publishing a package with this generated SDK from Elm Open API, then the, the breaking changes thing is more of a, you know, you know, when it's broken because your code doesn't compile, but, but from the point of view of like publishing breaking changes, you can add an enum variant and it's not a breaking change with an opaque type because you're exposing a new function. So that's a minor version bump, not a major version bump. But if you remove one, then it's a major breaking change, uh, because, because it is, because it could break someone's code. Uh, whereas with a non opaque custom type, you're in, we should, uh, we should lobby to, to call them explicitly non opaque. We should lobby to call them non opaque types. Transparent types. Bad types. Yeah. Bad types, the good types and bad types. It is considered in a published API, a breaking change. If you, um, if you add a variant, because, you know, you could have a case expression that you now have to handle that one additional thing. Now, on the other hand, so, so I'm able to do case expressions is handy. So that's a trade off too. Yeah. But, uh, you mentioned if you remove, uh, a possibility in the API, then that is also a breaking change for the backend, right? Because you're not, not, you're now not able to call it to call, get article with type, uh, some vegetable. I don't know, like vegetables are not sold anymore. Vegetables don't exist anymore. Well, now you can't send that anymore. And that's a breaking change also. So that shouldn't happen in that case. Usually the people go to a V two or V three of that same endpoint, right? So usually what would happen is you would add a new, new, new vegetable type. Uh, so carrots for whatever reason, you forgot to add carrots in, in your initial release of your backend and you go and you're like, shoot, I forgot those. You add carrots. That's, that's stupid because that's the best vegetable. I, I is a fantastic vegetable. It's probably up in my top three. But hey, you know, you were busy. You, you were working 12 hour days for whatever reason to get this shipped. Cause you're full and work. Carrots before hands. Yup. Exactly. So you ship it, a customer goes and installs version 1.0.0 of your SDK and they go and they use it and they're like, great. And they don't actually use that vegetable type. They don't care about it, but it's still part of the response. So the response decoder has to decode it. They don't care about it where they don't care it. Sorry. Nice. Nice. They don't care about it. So they, you go, you add carrots. It's a non-breaking change. It is. It's a minor, minor thing. You've added a new type. Their decoder doesn't support it. So their decoder, if it got carrots would fail because it doesn't recognize it, but they're not actually using it. So now you've broken their, their, their SDK for no, no apparent reason. But if you wrapped it in just a very small, slim, opaque type that took it as just like unknown, unsupported type, they can keep working. They're fine. And everyone else who wants carrots, they still have access to carrots. The end user might still be defining a custom type that, you know, they're going to be adding carrots to and handling that in their own code, right? So in a way it, it passes the buck downstream. So it's, it's an, it's an inherently challenging question. There's no easy magic bullet to it. Yeah. If a new possibility arises, like a carrot gets introduced by the back end, then your whole front end doesn't compile anymore because you've upgraded this back. And now the front end needs to real quick handle that case. I think there's, so there is an edge case here. If you are working with open APIs internally within your company, odds are you do want the custom type, regardless if it's opaque or not, you do want, you don't want to use stringly typed enums. You want the custom type because if, if your back end does change something, you want that reflected in your front end. However, if you are using this between companies, possibly between multiple companies like, like Spotify or something like that, then the custom type isn't, I think there's more risk involved with the custom type in terms of breaking the customers, the customer's application, which you don't want to do. There is still the edge case that they could forget to or customly handle carrots or rutabagas or whatever else they decide they need to support or you forgot to support. But that's like one of the worst vegetables. Oh, poor Vegas. They deserve it. But yeah, you don't want to, you don't want to break your customers and point, and they might not have the time. There isn't the same level of communication that you have between teams within a company. So maybe that's something that is like a flag maybe on or in your config for when you generate your own SDK from this. Maybe I have something that says, I want custom types for enums or I want strings for enums and let, let them choose or let them override to whatever fits their product the best. At work, we've had some, we use Elm GraphQL Dillon's version. And some of the things are using custom types. And that creates some problems. For instance, when we do migrations, like the backend migrates to a new version where it needs to support having older versions or newer versions of the front end talking to it or the other way around. Like you've seen Mario Rogers talk like all those variations. Well, we need to support those. And therefore we are thinking of like moving those custom types into stringified, stringified types, which is not great. But it is a lot more flexible. It is a lot easier to ignore some things that are unknown. And you just say, okay, well, the front end doesn't know about carrots. But if it gets a carrot, what should, what should we do? Should we just ignore the message or should we crash hard? And in some cases, it's fine to just ignore it. It really depends on the use case, obviously, or the situation. So we also use GraphQL at work. And I think the way we handle that is we generate a hash, essentially, of our GraphQL schema or whole GraphQL schema and just upload that as a single file and then use that as a versioning thing. And if the front end sees that that has changed, then it will refresh the page, essentially. Kind of like if you're on Slack on the browser or Discord or many other messaging apps, and they're always like, a new version is available. Click this button to update when it just reloads the page. Or Discord, which will just not let you look at anything if it's outdated, which I get. I totally get that. That's fair. Well, it really depends on the situation, right? Because if you're on Discord, you're typing a message and it says, oh, I need to update. Okay, well, let's refresh. Your message is still in the box. It's fine. But if you're editing a very complex form, you don't want to lose your things. If you're doing something that takes a long time to set up somehow, then you don't want to lose that. Right? I think it depends on the size of the company, too. I don't know how big you're at CrowdStrike, right? And I don't know how big CrowdStrike is, but if you get to the point where you have multiple back-end teams interfacing with multiple front-end apps and one of them wants to change an enum and add or remove that enum, that can be very difficult to coordinate across teams. A lot more difficult than one back-end team and one front-end team, essentially. Or like one front-end app and one back-end app. Yeah. So it mostly depends on who is your customer. Is your customer the same mono repo application with one back-end, one front-end, and then they're always in sync because there's always the same person who makes the back-end changes knows how to do the front-end changes as well. Or do you have back-end that is not always in sync with front-end or you use web components or you're shipping part of your products in not the same way? So yeah. So the flexibility, right? Yeah, it's the hard part. I always like to view it as the hard part is communication. And if the communication is hard, maybe strings are easier. Communication is easy. Then go with custom types when you can. I mean, you communicate with chat GPT only with text. Text. Image is nowadays as well, right? But yeah. Yeah. Yeah. Yeah. I was also thinking too, other things I've gotten to look at using this with, I know super base is a back-end service like database slash back-end as a service. You can also, I believe generate, I think they're swagger specs, but you can generate those from it, which means you could build an L map on top of super base with generating all the rest end points or the rest SDKs for your front-end. I started experimenting with it, but yeah. Again, have no business business ideas to actually build with it. Is your Elm open API the first time that a production ready version has been shipped? Like that. I feel like there have been murmurings of this for a long time. I mean, it's like an obvious fit for Elm because of its types. There is a, if you're talking open API and Elm, specifically, there is a Java based one. Trying to remember, it was pointed out, I might have come across it a while ago, but someone pointed it out to me recently. That one, I think that one is, it's from open API, general or open API tools. They release an open API generator and it can generate Elm. There's also a swagger decoder Elm package, but not, it doesn't generate code as far as I can sell. Do you think everyone ever seen that at one point? I probably did and then promptly forgot about it while looking at other packages. The, so when the open API tools one got brought up to me, I was very curious to see how ours differed because I didn't look at it at all while working on my stuff. It is one thing I do like that it does is it splits out into multiple Elm modules, kind of around types a little bit. It splits it out based on, so it splits out your endpoints into one module, your request body, response body objects into another file. And I think there might be a third one it generates as well, which is kind of nice because it breaks things up a bit. I would like to do the same though, I think more around types themselves instead of around functions. I think that feels a little bit more Elm like to me. I think the open API stool feels a little bit more Java to me. And then same with, I noticed too, with how you call functions feels a little bit more Java like and how they're written for it, bringing back a little bit to off off stuff and tokens. The API tools generation provide like a with authentication function to allow you to add that bearer token or JWT or whatever it is to your request. But it's always optional and you don't know if you need it or not for your endpoint, which doesn't feel Elm like to me. Like if this function requires, if this endpoint requires a token, then it's, you should be forced to pass it in, which is what mine does. If for every endpoint that requires a token, you're forced to pass in that token. Otherwise, what's the point in requesting the data because it's just going to fail? So I think, yeah. So I think there's something there where maybe I definitely need to break mine out so that you don't have an API module that's 5,000 lines long, 10,000 lines long. How do you manage like staging API URLs versus dev URLs versus production URLs? Is there a way to handle that? I think there is. I hadn't actually thought about it too much. So the way the schema works is there is like a top level URL defined. So Spotify's might be like HTTPS colon slash slash Spotify.com slash API. That is then used for all the endpoints and the endpoints are then just slash playlist or slash artists. And it all gets joined together. I believe you can define multiple top level API or yeah, top level URLs. So I guess I'm not sure what that looks like, but that probably should be something that's handled. Yeah, because you could talk to like a real world backend. But you can just say, well, whatever URL, whatever backend I'm targeting, the URL is defined not at code generation time, but at runtime or right. But that doesn't work with your approach as far as I can tell because you generate that URL inside the you put that URL inside the generated code. But that could be an argument rights as well. Yeah, there there likely are a lot of additional arguments that need to be optional ones, like completely optional. The real world server might be behind a proxy of some kind or something like that. And so being able to say with proxy URL or something along those lines would be a very nice thing to prepend every URL requests that you make with that. There could be other things as well, maybe cuss, maybe for whatever reason you want to handle custom decoding just locally in your app. You're like, the generated coding is great, but I want a little tweak on it. Maybe in the special case, so like with decode map or something along those lines might be might be a nice thing to have. There are other ones too. I know I want to add retry at some point. Retry is something that's usually usually nice to have. For request fails, maybe retry every 30 seconds incrementing by 20 seconds or something along those lines for 15 times. Wouldn't you be able to do that through the task API, the Elm task API anyway? You could write your own retry. Yeah, but it'd be nice to this is mostly a lot of these things are things that I knew were request when I was working at Square from users that were just really nice to have built in for the language. So why make you write your own retry logic when I can just generate it for you? Because it's never exactly what I want. Yeah, I get the feeling too that you're talking Wolfgang about a sort of pre-packaged SDK, something, the kind of thing that would be like NPM install Spotify API or GitHub API or something. And then it installs some JavaScript functions that let you use the API. So you're talking about that sort of pre-packaged SDK, right? Yes, yes. Because that's, I find it very useful when I when I'm doing JavaScript and I'm like, I need to talk to the service. Oh, hey, there's a package I can talk to the service and it's no setup. You there's a lot like you usually have to pass on a token once your API token for that service once and then you get some type of pre-pre-setup request. And then you just say, OK, go get me this data and you're done. And then if you look at I'm trying to think of some but with those pre-packaged ones, if you look at their REST APIs and you go to the rest docs, it's it's a fair amount of work to get back to that same point. And you're not sure, like, oh, shoot, did I type that wrong? Did I type the URL correctly? Did I remember to pass in the off token? Did did they change their their timeout like or some limits? Like maybe the API only allows you to make a request every 15 seconds. Did I actually set that to 15 seconds or did I set it to 10 seconds because I was looking at something else and got a number wrong. So having those types of experiences is very nice. And and I don't expect companies to go make Elm SDKs. Like that's a lot of work to go and make SDKs for other languages. So maybe I can help bring that experience, that nice quick. I want to use a service with minimal effort, that experience to Elm. Would you so let's say Spotify got big into Elm? And they wanted to make it easy for people to use their SDK. Would you recommend that they would generate an Elm package or that they make their open API spec readily available and give good instructions on how to generate it's using your package, like especially with the idea of having custom opaque types and all that. Maybe it's better to have it to generate it yourself so you can edit a bit instead of having a readily but fixed Elm package. I think handwriting your own SDK for especially a larger API can be a lot of work. Oh, no, in both cases, it would be using your your package, but it would be publish publishing the results or telling people to generate themselves using your tool. I think it would be I think it'd be publishing would be my guess. I think that if Spotify were really big into Elm, my my thought would be you would go to their developer page. They would have their first listed out their SDKs. And those would be say then Elm and JavaScript and maybe Python. And then they would after that list their API spec and their rest endpoint documentation. You'd probably still want the rest. I mean, you'd still want the rest documentation regardless because it's helpful for understanding the SDKs. But you would list the SDKs for saying like, hey, we support these. We put time into these because we want you to quickly get up to speed and build an app with our stuff. If you're using a language that doesn't isn't listed here. Here's another way to go about an interface with our with our API. Also, if I had one once I have some way to like customize the generated SDK with it like extra config, they could go into and like define their own extra stuff that they want. Like, like, you know, a custom. I'm trying to think what would be Spotify. I don't know what Spotify I was going to say track length always a positive number, but you can have tracks that are negative have negative time as well. Oh, yeah, great way to hide hide audio in tracks is to go negative. Yeah. And you can't if I remember. So I know this from back to listening to CDs because you can't really do that on a tape. There's tape has a start and an end. But in CDs, you have tracks that have a start and stop time. If you have negative time, the only easy way to get there is to finish the previous song and it will start the next song at the negative time stamp. If you skip to the next song, it starts at zero. And so you would completely miss that like hidden audio. Oh, is that for instance, like to make the transition between songs nicer? I don't know. I maybe but maybe like a translate type thing. Yeah. Oh, you can hide. You could hide an entire song there. I've had CDs back in the day where they had like negative four minutes and would hide entire songs in a track. What the hell? Yeah. Oh, yeah. I mean, there are songs where the play song, then they have like two minutes of of nothing and then a little thing at the end. Is that it as well? Like and that's somehow good. That's that's kind of. But that's not the same because you can like you can easily tell. Oh, my my track length is listed as six minutes and you get to four minutes. And then it's just two minutes of white noise. And you're like, why is it listed six minutes? Let me like skip forward to it. But if it's negative, that's not listed. So the only way to really know it's there is to either like rewind. Maybe I don't even know if that works. I think you have to like finish the previous song and let it auto go to the next section. So so if you're if you're a sucker for shuffling songs in an album, like, no, you're not going to have it. I knew shuffle as long as you finish the previous song. But I don't know if that works in web with MP3s. I don't know if MP3s can go negative. It might just be a it might be a forgotten thing, a lost thing with CDs. I'm very curious now. I might have to after this, I'm going to look all this up now. I'm really curious because I forgot. You just play this. Yeah. So yeah, no big type for that one. No, in less in less than MP3s, you can't have negative time. I don't know. Now I have to kind of hope it doesn't. It is pretty cool to be able to do that. So yeah, so they Spotify could go and add their own flavor onto the SDK generation that really makes it shine. That if you or I went and generated their their SDK ourselves, we wouldn't have. So I know we've talked about it during one of many Elm Cogen episodes. But it's still worth pointing out that it's really cool that in Elm, when you generate these huge API files, you only get in a ship, whatever you use. And I find that to be so cool. But like you just said, like, oh, yeah, like you forgot about it because you don't have to think about it anymore. It's just fun. It's so cool. Yeah, I was when I initially started this out, I was playing with the GitHub Open API spec. And I think at one point I was getting during some of the experimentation, we were getting up around like 10 or 15,000 lines of code, I want to say. Maybe more than it was probably more than that. Yeah, that's actually not as bad as I would have imagined. It's not it's definitely not the biggest Elm module by far. But I don't think GitHub's API is the largest API either. So I do wonder if there are APIs out there. There probably are that would that would break Elm if I generated it right now. I would imagine there are. I should try and find one. I wonder if there are any listeners who know of one that is big enough that would that would break the Elm compiler because the generated code is just too large. Yeah, if you generate you generate your SDK, you only use the commands. You never use a task. That's a third, at least, if not half of the Elm module that is just completely ignored and never shipped. Yeah, in your code. Yeah. And if you expose all the decoders and you don't use them, that's fine. And therefore, that's why it's fine for you to expose all those things as well. And I can't imagine like if you do this in TypeScript, like how how do you avoid bundling all that code with with your production bundle? Right. Yeah. So there's like module that code in a donation. Like if you don't import a module, it won't be imported. So you need to split things up by module, but it also uses objects, right? So anything or classes. So if you do anything with a product, then that for sure, all the things that are related to products are going to get shipped. Yeah. Like if you're if you're using Spotify, for example, maybe you never hit put or artists, you never hit the artist endpoint, but every song has an artist. So you need the artist decoder for every song. Yes. So regardless, that's going to get shipped. So you want the banana, you get the gorilla and the giant. Yeah. Yeah. Yeah. I do wonder how that works from language to language. I should I should generate trying to generate the square square SDK for Elm and see how big it ends up compared to the other languages. Because I know they ship like six to eight languages, I think it is. I'm guessing it's on NPM and you can find it there. See how big the API is. It is MPM. They ship a square. It's just square. I think square square up square. It is unpacked. It is six just shy of seven megabytes. That's for the node SDK. I don't know how big they they also have the Java one on Maven. There's a PHP one Rails C sharp. Right. Okay. So this is really specifically for node. It's not for front end JavaScript. That would definitely be a problem for your bundle size. That's that's a lot. Yeah. That does I wonder though, like does that affect I don't know enough about various languages that have start like like Java has a startup time and does the size of your dependencies drastically affect the startup time of your server then? It's a problem for for cold starts for serverless functions. That's for sure. It there's a there's a hard limit and it slows it down. So yeah, so that is an interesting thing. I never considered that. So we talked about cogeneration for open API specs. We talked about cogeneration for GraphQL. What else is missing in that realm? Like do we have protobufs, cogeneration that is missing? Do we have other applications that that are not even related to SDKs? There are. I think there is. I want to say there's a protobuf some some Elmstaffer protobuf. Yeah, there is actually. What is it? There is one for NATS that was just released this week. I don't know what NATS is yet, but I am kind of curious. I think there's gRPC is another one. I think I've seen something for that realm. There's always a few new ones here and there. There's one that's message something. Web message or something like along those lines. I think most have at least a minimal implementation in Elm. I do. So the other code gen aspect that I would like to explore someday. Maybe maybe I'm not sure if I'm going to go to it before or after forms is testing. I figure if I can generate the endpoints, can I not generate Elm test code as well for those endpoints so that like if you were right, say you were using Elm program test, you could send out your requests and have it respond with fake data essentially that is generated. Yes. Yeah. I've thought about this for Elm GraphQL as well. Like there's this factory girl Ruby gem where you're using these testing factories to basically say it knows what kind of data it's generating. You can let it generate some random things or give specific data in some of those pieces. But that would be very interesting for sure. But generating a nice API for it is a challenge. Yeah. Also like generating results that make sense can be difficult, I think. For instance, if you like create a user, then you need to return the same data that you gave to generate the user. So the same name as the input, the same age as the input, but a new ID and that ID has to be different from all the ones that you created previously probably, right? So like because with Swagger back in the day, there was the ability to generate test code like this. But in some cases, it's not exactly what you want, especially if you want things to be connected in some shape or form through IDs or something. It gets tricky. So very challenging, I think. But if it works, super interesting. Yeah, maybe I'll do that after form generation. No, no, no, I'm still motivated to do it. It's more what given a year, what can I do in a year and what will be the most beneficial? Testing is still very beneficial, but I think more people benefit from forms than tests is my guess. I think the testing side is more, I think there's an interesting challenge there and also provides a benefit, whereas forms, it's mostly benefit. Like the challenge I don't think is enormous or insurmountable, but the gains that people have from it are significantly higher. Also, if you wanted to make it work for an program test, you would need to generate a command, a task, and some kind of effect that goes well with the effects that the user defined. Yeah, yeah. I think you might just consider having like a wrapper function where every endpoint calls that function to generate a whatever, whether it's a command, a task, a backend task, an elm concurrent task, an effect, whatever it might be, like just let the user define that's using the code gen part, define a function that takes the decoder and headers and HTTP method and all that as input, and then turns that into a something, and then you let them do that. The hard part is in the generated code, then having the appropriate type signature for all of that generated code, which you kind of want, and that becomes a pain. It is, it's a type gymnastics, basically. Absolutely. You have four hours and you are not allowed to use more than three type variables. Well, the exam starts now. Well, Wolfgang, this is a really great asset for the community. So, thank you for pushing across the finish line. I really do believe the more pieces like this we have in the ecosystem, the fewer caveats there are to like, oh, but does the Elm ecosystem, does it have good GraphQL support, does it have good OpenAPI support? So, thank you. And if someone wants to get started, what's a good place to start, where can they learn more? They're completely, if they want to go down the road of learning OpenAPI, I would say just Google OpenAPI. Don't confuse it with OpenAI. You and I have been doing it lately. Yeah, OpenAPI. If you are looking to use it, check out the NPM package. Most likely, if you're looking to generate, and then whatever API endpoints you're, whatever service you're looking to communicate with, generate something. If you're looking to contribute to new features like form generation or testing, if you're feeling ambitious, feel free to reach out to me on Slack or Discord, or however else you know how to reach me. Amazing. And the real world example that you have where you're hitting the API using using OpenAPI is also very handy. So, worth taking a look if you want to see what it looks like calling these generated functions. Thank you so much. Wolfgang, great having you on. Thank you for having me. It was great. And you're in. Until next time. Until next time. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.6, "text": " Hello, you're in.", "tokens": [50364, 2425, 11, 291, 434, 294, 13, 50494], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 1, "seek": 0, "start": 2.6, "end": 3.6, "text": " Hello, Dillon.", "tokens": [50494, 2425, 11, 28160, 13, 50544], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 2, "seek": 0, "start": 3.6, "end": 8.540000000000001, "text": " Well, typically, when we have a guest on, it's nice to give them a bit of a break and", "tokens": [50544, 1042, 11, 5850, 11, 562, 321, 362, 257, 8341, 322, 11, 309, 311, 1481, 281, 976, 552, 257, 857, 295, 257, 1821, 293, 50791], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 3, "seek": 0, "start": 8.540000000000001, "end": 9.92, "text": " let them rest a little.", "tokens": [50791, 718, 552, 1472, 257, 707, 13, 50860], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 4, "seek": 0, "start": 9.92, "end": 15.18, "text": " But recently, we had Wolfgang Schuster on, and I thought, what better way to let him", "tokens": [50860, 583, 3938, 11, 321, 632, 16634, 19619, 2065, 8393, 322, 11, 293, 286, 1194, 11, 437, 1101, 636, 281, 718, 796, 51123], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 5, "seek": 0, "start": 15.18, "end": 21.84, "text": " rest than having him back on the podcast to talk about rest endpoints and auto-generating", "tokens": [51123, 1472, 813, 1419, 796, 646, 322, 264, 7367, 281, 751, 466, 1472, 917, 20552, 293, 8399, 12, 21848, 990, 51456], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 6, "seek": 0, "start": 21.84, "end": 24.04, "text": " APIs with Elm Open API?", "tokens": [51456, 21445, 365, 2699, 76, 7238, 9362, 30, 51566], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 7, "seek": 0, "start": 24.04, "end": 28.92, "text": " Wolfgang Schuster, welcome back to the podcast.", "tokens": [51566, 16634, 19619, 2065, 8393, 11, 2928, 646, 281, 264, 7367, 13, 51810], "temperature": 0.0, "avg_logprob": -0.21418387371560801, "compression_ratio": 1.5967078189300412, "no_speech_prob": 0.2240120768547058}, {"id": 8, "seek": 2892, "start": 28.92, "end": 29.92, "text": " Thank you for having me back.", "tokens": [50364, 1044, 291, 337, 1419, 385, 646, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 9, "seek": 2892, "start": 29.92, "end": 30.92, "text": " Great to have you.", "tokens": [50414, 3769, 281, 362, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 10, "seek": 2892, "start": 30.92, "end": 32.92, "text": " That was a good one.", "tokens": [50464, 663, 390, 257, 665, 472, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 11, "seek": 2892, "start": 32.92, "end": 37.0, "text": " Yeah, this was, again, one of those episodes where I'm like, hello, you're in.", "tokens": [50564, 865, 11, 341, 390, 11, 797, 11, 472, 295, 729, 9313, 689, 286, 478, 411, 11, 7751, 11, 291, 434, 294, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 12, "seek": 2892, "start": 37.0, "end": 38.0, "text": " Hello, Dillon.", "tokens": [50768, 2425, 11, 28160, 13, 50818], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 13, "seek": 2892, "start": 38.0, "end": 40.28, "text": " And I'm like, oh, no, he's going to make a pun now.", "tokens": [50818, 400, 286, 478, 411, 11, 1954, 11, 572, 11, 415, 311, 516, 281, 652, 257, 4468, 586, 13, 50932], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 14, "seek": 2892, "start": 40.28, "end": 43.480000000000004, "text": " Prepare for this.", "tokens": [50932, 29689, 337, 341, 13, 51092], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 15, "seek": 2892, "start": 43.480000000000004, "end": 44.68, "text": " And this was a good one.", "tokens": [51092, 400, 341, 390, 257, 665, 472, 13, 51152], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 16, "seek": 2892, "start": 44.68, "end": 46.68000000000001, "text": " This was a good one.", "tokens": [51152, 639, 390, 257, 665, 472, 13, 51252], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 17, "seek": 2892, "start": 46.68000000000001, "end": 50.44, "text": " You actually surprised me with it.", "tokens": [51252, 509, 767, 6100, 385, 365, 309, 13, 51440], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 18, "seek": 2892, "start": 50.44, "end": 54.08, "text": " Surprised you with it actually being a good one or with me doing it?", "tokens": [51440, 6732, 42093, 291, 365, 309, 767, 885, 257, 665, 472, 420, 365, 385, 884, 309, 30, 51622], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 19, "seek": 2892, "start": 54.08, "end": 55.08, "text": " Yeah.", "tokens": [51622, 865, 13, 51672], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 20, "seek": 2892, "start": 55.08, "end": 56.08, "text": " No, no.", "tokens": [51672, 883, 11, 572, 13, 51722], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 21, "seek": 2892, "start": 56.08, "end": 57.08, "text": " Let's say both.", "tokens": [51722, 961, 311, 584, 1293, 13, 51772], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 22, "seek": 2892, "start": 57.08, "end": 58.08, "text": " I didn't even see it coming.", "tokens": [51772, 286, 994, 380, 754, 536, 309, 1348, 13, 51822], "temperature": 0.0, "avg_logprob": -0.3007068509369894, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.29024603962898254}, {"id": 23, "seek": 5808, "start": 58.239999999999995, "end": 59.239999999999995, "text": " I was like, rest.", "tokens": [50372, 286, 390, 411, 11, 1472, 13, 50422], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 24, "seek": 5808, "start": 59.239999999999995, "end": 60.239999999999995, "text": " Yeah, yeah.", "tokens": [50422, 865, 11, 1338, 13, 50472], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 25, "seek": 5808, "start": 60.239999999999995, "end": 61.239999999999995, "text": " Okay.", "tokens": [50472, 1033, 13, 50522], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 26, "seek": 5808, "start": 61.239999999999995, "end": 66.24, "text": " Oh, oh, rest endpoints.", "tokens": [50522, 876, 11, 1954, 11, 1472, 917, 20552, 13, 50772], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 27, "seek": 5808, "start": 66.24, "end": 71.03999999999999, "text": " It does feel like we're giving a little balance to the universe, getting non-Martin guests", "tokens": [50772, 467, 775, 841, 411, 321, 434, 2902, 257, 707, 4772, 281, 264, 6445, 11, 1242, 2107, 12, 29612, 259, 9804, 51012], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 28, "seek": 5808, "start": 71.03999999999999, "end": 72.24, "text": " a chance as well.", "tokens": [51012, 257, 2931, 382, 731, 13, 51072], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 29, "seek": 5808, "start": 72.24, "end": 75.4, "text": " So thank you for representing the non-Martins.", "tokens": [51072, 407, 1309, 291, 337, 13460, 264, 2107, 12, 29612, 1292, 13, 51230], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 30, "seek": 5808, "start": 75.4, "end": 76.4, "text": " Happy to.", "tokens": [51230, 8277, 281, 13, 51280], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 31, "seek": 5808, "start": 76.4, "end": 77.4, "text": " Happy to.", "tokens": [51280, 8277, 281, 13, 51330], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 32, "seek": 5808, "start": 77.4, "end": 84.84, "text": " So, so Wolfgang, you recently had a big release of Elm Open API.", "tokens": [51330, 407, 11, 370, 16634, 19619, 11, 291, 3938, 632, 257, 955, 4374, 295, 2699, 76, 7238, 9362, 13, 51702], "temperature": 0.0, "avg_logprob": -0.24189984564687692, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.02226846106350422}, {"id": 33, "seek": 8484, "start": 84.84, "end": 89.28, "text": " And why don't you give us a quick intro to what it is?", "tokens": [50364, 400, 983, 500, 380, 291, 976, 505, 257, 1702, 12897, 281, 437, 309, 307, 30, 50586], "temperature": 0.0, "avg_logprob": -0.2308309578601225, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.13635887205600739}, {"id": 34, "seek": 8484, "start": 89.28, "end": 90.64, "text": " Sure, sure.", "tokens": [50586, 4894, 11, 988, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2308309578601225, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.13635887205600739}, {"id": 35, "seek": 8484, "start": 90.64, "end": 102.16, "text": " Elm Open API is a combination Elm package and Node CLI tool for generating, as you pointed", "tokens": [50654, 2699, 76, 7238, 9362, 307, 257, 6562, 2699, 76, 7372, 293, 38640, 12855, 40, 2290, 337, 17746, 11, 382, 291, 10932, 51230], "temperature": 0.0, "avg_logprob": -0.2308309578601225, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.13635887205600739}, {"id": 36, "seek": 8484, "start": 102.16, "end": 106.24000000000001, "text": " out earlier, rest endpoints for Elm.", "tokens": [51230, 484, 3071, 11, 1472, 917, 20552, 337, 2699, 76, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2308309578601225, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.13635887205600739}, {"id": 37, "seek": 8484, "start": 106.24000000000001, "end": 112.04, "text": " The idea being kind of, I want to talk to some third party service or maybe an internal", "tokens": [51434, 440, 1558, 885, 733, 295, 11, 286, 528, 281, 751, 281, 512, 2636, 3595, 2643, 420, 1310, 364, 6920, 51724], "temperature": 0.0, "avg_logprob": -0.2308309578601225, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.13635887205600739}, {"id": 38, "seek": 11204, "start": 112.04, "end": 117.16000000000001, "text": " one and I need an SDK and no one has made an Elm one.", "tokens": [50364, 472, 293, 286, 643, 364, 37135, 293, 572, 472, 575, 1027, 364, 2699, 76, 472, 13, 50620], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 39, "seek": 11204, "start": 117.16000000000001, "end": 119.12, "text": " So hey, now I have an Elm one.", "tokens": [50620, 407, 4177, 11, 586, 286, 362, 364, 2699, 76, 472, 13, 50718], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 40, "seek": 11204, "start": 119.12, "end": 122.32000000000001, "text": " What is an SDK?", "tokens": [50718, 708, 307, 364, 37135, 30, 50878], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 41, "seek": 11204, "start": 122.32000000000001, "end": 124.88000000000001, "text": " Software development kit, technically, I think.", "tokens": [50878, 27428, 3250, 8260, 11, 12120, 11, 286, 519, 13, 51006], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 42, "seek": 11204, "start": 124.88000000000001, "end": 126.28, "text": " I don't know.", "tokens": [51006, 286, 500, 380, 458, 13, 51076], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 43, "seek": 11204, "start": 126.28, "end": 127.28, "text": " Yeah.", "tokens": [51076, 865, 13, 51126], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 44, "seek": 11204, "start": 127.28, "end": 133.24, "text": " Basically a way to talk to series of functions, endpoints, whatever to talk to some other", "tokens": [51126, 8537, 257, 636, 281, 751, 281, 2638, 295, 6828, 11, 917, 20552, 11, 2035, 281, 751, 281, 512, 661, 51424], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 45, "seek": 11204, "start": 133.24, "end": 136.52, "text": " service or, yeah, yeah.", "tokens": [51424, 2643, 420, 11, 1338, 11, 1338, 13, 51588], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 46, "seek": 11204, "start": 136.52, "end": 138.84, "text": " In this case, some other service.", "tokens": [51588, 682, 341, 1389, 11, 512, 661, 2643, 13, 51704], "temperature": 0.0, "avg_logprob": -0.24512178757611444, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.25053244829177856}, {"id": 47, "seek": 13884, "start": 138.84, "end": 144.32, "text": " So yeah, let's, maybe let's give a definition of what Open API is.", "tokens": [50364, 407, 1338, 11, 718, 311, 11, 1310, 718, 311, 976, 257, 7123, 295, 437, 7238, 9362, 307, 13, 50638], "temperature": 0.0, "avg_logprob": -0.20399958947125604, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.028418580070137978}, {"id": 48, "seek": 13884, "start": 144.32, "end": 148.36, "text": " So it's a specification.", "tokens": [50638, 407, 309, 311, 257, 31256, 13, 50840], "temperature": 0.0, "avg_logprob": -0.20399958947125604, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.028418580070137978}, {"id": 49, "seek": 13884, "start": 148.36, "end": 151.2, "text": " What does that specification tell you?", "tokens": [50840, 708, 775, 300, 31256, 980, 291, 30, 50982], "temperature": 0.0, "avg_logprob": -0.20399958947125604, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.028418580070137978}, {"id": 50, "seek": 13884, "start": 151.2, "end": 162.48000000000002, "text": " It tells you which URI endpoints service provides, usually slash API slash products or slash catalog", "tokens": [50982, 467, 5112, 291, 597, 624, 5577, 917, 20552, 2643, 6417, 11, 2673, 17330, 9362, 17330, 3383, 420, 17330, 19746, 51546], "temperature": 0.0, "avg_logprob": -0.20399958947125604, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.028418580070137978}, {"id": 51, "seek": 13884, "start": 162.48000000000002, "end": 164.96, "text": " or something along those lines.", "tokens": [51546, 420, 746, 2051, 729, 3876, 13, 51670], "temperature": 0.0, "avg_logprob": -0.20399958947125604, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.028418580070137978}, {"id": 52, "seek": 16496, "start": 164.96, "end": 172.12, "text": " Incontain version information, whether it's a get post, put, delete, et cetera.", "tokens": [50364, 682, 9000, 491, 3037, 1589, 11, 1968, 309, 311, 257, 483, 2183, 11, 829, 11, 12097, 11, 1030, 11458, 13, 50722], "temperature": 0.0, "avg_logprob": -0.14699891236451296, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.7177669405937195}, {"id": 53, "seek": 16496, "start": 172.12, "end": 176.12, "text": " What body needs to be passed if you're creating something, if you're creating something in", "tokens": [50722, 708, 1772, 2203, 281, 312, 4678, 498, 291, 434, 4084, 746, 11, 498, 291, 434, 4084, 746, 294, 50922], "temperature": 0.0, "avg_logprob": -0.14699891236451296, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.7177669405937195}, {"id": 54, "seek": 16496, "start": 176.12, "end": 179.24, "text": " your catalog, what do you need to pass in?", "tokens": [50922, 428, 19746, 11, 437, 360, 291, 643, 281, 1320, 294, 30, 51078], "temperature": 0.0, "avg_logprob": -0.14699891236451296, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.7177669405937195}, {"id": 55, "seek": 16496, "start": 179.24, "end": 184.12, "text": " If you're retrieving a list of catalog information, what does that catalog information look like?", "tokens": [51078, 759, 291, 434, 19817, 798, 257, 1329, 295, 19746, 1589, 11, 437, 775, 300, 19746, 1589, 574, 411, 30, 51322], "temperature": 0.0, "avg_logprob": -0.14699891236451296, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.7177669405937195}, {"id": 56, "seek": 16496, "start": 184.12, "end": 186.04000000000002, "text": " How is it structured?", "tokens": [51322, 1012, 307, 309, 18519, 30, 51418], "temperature": 0.0, "avg_logprob": -0.14699891236451296, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.7177669405937195}, {"id": 57, "seek": 16496, "start": 186.04000000000002, "end": 188.16, "text": " What optional things you need to send in?", "tokens": [51418, 708, 17312, 721, 291, 643, 281, 2845, 294, 30, 51524], "temperature": 0.0, "avg_logprob": -0.14699891236451296, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.7177669405937195}, {"id": 58, "seek": 16496, "start": 188.16, "end": 193.28, "text": " What security requirements, headers, bearer tokens, et cetera.", "tokens": [51524, 708, 3825, 7728, 11, 45101, 11, 6155, 260, 22667, 11, 1030, 11458, 13, 51780], "temperature": 0.0, "avg_logprob": -0.14699891236451296, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.7177669405937195}, {"id": 59, "seek": 19328, "start": 193.28, "end": 194.28, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2251728057861328, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.03729952871799469}, {"id": 60, "seek": 19328, "start": 194.28, "end": 200.56, "text": " It's a way, it is in some ways kind of like GraphQL schemas, if people are familiar with", "tokens": [50414, 467, 311, 257, 636, 11, 309, 307, 294, 512, 2098, 733, 295, 411, 21884, 13695, 22627, 296, 11, 498, 561, 366, 4963, 365, 50728], "temperature": 0.0, "avg_logprob": -0.2251728057861328, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.03729952871799469}, {"id": 61, "seek": 19328, "start": 200.56, "end": 201.56, "text": " those.", "tokens": [50728, 729, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2251728057861328, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.03729952871799469}, {"id": 62, "seek": 19328, "start": 201.56, "end": 207.68, "text": " It is a similar type of description, but for traditional rest endpoints.", "tokens": [50778, 467, 307, 257, 2531, 2010, 295, 3855, 11, 457, 337, 5164, 1472, 917, 20552, 13, 51084], "temperature": 0.0, "avg_logprob": -0.2251728057861328, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.03729952871799469}, {"id": 63, "seek": 19328, "start": 207.68, "end": 208.68, "text": " Right.", "tokens": [51084, 1779, 13, 51134], "temperature": 0.0, "avg_logprob": -0.2251728057861328, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.03729952871799469}, {"id": 64, "seek": 19328, "start": 208.68, "end": 209.68, "text": " Right.", "tokens": [51134, 1779, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2251728057861328, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.03729952871799469}, {"id": 65, "seek": 19328, "start": 209.68, "end": 217.0, "text": " And so because it lives separate the API itself, right, or at least it can.", "tokens": [51184, 400, 370, 570, 309, 2909, 4994, 264, 9362, 2564, 11, 558, 11, 420, 412, 1935, 309, 393, 13, 51550], "temperature": 0.0, "avg_logprob": -0.2251728057861328, "compression_ratio": 1.427027027027027, "no_speech_prob": 0.03729952871799469}, {"id": 66, "seek": 21700, "start": 217.0, "end": 225.92, "text": " So you could take an existing popular API and then create a JSON file that describes", "tokens": [50364, 407, 291, 727, 747, 364, 6741, 3743, 9362, 293, 550, 1884, 257, 31828, 3991, 300, 15626, 50810], "temperature": 0.0, "avg_logprob": -0.13067714466768152, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.18227429687976837}, {"id": 67, "seek": 21700, "start": 225.92, "end": 229.2, "text": " that API using the open API specification.", "tokens": [50810, 300, 9362, 1228, 264, 1269, 9362, 31256, 13, 50974], "temperature": 0.0, "avg_logprob": -0.13067714466768152, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.18227429687976837}, {"id": 68, "seek": 21700, "start": 229.2, "end": 230.2, "text": " Is that correct?", "tokens": [50974, 1119, 300, 3006, 30, 51024], "temperature": 0.0, "avg_logprob": -0.13067714466768152, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.18227429687976837}, {"id": 69, "seek": 21700, "start": 230.2, "end": 231.2, "text": " Correct.", "tokens": [51024, 12753, 13, 51074], "temperature": 0.0, "avg_logprob": -0.13067714466768152, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.18227429687976837}, {"id": 70, "seek": 21700, "start": 231.2, "end": 233.28, "text": " Or you can go the other way too.", "tokens": [51074, 1610, 291, 393, 352, 264, 661, 636, 886, 13, 51178], "temperature": 0.0, "avg_logprob": -0.13067714466768152, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.18227429687976837}, {"id": 71, "seek": 21700, "start": 233.28, "end": 239.56, "text": " There are tools to generate backend services, backend servers.", "tokens": [51178, 821, 366, 3873, 281, 8460, 38087, 3328, 11, 38087, 15909, 13, 51492], "temperature": 0.0, "avg_logprob": -0.13067714466768152, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.18227429687976837}, {"id": 72, "seek": 21700, "start": 239.56, "end": 244.68, "text": " So you have a specification and you're like, I need this to talk to my database.", "tokens": [51492, 407, 291, 362, 257, 31256, 293, 291, 434, 411, 11, 286, 643, 341, 281, 751, 281, 452, 8149, 13, 51748], "temperature": 0.0, "avg_logprob": -0.13067714466768152, "compression_ratio": 1.5639810426540284, "no_speech_prob": 0.18227429687976837}, {"id": 73, "seek": 24468, "start": 244.68, "end": 247.96, "text": " You can go that way.", "tokens": [50364, 509, 393, 352, 300, 636, 13, 50528], "temperature": 0.0, "avg_logprob": -0.21189129737115675, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.058316249400377274}, {"id": 74, "seek": 24468, "start": 247.96, "end": 253.72, "text": " There are other things that are like PostgresQL can generate one of these schemas for you", "tokens": [50528, 821, 366, 661, 721, 300, 366, 411, 10223, 45189, 13695, 393, 8460, 472, 295, 613, 22627, 296, 337, 291, 50816], "temperature": 0.0, "avg_logprob": -0.21189129737115675, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.058316249400377274}, {"id": 75, "seek": 24468, "start": 253.72, "end": 256.16, "text": " in JSON or even YAML as well.", "tokens": [50816, 294, 31828, 420, 754, 398, 2865, 43, 382, 731, 13, 50938], "temperature": 0.0, "avg_logprob": -0.21189129737115675, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.058316249400377274}, {"id": 76, "seek": 24468, "start": 256.16, "end": 265.08, "text": " And then you can then generate an SDK from your Postgres schema and have Elm directly", "tokens": [50938, 400, 550, 291, 393, 550, 8460, 364, 14638, 42, 490, 428, 10223, 45189, 34078, 293, 362, 2699, 76, 3838, 51384], "temperature": 0.0, "avg_logprob": -0.21189129737115675, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.058316249400377274}, {"id": 77, "seek": 24468, "start": 265.08, "end": 268.4, "text": " talk to a Postgres database.", "tokens": [51384, 751, 281, 257, 10223, 45189, 8149, 13, 51550], "temperature": 0.0, "avg_logprob": -0.21189129737115675, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.058316249400377274}, {"id": 78, "seek": 24468, "start": 268.4, "end": 269.4, "text": " Yeah.", "tokens": [51550, 865, 13, 51600], "temperature": 0.0, "avg_logprob": -0.21189129737115675, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.058316249400377274}, {"id": 79, "seek": 24468, "start": 269.4, "end": 273.28000000000003, "text": " There's a whole bunch of different directions you can go with it.", "tokens": [51600, 821, 311, 257, 1379, 3840, 295, 819, 11095, 291, 393, 352, 365, 309, 13, 51794], "temperature": 0.0, "avg_logprob": -0.21189129737115675, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.058316249400377274}, {"id": 80, "seek": 27328, "start": 273.28, "end": 278.15999999999997, "text": " There's even a, there's a cool tool company called Akita.", "tokens": [50364, 821, 311, 754, 257, 11, 456, 311, 257, 1627, 2290, 2237, 1219, 9629, 2786, 13, 50608], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 81, "seek": 27328, "start": 278.15999999999997, "end": 281.64, "text": " I think they got, I think they're now part of Postman.", "tokens": [50608, 286, 519, 436, 658, 11, 286, 519, 436, 434, 586, 644, 295, 10223, 1601, 13, 50782], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 82, "seek": 27328, "start": 281.64, "end": 282.64, "text": " Yeah.", "tokens": [50782, 865, 13, 50832], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 83, "seek": 27328, "start": 282.64, "end": 288.08, "text": " But they, they provided a way to watch your traffic from over your network and generate", "tokens": [50832, 583, 436, 11, 436, 5649, 257, 636, 281, 1159, 428, 6419, 490, 670, 428, 3209, 293, 8460, 51104], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 84, "seek": 27328, "start": 288.08, "end": 289.44, "text": " one of these schemas from that.", "tokens": [51104, 472, 295, 613, 22627, 296, 490, 300, 13, 51172], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 85, "seek": 27328, "start": 289.44, "end": 291.79999999999995, "text": " So you could eat, you don't even have to hand write it.", "tokens": [51172, 407, 291, 727, 1862, 11, 291, 500, 380, 754, 362, 281, 1011, 2464, 309, 13, 51290], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 86, "seek": 27328, "start": 291.79999999999995, "end": 297.84, "text": " You could just watch network traffic with the, the Postman tool, generate a schema, generate", "tokens": [51290, 509, 727, 445, 1159, 3209, 6419, 365, 264, 11, 264, 10223, 1601, 2290, 11, 8460, 257, 34078, 11, 8460, 51592], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 87, "seek": 27328, "start": 297.84, "end": 301.59999999999997, "text": " an SDK, which is kind of a funky, cool little thing.", "tokens": [51592, 364, 14638, 42, 11, 597, 307, 733, 295, 257, 33499, 11, 1627, 707, 551, 13, 51780], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 88, "seek": 27328, "start": 301.59999999999997, "end": 302.59999999999997, "text": " Yeah.", "tokens": [51780, 865, 13, 51830], "temperature": 0.0, "avg_logprob": -0.18755321204662323, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.05179532989859581}, {"id": 89, "seek": 30260, "start": 302.6, "end": 308.40000000000003, "text": " And then you hope that everyone was using your API the correct way and that they were", "tokens": [50364, 400, 550, 291, 1454, 300, 1518, 390, 1228, 428, 9362, 264, 3006, 636, 293, 300, 436, 645, 50654], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 90, "seek": 30260, "start": 308.40000000000003, "end": 310.88, "text": " using everything.", "tokens": [50654, 1228, 1203, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 91, "seek": 30260, "start": 310.88, "end": 312.16, "text": " You always help a little bit.", "tokens": [50778, 509, 1009, 854, 257, 707, 857, 13, 50842], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 92, "seek": 30260, "start": 312.16, "end": 313.16, "text": " You never know for certain.", "tokens": [50842, 509, 1128, 458, 337, 1629, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 93, "seek": 30260, "start": 313.16, "end": 315.12, "text": " There's always, there's always education.", "tokens": [50892, 821, 311, 1009, 11, 456, 311, 1009, 3309, 13, 50990], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 94, "seek": 30260, "start": 315.12, "end": 316.12, "text": " Always.", "tokens": [50990, 11270, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 95, "seek": 30260, "start": 316.12, "end": 317.12, "text": " Yeah.", "tokens": [51040, 865, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 96, "seek": 30260, "start": 317.12, "end": 322.04, "text": " I'm, I'm sure there are things that are neatly described by an open API specification, but", "tokens": [51090, 286, 478, 11, 286, 478, 988, 456, 366, 721, 300, 366, 36634, 7619, 538, 364, 1269, 9362, 31256, 11, 457, 51336], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 97, "seek": 30260, "start": 322.04, "end": 329.92, "text": " then it suddenly returns HTML instead of JSON in the response and definitely doesn't conform.", "tokens": [51336, 550, 309, 5800, 11247, 17995, 2602, 295, 31828, 294, 264, 4134, 293, 2138, 1177, 380, 18975, 13, 51730], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 98, "seek": 30260, "start": 329.92, "end": 332.36, "text": " We've all been there.", "tokens": [51730, 492, 600, 439, 668, 456, 13, 51852], "temperature": 0.0, "avg_logprob": -0.2608806335174286, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0024717252235859632}, {"id": 99, "seek": 33236, "start": 332.36, "end": 337.32, "text": " There's also companies like GitHub who, who define their own schemas and instead of an", "tokens": [50364, 821, 311, 611, 3431, 411, 23331, 567, 11, 567, 6964, 641, 1065, 22627, 296, 293, 2602, 295, 364, 50612], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 100, "seek": 33236, "start": 337.32, "end": 344.24, "text": " octet stream, define an octocat stream, which is not an actual thing, but hey, they have", "tokens": [50612, 13350, 302, 4309, 11, 6964, 364, 13350, 905, 267, 4309, 11, 597, 307, 406, 364, 3539, 551, 11, 457, 4177, 11, 436, 362, 50958], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 101, "seek": 33236, "start": 344.24, "end": 347.48, "text": " it returns octocat for you.", "tokens": [50958, 309, 11247, 13350, 905, 267, 337, 291, 13, 51120], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 102, "seek": 33236, "start": 347.48, "end": 350.28000000000003, "text": " It's fun unless you actually need it.", "tokens": [51120, 467, 311, 1019, 5969, 291, 767, 643, 309, 13, 51260], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 103, "seek": 33236, "start": 350.28000000000003, "end": 351.28000000000003, "text": " Maybe.", "tokens": [51260, 2704, 13, 51310], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 104, "seek": 33236, "start": 351.28000000000003, "end": 352.28000000000003, "text": " Yeah.", "tokens": [51310, 865, 13, 51360], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 105, "seek": 33236, "start": 352.28000000000003, "end": 353.56, "text": " Well, it is pretty cool.", "tokens": [51360, 1042, 11, 309, 307, 1238, 1627, 13, 51424], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 106, "seek": 33236, "start": 353.56, "end": 361.52000000000004, "text": " The, for, for anyone who hasn't seen open API specifications or, or use them in any way,", "tokens": [51424, 440, 11, 337, 11, 337, 2878, 567, 6132, 380, 1612, 1269, 9362, 29448, 420, 11, 420, 764, 552, 294, 604, 636, 11, 51822], "temperature": 0.0, "avg_logprob": -0.25001860102382273, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.019706018269062042}, {"id": 107, "seek": 36152, "start": 362.32, "end": 364.44, "text": " it's more than just saying that this is a string.", "tokens": [50404, 309, 311, 544, 813, 445, 1566, 300, 341, 307, 257, 6798, 13, 50510], "temperature": 0.0, "avg_logprob": -0.13374767912195085, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.2474994957447052}, {"id": 108, "seek": 36152, "start": 364.44, "end": 369.79999999999995, "text": " Like you can specify that something is an enum, you know, even if you're not defining", "tokens": [50510, 1743, 291, 393, 16500, 300, 746, 307, 364, 465, 449, 11, 291, 458, 11, 754, 498, 291, 434, 406, 17827, 50778], "temperature": 0.0, "avg_logprob": -0.13374767912195085, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.2474994957447052}, {"id": 109, "seek": 36152, "start": 369.79999999999995, "end": 375.32, "text": " types for your schema, you can just say, well, these are the four possible string values.", "tokens": [50778, 3467, 337, 428, 34078, 11, 291, 393, 445, 584, 11, 731, 11, 613, 366, 264, 1451, 1944, 6798, 4190, 13, 51054], "temperature": 0.0, "avg_logprob": -0.13374767912195085, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.2474994957447052}, {"id": 110, "seek": 36152, "start": 375.32, "end": 382.12, "text": " This response can return or this is an integer in this range and things like that.", "tokens": [51054, 639, 4134, 393, 2736, 420, 341, 307, 364, 24922, 294, 341, 3613, 293, 721, 411, 300, 13, 51394], "temperature": 0.0, "avg_logprob": -0.13374767912195085, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.2474994957447052}, {"id": 111, "seek": 36152, "start": 382.12, "end": 387.64, "text": " So you can actually put constraints on both input and output values.", "tokens": [51394, 407, 291, 393, 767, 829, 18491, 322, 1293, 4846, 293, 5598, 4190, 13, 51670], "temperature": 0.0, "avg_logprob": -0.13374767912195085, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.2474994957447052}, {"id": 112, "seek": 38764, "start": 387.68, "end": 389.15999999999997, "text": " And that's us.", "tokens": [50366, 400, 300, 311, 505, 13, 50440], "temperature": 0.0, "avg_logprob": -0.1577243275112576, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0018100180895999074}, {"id": 113, "seek": 38764, "start": 389.15999999999997, "end": 394.91999999999996, "text": " So it kind of sits on top of or sits next to JSON schema.", "tokens": [50440, 407, 309, 733, 295, 12696, 322, 1192, 295, 420, 12696, 958, 281, 31828, 34078, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1577243275112576, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0018100180895999074}, {"id": 114, "seek": 38764, "start": 394.91999999999996, "end": 400.28, "text": " And so JSON schema is used to define all of the, the, the bodies essentially.", "tokens": [50728, 400, 370, 31828, 34078, 307, 1143, 281, 6964, 439, 295, 264, 11, 264, 11, 264, 7510, 4476, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1577243275112576, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0018100180895999074}, {"id": 115, "seek": 38764, "start": 400.28, "end": 406.44, "text": " So those limits, like you mentioned, like if you're requesting price information and", "tokens": [50996, 407, 729, 10406, 11, 411, 291, 2835, 11, 411, 498, 291, 434, 31937, 3218, 1589, 293, 51304], "temperature": 0.0, "avg_logprob": -0.1577243275112576, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0018100180895999074}, {"id": 116, "seek": 38764, "start": 406.44, "end": 411.32, "text": " that price has, you don't want negative prices, you could set a minimum, minimum of zero and", "tokens": [51304, 300, 3218, 575, 11, 291, 500, 380, 528, 3671, 7901, 11, 291, 727, 992, 257, 7285, 11, 7285, 295, 4018, 293, 51548], "temperature": 0.0, "avg_logprob": -0.1577243275112576, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0018100180895999074}, {"id": 117, "seek": 38764, "start": 411.32, "end": 416.64, "text": " that's using JSON schema definitions, which is kind of fun too, that it's building on", "tokens": [51548, 300, 311, 1228, 31828, 34078, 21988, 11, 597, 307, 733, 295, 1019, 886, 11, 300, 309, 311, 2390, 322, 51814], "temperature": 0.0, "avg_logprob": -0.1577243275112576, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0018100180895999074}, {"id": 118, "seek": 41664, "start": 416.64, "end": 421.08, "text": " top of these prior art and existing things, existing tools.", "tokens": [50364, 1192, 295, 613, 4059, 1523, 293, 6741, 721, 11, 6741, 3873, 13, 50586], "temperature": 0.0, "avg_logprob": -0.14658722826229628, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.002980699297040701}, {"id": 119, "seek": 41664, "start": 421.08, "end": 422.08, "text": " Right.", "tokens": [50586, 1779, 13, 50636], "temperature": 0.0, "avg_logprob": -0.14658722826229628, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.002980699297040701}, {"id": 120, "seek": 41664, "start": 422.08, "end": 428.47999999999996, "text": " And then there are also JSON schema validator tools in Node.js or other ecosystems where", "tokens": [50636, 400, 550, 456, 366, 611, 31828, 34078, 7363, 1639, 3873, 294, 38640, 13, 25530, 420, 661, 32647, 689, 50956], "temperature": 0.0, "avg_logprob": -0.14658722826229628, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.002980699297040701}, {"id": 121, "seek": 41664, "start": 428.47999999999996, "end": 436.88, "text": " you can actually verify before you consume the, the data, the request data, you can", "tokens": [50956, 291, 393, 767, 16888, 949, 291, 14732, 264, 11, 264, 1412, 11, 264, 5308, 1412, 11, 291, 393, 51376], "temperature": 0.0, "avg_logprob": -0.14658722826229628, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.002980699297040701}, {"id": 122, "seek": 41664, "start": 436.88, "end": 440.08, "text": " verify that it conforms to that specification.", "tokens": [51376, 16888, 300, 309, 18975, 82, 281, 300, 31256, 13, 51536], "temperature": 0.0, "avg_logprob": -0.14658722826229628, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.002980699297040701}, {"id": 123, "seek": 41664, "start": 440.08, "end": 445.84, "text": " Which is also what made my job easy is I used Elm JSON schema packages that already existed.", "tokens": [51536, 3013, 307, 611, 437, 1027, 452, 1691, 1858, 307, 286, 1143, 2699, 76, 31828, 34078, 17401, 300, 1217, 13135, 13, 51824], "temperature": 0.0, "avg_logprob": -0.14658722826229628, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.002980699297040701}, {"id": 124, "seek": 44584, "start": 446.03999999999996, "end": 450.71999999999997, "text": " I got to build on others work, just like open APIs building on others work.", "tokens": [50374, 286, 658, 281, 1322, 322, 2357, 589, 11, 445, 411, 1269, 21445, 2390, 322, 2357, 589, 13, 50608], "temperature": 0.0, "avg_logprob": -0.20845807155716084, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.10078740864992142}, {"id": 125, "seek": 44584, "start": 450.71999999999997, "end": 451.71999999999997, "text": " Oh, cool.", "tokens": [50608, 876, 11, 1627, 13, 50658], "temperature": 0.0, "avg_logprob": -0.20845807155716084, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.10078740864992142}, {"id": 126, "seek": 44584, "start": 451.71999999999997, "end": 455.35999999999996, "text": " What did, what did those tools help you do?", "tokens": [50658, 708, 630, 11, 437, 630, 729, 3873, 854, 291, 360, 30, 50840], "temperature": 0.0, "avg_logprob": -0.20845807155716084, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.10078740864992142}, {"id": 127, "seek": 44584, "start": 455.35999999999996, "end": 462.4, "text": " Instead of having to write a parser for everything, I only had to write a parser for or decoder", "tokens": [50840, 7156, 295, 1419, 281, 2464, 257, 21156, 260, 337, 1203, 11, 286, 787, 632, 281, 2464, 257, 21156, 260, 337, 420, 979, 19866, 51192], "temperature": 0.0, "avg_logprob": -0.20845807155716084, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.10078740864992142}, {"id": 128, "seek": 44584, "start": 462.4, "end": 466.71999999999997, "text": " parser for just the open API portion of the schema.", "tokens": [51192, 21156, 260, 337, 445, 264, 1269, 9362, 8044, 295, 264, 34078, 13, 51408], "temperature": 0.0, "avg_logprob": -0.20845807155716084, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.10078740864992142}, {"id": 129, "seek": 44584, "start": 466.71999999999997, "end": 471.76, "text": " So all like the body decoders and encoders and everything that's all handled by existing", "tokens": [51408, 407, 439, 411, 264, 1772, 979, 378, 433, 293, 2058, 378, 433, 293, 1203, 300, 311, 439, 18033, 538, 6741, 51660], "temperature": 0.0, "avg_logprob": -0.20845807155716084, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.10078740864992142}, {"id": 130, "seek": 44584, "start": 471.76, "end": 473.71999999999997, "text": " Elm packages.", "tokens": [51660, 2699, 76, 17401, 13, 51758], "temperature": 0.0, "avg_logprob": -0.20845807155716084, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.10078740864992142}, {"id": 131, "seek": 47372, "start": 473.72, "end": 479.32000000000005, "text": " So it probably saved me, I would guess a few, few good months at least.", "tokens": [50364, 407, 309, 1391, 6624, 385, 11, 286, 576, 2041, 257, 1326, 11, 1326, 665, 2493, 412, 1935, 13, 50644], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 132, "seek": 47372, "start": 479.32000000000005, "end": 480.32000000000005, "text": " That makes sense.", "tokens": [50644, 663, 1669, 2020, 13, 50694], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 133, "seek": 47372, "start": 480.32000000000005, "end": 485.56, "text": " So what are some popular APIs out there that, that use open API?", "tokens": [50694, 407, 437, 366, 512, 3743, 21445, 484, 456, 300, 11, 300, 764, 1269, 9362, 30, 50956], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 134, "seek": 47372, "start": 485.56, "end": 487.84000000000003, "text": " Well, I don't know how popular it is.", "tokens": [50956, 1042, 11, 286, 500, 380, 458, 577, 3743, 309, 307, 13, 51070], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 135, "seek": 47372, "start": 487.84000000000003, "end": 490.40000000000003, "text": " My start was actually with Square.", "tokens": [51070, 1222, 722, 390, 767, 365, 16463, 13, 51198], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 136, "seek": 47372, "start": 490.40000000000003, "end": 496.6, "text": " When I was working at Square some years ago, they produced an open API schema through a", "tokens": [51198, 1133, 286, 390, 1364, 412, 16463, 512, 924, 2057, 11, 436, 7126, 364, 1269, 9362, 34078, 807, 257, 51508], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 137, "seek": 47372, "start": 496.6, "end": 497.6, "text": " series.", "tokens": [51508, 2638, 13, 51558], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 138, "seek": 47372, "start": 497.6, "end": 498.6, "text": " There's, there's was not handwritten.", "tokens": [51558, 821, 311, 11, 456, 311, 390, 406, 1011, 26859, 13, 51608], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 139, "seek": 47372, "start": 498.6, "end": 499.6, "text": " There's was actually generated.", "tokens": [51608, 821, 311, 390, 767, 10833, 13, 51658], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 140, "seek": 47372, "start": 499.6, "end": 502.12, "text": " I believe it was proto buffs.", "tokens": [51658, 286, 1697, 309, 390, 47896, 50164, 13, 51784], "temperature": 0.0, "avg_logprob": -0.20421905517578126, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.028430301696062088}, {"id": 141, "seek": 50212, "start": 502.12, "end": 508.6, "text": " They used to generate the open API schema and then that was both available to customers", "tokens": [50364, 814, 1143, 281, 8460, 264, 1269, 9362, 34078, 293, 550, 300, 390, 1293, 2435, 281, 4581, 50688], "temperature": 0.0, "avg_logprob": -0.19982396472584119, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0174361914396286}, {"id": 142, "seek": 50212, "start": 508.6, "end": 514.32, "text": " to use as from a development standpoint, as well as through various other things.", "tokens": [50688, 281, 764, 382, 490, 257, 3250, 15827, 11, 382, 731, 382, 807, 3683, 661, 721, 13, 50974], "temperature": 0.0, "avg_logprob": -0.19982396472584119, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0174361914396286}, {"id": 143, "seek": 50212, "start": 514.32, "end": 516.2, "text": " So that's where I got my start.", "tokens": [50974, 407, 300, 311, 689, 286, 658, 452, 722, 13, 51068], "temperature": 0.0, "avg_logprob": -0.19982396472584119, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0174361914396286}, {"id": 144, "seek": 50212, "start": 516.2, "end": 520.84, "text": " They generated an open API specification from proto buff.", "tokens": [51068, 814, 10833, 364, 1269, 9362, 31256, 490, 47896, 9204, 13, 51300], "temperature": 0.0, "avg_logprob": -0.19982396472584119, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0174361914396286}, {"id": 145, "seek": 50212, "start": 520.84, "end": 522.32, "text": " I believe that's what it was.", "tokens": [51300, 286, 1697, 300, 311, 437, 309, 390, 13, 51374], "temperature": 0.0, "avg_logprob": -0.19982396472584119, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0174361914396286}, {"id": 146, "seek": 50212, "start": 522.32, "end": 527.12, "text": " If I remember right, yes, various teams write their own proto buffs.", "tokens": [51374, 759, 286, 1604, 558, 11, 2086, 11, 3683, 5491, 2464, 641, 1065, 47896, 50164, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19982396472584119, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0174361914396286}, {"id": 147, "seek": 52712, "start": 527.12, "end": 533.52, "text": " Those are merged into one giant proto buff essentially, which then generates an open", "tokens": [50364, 3950, 366, 36427, 666, 472, 7410, 47896, 9204, 4476, 11, 597, 550, 23815, 364, 1269, 50684], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 148, "seek": 52712, "start": 533.52, "end": 535.2, "text": " API schema.", "tokens": [50684, 9362, 34078, 13, 50768], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 149, "seek": 52712, "start": 535.2, "end": 537.52, "text": " Is it like a proto buff specification?", "tokens": [50768, 1119, 309, 411, 257, 47896, 9204, 31256, 30, 50884], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 150, "seek": 52712, "start": 537.52, "end": 538.52, "text": " Okay.", "tokens": [50884, 1033, 13, 50934], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 151, "seek": 52712, "start": 538.52, "end": 539.52, "text": " Okay.", "tokens": [50934, 1033, 13, 50984], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 152, "seek": 52712, "start": 539.52, "end": 541.28, "text": " Cause I have never used proto buff.", "tokens": [50984, 10865, 286, 362, 1128, 1143, 47896, 9204, 13, 51072], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 153, "seek": 52712, "start": 541.28, "end": 546.6, "text": " The only reason I know about proto buff is about, is because Evan talks about proto", "tokens": [51072, 440, 787, 1778, 286, 458, 466, 47896, 9204, 307, 466, 11, 307, 570, 22613, 6686, 466, 47896, 51338], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 154, "seek": 52712, "start": 546.6, "end": 549.5600000000001, "text": " buff in relation to Jason and all that.", "tokens": [51338, 9204, 294, 9721, 281, 11181, 293, 439, 300, 13, 51486], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 155, "seek": 52712, "start": 549.5600000000001, "end": 551.04, "text": " And was like, Oh, okay.", "tokens": [51486, 400, 390, 411, 11, 876, 11, 1392, 13, 51560], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 156, "seek": 52712, "start": 551.04, "end": 555.76, "text": " Proto buff is a, is like Jason with different semantics.", "tokens": [51560, 2114, 6738, 9204, 307, 257, 11, 307, 411, 11181, 365, 819, 4361, 45298, 13, 51796], "temperature": 0.0, "avg_logprob": -0.26907196751347295, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.1559390276670456}, {"id": 157, "seek": 55576, "start": 555.8, "end": 560.2, "text": " It's like, so how do you make a specification from Jason?", "tokens": [50366, 467, 311, 411, 11, 370, 577, 360, 291, 652, 257, 31256, 490, 11181, 30, 50586], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 158, "seek": 55576, "start": 560.2, "end": 561.2, "text": " Which, okay.", "tokens": [50586, 3013, 11, 1392, 13, 50636], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 159, "seek": 55576, "start": 561.2, "end": 565.92, "text": " You get, you have a Jason specification of the, sorry, as a open API specification.", "tokens": [50636, 509, 483, 11, 291, 362, 257, 11181, 31256, 295, 264, 11, 2597, 11, 382, 257, 1269, 9362, 31256, 13, 50872], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 160, "seek": 55576, "start": 565.92, "end": 566.92, "text": " But okay.", "tokens": [50872, 583, 1392, 13, 50922], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 161, "seek": 55576, "start": 566.92, "end": 567.92, "text": " You get what I mean.", "tokens": [50922, 509, 483, 437, 286, 914, 13, 50972], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 162, "seek": 55576, "start": 567.92, "end": 568.92, "text": " Yeah.", "tokens": [50972, 865, 13, 51022], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 163, "seek": 55576, "start": 568.92, "end": 569.92, "text": " Yeah.", "tokens": [51022, 865, 13, 51072], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 164, "seek": 55576, "start": 569.92, "end": 571.48, "text": " It's just a series of specification transformers.", "tokens": [51072, 467, 311, 445, 257, 2638, 295, 31256, 4088, 433, 13, 51150], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 165, "seek": 55576, "start": 571.48, "end": 577.12, "text": " Essentially you're going from one specification to another format to another format and eventually", "tokens": [51150, 23596, 291, 434, 516, 490, 472, 31256, 281, 1071, 7877, 281, 1071, 7877, 293, 4728, 51432], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 166, "seek": 55576, "start": 577.12, "end": 580.88, "text": " you get SDKs and can write code and yeah, send data.", "tokens": [51432, 291, 483, 37135, 82, 293, 393, 2464, 3089, 293, 1338, 11, 2845, 1412, 13, 51620], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 167, "seek": 55576, "start": 580.88, "end": 582.6, "text": " But yeah.", "tokens": [51620, 583, 1338, 13, 51706], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 168, "seek": 55576, "start": 582.6, "end": 585.16, "text": " So that was, that was my first experience.", "tokens": [51706, 407, 300, 390, 11, 300, 390, 452, 700, 1752, 13, 51834], "temperature": 0.0, "avg_logprob": -0.3123440146446228, "compression_ratio": 1.7936507936507937, "no_speech_prob": 0.005552951712161303}, {"id": 169, "seek": 58516, "start": 585.16, "end": 591.04, "text": " I had seen it before them, but that was my first like real hands on Leonardo.", "tokens": [50364, 286, 632, 1612, 309, 949, 552, 11, 457, 300, 390, 452, 700, 411, 957, 2377, 322, 36523, 13, 50658], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 170, "seek": 58516, "start": 591.04, "end": 594.9599999999999, "text": " Most people in the, in the Elm community know him as mini bill.", "tokens": [50658, 4534, 561, 294, 264, 11, 294, 264, 2699, 76, 1768, 458, 796, 382, 8382, 2961, 13, 50854], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 171, "seek": 58516, "start": 594.9599999999999, "end": 601.88, "text": " He actually helped me a ton with the package and both the Elm package and the CLI tool", "tokens": [50854, 634, 767, 4254, 385, 257, 2952, 365, 264, 7372, 293, 1293, 264, 2699, 76, 7372, 293, 264, 12855, 40, 2290, 51200], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 172, "seek": 58516, "start": 601.88, "end": 603.8, "text": " ton to ton.", "tokens": [51200, 2952, 281, 2952, 13, 51296], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 173, "seek": 58516, "start": 603.8, "end": 606.64, "text": " He was using it for Spotify.", "tokens": [51296, 634, 390, 1228, 309, 337, 29036, 13, 51438], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 174, "seek": 58516, "start": 606.64, "end": 610.16, "text": " So they, they have their own open API spec.", "tokens": [51438, 407, 436, 11, 436, 362, 641, 1065, 1269, 9362, 1608, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 175, "seek": 58516, "start": 610.16, "end": 611.16, "text": " He was using it.", "tokens": [51614, 634, 390, 1228, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 176, "seek": 58516, "start": 611.16, "end": 612.8, "text": " I have never actually seen what he built with it.", "tokens": [51664, 286, 362, 1128, 767, 1612, 437, 415, 3094, 365, 309, 13, 51746], "temperature": 0.0, "avg_logprob": -0.18817672729492188, "compression_ratio": 1.623931623931624, "no_speech_prob": 0.0003682634560391307}, {"id": 177, "seek": 61280, "start": 612.8, "end": 616.0799999999999, "text": " I know he was building some Spotify tool for himself though.", "tokens": [50364, 286, 458, 415, 390, 2390, 512, 29036, 2290, 337, 3647, 1673, 13, 50528], "temperature": 0.0, "avg_logprob": -0.19618378029213296, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0037059313617646694}, {"id": 178, "seek": 61280, "start": 616.0799999999999, "end": 618.28, "text": " So and it works for him, which is awesome.", "tokens": [50528, 407, 293, 309, 1985, 337, 796, 11, 597, 307, 3476, 13, 50638], "temperature": 0.0, "avg_logprob": -0.19618378029213296, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0037059313617646694}, {"id": 179, "seek": 61280, "start": 618.28, "end": 623.12, "text": " That's the whole reason I built this for, for people to be able to use it.", "tokens": [50638, 663, 311, 264, 1379, 1778, 286, 3094, 341, 337, 11, 337, 561, 281, 312, 1075, 281, 764, 309, 13, 50880], "temperature": 0.0, "avg_logprob": -0.19618378029213296, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0037059313617646694}, {"id": 180, "seek": 61280, "start": 623.12, "end": 627.64, "text": " Stripe I know from my time at square stripe being a competitive squares, like they have", "tokens": [50880, 20390, 494, 286, 458, 490, 452, 565, 412, 3732, 42957, 885, 257, 10043, 19368, 11, 411, 436, 362, 51106], "temperature": 0.0, "avg_logprob": -0.19618378029213296, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0037059313617646694}, {"id": 181, "seek": 61280, "start": 627.64, "end": 631.4, "text": " their own open API spec that people can use.", "tokens": [51106, 641, 1065, 1269, 9362, 1608, 300, 561, 393, 764, 13, 51294], "temperature": 0.0, "avg_logprob": -0.19618378029213296, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0037059313617646694}, {"id": 182, "seek": 61280, "start": 631.4, "end": 633.04, "text": " I know there are lots of others.", "tokens": [51294, 286, 458, 456, 366, 3195, 295, 2357, 13, 51376], "temperature": 0.0, "avg_logprob": -0.19618378029213296, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0037059313617646694}, {"id": 183, "seek": 61280, "start": 633.04, "end": 637.92, "text": " Most, I feel like most companies these days have some type of open API spec.", "tokens": [51376, 4534, 11, 286, 841, 411, 881, 3431, 613, 1708, 362, 512, 2010, 295, 1269, 9362, 1608, 13, 51620], "temperature": 0.0, "avg_logprob": -0.19618378029213296, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0037059313617646694}, {"id": 184, "seek": 63792, "start": 637.92, "end": 644.8, "text": " I remember at my, at one of my previous companies, we, we had an open API spec or swagger spec", "tokens": [50364, 286, 1604, 412, 452, 11, 412, 472, 295, 452, 3894, 3431, 11, 321, 11, 321, 632, 364, 1269, 9362, 1608, 420, 1693, 11062, 1608, 50708], "temperature": 0.0, "avg_logprob": -0.15718289260025864, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.021890774369239807}, {"id": 185, "seek": 63792, "start": 644.8, "end": 648.1999999999999, "text": " because that's what it was called back then, right?", "tokens": [50708, 570, 300, 311, 437, 309, 390, 1219, 646, 550, 11, 558, 30, 50878], "temperature": 0.0, "avg_logprob": -0.15718289260025864, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.021890774369239807}, {"id": 186, "seek": 63792, "start": 648.1999999999999, "end": 652.9599999999999, "text": " And yeah, it was only for internal use, but that helped us make sure that the back end", "tokens": [50878, 400, 1338, 11, 309, 390, 787, 337, 6920, 764, 11, 457, 300, 4254, 505, 652, 988, 300, 264, 646, 917, 51116], "temperature": 0.0, "avg_logprob": -0.15718289260025864, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.021890774369239807}, {"id": 187, "seek": 63792, "start": 652.9599999999999, "end": 658.36, "text": " and the front end were in sync and having the spec also allowed for automated tests.", "tokens": [51116, 293, 264, 1868, 917, 645, 294, 20271, 293, 1419, 264, 1608, 611, 4350, 337, 18473, 6921, 13, 51386], "temperature": 0.0, "avg_logprob": -0.15718289260025864, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.021890774369239807}, {"id": 188, "seek": 63792, "start": 658.36, "end": 660.36, "text": " So that was quite nice.", "tokens": [51386, 407, 300, 390, 1596, 1481, 13, 51486], "temperature": 0.0, "avg_logprob": -0.15718289260025864, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.021890774369239807}, {"id": 189, "seek": 66036, "start": 660.36, "end": 667.52, "text": " Did you build any back end with those specific, with those specifications as well?", "tokens": [50364, 2589, 291, 1322, 604, 646, 917, 365, 729, 2685, 11, 365, 729, 29448, 382, 731, 30, 50722], "temperature": 0.0, "avg_logprob": -0.243100502911736, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.34829211235046387}, {"id": 190, "seek": 66036, "start": 667.52, "end": 669.64, "text": " Or I have not.", "tokens": [50722, 1610, 286, 362, 406, 13, 50828], "temperature": 0.0, "avg_logprob": -0.243100502911736, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.34829211235046387}, {"id": 191, "seek": 66036, "start": 669.64, "end": 674.16, "text": " I have been dreaming that somebody will use it on something like Lambda era for their", "tokens": [50828, 286, 362, 668, 21475, 300, 2618, 486, 764, 309, 322, 746, 411, 45691, 4249, 337, 641, 51054], "temperature": 0.0, "avg_logprob": -0.243100502911736, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.34829211235046387}, {"id": 192, "seek": 66036, "start": 674.16, "end": 680.2, "text": " back end, because a lot of these times these SDKs are more designed for back end because", "tokens": [51054, 646, 917, 11, 570, 257, 688, 295, 613, 1413, 613, 37135, 82, 366, 544, 4761, 337, 646, 917, 570, 51356], "temperature": 0.0, "avg_logprob": -0.243100502911736, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.34829211235046387}, {"id": 193, "seek": 66036, "start": 680.2, "end": 686.64, "text": " you're holding API keys and other secret keys that you don't want to expose to the client,", "tokens": [51356, 291, 434, 5061, 9362, 9317, 293, 661, 4054, 9317, 300, 291, 500, 380, 528, 281, 19219, 281, 264, 6423, 11, 51678], "temperature": 0.0, "avg_logprob": -0.243100502911736, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.34829211235046387}, {"id": 194, "seek": 66036, "start": 686.64, "end": 688.04, "text": " especially payments.", "tokens": [51678, 2318, 14348, 13, 51748], "temperature": 0.0, "avg_logprob": -0.243100502911736, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.34829211235046387}, {"id": 195, "seek": 66036, "start": 688.04, "end": 689.72, "text": " Payments is a great example.", "tokens": [51748, 11431, 1117, 307, 257, 869, 1365, 13, 51832], "temperature": 0.0, "avg_logprob": -0.243100502911736, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.34829211235046387}, {"id": 196, "seek": 68972, "start": 689.72, "end": 694.36, "text": " If you were connecting up to Stripe, you're given an API, a secret API key that is just", "tokens": [50364, 759, 291, 645, 11015, 493, 281, 20390, 494, 11, 291, 434, 2212, 364, 9362, 11, 257, 4054, 9362, 2141, 300, 307, 445, 50596], "temperature": 0.0, "avg_logprob": -0.17923439466036284, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0015484780305996537}, {"id": 197, "seek": 68972, "start": 694.36, "end": 695.6800000000001, "text": " for your company.", "tokens": [50596, 337, 428, 2237, 13, 50662], "temperature": 0.0, "avg_logprob": -0.17923439466036284, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0015484780305996537}, {"id": 198, "seek": 68972, "start": 695.6800000000001, "end": 700.12, "text": " You don't want to expose that to your customers to the front end.", "tokens": [50662, 509, 500, 380, 528, 281, 19219, 300, 281, 428, 4581, 281, 264, 1868, 917, 13, 50884], "temperature": 0.0, "avg_logprob": -0.17923439466036284, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0015484780305996537}, {"id": 199, "seek": 68972, "start": 700.12, "end": 701.12, "text": " Yep.", "tokens": [50884, 7010, 13, 50934], "temperature": 0.0, "avg_logprob": -0.17923439466036284, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0015484780305996537}, {"id": 200, "seek": 68972, "start": 701.12, "end": 707.32, "text": " So you put that, you know, in your Lambda era back end and can do your communication with", "tokens": [50934, 407, 291, 829, 300, 11, 291, 458, 11, 294, 428, 45691, 4249, 646, 917, 293, 393, 360, 428, 6101, 365, 51244], "temperature": 0.0, "avg_logprob": -0.17923439466036284, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0015484780305996537}, {"id": 201, "seek": 68972, "start": 707.32, "end": 712.44, "text": " Stripe from Lambda era back end and present some type of interface to the front end.", "tokens": [51244, 20390, 494, 490, 45691, 4249, 646, 917, 293, 1974, 512, 2010, 295, 9226, 281, 264, 1868, 917, 13, 51500], "temperature": 0.0, "avg_logprob": -0.17923439466036284, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0015484780305996537}, {"id": 202, "seek": 68972, "start": 712.44, "end": 714.5600000000001, "text": " And yeah, there you go.", "tokens": [51500, 400, 1338, 11, 456, 291, 352, 13, 51606], "temperature": 0.0, "avg_logprob": -0.17923439466036284, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.0015484780305996537}, {"id": 203, "seek": 71456, "start": 714.56, "end": 715.56, "text": " Yes.", "tokens": [50364, 1079, 13, 50414], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 204, "seek": 71456, "start": 715.56, "end": 721.1999999999999, "text": " So I don't have the business ideas yet to implement that and use that, but I know it's, I know", "tokens": [50414, 407, 286, 500, 380, 362, 264, 1606, 3487, 1939, 281, 4445, 300, 293, 764, 300, 11, 457, 286, 458, 309, 311, 11, 286, 458, 50696], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 205, "seek": 71456, "start": 721.1999999999999, "end": 722.56, "text": " there's something there.", "tokens": [50696, 456, 311, 746, 456, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 206, "seek": 71456, "start": 722.56, "end": 723.56, "text": " Yeah.", "tokens": [50764, 865, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 207, "seek": 71456, "start": 723.56, "end": 724.8, "text": " That's very cool.", "tokens": [50814, 663, 311, 588, 1627, 13, 50876], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 208, "seek": 71456, "start": 724.8, "end": 732.4799999999999, "text": " I've heard too that the ChatGPT plugins are built around open API specifications, which", "tokens": [50876, 286, 600, 2198, 886, 300, 264, 27503, 38, 47, 51, 33759, 366, 3094, 926, 1269, 9362, 29448, 11, 597, 51260], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 209, "seek": 71456, "start": 732.4799999999999, "end": 737.1999999999999, "text": " is really confusing because the company is open AI, but they use open API, but they're", "tokens": [51260, 307, 534, 13181, 570, 264, 2237, 307, 1269, 7318, 11, 457, 436, 764, 1269, 9362, 11, 457, 436, 434, 51496], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 210, "seek": 71456, "start": 737.1999999999999, "end": 738.5999999999999, "text": " not related.", "tokens": [51496, 406, 4077, 13, 51566], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 211, "seek": 71456, "start": 738.5999999999999, "end": 742.9599999999999, "text": " And also there's nothing open about open AI anymore.", "tokens": [51566, 400, 611, 456, 311, 1825, 1269, 466, 1269, 7318, 3602, 13, 51784], "temperature": 0.0, "avg_logprob": -0.23815730701793325, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.2251983880996704}, {"id": 212, "seek": 74296, "start": 742.96, "end": 744.48, "text": " All that aside, it's...", "tokens": [50364, 1057, 300, 7359, 11, 309, 311, 485, 50440], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 213, "seek": 74296, "start": 744.48, "end": 749.36, "text": " I've also been confused about reading open AI and...", "tokens": [50440, 286, 600, 611, 668, 9019, 466, 3760, 1269, 7318, 293, 485, 50684], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 214, "seek": 74296, "start": 749.36, "end": 750.36, "text": " Yeah.", "tokens": [50684, 865, 13, 50734], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 215, "seek": 74296, "start": 750.36, "end": 755.88, "text": " Like in my mind it's like open API because I was researching this episode or the other", "tokens": [50734, 1743, 294, 452, 1575, 309, 311, 411, 1269, 9362, 570, 286, 390, 24176, 341, 3500, 420, 264, 661, 51010], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 216, "seek": 74296, "start": 755.88, "end": 757.6800000000001, "text": " way around.", "tokens": [51010, 636, 926, 13, 51100], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 217, "seek": 74296, "start": 757.6800000000001, "end": 759.48, "text": " Gets me every time.", "tokens": [51100, 460, 1385, 385, 633, 565, 13, 51190], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 218, "seek": 74296, "start": 759.48, "end": 765.96, "text": " And yet they actually are related because ChatGPT uses open API specifications to basically", "tokens": [51190, 400, 1939, 436, 767, 366, 4077, 570, 27503, 38, 47, 51, 4960, 1269, 9362, 29448, 281, 1936, 51514], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 219, "seek": 74296, "start": 765.96, "end": 767.2, "text": " build a plugin.", "tokens": [51514, 1322, 257, 23407, 13, 51576], "temperature": 0.0, "avg_logprob": -0.24880198545234147, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0518069751560688}, {"id": 220, "seek": 76720, "start": 767.2, "end": 774.84, "text": " You give it a specification that tells it how to interact with your API and then use", "tokens": [50364, 509, 976, 309, 257, 31256, 300, 5112, 309, 577, 281, 4648, 365, 428, 9362, 293, 550, 764, 50746], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 221, "seek": 76720, "start": 774.84, "end": 779.32, "text": " plain English to describe how to use those endpoints.", "tokens": [50746, 11121, 3669, 281, 6786, 577, 281, 764, 729, 917, 20552, 13, 50970], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 222, "seek": 76720, "start": 779.32, "end": 783.8000000000001, "text": " So can you generate an open API spec using open AI?", "tokens": [50970, 407, 393, 291, 8460, 364, 1269, 9362, 1608, 1228, 1269, 7318, 30, 51194], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 223, "seek": 76720, "start": 783.8000000000001, "end": 786.6400000000001, "text": " You totally can and it's really good at that too.", "tokens": [51194, 509, 3879, 393, 293, 309, 311, 534, 665, 412, 300, 886, 13, 51336], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 224, "seek": 76720, "start": 786.6400000000001, "end": 787.6400000000001, "text": " Oh.", "tokens": [51336, 876, 13, 51386], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 225, "seek": 76720, "start": 787.6400000000001, "end": 788.6400000000001, "text": " Yeah.", "tokens": [51386, 865, 13, 51436], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 226, "seek": 76720, "start": 788.6400000000001, "end": 789.6400000000001, "text": " Full circle?", "tokens": [51436, 13841, 6329, 30, 51486], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 227, "seek": 76720, "start": 789.6400000000001, "end": 790.6400000000001, "text": " Short circle, but...", "tokens": [51486, 16881, 6329, 11, 457, 485, 51536], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 228, "seek": 76720, "start": 790.6400000000001, "end": 795.36, "text": " See, could use open AI to talk to Lambda era back end.", "tokens": [51536, 3008, 11, 727, 764, 1269, 7318, 281, 751, 281, 45691, 4249, 646, 917, 13, 51772], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 229, "seek": 76720, "start": 795.36, "end": 796.36, "text": " Yeah.", "tokens": [51772, 865, 13, 51822], "temperature": 0.0, "avg_logprob": -0.24835526709463082, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.1440841108560562}, {"id": 230, "seek": 79636, "start": 796.36, "end": 797.36, "text": " Yes.", "tokens": [50364, 1079, 13, 50414], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 231, "seek": 79636, "start": 797.36, "end": 798.36, "text": " Exactly.", "tokens": [50414, 7587, 13, 50464], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 232, "seek": 79636, "start": 798.36, "end": 799.36, "text": " I like it.", "tokens": [50464, 286, 411, 309, 13, 50514], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 233, "seek": 79636, "start": 799.36, "end": 803.96, "text": " Wait, does Lambda have endpoints, back end endpoints?", "tokens": [50514, 3802, 11, 775, 45691, 362, 917, 20552, 11, 646, 917, 917, 20552, 30, 50744], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 234, "seek": 79636, "start": 803.96, "end": 805.36, "text": " I believe...", "tokens": [50744, 286, 1697, 485, 50814], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 235, "seek": 79636, "start": 805.36, "end": 813.44, "text": " I don't know if they're still in the labs testing area or if they're released fully or not,", "tokens": [50814, 286, 500, 380, 458, 498, 436, 434, 920, 294, 264, 20339, 4997, 1859, 420, 498, 436, 434, 4736, 4498, 420, 406, 11, 51218], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 236, "seek": 79636, "start": 813.44, "end": 817.92, "text": " but it does have a way to communicate through REST to Lambda era back end.", "tokens": [51218, 457, 309, 775, 362, 257, 636, 281, 7890, 807, 497, 14497, 281, 45691, 4249, 646, 917, 13, 51442], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 237, "seek": 79636, "start": 817.92, "end": 818.92, "text": " Okay.", "tokens": [51442, 1033, 13, 51492], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 238, "seek": 79636, "start": 818.92, "end": 819.92, "text": " I missed that.", "tokens": [51492, 286, 6721, 300, 13, 51542], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 239, "seek": 79636, "start": 819.92, "end": 820.9200000000001, "text": " Yeah.", "tokens": [51542, 865, 13, 51592], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 240, "seek": 79636, "start": 820.9200000000001, "end": 821.9200000000001, "text": " Interesting.", "tokens": [51592, 14711, 13, 51642], "temperature": 0.0, "avg_logprob": -0.23633673756392962, "compression_ratio": 1.4825870646766168, "no_speech_prob": 0.05256131291389465}, {"id": 241, "seek": 82192, "start": 821.92, "end": 827.7199999999999, "text": " Makes me wonder now, could you use open AI or co-pilot or something to look at your", "tokens": [50364, 25245, 385, 2441, 586, 11, 727, 291, 764, 1269, 7318, 420, 598, 12, 79, 31516, 420, 746, 281, 574, 412, 428, 50654], "temperature": 0.0, "avg_logprob": -0.28534038587548266, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.026737017557024956}, {"id": 242, "seek": 82192, "start": 827.7199999999999, "end": 833.1999999999999, "text": " Lambda era back end code to generate an open API spec that you could then feed back into", "tokens": [50654, 45691, 4249, 646, 917, 3089, 281, 8460, 364, 1269, 9362, 1608, 300, 291, 727, 550, 3154, 646, 666, 50928], "temperature": 0.0, "avg_logprob": -0.28534038587548266, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.026737017557024956}, {"id": 243, "seek": 82192, "start": 833.1999999999999, "end": 837.8399999999999, "text": " open AI to communicate to Lambda era in production?", "tokens": [50928, 1269, 7318, 281, 7890, 281, 45691, 4249, 294, 4265, 30, 51160], "temperature": 0.0, "avg_logprob": -0.28534038587548266, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.026737017557024956}, {"id": 244, "seek": 82192, "start": 837.8399999999999, "end": 842.0799999999999, "text": " Snake eating his tail.", "tokens": [51160, 33885, 3936, 702, 6838, 13, 51372], "temperature": 0.0, "avg_logprob": -0.28534038587548266, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.026737017557024956}, {"id": 245, "seek": 82192, "start": 842.0799999999999, "end": 846.4, "text": " We are in the inception part of the episode.", "tokens": [51372, 492, 366, 294, 264, 49834, 644, 295, 264, 3500, 13, 51588], "temperature": 0.0, "avg_logprob": -0.28534038587548266, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.026737017557024956}, {"id": 246, "seek": 82192, "start": 846.4, "end": 847.4, "text": " Yes.", "tokens": [51588, 1079, 13, 51638], "temperature": 0.0, "avg_logprob": -0.28534038587548266, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.026737017557024956}, {"id": 247, "seek": 82192, "start": 847.4, "end": 850.4, "text": " It's fun though.", "tokens": [51638, 467, 311, 1019, 1673, 13, 51788], "temperature": 0.0, "avg_logprob": -0.28534038587548266, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.026737017557024956}, {"id": 248, "seek": 85040, "start": 850.4, "end": 851.4, "text": " I don't know.", "tokens": [50364, 286, 500, 380, 458, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 249, "seek": 85040, "start": 851.4, "end": 854.72, "text": " I like the code gen side of things.", "tokens": [50414, 286, 411, 264, 3089, 1049, 1252, 295, 721, 13, 50580], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 250, "seek": 85040, "start": 854.72, "end": 862.92, "text": " With open API makes for a lot of, I won't even say code, just generation of information,", "tokens": [50580, 2022, 1269, 9362, 1669, 337, 257, 688, 295, 11, 286, 1582, 380, 754, 584, 3089, 11, 445, 5125, 295, 1589, 11, 50990], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 251, "seek": 85040, "start": 862.92, "end": 863.92, "text": " open API.", "tokens": [50990, 1269, 9362, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 252, "seek": 85040, "start": 863.92, "end": 866.88, "text": " Makes it really, already messing it up.", "tokens": [51040, 25245, 309, 534, 11, 1217, 23258, 309, 493, 13, 51188], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 253, "seek": 85040, "start": 866.88, "end": 868.8, "text": " Makes it really easy.", "tokens": [51188, 25245, 309, 534, 1858, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 254, "seek": 85040, "start": 868.8, "end": 869.8, "text": " Just go to Swagger.", "tokens": [51284, 1449, 352, 281, 3926, 11062, 13, 51334], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 255, "seek": 85040, "start": 869.8, "end": 870.8, "text": " Swagger.", "tokens": [51334, 3926, 11062, 13, 51384], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 256, "seek": 85040, "start": 870.8, "end": 871.8, "text": " Swagger, yeah.", "tokens": [51384, 3926, 11062, 11, 1338, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 257, "seek": 85040, "start": 871.8, "end": 872.8, "text": " Swagger is...", "tokens": [51434, 3926, 11062, 307, 485, 51484], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 258, "seek": 85040, "start": 872.8, "end": 878.52, "text": " So yeah, those who aren't familiar with open API or only familiar with Swagger or confused", "tokens": [51484, 407, 1338, 11, 729, 567, 3212, 380, 4963, 365, 1269, 9362, 420, 787, 4963, 365, 3926, 11062, 420, 9019, 51770], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 259, "seek": 85040, "start": 878.52, "end": 879.52, "text": " by it all.", "tokens": [51770, 538, 309, 439, 13, 51820], "temperature": 0.0, "avg_logprob": -0.2540138949867056, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.23332828283309937}, {"id": 260, "seek": 87952, "start": 879.52, "end": 884.36, "text": " Swagger is open API version two or lower.", "tokens": [50364, 3926, 11062, 307, 1269, 9362, 3037, 732, 420, 3126, 13, 50606], "temperature": 0.0, "avg_logprob": -0.2801305225917271, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.03511728346347809}, {"id": 261, "seek": 87952, "start": 884.36, "end": 890.0, "text": " I don't know the history there, but they changed names between version two and three.", "tokens": [50606, 286, 500, 380, 458, 264, 2503, 456, 11, 457, 436, 3105, 5288, 1296, 3037, 732, 293, 1045, 13, 50888], "temperature": 0.0, "avg_logprob": -0.2801305225917271, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.03511728346347809}, {"id": 262, "seek": 87952, "start": 890.0, "end": 892.76, "text": " They didn't have enough swag.", "tokens": [50888, 814, 994, 380, 362, 1547, 42064, 13, 51026], "temperature": 0.0, "avg_logprob": -0.2801305225917271, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.03511728346347809}, {"id": 263, "seek": 87952, "start": 892.76, "end": 894.88, "text": " They lost all their Swagger when they became open.", "tokens": [51026, 814, 2731, 439, 641, 3926, 11062, 562, 436, 3062, 1269, 13, 51132], "temperature": 0.0, "avg_logprob": -0.2801305225917271, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.03511728346347809}, {"id": 264, "seek": 87952, "start": 894.88, "end": 898.88, "text": " But they were open about changing the name.", "tokens": [51132, 583, 436, 645, 1269, 466, 4473, 264, 1315, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2801305225917271, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.03511728346347809}, {"id": 265, "seek": 87952, "start": 898.88, "end": 903.96, "text": " So you mentioned, yeah, that it was for V2 and below.", "tokens": [51332, 407, 291, 2835, 11, 1338, 11, 300, 309, 390, 337, 691, 17, 293, 2507, 13, 51586], "temperature": 0.0, "avg_logprob": -0.2801305225917271, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.03511728346347809}, {"id": 266, "seek": 90396, "start": 903.96, "end": 909.76, "text": " And when you open the Elm package you wrote, it says here are all the features we support", "tokens": [50364, 400, 562, 291, 1269, 264, 2699, 76, 7372, 291, 4114, 11, 309, 1619, 510, 366, 439, 264, 4122, 321, 1406, 50654], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 267, "seek": 90396, "start": 909.76, "end": 913.6, "text": " from V3 and V3.1.", "tokens": [50654, 490, 691, 18, 293, 691, 18, 13, 16, 13, 50846], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 268, "seek": 90396, "start": 913.6, "end": 917.32, "text": " And V2 is in the works, and maybe V3 is in the works as well.", "tokens": [50846, 400, 691, 17, 307, 294, 264, 1985, 11, 293, 1310, 691, 18, 307, 294, 264, 1985, 382, 731, 13, 51032], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 269, "seek": 90396, "start": 917.32, "end": 918.32, "text": " I don't remember.", "tokens": [51032, 286, 500, 380, 1604, 13, 51082], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 270, "seek": 90396, "start": 918.32, "end": 919.32, "text": " Kind of.", "tokens": [51082, 9242, 295, 13, 51132], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 271, "seek": 90396, "start": 919.32, "end": 921.96, "text": " When I have time, I work on it.", "tokens": [51132, 1133, 286, 362, 565, 11, 286, 589, 322, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 272, "seek": 90396, "start": 921.96, "end": 922.96, "text": " Yeah.", "tokens": [51264, 865, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 273, "seek": 90396, "start": 922.96, "end": 925.36, "text": " So 3.0 points.", "tokens": [51314, 407, 805, 13, 15, 2793, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 274, "seek": 90396, "start": 925.36, "end": 926.36, "text": " Why?", "tokens": [51434, 1545, 30, 51484], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 275, "seek": 90396, "start": 926.36, "end": 929.52, "text": " I don't even know why there's a why there.", "tokens": [51484, 286, 500, 380, 754, 458, 983, 456, 311, 257, 983, 456, 13, 51642], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 276, "seek": 90396, "start": 929.52, "end": 930.9200000000001, "text": " Is not supported yet.", "tokens": [51642, 1119, 406, 8104, 1939, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 277, "seek": 90396, "start": 930.9200000000001, "end": 932.96, "text": " 3.1 is mostly.", "tokens": [51712, 805, 13, 16, 307, 5240, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2820323181152344, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5858955383300781}, {"id": 278, "seek": 93296, "start": 933.96, "end": 939.96, "text": " What is hard about supporting that or is it just like it needs to put in some work or", "tokens": [50414, 708, 307, 1152, 466, 7231, 300, 420, 307, 309, 445, 411, 309, 2203, 281, 829, 294, 512, 589, 420, 50714], "temperature": 0.0, "avg_logprob": -0.17634888128800827, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.05403931438922882}, {"id": 279, "seek": 93296, "start": 939.96, "end": 943.2, "text": " is it like a very different system?", "tokens": [50714, 307, 309, 411, 257, 588, 819, 1185, 30, 50876], "temperature": 0.0, "avg_logprob": -0.17634888128800827, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.05403931438922882}, {"id": 280, "seek": 93296, "start": 943.2, "end": 944.2, "text": " Not very different.", "tokens": [50876, 1726, 588, 819, 13, 50926], "temperature": 0.0, "avg_logprob": -0.17634888128800827, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.05403931438922882}, {"id": 281, "seek": 93296, "start": 944.2, "end": 951.12, "text": " No, just time to go back through their docs and see what changed between the two versions.", "tokens": [50926, 883, 11, 445, 565, 281, 352, 646, 807, 641, 45623, 293, 536, 437, 3105, 1296, 264, 732, 9606, 13, 51272], "temperature": 0.0, "avg_logprob": -0.17634888128800827, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.05403931438922882}, {"id": 282, "seek": 93296, "start": 951.12, "end": 953.76, "text": " They are mostly compatible versions.", "tokens": [51272, 814, 366, 5240, 18218, 9606, 13, 51404], "temperature": 0.0, "avg_logprob": -0.17634888128800827, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.05403931438922882}, {"id": 283, "seek": 93296, "start": 953.76, "end": 960.44, "text": " I haven't heard of anyone with like 3.0 not being able to use it.", "tokens": [51404, 286, 2378, 380, 2198, 295, 2878, 365, 411, 805, 13, 15, 406, 885, 1075, 281, 764, 309, 13, 51738], "temperature": 0.0, "avg_logprob": -0.17634888128800827, "compression_ratio": 1.543778801843318, "no_speech_prob": 0.05403931438922882}, {"id": 284, "seek": 96044, "start": 960.44, "end": 966.9200000000001, "text": " I mean if you go back to Swagger, any of the two 2.x versions, you might have to do a few", "tokens": [50364, 286, 914, 498, 291, 352, 646, 281, 3926, 11062, 11, 604, 295, 264, 732, 568, 13, 87, 9606, 11, 291, 1062, 362, 281, 360, 257, 1326, 50688], "temperature": 0.0, "avg_logprob": -0.2377847551225542, "compression_ratio": 1.6, "no_speech_prob": 0.002631119219586253}, {"id": 285, "seek": 96044, "start": 966.9200000000001, "end": 970.0400000000001, "text": " tiny changes, but they're mostly compatible.", "tokens": [50688, 5870, 2962, 11, 457, 436, 434, 5240, 18218, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2377847551225542, "compression_ratio": 1.6, "no_speech_prob": 0.002631119219586253}, {"id": 286, "seek": 96044, "start": 970.0400000000001, "end": 973.84, "text": " So there's the CLI and there's the Elm package.", "tokens": [50844, 407, 456, 311, 264, 12855, 40, 293, 456, 311, 264, 2699, 76, 7372, 13, 51034], "temperature": 0.0, "avg_logprob": -0.2377847551225542, "compression_ratio": 1.6, "no_speech_prob": 0.002631119219586253}, {"id": 287, "seek": 96044, "start": 973.84, "end": 980.7600000000001, "text": " The CLI is there to generate Elm code for people to use the SDK.", "tokens": [51034, 440, 12855, 40, 307, 456, 281, 8460, 2699, 76, 3089, 337, 561, 281, 764, 264, 37135, 13, 51380], "temperature": 0.0, "avg_logprob": -0.2377847551225542, "compression_ratio": 1.6, "no_speech_prob": 0.002631119219586253}, {"id": 288, "seek": 96044, "start": 980.7600000000001, "end": 983.24, "text": " Or I guess it is the SDK then.", "tokens": [51380, 1610, 286, 2041, 309, 307, 264, 37135, 550, 13, 51504], "temperature": 0.0, "avg_logprob": -0.2377847551225542, "compression_ratio": 1.6, "no_speech_prob": 0.002631119219586253}, {"id": 289, "seek": 96044, "start": 983.24, "end": 987.72, "text": " Or yeah, to use the rest, the rest endpoints.", "tokens": [51504, 1610, 1338, 11, 281, 764, 264, 1472, 11, 264, 1472, 917, 20552, 13, 51728], "temperature": 0.0, "avg_logprob": -0.2377847551225542, "compression_ratio": 1.6, "no_speech_prob": 0.002631119219586253}, {"id": 290, "seek": 96044, "start": 987.72, "end": 989.72, "text": " What about the Elm package?", "tokens": [51728, 708, 466, 264, 2699, 76, 7372, 30, 51828], "temperature": 0.0, "avg_logprob": -0.2377847551225542, "compression_ratio": 1.6, "no_speech_prob": 0.002631119219586253}, {"id": 291, "seek": 98972, "start": 989.72, "end": 997.76, "text": " Is that meant to be used by people or is it just like you built the parser for the open", "tokens": [50364, 1119, 300, 4140, 281, 312, 1143, 538, 561, 420, 307, 309, 445, 411, 291, 3094, 264, 21156, 260, 337, 264, 1269, 50766], "temperature": 0.0, "avg_logprob": -0.24241759187431747, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.0011157267726957798}, {"id": 292, "seek": 98972, "start": 997.76, "end": 1002.4, "text": " API spec and you thought it might be useful for other people as well?", "tokens": [50766, 9362, 1608, 293, 291, 1194, 309, 1062, 312, 4420, 337, 661, 561, 382, 731, 30, 50998], "temperature": 0.0, "avg_logprob": -0.24241759187431747, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.0011157267726957798}, {"id": 293, "seek": 98972, "start": 1002.4, "end": 1005.84, "text": " Should I ever look at this package is what I meant?", "tokens": [50998, 6454, 286, 1562, 574, 412, 341, 7372, 307, 437, 286, 4140, 30, 51170], "temperature": 0.0, "avg_logprob": -0.24241759187431747, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.0011157267726957798}, {"id": 294, "seek": 98972, "start": 1005.84, "end": 1006.84, "text": " Yes.", "tokens": [51170, 1079, 13, 51220], "temperature": 0.0, "avg_logprob": -0.24241759187431747, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.0011157267726957798}, {"id": 295, "seek": 98972, "start": 1006.84, "end": 1010.64, "text": " I do think there are benefits to it.", "tokens": [51220, 286, 360, 519, 456, 366, 5311, 281, 309, 13, 51410], "temperature": 0.0, "avg_logprob": -0.24241759187431747, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.0011157267726957798}, {"id": 296, "seek": 98972, "start": 1010.64, "end": 1017.6, "text": " So going back to Square, all this came to me as something I wanted to work on when I", "tokens": [51410, 407, 516, 646, 281, 16463, 11, 439, 341, 1361, 281, 385, 382, 746, 286, 1415, 281, 589, 322, 562, 286, 51758], "temperature": 0.0, "avg_logprob": -0.24241759187431747, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.0011157267726957798}, {"id": 297, "seek": 101760, "start": 1017.6, "end": 1021.96, "text": " was at Square because of how we used it.", "tokens": [50364, 390, 412, 16463, 570, 295, 577, 321, 1143, 309, 13, 50582], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 298, "seek": 101760, "start": 1021.96, "end": 1027.6, "text": " So I mentioned earlier that we would generate this open API spec.", "tokens": [50582, 407, 286, 2835, 3071, 300, 321, 576, 8460, 341, 1269, 9362, 1608, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 299, "seek": 101760, "start": 1027.6, "end": 1028.68, "text": " But what happens from it there?", "tokens": [50864, 583, 437, 2314, 490, 309, 456, 30, 50918], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 300, "seek": 101760, "start": 1028.68, "end": 1031.72, "text": " We didn't just give it to users.", "tokens": [50918, 492, 994, 380, 445, 976, 309, 281, 5022, 13, 51070], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 301, "seek": 101760, "start": 1031.72, "end": 1035.72, "text": " One of the things we would do with it was send it off to another company called API", "tokens": [51070, 1485, 295, 264, 721, 321, 576, 360, 365, 309, 390, 2845, 309, 766, 281, 1071, 2237, 1219, 9362, 51270], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 302, "seek": 101760, "start": 1035.72, "end": 1039.1200000000001, "text": " Matic to generate SDKs.", "tokens": [51270, 376, 2399, 281, 8460, 37135, 82, 13, 51440], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 303, "seek": 101760, "start": 1039.1200000000001, "end": 1043.56, "text": " So that's kind of where I was like, oh, that's really cool, but they don't make an Elm SDK.", "tokens": [51440, 407, 300, 311, 733, 295, 689, 286, 390, 411, 11, 1954, 11, 300, 311, 534, 1627, 11, 457, 436, 500, 380, 652, 364, 2699, 76, 37135, 13, 51662], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 304, "seek": 101760, "start": 1043.56, "end": 1045.1200000000001, "text": " I want an Elm SDK.", "tokens": [51662, 286, 528, 364, 2699, 76, 37135, 13, 51740], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 305, "seek": 101760, "start": 1045.1200000000001, "end": 1046.84, "text": " I'm not going to pay them to make one.", "tokens": [51740, 286, 478, 406, 516, 281, 1689, 552, 281, 652, 472, 13, 51826], "temperature": 0.0, "avg_logprob": -0.15013072287389476, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.017436515539884567}, {"id": 306, "seek": 104684, "start": 1046.8799999999999, "end": 1048.08, "text": " I'll just do it myself.", "tokens": [50366, 286, 603, 445, 360, 309, 2059, 13, 50426], "temperature": 0.0, "avg_logprob": -0.1507461882129158, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008570640347898006}, {"id": 307, "seek": 104684, "start": 1048.08, "end": 1052.8799999999999, "text": " So it's like, all right, well, in order to do that, I need to be able to parse the spec.", "tokens": [50426, 407, 309, 311, 411, 11, 439, 558, 11, 731, 11, 294, 1668, 281, 360, 300, 11, 286, 643, 281, 312, 1075, 281, 48377, 264, 1608, 13, 50666], "temperature": 0.0, "avg_logprob": -0.1507461882129158, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008570640347898006}, {"id": 308, "seek": 104684, "start": 1052.8799999999999, "end": 1056.1599999999999, "text": " So that's kind of where the package started.", "tokens": [50666, 407, 300, 311, 733, 295, 689, 264, 7372, 1409, 13, 50830], "temperature": 0.0, "avg_logprob": -0.1507461882129158, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008570640347898006}, {"id": 309, "seek": 104684, "start": 1056.1599999999999, "end": 1062.6799999999998, "text": " And then in addition to that, we would also take the spec and we would generate documentation.", "tokens": [50830, 400, 550, 294, 4500, 281, 300, 11, 321, 576, 611, 747, 264, 1608, 293, 321, 576, 8460, 14333, 13, 51156], "temperature": 0.0, "avg_logprob": -0.1507461882129158, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008570640347898006}, {"id": 310, "seek": 104684, "start": 1062.6799999999998, "end": 1070.9199999999998, "text": " So if you go to Square's developer docs, a significant portion of that is run through", "tokens": [51156, 407, 498, 291, 352, 281, 16463, 311, 10754, 45623, 11, 257, 4776, 8044, 295, 300, 307, 1190, 807, 51568], "temperature": 0.0, "avg_logprob": -0.1507461882129158, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008570640347898006}, {"id": 311, "seek": 104684, "start": 1070.9199999999998, "end": 1071.9199999999998, "text": " the spec.", "tokens": [51568, 264, 1608, 13, 51618], "temperature": 0.0, "avg_logprob": -0.1507461882129158, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.008570640347898006}, {"id": 312, "seek": 107192, "start": 1071.92, "end": 1078.5600000000002, "text": " We take the spec and they would add markdown and other documentation into the spec written", "tokens": [50364, 492, 747, 264, 1608, 293, 436, 576, 909, 1491, 5093, 293, 661, 14333, 666, 264, 1608, 3720, 50696], "temperature": 0.0, "avg_logprob": -0.14719899878444442, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00884548481553793}, {"id": 313, "seek": 107192, "start": 1078.5600000000002, "end": 1081.44, "text": " by tech writers.", "tokens": [50696, 538, 7553, 13491, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14719899878444442, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00884548481553793}, {"id": 314, "seek": 107192, "start": 1081.44, "end": 1085.5600000000002, "text": " And you have a website, basically, then from that.", "tokens": [50840, 400, 291, 362, 257, 3144, 11, 1936, 11, 550, 490, 300, 13, 51046], "temperature": 0.0, "avg_logprob": -0.14719899878444442, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00884548481553793}, {"id": 315, "seek": 107192, "start": 1085.5600000000002, "end": 1093.64, "text": " So if you wanted to, if your company exposed an open API spec that other people could use,", "tokens": [51046, 407, 498, 291, 1415, 281, 11, 498, 428, 2237, 9495, 364, 1269, 9362, 1608, 300, 661, 561, 727, 764, 11, 51450], "temperature": 0.0, "avg_logprob": -0.14719899878444442, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00884548481553793}, {"id": 316, "seek": 107192, "start": 1093.64, "end": 1099.16, "text": " you could, in addition to that, generate all of your documentation from that same spec,", "tokens": [51450, 291, 727, 11, 294, 4500, 281, 300, 11, 8460, 439, 295, 428, 14333, 490, 300, 912, 1608, 11, 51726], "temperature": 0.0, "avg_logprob": -0.14719899878444442, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.00884548481553793}, {"id": 317, "seek": 109916, "start": 1099.16, "end": 1104.0, "text": " which is quite handy, then your documentation always matches your SDK, always matches your", "tokens": [50364, 597, 307, 1596, 13239, 11, 550, 428, 14333, 1009, 10676, 428, 37135, 11, 1009, 10676, 428, 50606], "temperature": 0.0, "avg_logprob": -0.14309298515319824, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04466429352760315}, {"id": 318, "seek": 109916, "start": 1104.0, "end": 1105.0, "text": " backend.", "tokens": [50606, 38087, 13, 50656], "temperature": 0.0, "avg_logprob": -0.14309298515319824, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04466429352760315}, {"id": 319, "seek": 109916, "start": 1105.0, "end": 1106.0, "text": " Yeah.", "tokens": [50656, 865, 13, 50706], "temperature": 0.0, "avg_logprob": -0.14309298515319824, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04466429352760315}, {"id": 320, "seek": 109916, "start": 1106.0, "end": 1110.8400000000001, "text": " It seems like a, I mean, if you're using REST endpoints, it seems like a great way to go", "tokens": [50706, 467, 2544, 411, 257, 11, 286, 914, 11, 498, 291, 434, 1228, 497, 14497, 917, 20552, 11, 309, 2544, 411, 257, 869, 636, 281, 352, 50948], "temperature": 0.0, "avg_logprob": -0.14309298515319824, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04466429352760315}, {"id": 321, "seek": 109916, "start": 1110.8400000000001, "end": 1115.3200000000002, "text": " to get a lot of the tooling benefits that GraphQL would provide.", "tokens": [50948, 281, 483, 257, 688, 295, 264, 46593, 5311, 300, 21884, 13695, 576, 2893, 13, 51172], "temperature": 0.0, "avg_logprob": -0.14309298515319824, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04466429352760315}, {"id": 322, "seek": 109916, "start": 1115.3200000000002, "end": 1118.88, "text": " It seems like a great alternative to GraphQL.", "tokens": [51172, 467, 2544, 411, 257, 869, 8535, 281, 21884, 13695, 13, 51350], "temperature": 0.0, "avg_logprob": -0.14309298515319824, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04466429352760315}, {"id": 323, "seek": 109916, "start": 1118.88, "end": 1124.0800000000002, "text": " And of course, GraphQL has its share of tradeoffs as well.", "tokens": [51350, 400, 295, 1164, 11, 21884, 13695, 575, 1080, 2073, 295, 4923, 19231, 382, 731, 13, 51610], "temperature": 0.0, "avg_logprob": -0.14309298515319824, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.04466429352760315}, {"id": 324, "seek": 112408, "start": 1124.08, "end": 1133.6399999999999, "text": " The whole overfetching, GraphQL is trying to solve overfetching and N plus one database,", "tokens": [50364, 440, 1379, 670, 69, 7858, 278, 11, 21884, 13695, 307, 1382, 281, 5039, 670, 69, 7858, 278, 293, 426, 1804, 472, 8149, 11, 50842], "temperature": 0.0, "avg_logprob": -0.14944018017161975, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.4918939173221588}, {"id": 325, "seek": 112408, "start": 1133.6399999999999, "end": 1140.24, "text": " you know, N plus one queries from the front end, but it makes it really hard to avoid", "tokens": [50842, 291, 458, 11, 426, 1804, 472, 24109, 490, 264, 1868, 917, 11, 457, 309, 1669, 309, 534, 1152, 281, 5042, 51172], "temperature": 0.0, "avg_logprob": -0.14944018017161975, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.4918939173221588}, {"id": 326, "seek": 112408, "start": 1140.24, "end": 1146.72, "text": " N plus one queries in the database because you have to tack on all these additional queries", "tokens": [51172, 426, 1804, 472, 24109, 294, 264, 8149, 570, 291, 362, 281, 9426, 322, 439, 613, 4497, 24109, 51496], "temperature": 0.0, "avg_logprob": -0.14944018017161975, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.4918939173221588}, {"id": 327, "seek": 112408, "start": 1146.72, "end": 1153.48, "text": " and use creative ways to like aggregate them into one sort of query runner, which is very", "tokens": [51496, 293, 764, 5880, 2098, 281, 411, 26118, 552, 666, 472, 1333, 295, 14581, 24376, 11, 597, 307, 588, 51834], "temperature": 0.0, "avg_logprob": -0.14944018017161975, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.4918939173221588}, {"id": 328, "seek": 115348, "start": 1153.48, "end": 1155.0, "text": " difficult is a difficult problem.", "tokens": [50364, 2252, 307, 257, 2252, 1154, 13, 50440], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 329, "seek": 115348, "start": 1155.0, "end": 1158.76, "text": " So, so some people like using REST APIs for their back end.", "tokens": [50440, 407, 11, 370, 512, 561, 411, 1228, 497, 14497, 21445, 337, 641, 646, 917, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 330, "seek": 115348, "start": 1158.76, "end": 1164.4, "text": " And if you do, why not use open API and if you get a nice specification, it's way nicer", "tokens": [50628, 400, 498, 291, 360, 11, 983, 406, 764, 1269, 9362, 293, 498, 291, 483, 257, 1481, 31256, 11, 309, 311, 636, 22842, 50910], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 331, "seek": 115348, "start": 1164.4, "end": 1166.4, "text": " to use from, from Elm.", "tokens": [50910, 281, 764, 490, 11, 490, 2699, 76, 13, 51010], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 332, "seek": 115348, "start": 1166.4, "end": 1172.6, "text": " Also not everything goes through a get request with, with GraphQL, everything goes through", "tokens": [51010, 2743, 406, 1203, 1709, 807, 257, 483, 5308, 365, 11, 365, 21884, 13695, 11, 1203, 1709, 807, 51320], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 333, "seek": 115348, "start": 1172.6, "end": 1174.24, "text": " get request, right?", "tokens": [51320, 483, 5308, 11, 558, 30, 51402], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 334, "seek": 115348, "start": 1174.24, "end": 1176.04, "text": " Or post or post request.", "tokens": [51402, 1610, 2183, 420, 2183, 5308, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 335, "seek": 115348, "start": 1176.04, "end": 1177.04, "text": " Yeah.", "tokens": [51492, 865, 13, 51542], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 336, "seek": 115348, "start": 1177.04, "end": 1178.04, "text": " Oh, post request.", "tokens": [51542, 876, 11, 2183, 5308, 13, 51592], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 337, "seek": 115348, "start": 1178.04, "end": 1179.04, "text": " Was that it?", "tokens": [51592, 3027, 300, 309, 30, 51642], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 338, "seek": 115348, "start": 1179.04, "end": 1180.04, "text": " Yeah.", "tokens": [51642, 865, 13, 51692], "temperature": 0.0, "avg_logprob": -0.2763076321832065, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.09936407953500748}, {"id": 339, "seek": 118004, "start": 1180.04, "end": 1181.04, "text": " Yeah, I think it's two either.", "tokens": [50364, 865, 11, 286, 519, 309, 311, 732, 2139, 13, 50414], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 340, "seek": 118004, "start": 1181.04, "end": 1184.3999999999999, "text": " But yeah, usually people do post request with a body.", "tokens": [50414, 583, 1338, 11, 2673, 561, 360, 2183, 5308, 365, 257, 1772, 13, 50582], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 341, "seek": 118004, "start": 1184.3999999999999, "end": 1185.92, "text": " Wait, you can do either.", "tokens": [50582, 3802, 11, 291, 393, 360, 2139, 13, 50658], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 342, "seek": 118004, "start": 1185.92, "end": 1188.2, "text": " Can you also do it with a delete request?", "tokens": [50658, 1664, 291, 611, 360, 309, 365, 257, 12097, 5308, 30, 50772], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 343, "seek": 118004, "start": 1188.2, "end": 1189.2, "text": " Maybe.", "tokens": [50772, 2704, 13, 50822], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 344, "seek": 118004, "start": 1189.2, "end": 1194.2, "text": " If you're a psychopath, then yes.", "tokens": [50822, 759, 291, 434, 257, 47577, 11, 550, 2086, 13, 51072], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 345, "seek": 118004, "start": 1194.2, "end": 1195.2, "text": " Yeah.", "tokens": [51072, 865, 13, 51122], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 346, "seek": 118004, "start": 1195.2, "end": 1196.2, "text": " Yeah.", "tokens": [51122, 865, 13, 51172], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 347, "seek": 118004, "start": 1196.2, "end": 1203.2, "text": " No, I think the comparisons to the GraphQL, I think are, are very warranted.", "tokens": [51172, 883, 11, 286, 519, 264, 33157, 281, 264, 21884, 13695, 11, 286, 519, 366, 11, 366, 588, 16354, 292, 13, 51522], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 348, "seek": 118004, "start": 1203.2, "end": 1207.52, "text": " It is very, very similar in many ways to, to working with GraphQL.", "tokens": [51522, 467, 307, 588, 11, 588, 2531, 294, 867, 2098, 281, 11, 281, 1364, 365, 21884, 13695, 13, 51738], "temperature": 0.0, "avg_logprob": -0.4003607545580183, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.09131862223148346}, {"id": 349, "seek": 120752, "start": 1207.52, "end": 1213.68, "text": " You get a lot of similar tooling, a lot of similar typed benefits in a way, a lot of", "tokens": [50364, 509, 483, 257, 688, 295, 2531, 46593, 11, 257, 688, 295, 2531, 33941, 5311, 294, 257, 636, 11, 257, 688, 295, 50672], "temperature": 0.0, "avg_logprob": -0.12148407409931052, "compression_ratio": 1.7587719298245614, "no_speech_prob": 0.03617960587143898}, {"id": 350, "seek": 120752, "start": 1213.68, "end": 1216.36, "text": " similar guarantees.", "tokens": [50672, 2531, 32567, 13, 50806], "temperature": 0.0, "avg_logprob": -0.12148407409931052, "compression_ratio": 1.7587719298245614, "no_speech_prob": 0.03617960587143898}, {"id": 351, "seek": 120752, "start": 1216.36, "end": 1220.12, "text": " I think I haven't worked with GraphQL long enough.", "tokens": [50806, 286, 519, 286, 2378, 380, 2732, 365, 21884, 13695, 938, 1547, 13, 50994], "temperature": 0.0, "avg_logprob": -0.12148407409931052, "compression_ratio": 1.7587719298245614, "no_speech_prob": 0.03617960587143898}, {"id": 352, "seek": 120752, "start": 1220.12, "end": 1222.36, "text": " I've worked with REST for over a decade now.", "tokens": [50994, 286, 600, 2732, 365, 497, 14497, 337, 670, 257, 10378, 586, 13, 51106], "temperature": 0.0, "avg_logprob": -0.12148407409931052, "compression_ratio": 1.7587719298245614, "no_speech_prob": 0.03617960587143898}, {"id": 353, "seek": 120752, "start": 1222.36, "end": 1225.08, "text": " I've worked with GraphQL for over a year.", "tokens": [51106, 286, 600, 2732, 365, 21884, 13695, 337, 670, 257, 1064, 13, 51242], "temperature": 0.0, "avg_logprob": -0.12148407409931052, "compression_ratio": 1.7587719298245614, "no_speech_prob": 0.03617960587143898}, {"id": 354, "seek": 120752, "start": 1225.08, "end": 1230.6399999999999, "text": " So I feel like I still have a lot of learning on how to best use GraphQL.", "tokens": [51242, 407, 286, 841, 411, 286, 920, 362, 257, 688, 295, 2539, 322, 577, 281, 1151, 764, 21884, 13695, 13, 51520], "temperature": 0.0, "avg_logprob": -0.12148407409931052, "compression_ratio": 1.7587719298245614, "no_speech_prob": 0.03617960587143898}, {"id": 355, "seek": 120752, "start": 1230.6399999999999, "end": 1236.56, "text": " I think with REST, it's a little bit more fixed in terms of what to expect, which is", "tokens": [51520, 286, 519, 365, 497, 14497, 11, 309, 311, 257, 707, 857, 544, 6806, 294, 2115, 295, 437, 281, 2066, 11, 597, 307, 51816], "temperature": 0.0, "avg_logprob": -0.12148407409931052, "compression_ratio": 1.7587719298245614, "no_speech_prob": 0.03617960587143898}, {"id": 356, "seek": 123656, "start": 1236.56, "end": 1238.2, "text": " kind of nice.", "tokens": [50364, 733, 295, 1481, 13, 50446], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 357, "seek": 123656, "start": 1238.2, "end": 1245.3999999999999, "text": " There's also, so other, other ways, since we were talking about, would, would you rune", "tokens": [50446, 821, 311, 611, 11, 370, 661, 11, 661, 2098, 11, 1670, 321, 645, 1417, 466, 11, 576, 11, 576, 291, 1190, 68, 50806], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 358, "seek": 123656, "start": 1245.3999999999999, "end": 1247.8799999999999, "text": " use, use the Elm package?", "tokens": [50806, 764, 11, 764, 264, 2699, 76, 7372, 30, 50930], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 359, "seek": 123656, "start": 1247.8799999999999, "end": 1253.52, "text": " Another thing I've been wanting to explore and bring it back to beginning with Martins.", "tokens": [50930, 3996, 551, 286, 600, 668, 7935, 281, 6839, 293, 1565, 309, 646, 281, 2863, 365, 5807, 1292, 13, 51212], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 360, "seek": 123656, "start": 1253.52, "end": 1256.36, "text": " I know there is a Martin already exploring this.", "tokens": [51212, 286, 458, 456, 307, 257, 9184, 1217, 12736, 341, 13, 51354], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 361, "seek": 123656, "start": 1256.36, "end": 1258.36, "text": " Which one?", "tokens": [51354, 3013, 472, 30, 51454], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 362, "seek": 123656, "start": 1258.36, "end": 1260.36, "text": " Which one?", "tokens": [51454, 3013, 472, 30, 51554], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 363, "seek": 123656, "start": 1260.36, "end": 1264.52, "text": " Oh, I'm trying to, I'm blanking on his last name.", "tokens": [51554, 876, 11, 286, 478, 1382, 281, 11, 286, 478, 8247, 278, 322, 702, 1036, 1315, 13, 51762], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 364, "seek": 123656, "start": 1264.52, "end": 1265.52, "text": " Stuart?", "tokens": [51762, 36236, 30, 51812], "temperature": 0.0, "avg_logprob": -0.31142331969063236, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.12224245071411133}, {"id": 365, "seek": 126552, "start": 1265.6399999999999, "end": 1267.6399999999999, "text": " No, no, no.", "tokens": [50370, 883, 11, 572, 11, 572, 13, 50470], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 366, "seek": 126552, "start": 1267.6399999999999, "end": 1269.6399999999999, "text": " Wait, another one?", "tokens": [50470, 3802, 11, 1071, 472, 30, 50570], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 367, "seek": 126552, "start": 1269.6399999999999, "end": 1271.6399999999999, "text": " Yes.", "tokens": [50570, 1079, 13, 50670], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 368, "seek": 126552, "start": 1271.6399999999999, "end": 1281.12, "text": " We were talking in Elm, the Elm online meetup a few weeks ago about OpenAPI because the", "tokens": [50670, 492, 645, 1417, 294, 2699, 76, 11, 264, 2699, 76, 2950, 1677, 1010, 257, 1326, 3259, 2057, 466, 7238, 4715, 40, 570, 264, 51144], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 369, "seek": 126552, "start": 1281.12, "end": 1283.4, "text": " company works for is using it.", "tokens": [51144, 2237, 1985, 337, 307, 1228, 309, 13, 51258], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 370, "seek": 126552, "start": 1283.4, "end": 1286.32, "text": " They're actually using the package, not the CLI tool.", "tokens": [51258, 814, 434, 767, 1228, 264, 7372, 11, 406, 264, 12855, 40, 2290, 13, 51404], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 371, "seek": 126552, "start": 1286.32, "end": 1288.96, "text": " So you can too.", "tokens": [51404, 407, 291, 393, 886, 13, 51536], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 372, "seek": 126552, "start": 1288.96, "end": 1293.44, "text": " But they're looking at doing form generation.", "tokens": [51536, 583, 436, 434, 1237, 412, 884, 1254, 5125, 13, 51760], "temperature": 0.0, "avg_logprob": -0.3112996058030562, "compression_ratio": 1.40625, "no_speech_prob": 0.004901488311588764}, {"id": 373, "seek": 129344, "start": 1293.44, "end": 1301.16, "text": " So because it's using JSON schema under the hood for the bodies of the posts and gets", "tokens": [50364, 407, 570, 309, 311, 1228, 31828, 34078, 833, 264, 13376, 337, 264, 7510, 295, 264, 12300, 293, 2170, 50750], "temperature": 0.0, "avg_logprob": -0.195618359010611, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.0007790200179442763}, {"id": 374, "seek": 129344, "start": 1301.16, "end": 1310.1200000000001, "text": " and deletes, you could just use whatever else exists for JSON schema and you could do form", "tokens": [50750, 293, 1103, 37996, 11, 291, 727, 445, 764, 2035, 1646, 8198, 337, 31828, 34078, 293, 291, 727, 360, 1254, 51198], "temperature": 0.0, "avg_logprob": -0.195618359010611, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.0007790200179442763}, {"id": 375, "seek": 129344, "start": 1310.1200000000001, "end": 1311.1200000000001, "text": " generation.", "tokens": [51198, 5125, 13, 51248], "temperature": 0.0, "avg_logprob": -0.195618359010611, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.0007790200179442763}, {"id": 376, "seek": 129344, "start": 1311.1200000000001, "end": 1316.24, "text": " I think he is actually using Dillon, your library as well, for the form side.", "tokens": [51248, 286, 519, 415, 307, 767, 1228, 28160, 11, 428, 6405, 382, 731, 11, 337, 264, 1254, 1252, 13, 51504], "temperature": 0.0, "avg_logprob": -0.195618359010611, "compression_ratio": 1.5406976744186047, "no_speech_prob": 0.0007790200179442763}, {"id": 377, "seek": 131624, "start": 1316.24, "end": 1323.88, "text": " So he's kind of combining the Elm Open API and Elm forms to do form generation.", "tokens": [50364, 407, 415, 311, 733, 295, 21928, 264, 2699, 76, 7238, 9362, 293, 2699, 76, 6422, 281, 360, 1254, 5125, 13, 50746], "temperature": 0.0, "avg_logprob": -0.2038273116917286, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.15796272456645966}, {"id": 378, "seek": 131624, "start": 1323.88, "end": 1330.08, "text": " All of these schemas definitely, you squint your eyes and you're like, hmm, these do look", "tokens": [50746, 1057, 295, 613, 22627, 296, 2138, 11, 291, 2339, 686, 428, 2575, 293, 291, 434, 411, 11, 16478, 11, 613, 360, 574, 51056], "temperature": 0.0, "avg_logprob": -0.2038273116917286, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.15796272456645966}, {"id": 379, "seek": 131624, "start": 1330.08, "end": 1331.08, "text": " similar.", "tokens": [51056, 2531, 13, 51106], "temperature": 0.0, "avg_logprob": -0.2038273116917286, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.15796272456645966}, {"id": 380, "seek": 131624, "start": 1331.08, "end": 1339.2, "text": " Yeah, in the blog post you wrote about effortless SDKs, which introduces Elm Open API.", "tokens": [51106, 865, 11, 294, 264, 6968, 2183, 291, 4114, 466, 4630, 1832, 37135, 82, 11, 597, 31472, 2699, 76, 7238, 9362, 13, 51512], "temperature": 0.0, "avg_logprob": -0.2038273116917286, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.15796272456645966}, {"id": 381, "seek": 131624, "start": 1339.2, "end": 1340.52, "text": " You do talk about forms.", "tokens": [51512, 509, 360, 751, 466, 6422, 13, 51578], "temperature": 0.0, "avg_logprob": -0.2038273116917286, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.15796272456645966}, {"id": 382, "seek": 131624, "start": 1340.52, "end": 1344.8, "text": " So is this something that people can do already or is this something that you want to work", "tokens": [51578, 407, 307, 341, 746, 300, 561, 393, 360, 1217, 420, 307, 341, 746, 300, 291, 528, 281, 589, 51792], "temperature": 0.0, "avg_logprob": -0.2038273116917286, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.15796272456645966}, {"id": 383, "seek": 134480, "start": 1344.8, "end": 1347.12, "text": " on later?", "tokens": [50364, 322, 1780, 30, 50480], "temperature": 0.0, "avg_logprob": -0.24639964384191176, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.20155459642410278}, {"id": 384, "seek": 134480, "start": 1347.12, "end": 1348.96, "text": " Something I want to work on later.", "tokens": [50480, 6595, 286, 528, 281, 589, 322, 1780, 13, 50572], "temperature": 0.0, "avg_logprob": -0.24639964384191176, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.20155459642410278}, {"id": 385, "seek": 134480, "start": 1348.96, "end": 1355.0, "text": " When I was at Strange Loop, I was it was chatting with someone who they know they're Martin.", "tokens": [50572, 1133, 286, 390, 412, 29068, 45660, 11, 286, 390, 309, 390, 24654, 365, 1580, 567, 436, 458, 436, 434, 9184, 13, 50874], "temperature": 0.0, "avg_logprob": -0.24639964384191176, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.20155459642410278}, {"id": 386, "seek": 134480, "start": 1355.0, "end": 1357.1599999999999, "text": " No, no.", "tokens": [50874, 883, 11, 572, 13, 50982], "temperature": 0.0, "avg_logprob": -0.24639964384191176, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.20155459642410278}, {"id": 387, "seek": 134480, "start": 1357.1599999999999, "end": 1363.08, "text": " I want to say if my memory is not failing me, I believe his name was Kevin.", "tokens": [50982, 286, 528, 281, 584, 498, 452, 4675, 307, 406, 18223, 385, 11, 286, 1697, 702, 1315, 390, 9954, 13, 51278], "temperature": 0.0, "avg_logprob": -0.24639964384191176, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.20155459642410278}, {"id": 388, "seek": 134480, "start": 1363.08, "end": 1371.84, "text": " He was doing some biomedical stuff prior and doing some form generation and had been using", "tokens": [51278, 634, 390, 884, 512, 49775, 1507, 4059, 293, 884, 512, 1254, 5125, 293, 632, 668, 1228, 51716], "temperature": 0.0, "avg_logprob": -0.24639964384191176, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.20155459642410278}, {"id": 389, "seek": 137184, "start": 1371.84, "end": 1377.24, "text": " ClosureScript for the form generation and was just trying to explore what else was out", "tokens": [50364, 2033, 7641, 14237, 337, 264, 1254, 5125, 293, 390, 445, 1382, 281, 6839, 437, 1646, 390, 484, 50634], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 390, "seek": 137184, "start": 1377.24, "end": 1378.24, "text": " there.", "tokens": [50634, 456, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 391, "seek": 137184, "start": 1378.24, "end": 1380.36, "text": " His back end was in Rust.", "tokens": [50684, 2812, 646, 917, 390, 294, 34952, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 392, "seek": 137184, "start": 1380.36, "end": 1384.3999999999999, "text": " That's how the machines were communicating with the web interface.", "tokens": [50790, 663, 311, 577, 264, 8379, 645, 17559, 365, 264, 3670, 9226, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 393, "seek": 137184, "start": 1384.3999999999999, "end": 1386.52, "text": " And so he was exploring what else is out there.", "tokens": [50992, 400, 370, 415, 390, 12736, 437, 1646, 307, 484, 456, 13, 51098], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 394, "seek": 137184, "start": 1386.52, "end": 1390.52, "text": " I was like, you know, I think there is something Elm could do there.", "tokens": [51098, 286, 390, 411, 11, 291, 458, 11, 286, 519, 456, 307, 746, 2699, 76, 727, 360, 456, 13, 51298], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 395, "seek": 137184, "start": 1390.52, "end": 1393.84, "text": " I don't know for certain, but I think there is something.", "tokens": [51298, 286, 500, 380, 458, 337, 1629, 11, 457, 286, 519, 456, 307, 746, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 396, "seek": 137184, "start": 1393.84, "end": 1400.4399999999998, "text": " We got to chatting about JSON schemas, which does most of the data portion.", "tokens": [51464, 492, 658, 281, 24654, 466, 31828, 22627, 296, 11, 597, 775, 881, 295, 264, 1412, 8044, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1435077272612473, "compression_ratio": 1.66793893129771, "no_speech_prob": 0.6503519415855408}, {"id": 397, "seek": 140044, "start": 1400.44, "end": 1408.8400000000001, "text": " And then there's also UI schema, which is UI definitions for rows, columns, and a bit", "tokens": [50364, 400, 550, 456, 311, 611, 15682, 34078, 11, 597, 307, 15682, 21988, 337, 13241, 11, 13766, 11, 293, 257, 857, 50784], "temperature": 0.0, "avg_logprob": -0.1064926988335066, "compression_ratio": 1.65, "no_speech_prob": 0.06183608993887901}, {"id": 398, "seek": 140044, "start": 1408.8400000000001, "end": 1414.68, "text": " of other stuff to use in conjunction with JSON schema for defining forms.", "tokens": [50784, 295, 661, 1507, 281, 764, 294, 27482, 365, 31828, 34078, 337, 17827, 6422, 13, 51076], "temperature": 0.0, "avg_logprob": -0.1064926988335066, "compression_ratio": 1.65, "no_speech_prob": 0.06183608993887901}, {"id": 399, "seek": 140044, "start": 1414.68, "end": 1420.24, "text": " So there is there is an existing schema for defining forms on top of JSON schema.", "tokens": [51076, 407, 456, 307, 456, 307, 364, 6741, 34078, 337, 17827, 6422, 322, 1192, 295, 31828, 34078, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1064926988335066, "compression_ratio": 1.65, "no_speech_prob": 0.06183608993887901}, {"id": 400, "seek": 140044, "start": 1420.24, "end": 1426.88, "text": " So I think I think I could probably my my idea is to try out something in that area.", "tokens": [51354, 407, 286, 519, 286, 519, 286, 727, 1391, 452, 452, 1558, 307, 281, 853, 484, 746, 294, 300, 1859, 13, 51686], "temperature": 0.0, "avg_logprob": -0.1064926988335066, "compression_ratio": 1.65, "no_speech_prob": 0.06183608993887901}, {"id": 401, "seek": 140044, "start": 1426.88, "end": 1428.16, "text": " I don't know what it looks like yet.", "tokens": [51686, 286, 500, 380, 458, 437, 309, 1542, 411, 1939, 13, 51750], "temperature": 0.0, "avg_logprob": -0.1064926988335066, "compression_ratio": 1.65, "no_speech_prob": 0.06183608993887901}, {"id": 402, "seek": 142816, "start": 1428.16, "end": 1431.3200000000002, "text": " I haven't haven't had the time to get around to it.", "tokens": [50364, 286, 2378, 380, 2378, 380, 632, 264, 565, 281, 483, 926, 281, 309, 13, 50522], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 403, "seek": 142816, "start": 1431.3200000000002, "end": 1434.64, "text": " But I think there could be something there.", "tokens": [50522, 583, 286, 519, 456, 727, 312, 746, 456, 13, 50688], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 404, "seek": 142816, "start": 1434.64, "end": 1436.44, "text": " So a little bit confused.", "tokens": [50688, 407, 257, 707, 857, 9019, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 405, "seek": 142816, "start": 1436.44, "end": 1443.48, "text": " Can you define all the necessary validations on data through the open API spec?", "tokens": [50778, 1664, 291, 6964, 439, 264, 4818, 7363, 763, 322, 1412, 807, 264, 1269, 9362, 1608, 30, 51130], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 406, "seek": 142816, "start": 1443.48, "end": 1445.6000000000001, "text": " Or is it too limited?", "tokens": [51130, 1610, 307, 309, 886, 5567, 30, 51236], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 407, "seek": 142816, "start": 1445.6000000000001, "end": 1449.88, "text": " I'm guessing you can do relationships between two objects or whatever.", "tokens": [51236, 286, 478, 17939, 291, 393, 360, 6159, 1296, 732, 6565, 420, 2035, 13, 51450], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 408, "seek": 142816, "start": 1449.88, "end": 1452.24, "text": " But relationships are not certain.", "tokens": [51450, 583, 6159, 366, 406, 1629, 13, 51568], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 409, "seek": 142816, "start": 1452.24, "end": 1453.24, "text": " You might be able to.", "tokens": [51568, 509, 1062, 312, 1075, 281, 13, 51618], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 410, "seek": 142816, "start": 1453.24, "end": 1455.5600000000002, "text": " I'm not 100% certain offhand.", "tokens": [51618, 286, 478, 406, 2319, 4, 1629, 766, 5543, 13, 51734], "temperature": 0.0, "avg_logprob": -0.2485702877952939, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.012819345109164715}, {"id": 411, "seek": 145556, "start": 1455.56, "end": 1458.8799999999999, "text": " You can define so it's not on the open API that's on.", "tokens": [50364, 509, 393, 6964, 370, 309, 311, 406, 322, 264, 1269, 9362, 300, 311, 322, 13, 50530], "temperature": 0.0, "avg_logprob": -0.26193934473498115, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.016901202499866486}, {"id": 412, "seek": 145556, "start": 1458.8799999999999, "end": 1463.6799999999998, "text": " So the validation is done on JSON schema, which is confusing.", "tokens": [50530, 407, 264, 24071, 307, 1096, 322, 31828, 34078, 11, 597, 307, 13181, 13, 50770], "temperature": 0.0, "avg_logprob": -0.26193934473498115, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.016901202499866486}, {"id": 413, "seek": 145556, "start": 1463.6799999999998, "end": 1469.6799999999998, "text": " But yeah, but open API spec is JSON schema also.", "tokens": [50770, 583, 1338, 11, 457, 1269, 9362, 1608, 307, 31828, 34078, 611, 13, 51070], "temperature": 0.0, "avg_logprob": -0.26193934473498115, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.016901202499866486}, {"id": 414, "seek": 145556, "start": 1469.6799999999998, "end": 1476.48, "text": " It uses JSON schema for defining request bodies and response bodies.", "tokens": [51070, 467, 4960, 31828, 34078, 337, 17827, 5308, 7510, 293, 4134, 7510, 13, 51410], "temperature": 0.0, "avg_logprob": -0.26193934473498115, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.016901202499866486}, {"id": 415, "seek": 145556, "start": 1476.48, "end": 1483.36, "text": " OK, because you did mention like, yeah, you can say that a price has to be a minimum zero.", "tokens": [51410, 2264, 11, 570, 291, 630, 2152, 411, 11, 1338, 11, 291, 393, 584, 300, 257, 3218, 575, 281, 312, 257, 7285, 4018, 13, 51754], "temperature": 0.0, "avg_logprob": -0.26193934473498115, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.016901202499866486}, {"id": 416, "seek": 148336, "start": 1483.3999999999999, "end": 1485.9599999999998, "text": " So there are some validations.", "tokens": [50366, 407, 456, 366, 512, 7363, 763, 13, 50494], "temperature": 0.0, "avg_logprob": -0.16836186741175277, "compression_ratio": 1.5408163265306123, "no_speech_prob": 0.0017004305263981223}, {"id": 417, "seek": 148336, "start": 1485.9599999999998, "end": 1496.24, "text": " So I'm curious about like, if I'm using Elm open API, so I've I've run the CLI tool.", "tokens": [50494, 407, 286, 478, 6369, 466, 411, 11, 498, 286, 478, 1228, 2699, 76, 1269, 9362, 11, 370, 286, 600, 286, 600, 1190, 264, 12855, 40, 2290, 13, 51008], "temperature": 0.0, "avg_logprob": -0.16836186741175277, "compression_ratio": 1.5408163265306123, "no_speech_prob": 0.0017004305263981223}, {"id": 418, "seek": 148336, "start": 1496.24, "end": 1502.8799999999999, "text": " I've generated some code for the Spotify API or some open API spec.", "tokens": [51008, 286, 600, 10833, 512, 3089, 337, 264, 29036, 9362, 420, 512, 1269, 9362, 1608, 13, 51340], "temperature": 0.0, "avg_logprob": -0.16836186741175277, "compression_ratio": 1.5408163265306123, "no_speech_prob": 0.0017004305263981223}, {"id": 419, "seek": 148336, "start": 1502.8799999999999, "end": 1510.28, "text": " And now I've got a folder full of generated Elm code that I can use for all of the rest", "tokens": [51340, 400, 586, 286, 600, 658, 257, 10820, 1577, 295, 10833, 2699, 76, 3089, 300, 286, 393, 764, 337, 439, 295, 264, 1472, 51710], "temperature": 0.0, "avg_logprob": -0.16836186741175277, "compression_ratio": 1.5408163265306123, "no_speech_prob": 0.0017004305263981223}, {"id": 420, "seek": 148336, "start": 1510.28, "end": 1512.36, "text": " endpoints in my specification.", "tokens": [51710, 917, 20552, 294, 452, 31256, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16836186741175277, "compression_ratio": 1.5408163265306123, "no_speech_prob": 0.0017004305263981223}, {"id": 421, "seek": 151236, "start": 1512.36, "end": 1514.32, "text": " Tell us a little bit about that code.", "tokens": [50364, 5115, 505, 257, 707, 857, 466, 300, 3089, 13, 50462], "temperature": 0.0, "avg_logprob": -0.18345069885253906, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020315855741500854}, {"id": 422, "seek": 151236, "start": 1514.32, "end": 1519.9599999999998, "text": " Is it like what are the what are the functions that it's giving giving you to consume these", "tokens": [50462, 1119, 309, 411, 437, 366, 264, 437, 366, 264, 6828, 300, 309, 311, 2902, 2902, 291, 281, 14732, 613, 50744], "temperature": 0.0, "avg_logprob": -0.18345069885253906, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020315855741500854}, {"id": 423, "seek": 151236, "start": 1519.9599999999998, "end": 1520.9599999999998, "text": " APIs?", "tokens": [50744, 21445, 30, 50794], "temperature": 0.0, "avg_logprob": -0.18345069885253906, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020315855741500854}, {"id": 424, "seek": 151236, "start": 1520.9599999999998, "end": 1521.9599999999998, "text": " What do those look like?", "tokens": [50794, 708, 360, 729, 574, 411, 30, 50844], "temperature": 0.0, "avg_logprob": -0.18345069885253906, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020315855741500854}, {"id": 425, "seek": 151236, "start": 1521.9599999999998, "end": 1529.9599999999998, "text": " Yeah, it is it is giving you your standard HTTP command requests.", "tokens": [50844, 865, 11, 309, 307, 309, 307, 2902, 291, 428, 3832, 33283, 5622, 12475, 13, 51244], "temperature": 0.0, "avg_logprob": -0.18345069885253906, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020315855741500854}, {"id": 426, "seek": 151236, "start": 1529.9599999999998, "end": 1538.84, "text": " Say you had a create product definition endpoint that so you would give you a create product", "tokens": [51244, 6463, 291, 632, 257, 1884, 1674, 7123, 35795, 300, 370, 291, 576, 976, 291, 257, 1884, 1674, 51688], "temperature": 0.0, "avg_logprob": -0.18345069885253906, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020315855741500854}, {"id": 427, "seek": 153884, "start": 1538.84, "end": 1544.72, "text": " definition, it would take whatever data is necessary for actually creating a product", "tokens": [50364, 7123, 11, 309, 576, 747, 2035, 1412, 307, 4818, 337, 767, 4084, 257, 1674, 50658], "temperature": 0.0, "avg_logprob": -0.17428506138813066, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.5224864482879639}, {"id": 428, "seek": 153884, "start": 1544.72, "end": 1550.12, "text": " name price, something along maybe a description, optional description.", "tokens": [50658, 1315, 3218, 11, 746, 2051, 1310, 257, 3855, 11, 17312, 3855, 13, 50928], "temperature": 0.0, "avg_logprob": -0.17428506138813066, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.5224864482879639}, {"id": 429, "seek": 153884, "start": 1550.12, "end": 1555.6, "text": " It would also provide you all the types for what a product is encoders decoders if you", "tokens": [50928, 467, 576, 611, 2893, 291, 439, 264, 3467, 337, 437, 257, 1674, 307, 2058, 378, 433, 979, 378, 433, 498, 291, 51202], "temperature": 0.0, "avg_logprob": -0.17428506138813066, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.5224864482879639}, {"id": 430, "seek": 153884, "start": 1555.6, "end": 1558.4399999999998, "text": " need to use it somewhere else.", "tokens": [51202, 643, 281, 764, 309, 4079, 1646, 13, 51344], "temperature": 0.0, "avg_logprob": -0.17428506138813066, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.5224864482879639}, {"id": 431, "seek": 153884, "start": 1558.4399999999998, "end": 1563.24, "text": " It will auto wire up all the encoding decoding for you in the request itself.", "tokens": [51344, 467, 486, 8399, 6234, 493, 439, 264, 43430, 979, 8616, 337, 291, 294, 264, 5308, 2564, 13, 51584], "temperature": 0.0, "avg_logprob": -0.17428506138813066, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.5224864482879639}, {"id": 432, "seek": 156324, "start": 1563.24, "end": 1570.24, "text": " But if you need to use it to store in local storage or some something else outside of", "tokens": [50364, 583, 498, 291, 643, 281, 764, 309, 281, 3531, 294, 2654, 6725, 420, 512, 746, 1646, 2380, 295, 50714], "temperature": 0.0, "avg_logprob": -0.1633755577935113, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.1665378212928772}, {"id": 433, "seek": 156324, "start": 1570.24, "end": 1574.72, "text": " the API, all that is provided for you are exposed for you.", "tokens": [50714, 264, 9362, 11, 439, 300, 307, 5649, 337, 291, 366, 9495, 337, 291, 13, 50938], "temperature": 0.0, "avg_logprob": -0.1633755577935113, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.1665378212928772}, {"id": 434, "seek": 156324, "start": 1574.72, "end": 1582.8, "text": " That is a pretty cool detail because we just talked about concurrent task in last episode.", "tokens": [50938, 663, 307, 257, 1238, 1627, 2607, 570, 321, 445, 2825, 466, 37702, 5633, 294, 1036, 3500, 13, 51342], "temperature": 0.0, "avg_logprob": -0.1633755577935113, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.1665378212928772}, {"id": 435, "seek": 156324, "start": 1582.8, "end": 1589.8, "text": " And it defines its own tasks, meaning that you could probably not reuse the HTTP request", "tokens": [51342, 400, 309, 23122, 1080, 1065, 9608, 11, 3620, 300, 291, 727, 1391, 406, 26225, 264, 33283, 5308, 51692], "temperature": 0.0, "avg_logprob": -0.1633755577935113, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.1665378212928772}, {"id": 436, "seek": 156324, "start": 1589.8, "end": 1592.36, "text": " if you wanted to be able to use them concurrently.", "tokens": [51692, 498, 291, 1415, 281, 312, 1075, 281, 764, 552, 37702, 356, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1633755577935113, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.1665378212928772}, {"id": 437, "seek": 159236, "start": 1592.36, "end": 1597.9199999999998, "text": " But if you have the building blocks to do that manually, you could still do it, although", "tokens": [50364, 583, 498, 291, 362, 264, 2390, 8474, 281, 360, 300, 16945, 11, 291, 727, 920, 360, 309, 11, 4878, 50642], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 438, "seek": 159236, "start": 1597.9199999999998, "end": 1602.1999999999998, "text": " you probably still would like to have some kind of cogeneration because it's a lot of", "tokens": [50642, 291, 1391, 920, 576, 411, 281, 362, 512, 733, 295, 598, 30372, 570, 309, 311, 257, 688, 295, 50856], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 439, "seek": 159236, "start": 1602.1999999999998, "end": 1603.1999999999998, "text": " code otherwise.", "tokens": [50856, 3089, 5911, 13, 50906], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 440, "seek": 159236, "start": 1603.1999999999998, "end": 1607.3999999999999, "text": " Don't recall from the episode or from reading the docs.", "tokens": [50906, 1468, 380, 9901, 490, 264, 3500, 420, 490, 3760, 264, 45623, 13, 51116], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 441, "seek": 159236, "start": 1607.3999999999999, "end": 1611.6399999999999, "text": " If you can mix Elm tasks with Elm concurrent tasks.", "tokens": [51116, 759, 291, 393, 2890, 2699, 76, 9608, 365, 2699, 76, 37702, 9608, 13, 51328], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 442, "seek": 159236, "start": 1611.6399999999999, "end": 1613.8799999999999, "text": " No, no, not really.", "tokens": [51328, 883, 11, 572, 11, 406, 534, 13, 51440], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 443, "seek": 159236, "start": 1613.8799999999999, "end": 1615.8, "text": " I mean, it's its own thing.", "tokens": [51440, 286, 914, 11, 309, 311, 1080, 1065, 551, 13, 51536], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 444, "seek": 159236, "start": 1615.8, "end": 1620.9199999999998, "text": " You definitely can't create an Elm concurrent task from an Elm task.", "tokens": [51536, 509, 2138, 393, 380, 1884, 364, 2699, 76, 37702, 5633, 490, 364, 2699, 76, 5633, 13, 51792], "temperature": 0.0, "avg_logprob": -0.22070236206054689, "compression_ratio": 1.6869918699186992, "no_speech_prob": 0.11588918417692184}, {"id": 445, "seek": 162092, "start": 1621.28, "end": 1622.28, "text": " Okay, okay.", "tokens": [50382, 1033, 11, 1392, 13, 50432], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 446, "seek": 162092, "start": 1622.28, "end": 1628.0, "text": " Yeah, so there would be a limitation there because I do I do also expose a task version.", "tokens": [50432, 865, 11, 370, 456, 576, 312, 257, 27432, 456, 570, 286, 360, 286, 360, 611, 19219, 257, 5633, 3037, 13, 50718], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 447, "seek": 162092, "start": 1628.0, "end": 1632.6000000000001, "text": " So there is like a there would be a create product, which we're going to turn a command.", "tokens": [50718, 407, 456, 307, 411, 257, 456, 576, 312, 257, 1884, 1674, 11, 597, 321, 434, 516, 281, 1261, 257, 5622, 13, 50948], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 448, "seek": 162092, "start": 1632.6000000000001, "end": 1637.3600000000001, "text": " And then there's also, I don't remember the naming scheme I use, but it is like create", "tokens": [50948, 400, 550, 456, 311, 611, 11, 286, 500, 380, 1604, 264, 25290, 12232, 286, 764, 11, 457, 309, 307, 411, 1884, 51186], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 449, "seek": 162092, "start": 1637.3600000000001, "end": 1641.52, "text": " product task that would create a task as well.", "tokens": [51186, 1674, 5633, 300, 576, 1884, 257, 5633, 382, 731, 13, 51394], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 450, "seek": 162092, "start": 1641.52, "end": 1645.6000000000001, "text": " Just in case you don't I don't know what what you're going to want to consume.", "tokens": [51394, 1449, 294, 1389, 291, 500, 380, 286, 500, 380, 458, 437, 437, 291, 434, 516, 281, 528, 281, 14732, 13, 51598], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 451, "seek": 162092, "start": 1645.6000000000001, "end": 1646.6000000000001, "text": " Maybe you need both.", "tokens": [51598, 2704, 291, 643, 1293, 13, 51648], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 452, "seek": 162092, "start": 1646.6000000000001, "end": 1647.6000000000001, "text": " Right.", "tokens": [51648, 1779, 13, 51698], "temperature": 0.0, "avg_logprob": -0.20688160982998935, "compression_ratio": 1.7551020408163265, "no_speech_prob": 0.19145779311656952}, {"id": 453, "seek": 164760, "start": 1647.6, "end": 1656.56, "text": " So you have the create article, create article task, or you have the decoder article encoder,", "tokens": [50364, 407, 291, 362, 264, 1884, 7222, 11, 1884, 7222, 5633, 11, 420, 291, 362, 264, 979, 19866, 7222, 2058, 19866, 11, 50812], "temperature": 0.0, "avg_logprob": -0.2924575362094613, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.015177378430962563}, {"id": 454, "seek": 164760, "start": 1656.56, "end": 1661.56, "text": " you have everything is generated and exposed.", "tokens": [50812, 291, 362, 1203, 307, 10833, 293, 9495, 13, 51062], "temperature": 0.0, "avg_logprob": -0.2924575362094613, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.015177378430962563}, {"id": 455, "seek": 164760, "start": 1661.56, "end": 1662.76, "text": " That's nice.", "tokens": [51062, 663, 311, 1481, 13, 51122], "temperature": 0.0, "avg_logprob": -0.2924575362094613, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.015177378430962563}, {"id": 456, "seek": 164760, "start": 1662.76, "end": 1667.04, "text": " The one thing that is not not released yet, I am working on it.", "tokens": [51122, 440, 472, 551, 300, 307, 406, 406, 4736, 1939, 11, 286, 669, 1364, 322, 309, 13, 51336], "temperature": 0.0, "avg_logprob": -0.2924575362094613, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.015177378430962563}, {"id": 457, "seek": 164760, "start": 1667.04, "end": 1673.1599999999999, "text": " I think I'm getting close to something that I like is don't make promises on this podcast.", "tokens": [51336, 286, 519, 286, 478, 1242, 1998, 281, 746, 300, 286, 411, 307, 500, 380, 652, 16403, 322, 341, 7367, 13, 51642], "temperature": 0.0, "avg_logprob": -0.2924575362094613, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.015177378430962563}, {"id": 458, "seek": 164760, "start": 1673.1599999999999, "end": 1674.1599999999999, "text": " Don't.", "tokens": [51642, 1468, 380, 13, 51692], "temperature": 0.0, "avg_logprob": -0.2924575362094613, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.015177378430962563}, {"id": 459, "seek": 167416, "start": 1675.16, "end": 1682.92, "text": " No, no, I am actually decently close, but is is is air handling instead of returning", "tokens": [50414, 883, 11, 572, 11, 286, 669, 767, 979, 2276, 1998, 11, 457, 307, 307, 307, 1988, 13175, 2602, 295, 12678, 50802], "temperature": 0.0, "avg_logprob": -0.24166607856750488, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0005192195530980825}, {"id": 460, "seek": 167416, "start": 1682.92, "end": 1685.76, "text": " just an HTTP error.", "tokens": [50802, 445, 364, 33283, 6713, 13, 50944], "temperature": 0.0, "avg_logprob": -0.24166607856750488, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0005192195530980825}, {"id": 461, "seek": 167416, "start": 1685.76, "end": 1693.68, "text": " The the spec or open API specs can also define what type of error to return what the error", "tokens": [50944, 440, 264, 1608, 420, 1269, 9362, 27911, 393, 611, 6964, 437, 2010, 295, 6713, 281, 2736, 437, 264, 6713, 51340], "temperature": 0.0, "avg_logprob": -0.24166607856750488, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0005192195530980825}, {"id": 462, "seek": 167416, "start": 1693.68, "end": 1700.0400000000002, "text": " messages can look like, which I think is very useful.", "tokens": [51340, 7897, 393, 574, 411, 11, 597, 286, 519, 307, 588, 4420, 13, 51658], "temperature": 0.0, "avg_logprob": -0.24166607856750488, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0005192195530980825}, {"id": 463, "seek": 167416, "start": 1700.0400000000002, "end": 1701.8000000000002, "text": " That's that is one thing that is.", "tokens": [51658, 663, 311, 300, 307, 472, 551, 300, 307, 13, 51746], "temperature": 0.0, "avg_logprob": -0.24166607856750488, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0005192195530980825}, {"id": 464, "seek": 167416, "start": 1701.8000000000002, "end": 1703.96, "text": " Yeah, that's really nice to have.", "tokens": [51746, 865, 11, 300, 311, 534, 1481, 281, 362, 13, 51854], "temperature": 0.0, "avg_logprob": -0.24166607856750488, "compression_ratio": 1.5314009661835748, "no_speech_prob": 0.0005192195530980825}, {"id": 465, "seek": 170396, "start": 1703.96, "end": 1711.72, "text": " So I'm trying to come up with a better air type that is specific to your API so that", "tokens": [50364, 407, 286, 478, 1382, 281, 808, 493, 365, 257, 1101, 1988, 2010, 300, 307, 2685, 281, 428, 9362, 370, 300, 50752], "temperature": 0.0, "avg_logprob": -0.17289512715441116, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0007094338070601225}, {"id": 466, "seek": 170396, "start": 1711.72, "end": 1714.24, "text": " you get this custom air type.", "tokens": [50752, 291, 483, 341, 2375, 1988, 2010, 13, 50878], "temperature": 0.0, "avg_logprob": -0.17289512715441116, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0007094338070601225}, {"id": 467, "seek": 170396, "start": 1714.24, "end": 1719.24, "text": " And then if it can't if for whatever reason you're back and returns something invalid", "tokens": [50878, 400, 550, 498, 309, 393, 380, 498, 337, 2035, 1778, 291, 434, 646, 293, 11247, 746, 34702, 51128], "temperature": 0.0, "avg_logprob": -0.17289512715441116, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0007094338070601225}, {"id": 468, "seek": 170396, "start": 1719.24, "end": 1727.2, "text": " or some other network traffic error occurs that you can fall back to to a regular error", "tokens": [51128, 420, 512, 661, 3209, 6419, 6713, 11843, 300, 291, 393, 2100, 646, 281, 281, 257, 3890, 6713, 51526], "temperature": 0.0, "avg_logprob": -0.17289512715441116, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0007094338070601225}, {"id": 469, "seek": 170396, "start": 1727.2, "end": 1728.72, "text": " of some kind.", "tokens": [51526, 295, 512, 733, 13, 51602], "temperature": 0.0, "avg_logprob": -0.17289512715441116, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0007094338070601225}, {"id": 470, "seek": 170396, "start": 1728.72, "end": 1732.96, "text": " So that's that is the my experiments with that are a little weak.", "tokens": [51602, 407, 300, 311, 300, 307, 264, 452, 12050, 365, 300, 366, 257, 707, 5336, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17289512715441116, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0007094338070601225}, {"id": 471, "seek": 173296, "start": 1732.96, "end": 1740.08, "text": " I've I've been using the real world app for a lot of my local testing and local experimentation.", "tokens": [50364, 286, 600, 286, 600, 668, 1228, 264, 957, 1002, 724, 337, 257, 688, 295, 452, 2654, 4997, 293, 2654, 37142, 13, 50720], "temperature": 0.0, "avg_logprob": -0.16430626524255632, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.001410098746418953}, {"id": 472, "seek": 173296, "start": 1740.08, "end": 1746.6000000000001, "text": " And that only defines two custom errors, which are there's a 401 on authorized error.", "tokens": [50720, 400, 300, 787, 23122, 732, 2375, 13603, 11, 597, 366, 456, 311, 257, 37510, 322, 28312, 6713, 13, 51046], "temperature": 0.0, "avg_logprob": -0.16430626524255632, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.001410098746418953}, {"id": 473, "seek": 173296, "start": 1746.6000000000001, "end": 1751.96, "text": " And there is another one called generic error in the spec I'm looking at, which is the error", "tokens": [51046, 400, 456, 307, 1071, 472, 1219, 19577, 6713, 294, 264, 1608, 286, 478, 1237, 412, 11, 597, 307, 264, 6713, 51314], "temperature": 0.0, "avg_logprob": -0.16430626524255632, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.001410098746418953}, {"id": 474, "seek": 173296, "start": 1751.96, "end": 1753.6000000000001, "text": " for everything else.", "tokens": [51314, 337, 1203, 1646, 13, 51396], "temperature": 0.0, "avg_logprob": -0.16430626524255632, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.001410098746418953}, {"id": 475, "seek": 173296, "start": 1753.6000000000001, "end": 1759.4, "text": " Yeah, so and I've I've also learned from this that there are actually multiple implementations", "tokens": [51396, 865, 11, 370, 293, 286, 600, 286, 600, 611, 3264, 490, 341, 300, 456, 366, 767, 3866, 4445, 763, 51686], "temperature": 0.0, "avg_logprob": -0.16430626524255632, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.001410098746418953}, {"id": 476, "seek": 175940, "start": 1759.4, "end": 1765.1200000000001, "text": " of the real world backend in terms of what that spec looks like, which means sometimes", "tokens": [50364, 295, 264, 957, 1002, 38087, 294, 2115, 295, 437, 300, 1608, 1542, 411, 11, 597, 1355, 2171, 50650], "temperature": 0.0, "avg_logprob": -0.18873656879771838, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.006096736062318087}, {"id": 477, "seek": 175940, "start": 1765.1200000000001, "end": 1769.3600000000001, "text": " my test work or sometimes my experiments work with the backend that I'm hitting and sometimes", "tokens": [50650, 452, 1500, 589, 420, 2171, 452, 12050, 589, 365, 264, 38087, 300, 286, 478, 8850, 293, 2171, 50862], "temperature": 0.0, "avg_logprob": -0.18873656879771838, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.006096736062318087}, {"id": 478, "seek": 175940, "start": 1769.3600000000001, "end": 1770.8600000000001, "text": " they don't.", "tokens": [50862, 436, 500, 380, 13, 50937], "temperature": 0.0, "avg_logprob": -0.18873656879771838, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.006096736062318087}, {"id": 479, "seek": 175940, "start": 1770.8600000000001, "end": 1776.96, "text": " So that's that's my latest or my current current work on the open API stuff.", "tokens": [50937, 407, 300, 311, 300, 311, 452, 6792, 420, 452, 2190, 2190, 589, 322, 264, 1269, 9362, 1507, 13, 51242], "temperature": 0.0, "avg_logprob": -0.18873656879771838, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.006096736062318087}, {"id": 480, "seek": 175940, "start": 1776.96, "end": 1785.44, "text": " Yeah, it it's very cool how if a if an endpoint requires an authorization token or a particular", "tokens": [51242, 865, 11, 309, 309, 311, 588, 1627, 577, 498, 257, 498, 364, 35795, 7029, 364, 33697, 14862, 420, 257, 1729, 51666], "temperature": 0.0, "avg_logprob": -0.18873656879771838, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.006096736062318087}, {"id": 481, "seek": 178544, "start": 1785.44, "end": 1791.8400000000001, "text": " query param with a given name, the code that elm open API generates shows reflects that", "tokens": [50364, 14581, 6220, 365, 257, 2212, 1315, 11, 264, 3089, 300, 806, 76, 1269, 9362, 23815, 3110, 18926, 300, 50684], "temperature": 0.0, "avg_logprob": -0.1655752735753213, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.6612383127212524}, {"id": 482, "seek": 178544, "start": 1791.8400000000001, "end": 1801.1200000000001, "text": " so it gives you the record argument to call the function has the authorization token and", "tokens": [50684, 370, 309, 2709, 291, 264, 2136, 6770, 281, 818, 264, 2445, 575, 264, 33697, 14862, 293, 51148], "temperature": 0.0, "avg_logprob": -0.1655752735753213, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.6612383127212524}, {"id": 483, "seek": 178544, "start": 1801.1200000000001, "end": 1805.4, "text": " the query programs that you need to pass in of the correct types.", "tokens": [51148, 264, 14581, 4268, 300, 291, 643, 281, 1320, 294, 295, 264, 3006, 3467, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1655752735753213, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.6612383127212524}, {"id": 484, "seek": 178544, "start": 1805.4, "end": 1807.76, "text": " One thing I can't help but think about.", "tokens": [51362, 1485, 551, 286, 393, 380, 854, 457, 519, 466, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1655752735753213, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.6612383127212524}, {"id": 485, "seek": 178544, "start": 1807.76, "end": 1812.68, "text": " And if you're playing Elm Radio bingo, now's the time to get out your bingo cards.", "tokens": [51480, 400, 498, 291, 434, 2433, 2699, 76, 17296, 272, 18459, 11, 586, 311, 264, 565, 281, 483, 484, 428, 272, 18459, 5632, 13, 51726], "temperature": 0.0, "avg_logprob": -0.1655752735753213, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.6612383127212524}, {"id": 486, "seek": 181268, "start": 1812.68, "end": 1816.4, "text": " What if you wanted to create some sort of opaque type, let's say,", "tokens": [50364, 708, 498, 291, 1415, 281, 1884, 512, 1333, 295, 42687, 2010, 11, 718, 311, 584, 11, 50550], "temperature": 0.2, "avg_logprob": -0.41227793974034926, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.27463287115097046}, {"id": 487, "seek": 181268, "start": 1816.4, "end": 1821.5600000000002, "text": " Ding, ding, ding, ding, ding, please.", "tokens": [50550, 20558, 11, 21211, 11, 21211, 11, 21211, 11, 21211, 11, 1767, 13, 50808], "temperature": 0.2, "avg_logprob": -0.41227793974034926, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.27463287115097046}, {"id": 488, "seek": 181268, "start": 1821.5600000000002, "end": 1827.88, "text": " I think that was going to be my next question as well.", "tokens": [50808, 286, 519, 300, 390, 516, 281, 312, 452, 958, 1168, 382, 731, 13, 51124], "temperature": 0.2, "avg_logprob": -0.41227793974034926, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.27463287115097046}, {"id": 489, "seek": 181268, "start": 1827.88, "end": 1828.88, "text": " What a surprise.", "tokens": [51124, 708, 257, 6365, 13, 51174], "temperature": 0.2, "avg_logprob": -0.41227793974034926, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.27463287115097046}, {"id": 490, "seek": 181268, "start": 1828.88, "end": 1836.8400000000001, "text": " Yeah, so like with with Done Kerns on GraphQL, for example, you know, I'm I'm a big fan of", "tokens": [51174, 865, 11, 370, 411, 365, 365, 1468, 68, 40224, 82, 322, 21884, 13695, 11, 337, 1365, 11, 291, 458, 11, 286, 478, 286, 478, 257, 955, 3429, 295, 51572], "temperature": 0.2, "avg_logprob": -0.41227793974034926, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.27463287115097046}, {"id": 491, "seek": 183684, "start": 1836.84, "end": 1843.52, "text": " using opaque types as much as possible, both decoding into and using, you know, custom", "tokens": [50364, 1228, 42687, 3467, 382, 709, 382, 1944, 11, 1293, 979, 8616, 666, 293, 1228, 11, 291, 458, 11, 2375, 50698], "temperature": 0.0, "avg_logprob": -0.1004386413388136, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.6506532430648804}, {"id": 492, "seek": 183684, "start": 1843.52, "end": 1848.8799999999999, "text": " input types and that sort of thing, like for an authorization token, for example, I might", "tokens": [50698, 4846, 3467, 293, 300, 1333, 295, 551, 11, 411, 337, 364, 33697, 14862, 11, 337, 1365, 11, 286, 1062, 50966], "temperature": 0.0, "avg_logprob": -0.1004386413388136, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.6506532430648804}, {"id": 493, "seek": 183684, "start": 1848.8799999999999, "end": 1855.9599999999998, "text": " want to have a wrapped opaque type that is actually an authorization token and maybe", "tokens": [50966, 528, 281, 362, 257, 14226, 42687, 2010, 300, 307, 767, 364, 33697, 14862, 293, 1310, 51320], "temperature": 0.0, "avg_logprob": -0.1004386413388136, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.6506532430648804}, {"id": 494, "seek": 183684, "start": 1855.9599999999998, "end": 1862.4399999999998, "text": " that I can't accidentally pass a string oops, that I mixed up the names of the authorization", "tokens": [51320, 300, 286, 393, 380, 15715, 1320, 257, 6798, 34166, 11, 300, 286, 7467, 493, 264, 5288, 295, 264, 33697, 51644], "temperature": 0.0, "avg_logprob": -0.1004386413388136, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.6506532430648804}, {"id": 495, "seek": 186244, "start": 1862.44, "end": 1868.52, "text": " token and the username and now that's getting logged to, you know, the console or something", "tokens": [50364, 14862, 293, 264, 30351, 293, 586, 300, 311, 1242, 27231, 281, 11, 291, 458, 11, 264, 11076, 420, 746, 50668], "temperature": 0.0, "avg_logprob": -0.13555225816745203, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.19671232998371124}, {"id": 496, "seek": 186244, "start": 1868.52, "end": 1870.1200000000001, "text": " like that, right?", "tokens": [50668, 411, 300, 11, 558, 30, 50748], "temperature": 0.0, "avg_logprob": -0.13555225816745203, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.19671232998371124}, {"id": 497, "seek": 186244, "start": 1870.1200000000001, "end": 1876.96, "text": " And same with like getting back data, things like enums or things like a non negative integer,", "tokens": [50748, 400, 912, 365, 411, 1242, 646, 1412, 11, 721, 411, 465, 8099, 420, 721, 411, 257, 2107, 3671, 24922, 11, 51090], "temperature": 0.0, "avg_logprob": -0.13555225816745203, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.19671232998371124}, {"id": 498, "seek": 186244, "start": 1876.96, "end": 1882.44, "text": " like a price, I might want to have an opaque type to make sure I don't cross wires for", "tokens": [51090, 411, 257, 3218, 11, 286, 1062, 528, 281, 362, 364, 42687, 2010, 281, 652, 988, 286, 500, 380, 3278, 15537, 337, 51364], "temperature": 0.0, "avg_logprob": -0.13555225816745203, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.19671232998371124}, {"id": 499, "seek": 186244, "start": 1882.44, "end": 1888.3200000000002, "text": " these different result types that I'm getting back and have certain guarantees that I that", "tokens": [51364, 613, 819, 1874, 3467, 300, 286, 478, 1242, 646, 293, 362, 1629, 32567, 300, 286, 300, 51658], "temperature": 0.0, "avg_logprob": -0.13555225816745203, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.19671232998371124}, {"id": 500, "seek": 186244, "start": 1888.3200000000002, "end": 1891.0, "text": " I know invariance about those values.", "tokens": [51658, 286, 458, 33270, 719, 466, 729, 4190, 13, 51792], "temperature": 0.0, "avg_logprob": -0.13555225816745203, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.19671232998371124}, {"id": 501, "seek": 189100, "start": 1891.88, "end": 1893.28, "text": " Has that been on your radar at all?", "tokens": [50408, 8646, 300, 668, 322, 428, 16544, 412, 439, 30, 50478], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 502, "seek": 189100, "start": 1893.28, "end": 1894.28, "text": " Have you thought about that?", "tokens": [50478, 3560, 291, 1194, 466, 300, 30, 50528], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 503, "seek": 189100, "start": 1894.28, "end": 1896.28, "text": " Is that compatible with this approach?", "tokens": [50528, 1119, 300, 18218, 365, 341, 3109, 30, 50628], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 504, "seek": 189100, "start": 1896.28, "end": 1898.28, "text": " Yes, yes, yes.", "tokens": [50628, 1079, 11, 2086, 11, 2086, 13, 50728], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 505, "seek": 189100, "start": 1898.28, "end": 1901.28, "text": " A thousand times yes.", "tokens": [50728, 316, 4714, 1413, 2086, 13, 50878], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 506, "seek": 189100, "start": 1901.28, "end": 1907.48, "text": " I'm glad you didn't put a no in there because I didn't know to which question it would", "tokens": [50878, 286, 478, 5404, 291, 994, 380, 829, 257, 572, 294, 456, 570, 286, 994, 380, 458, 281, 597, 1168, 309, 576, 51188], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 507, "seek": 189100, "start": 1907.48, "end": 1909.04, "text": " have been an answer to.", "tokens": [51188, 362, 668, 364, 1867, 281, 13, 51266], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 508, "seek": 189100, "start": 1909.04, "end": 1914.68, "text": " I honestly don't know which one I would have lined up with.", "tokens": [51266, 286, 6095, 500, 380, 458, 597, 472, 286, 576, 362, 17189, 493, 365, 13, 51548], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 509, "seek": 189100, "start": 1914.68, "end": 1920.4, "text": " So the the the tokens that had not specifically crossed my mind, like that specifically, I", "tokens": [51548, 407, 264, 264, 264, 22667, 300, 632, 406, 4682, 14622, 452, 1575, 11, 411, 300, 4682, 11, 286, 51834], "temperature": 0.0, "avg_logprob": -0.33889879499162945, "compression_ratio": 1.675, "no_speech_prob": 0.03307420015335083}, {"id": 510, "seek": 192040, "start": 1920.4, "end": 1922.44, "text": " do really like that, though.", "tokens": [50364, 360, 534, 411, 300, 11, 1673, 13, 50466], "temperature": 0.0, "avg_logprob": -0.17770731449127197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.004069003742188215}, {"id": 511, "seek": 192040, "start": 1922.44, "end": 1926.76, "text": " I don't know if I could make a special exception just for that or not.", "tokens": [50466, 286, 500, 380, 458, 498, 286, 727, 652, 257, 2121, 11183, 445, 337, 300, 420, 406, 13, 50682], "temperature": 0.0, "avg_logprob": -0.17770731449127197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.004069003742188215}, {"id": 512, "seek": 192040, "start": 1926.76, "end": 1928.64, "text": " I'm tempted to.", "tokens": [50682, 286, 478, 29941, 281, 13, 50776], "temperature": 0.0, "avg_logprob": -0.17770731449127197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.004069003742188215}, {"id": 513, "seek": 192040, "start": 1928.64, "end": 1929.64, "text": " I do.", "tokens": [50776, 286, 360, 13, 50826], "temperature": 0.0, "avg_logprob": -0.17770731449127197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.004069003742188215}, {"id": 514, "seek": 192040, "start": 1929.64, "end": 1937.24, "text": " So my thinking has been that in this kind of goes with the form generation as well, because", "tokens": [50826, 407, 452, 1953, 575, 668, 300, 294, 341, 733, 295, 1709, 365, 264, 1254, 5125, 382, 731, 11, 570, 51206], "temperature": 0.0, "avg_logprob": -0.17770731449127197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.004069003742188215}, {"id": 515, "seek": 192040, "start": 1937.24, "end": 1941.68, "text": " I think it kind of benefits both sides is the schemas.", "tokens": [51206, 286, 519, 309, 733, 295, 5311, 1293, 4881, 307, 264, 22627, 296, 13, 51428], "temperature": 0.0, "avg_logprob": -0.17770731449127197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.004069003742188215}, {"id": 516, "seek": 192040, "start": 1941.68, "end": 1946.64, "text": " All these schemas have a way to define your own properties on the schema.", "tokens": [51428, 1057, 613, 22627, 296, 362, 257, 636, 281, 6964, 428, 1065, 7221, 322, 264, 34078, 13, 51676], "temperature": 0.0, "avg_logprob": -0.17770731449127197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.004069003742188215}, {"id": 517, "seek": 194664, "start": 1946.88, "end": 1953.72, "text": " You usually form like X dash custom property name and that can do whatever you want.", "tokens": [50376, 509, 2673, 1254, 411, 1783, 8240, 2375, 4707, 1315, 293, 300, 393, 360, 2035, 291, 528, 13, 50718], "temperature": 0.0, "avg_logprob": -0.27178561223017705, "compression_ratio": 1.5876288659793814, "no_speech_prob": 0.02516692318022251}, {"id": 518, "seek": 194664, "start": 1953.72, "end": 1956.5200000000002, "text": " Essentially, it can mean whatever you want.", "tokens": [50718, 23596, 11, 309, 393, 914, 2035, 291, 528, 13, 50858], "temperature": 0.0, "avg_logprob": -0.27178561223017705, "compression_ratio": 1.5876288659793814, "no_speech_prob": 0.02516692318022251}, {"id": 519, "seek": 194664, "start": 1956.5200000000002, "end": 1965.44, "text": " And I think there might be a way to use that to define custom ways to handle encoding and", "tokens": [50858, 400, 286, 519, 456, 1062, 312, 257, 636, 281, 764, 300, 281, 6964, 2375, 2098, 281, 4813, 43430, 293, 51304], "temperature": 0.0, "avg_logprob": -0.27178561223017705, "compression_ratio": 1.5876288659793814, "no_speech_prob": 0.02516692318022251}, {"id": 520, "seek": 194664, "start": 1965.44, "end": 1970.68, "text": " decoding and maybe form, form handling as well.", "tokens": [51304, 979, 8616, 293, 1310, 1254, 11, 1254, 13175, 382, 731, 13, 51566], "temperature": 0.0, "avg_logprob": -0.27178561223017705, "compression_ratio": 1.5876288659793814, "no_speech_prob": 0.02516692318022251}, {"id": 521, "seek": 194664, "start": 1970.68, "end": 1975.5200000000002, "text": " So like an example, there would be units.", "tokens": [51566, 407, 411, 364, 1365, 11, 456, 576, 312, 6815, 13, 51808], "temperature": 0.0, "avg_logprob": -0.27178561223017705, "compression_ratio": 1.5876288659793814, "no_speech_prob": 0.02516692318022251}, {"id": 522, "seek": 197664, "start": 1976.72, "end": 1977.64, "text": " No one's familiar.", "tokens": [50368, 883, 472, 311, 4963, 13, 50414], "temperature": 0.0, "avg_logprob": -0.20302198714568837, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.01911327801644802}, {"id": 523, "seek": 197664, "start": 1977.64, "end": 1981.88, "text": " There there is a great package from Ian McKenzie called Elm units.", "tokens": [50414, 821, 456, 307, 257, 869, 7372, 490, 19595, 21765, 32203, 1219, 2699, 76, 6815, 13, 50626], "temperature": 0.0, "avg_logprob": -0.20302198714568837, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.01911327801644802}, {"id": 524, "seek": 197664, "start": 1981.88, "end": 1988.76, "text": " That is really fantastic for handling anything with any type of unit value, like feet to", "tokens": [50626, 663, 307, 534, 5456, 337, 13175, 1340, 365, 604, 2010, 295, 4985, 2158, 11, 411, 3521, 281, 50970], "temperature": 0.0, "avg_logprob": -0.20302198714568837, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.01911327801644802}, {"id": 525, "seek": 197664, "start": 1988.76, "end": 1991.5200000000002, "text": " meters, you don't want to mess that up.", "tokens": [50970, 8146, 11, 291, 500, 380, 528, 281, 2082, 300, 493, 13, 51108], "temperature": 0.0, "avg_logprob": -0.20302198714568837, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.01911327801644802}, {"id": 526, "seek": 197664, "start": 1991.5200000000002, "end": 1994.68, "text": " So why not use a package that just does it for you?", "tokens": [51108, 407, 983, 406, 764, 257, 7372, 300, 445, 775, 309, 337, 291, 30, 51266], "temperature": 0.0, "avg_logprob": -0.20302198714568837, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.01911327801644802}, {"id": 527, "seek": 197664, "start": 1994.68, "end": 1999.8400000000001, "text": " Similarly, if you're dealing with endpoints that are to have that kind of data similar", "tokens": [51266, 13157, 11, 498, 291, 434, 6260, 365, 917, 20552, 300, 366, 281, 362, 300, 733, 295, 1412, 2531, 51524], "temperature": 0.0, "avg_logprob": -0.20302198714568837, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.01911327801644802}, {"id": 528, "seek": 197664, "start": 1999.8400000000001, "end": 2003.76, "text": " to a token, like I don't want to pass in the wrong string for a token, I don't want to", "tokens": [51524, 281, 257, 14862, 11, 411, 286, 500, 380, 528, 281, 1320, 294, 264, 2085, 6798, 337, 257, 14862, 11, 286, 500, 380, 528, 281, 51720], "temperature": 0.0, "avg_logprob": -0.20302198714568837, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.01911327801644802}, {"id": 529, "seek": 200376, "start": 2003.76, "end": 2007.92, "text": " pass in feet when I want to use meters in my endpoint.", "tokens": [50364, 1320, 294, 3521, 562, 286, 528, 281, 764, 8146, 294, 452, 35795, 13, 50572], "temperature": 0.0, "avg_logprob": -0.15734669099371118, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.00186735310126096}, {"id": 530, "seek": 200376, "start": 2007.92, "end": 2012.2, "text": " That would that could be catastrophic in some cases.", "tokens": [50572, 663, 576, 300, 727, 312, 34915, 294, 512, 3331, 13, 50786], "temperature": 0.0, "avg_logprob": -0.15734669099371118, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.00186735310126096}, {"id": 531, "seek": 200376, "start": 2012.2, "end": 2015.24, "text": " There are spaceships that have blown up because of that.", "tokens": [50786, 821, 366, 7673, 7640, 300, 362, 16479, 493, 570, 295, 300, 13, 50938], "temperature": 0.0, "avg_logprob": -0.15734669099371118, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.00186735310126096}, {"id": 532, "seek": 200376, "start": 2015.24, "end": 2021.92, "text": " So I haven't had the chance to experiment yet in code itself, but I've been thinking", "tokens": [50938, 407, 286, 2378, 380, 632, 264, 2931, 281, 5120, 1939, 294, 3089, 2564, 11, 457, 286, 600, 668, 1953, 51272], "temperature": 0.0, "avg_logprob": -0.15734669099371118, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.00186735310126096}, {"id": 533, "seek": 200376, "start": 2021.92, "end": 2029.44, "text": " for months now that there is probably a way to handle automatically pulling in a package.", "tokens": [51272, 337, 2493, 586, 300, 456, 307, 1391, 257, 636, 281, 4813, 6772, 8407, 294, 257, 7372, 13, 51648], "temperature": 0.0, "avg_logprob": -0.15734669099371118, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.00186735310126096}, {"id": 534, "seek": 202944, "start": 2029.44, "end": 2036.68, "text": " So a way to find I want to use this package for this type of data and have it auto encode,", "tokens": [50364, 407, 257, 636, 281, 915, 286, 528, 281, 764, 341, 7372, 337, 341, 2010, 295, 1412, 293, 362, 309, 8399, 2058, 1429, 11, 50726], "temperature": 0.0, "avg_logprob": -0.22040863374693204, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.005384029354900122}, {"id": 535, "seek": 202944, "start": 2036.68, "end": 2042.0800000000002, "text": " decode that type of data, wrap that type of data in whatever wrapper it needs.", "tokens": [50726, 979, 1429, 300, 2010, 295, 1412, 11, 7019, 300, 2010, 295, 1412, 294, 2035, 46906, 309, 2203, 13, 50996], "temperature": 0.0, "avg_logprob": -0.22040863374693204, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.005384029354900122}, {"id": 536, "seek": 202944, "start": 2042.0800000000002, "end": 2047.28, "text": " Yeah, I really, I think there's a lot of potential in that area, a lot of potential.", "tokens": [50996, 865, 11, 286, 534, 11, 286, 519, 456, 311, 257, 688, 295, 3995, 294, 300, 1859, 11, 257, 688, 295, 3995, 13, 51256], "temperature": 0.0, "avg_logprob": -0.22040863374693204, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.005384029354900122}, {"id": 537, "seek": 202944, "start": 2047.28, "end": 2048.92, "text": " That would be, that would be amazing.", "tokens": [51256, 663, 576, 312, 11, 300, 576, 312, 2243, 13, 51338], "temperature": 0.0, "avg_logprob": -0.22040863374693204, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.005384029354900122}, {"id": 538, "seek": 202944, "start": 2048.92, "end": 2049.92, "text": " Yeah.", "tokens": [51338, 865, 13, 51388], "temperature": 0.0, "avg_logprob": -0.22040863374693204, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.005384029354900122}, {"id": 539, "seek": 202944, "start": 2049.92, "end": 2054.8, "text": " So like in the, in the case of Dillon Kern's Elm GraphQL, what I did is I introduced, you", "tokens": [51388, 407, 411, 294, 264, 11, 294, 264, 1389, 295, 28160, 40224, 311, 2699, 76, 21884, 13695, 11, 437, 286, 630, 307, 286, 7268, 11, 291, 51632], "temperature": 0.0, "avg_logprob": -0.22040863374693204, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.005384029354900122}, {"id": 540, "seek": 205480, "start": 2054.8, "end": 2063.1600000000003, "text": " know, this custom scalar codecs file where you, for each custom scalar in your schema,", "tokens": [50364, 458, 11, 341, 2375, 39684, 3089, 14368, 3991, 689, 291, 11, 337, 1184, 2375, 39684, 294, 428, 34078, 11, 50782], "temperature": 0.0, "avg_logprob": -0.12591462195674075, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.13287389278411865}, {"id": 541, "seek": 205480, "start": 2063.1600000000003, "end": 2069.2000000000003, "text": " you define a codec encoder and decoder pair.", "tokens": [50782, 291, 6964, 257, 3089, 66, 2058, 19866, 293, 979, 19866, 6119, 13, 51084], "temperature": 0.0, "avg_logprob": -0.12591462195674075, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.13287389278411865}, {"id": 542, "seek": 205480, "start": 2069.2000000000003, "end": 2076.4, "text": " And now the nice thing about the GraphQL spec in that regard is you do have this concept", "tokens": [51084, 400, 586, 264, 1481, 551, 466, 264, 21884, 13695, 1608, 294, 300, 3843, 307, 291, 360, 362, 341, 3410, 51444], "temperature": 0.0, "avg_logprob": -0.12591462195674075, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.13287389278411865}, {"id": 543, "seek": 205480, "start": 2076.4, "end": 2084.36, "text": " of a nominal type, a type which is not just a set of fields of particular types.", "tokens": [51444, 295, 257, 41641, 2010, 11, 257, 2010, 597, 307, 406, 445, 257, 992, 295, 7909, 295, 1729, 3467, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12591462195674075, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.13287389278411865}, {"id": 544, "seek": 208436, "start": 2084.4, "end": 2093.7200000000003, "text": " It's like, this is a, you know, a unit of, of feet or of meters, like you can specify", "tokens": [50366, 467, 311, 411, 11, 341, 307, 257, 11, 291, 458, 11, 257, 4985, 295, 11, 295, 3521, 420, 295, 8146, 11, 411, 291, 393, 16500, 50832], "temperature": 0.0, "avg_logprob": -0.16388705924705224, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03512076288461685}, {"id": 545, "seek": 208436, "start": 2093.7200000000003, "end": 2099.1600000000003, "text": " custom scalars and give it a name and say, well, the underlying representation might", "tokens": [50832, 2375, 15664, 685, 293, 976, 309, 257, 1315, 293, 584, 11, 731, 11, 264, 14217, 10290, 1062, 51104], "temperature": 0.0, "avg_logprob": -0.16388705924705224, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03512076288461685}, {"id": 546, "seek": 208436, "start": 2099.1600000000003, "end": 2103.6, "text": " be exactly the same as this other thing, but it's not like a, if it quacks like a duck,", "tokens": [51104, 312, 2293, 264, 912, 382, 341, 661, 551, 11, 457, 309, 311, 406, 411, 257, 11, 498, 309, 421, 7424, 411, 257, 12482, 11, 51326], "temperature": 0.0, "avg_logprob": -0.16388705924705224, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03512076288461685}, {"id": 547, "seek": 208436, "start": 2103.6, "end": 2110.76, "text": " it's like this is units and meters, not to be confused with some other unit that we use,", "tokens": [51326, 309, 311, 411, 341, 307, 6815, 293, 8146, 11, 406, 281, 312, 9019, 365, 512, 661, 4985, 300, 321, 764, 11, 51684], "temperature": 0.0, "avg_logprob": -0.16388705924705224, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03512076288461685}, {"id": 548, "seek": 208436, "start": 2110.76, "end": 2114.1600000000003, "text": " unit and millimeters or something, you know.", "tokens": [51684, 4985, 293, 24388, 420, 746, 11, 291, 458, 13, 51854], "temperature": 0.0, "avg_logprob": -0.16388705924705224, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.03512076288461685}, {"id": 549, "seek": 211416, "start": 2114.16, "end": 2120.68, "text": " So the GraphQL schema definition language has this notion of users being able to define,", "tokens": [50364, 407, 264, 21884, 13695, 34078, 7123, 2856, 575, 341, 10710, 295, 5022, 885, 1075, 281, 6964, 11, 50690], "temperature": 0.0, "avg_logprob": -0.1307376119825575, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.00010889044642681256}, {"id": 550, "seek": 211416, "start": 2120.68, "end": 2123.8399999999997, "text": " to define their own custom scalars.", "tokens": [50690, 281, 6964, 641, 1065, 2375, 15664, 685, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1307376119825575, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.00010889044642681256}, {"id": 551, "seek": 211416, "start": 2123.8399999999997, "end": 2129.96, "text": " And then Dillon Kern's Elm GraphQL just provides a way to define encoder, decoder pairs for", "tokens": [50848, 400, 550, 28160, 40224, 311, 2699, 76, 21884, 13695, 445, 6417, 257, 636, 281, 6964, 2058, 19866, 11, 979, 19866, 15494, 337, 51154], "temperature": 0.0, "avg_logprob": -0.1307376119825575, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.00010889044642681256}, {"id": 552, "seek": 211416, "start": 2129.96, "end": 2133.04, "text": " each of those custom scalars in the schema.", "tokens": [51154, 1184, 295, 729, 2375, 15664, 685, 294, 264, 34078, 13, 51308], "temperature": 0.0, "avg_logprob": -0.1307376119825575, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.00010889044642681256}, {"id": 553, "seek": 211416, "start": 2133.04, "end": 2140.72, "text": " Now with OpenAPI and JSON schema, I wonder like, is it more duck typed by its nature", "tokens": [51308, 823, 365, 7238, 4715, 40, 293, 31828, 34078, 11, 286, 2441, 411, 11, 307, 309, 544, 12482, 33941, 538, 1080, 3687, 51692], "temperature": 0.0, "avg_logprob": -0.1307376119825575, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.00010889044642681256}, {"id": 554, "seek": 214072, "start": 2140.7599999999998, "end": 2147.6, "text": " in the sense of it's just saying, well, this is an integer and its minimum value is this", "tokens": [50366, 294, 264, 2020, 295, 309, 311, 445, 1566, 11, 731, 11, 341, 307, 364, 24922, 293, 1080, 7285, 2158, 307, 341, 50708], "temperature": 0.0, "avg_logprob": -0.16225001925513857, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.01168497558683157}, {"id": 555, "seek": 214072, "start": 2147.6, "end": 2149.6, "text": " and it doesn't have a maximum value.", "tokens": [50708, 293, 309, 1177, 380, 362, 257, 6674, 2158, 13, 50808], "temperature": 0.0, "avg_logprob": -0.16225001925513857, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.01168497558683157}, {"id": 556, "seek": 214072, "start": 2149.6, "end": 2156.3599999999997, "text": " And you're not really naming things in one central location where you give something", "tokens": [50808, 400, 291, 434, 406, 534, 25290, 721, 294, 472, 5777, 4914, 689, 291, 976, 746, 51146], "temperature": 0.0, "avg_logprob": -0.16225001925513857, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.01168497558683157}, {"id": 557, "seek": 214072, "start": 2156.3599999999997, "end": 2160.64, "text": " a name and then you reference that named thing you defined at the top.", "tokens": [51146, 257, 1315, 293, 550, 291, 6408, 300, 4926, 551, 291, 7642, 412, 264, 1192, 13, 51360], "temperature": 0.0, "avg_logprob": -0.16225001925513857, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.01168497558683157}, {"id": 558, "seek": 214072, "start": 2161.12, "end": 2163.7999999999997, "text": " I will say it's limited by JSON a little bit.", "tokens": [51384, 286, 486, 584, 309, 311, 5567, 538, 31828, 257, 707, 857, 13, 51518], "temperature": 0.0, "avg_logprob": -0.16225001925513857, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.01168497558683157}, {"id": 559, "seek": 216380, "start": 2163.92, "end": 2173.76, "text": " So the defaults are things like a generic number or a string, an array, they're very limited", "tokens": [50370, 407, 264, 7576, 82, 366, 721, 411, 257, 19577, 1230, 420, 257, 6798, 11, 364, 10225, 11, 436, 434, 588, 5567, 50862], "temperature": 0.0, "avg_logprob": -0.25069908979462413, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.00781333725899458}, {"id": 560, "seek": 216380, "start": 2173.76, "end": 2175.2000000000003, "text": " in that scope initially.", "tokens": [50862, 294, 300, 11923, 9105, 13, 50934], "temperature": 0.0, "avg_logprob": -0.25069908979462413, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.00781333725899458}, {"id": 561, "seek": 216380, "start": 2175.6800000000003, "end": 2184.1200000000003, "text": " There is a portion of the schema that is just for defining, called objects for, since it", "tokens": [50958, 821, 307, 257, 8044, 295, 264, 34078, 300, 307, 445, 337, 17827, 11, 1219, 6565, 337, 11, 1670, 309, 51380], "temperature": 0.0, "avg_logprob": -0.25069908979462413, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.00781333725899458}, {"id": 562, "seek": 216380, "start": 2184.1200000000003, "end": 2186.32, "text": " kind of fits with JSON a little bit.", "tokens": [51380, 733, 295, 9001, 365, 31828, 257, 707, 857, 13, 51490], "temperature": 0.0, "avg_logprob": -0.25069908979462413, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.00781333725899458}, {"id": 563, "seek": 216380, "start": 2186.8, "end": 2192.2000000000003, "text": " So you, when you define your product, your product could be a ruler.", "tokens": [51514, 407, 291, 11, 562, 291, 6964, 428, 1674, 11, 428, 1674, 727, 312, 257, 19661, 13, 51784], "temperature": 0.0, "avg_logprob": -0.25069908979462413, "compression_ratio": 1.5145631067961165, "no_speech_prob": 0.00781333725899458}, {"id": 564, "seek": 219220, "start": 2193.2, "end": 2199.3999999999996, "text": " So a ruler measures things, but doesn't tell you just by the name if it's in imperial or metric,", "tokens": [50414, 407, 257, 19661, 8000, 721, 11, 457, 1177, 380, 980, 291, 445, 538, 264, 1315, 498, 309, 311, 294, 21143, 420, 20678, 11, 50724], "temperature": 0.0, "avg_logprob": -0.222281847681318, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.00030526897171512246}, {"id": 565, "seek": 219220, "start": 2199.64, "end": 2202.24, "text": " you have to actually see it or have something else.", "tokens": [50736, 291, 362, 281, 767, 536, 309, 420, 362, 746, 1646, 13, 50866], "temperature": 0.0, "avg_logprob": -0.222281847681318, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.00030526897171512246}, {"id": 566, "seek": 219220, "start": 2202.64, "end": 2208.8799999999997, "text": " And that might actually be defined more in that object schema portion of the full schema,", "tokens": [50886, 400, 300, 1062, 767, 312, 7642, 544, 294, 300, 2657, 34078, 8044, 295, 264, 1577, 34078, 11, 51198], "temperature": 0.0, "avg_logprob": -0.222281847681318, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.00030526897171512246}, {"id": 567, "seek": 219220, "start": 2208.9199999999996, "end": 2209.8399999999997, "text": " the full spec.", "tokens": [51200, 264, 1577, 1608, 13, 51246], "temperature": 0.0, "avg_logprob": -0.222281847681318, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.00030526897171512246}, {"id": 568, "seek": 219220, "start": 2210.8799999999997, "end": 2216.0, "text": " So there you could, you might have a minimum of like, a ruler is never going to be less", "tokens": [51298, 407, 456, 291, 727, 11, 291, 1062, 362, 257, 7285, 295, 411, 11, 257, 19661, 307, 1128, 516, 281, 312, 1570, 51554], "temperature": 0.0, "avg_logprob": -0.222281847681318, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.00030526897171512246}, {"id": 569, "seek": 219220, "start": 2216.0, "end": 2221.3599999999997, "text": " than zero, so a min of zero and a max of maybe 50 or something, I don't know.", "tokens": [51554, 813, 4018, 11, 370, 257, 923, 295, 4018, 293, 257, 11469, 295, 1310, 2625, 420, 746, 11, 286, 500, 380, 458, 13, 51822], "temperature": 0.0, "avg_logprob": -0.222281847681318, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.00030526897171512246}, {"id": 570, "seek": 222136, "start": 2222.32, "end": 2227.08, "text": " You, on top of that, if you wanted to define units, so if you wanted to know if it was", "tokens": [50412, 509, 11, 322, 1192, 295, 300, 11, 498, 291, 1415, 281, 6964, 6815, 11, 370, 498, 291, 1415, 281, 458, 498, 309, 390, 50650], "temperature": 0.0, "avg_logprob": -0.17900732897837227, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00034596104524098337}, {"id": 571, "seek": 222136, "start": 2227.08, "end": 2233.1200000000003, "text": " imperial or metric, you would most likely, I think, use a custom property that was unique", "tokens": [50650, 21143, 420, 20678, 11, 291, 576, 881, 3700, 11, 286, 519, 11, 764, 257, 2375, 4707, 300, 390, 3845, 50952], "temperature": 0.0, "avg_logprob": -0.17900732897837227, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00034596104524098337}, {"id": 572, "seek": 222136, "start": 2233.1200000000003, "end": 2235.04, "text": " to your API.", "tokens": [50952, 281, 428, 9362, 13, 51048], "temperature": 0.0, "avg_logprob": -0.17900732897837227, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00034596104524098337}, {"id": 573, "seek": 222136, "start": 2235.88, "end": 2239.1600000000003, "text": " Those, those are quite common using custom properties.", "tokens": [51090, 3950, 11, 729, 366, 1596, 2689, 1228, 2375, 7221, 13, 51254], "temperature": 0.0, "avg_logprob": -0.17900732897837227, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00034596104524098337}, {"id": 574, "seek": 222136, "start": 2239.1600000000003, "end": 2245.2400000000002, "text": " It's one thing that is probably the one thing I'm not sure entirely how to support yet in,", "tokens": [51254, 467, 311, 472, 551, 300, 307, 1391, 264, 472, 551, 286, 478, 406, 988, 7696, 577, 281, 1406, 1939, 294, 11, 51558], "temperature": 0.0, "avg_logprob": -0.17900732897837227, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00034596104524098337}, {"id": 575, "seek": 222136, "start": 2245.2400000000002, "end": 2251.04, "text": " in my package or my CLI tools, because they, they can be anything almost that", "tokens": [51558, 294, 452, 7372, 420, 452, 12855, 40, 3873, 11, 570, 436, 11, 436, 393, 312, 1340, 1920, 300, 51848], "temperature": 0.0, "avg_logprob": -0.17900732897837227, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00034596104524098337}, {"id": 576, "seek": 225104, "start": 2251.24, "end": 2255.16, "text": " basically any JSON schema definition can be used there.", "tokens": [50374, 1936, 604, 31828, 34078, 7123, 393, 312, 1143, 456, 13, 50570], "temperature": 0.0, "avg_logprob": -0.17888944289263556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0008295077714137733}, {"id": 577, "seek": 225104, "start": 2255.88, "end": 2260.64, "text": " Although I do like that you mentioned having a separate config specifically for some of this.", "tokens": [50606, 5780, 286, 360, 411, 300, 291, 2835, 1419, 257, 4994, 6662, 4682, 337, 512, 295, 341, 13, 50844], "temperature": 0.0, "avg_logprob": -0.17888944289263556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0008295077714137733}, {"id": 578, "seek": 225104, "start": 2261.32, "end": 2265.88, "text": " Think I hadn't considered that entirely, but I kind of like that because if I'm using,", "tokens": [50878, 6557, 286, 8782, 380, 4888, 300, 7696, 11, 457, 286, 733, 295, 411, 300, 570, 498, 286, 478, 1228, 11, 51106], "temperature": 0.0, "avg_logprob": -0.17888944289263556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0008295077714137733}, {"id": 579, "seek": 225104, "start": 2266.44, "end": 2273.84, "text": " if I'm using Spotify's API, I can't modify their, their API spec, their open API spec.", "tokens": [51134, 498, 286, 478, 1228, 29036, 311, 9362, 11, 286, 393, 380, 16927, 641, 11, 641, 9362, 1608, 11, 641, 1269, 9362, 1608, 13, 51504], "temperature": 0.0, "avg_logprob": -0.17888944289263556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0008295077714137733}, {"id": 580, "seek": 225104, "start": 2273.84, "end": 2277.32, "text": " I can't modify it's not my, I mean, I could download it and edit it.", "tokens": [51504, 286, 393, 380, 16927, 309, 311, 406, 452, 11, 286, 914, 11, 286, 727, 5484, 309, 293, 8129, 309, 13, 51678], "temperature": 0.0, "avg_logprob": -0.17888944289263556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0008295077714137733}, {"id": 581, "seek": 227732, "start": 2277.76, "end": 2281.36, "text": " But then when they release a new version, I have to go and manually edit it again.", "tokens": [50386, 583, 550, 562, 436, 4374, 257, 777, 3037, 11, 286, 362, 281, 352, 293, 16945, 8129, 309, 797, 13, 50566], "temperature": 0.0, "avg_logprob": -0.16521278871308773, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.011330116540193558}, {"id": 582, "seek": 227732, "start": 2281.56, "end": 2283.4, "text": " And that's, that's not fun for somebody.", "tokens": [50576, 400, 300, 311, 11, 300, 311, 406, 1019, 337, 2618, 13, 50668], "temperature": 0.0, "avg_logprob": -0.16521278871308773, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.011330116540193558}, {"id": 583, "seek": 227732, "start": 2284.44, "end": 2291.52, "text": " So having the option to either include your own custom properties or for a third party", "tokens": [50720, 407, 1419, 264, 3614, 281, 2139, 4090, 428, 1065, 2375, 7221, 420, 337, 257, 2636, 3595, 51074], "temperature": 0.0, "avg_logprob": -0.16521278871308773, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.011330116540193558}, {"id": 584, "seek": 227732, "start": 2292.0, "end": 2297.6400000000003, "text": " spec, being able to define your own local config to add on to it, essentially that", "tokens": [51098, 1608, 11, 885, 1075, 281, 6964, 428, 1065, 2654, 6662, 281, 909, 322, 281, 309, 11, 4476, 300, 51380], "temperature": 0.0, "avg_logprob": -0.16521278871308773, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.011330116540193558}, {"id": 585, "seek": 227732, "start": 2298.1600000000003, "end": 2300.6000000000004, "text": " that could be a very nice route to go.", "tokens": [51406, 300, 727, 312, 257, 588, 1481, 7955, 281, 352, 13, 51528], "temperature": 0.0, "avg_logprob": -0.16521278871308773, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.011330116540193558}, {"id": 586, "seek": 227732, "start": 2301.04, "end": 2305.36, "text": " Yeah, if there was some way to hook into that, that would, because I guess what often happens", "tokens": [51550, 865, 11, 498, 456, 390, 512, 636, 281, 6328, 666, 300, 11, 300, 576, 11, 570, 286, 2041, 437, 2049, 2314, 51766], "temperature": 0.0, "avg_logprob": -0.16521278871308773, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.011330116540193558}, {"id": 587, "seek": 230536, "start": 2305.4, "end": 2312.52, "text": " is like things, you know, tools around GraphQL and JSON schemas, you know, a lot of people", "tokens": [50366, 307, 411, 721, 11, 291, 458, 11, 3873, 926, 21884, 13695, 293, 31828, 22627, 296, 11, 291, 458, 11, 257, 688, 295, 561, 50722], "temperature": 0.0, "avg_logprob": -0.12097315710099017, "compression_ratio": 1.8063241106719368, "no_speech_prob": 0.01495325192809105}, {"id": 588, "seek": 230536, "start": 2312.52, "end": 2320.0, "text": " are using them with TypeScript maybe, and they're just like, okay, let's just add TypeScript types.", "tokens": [50722, 366, 1228, 552, 365, 15576, 14237, 1310, 11, 293, 436, 434, 445, 411, 11, 1392, 11, 718, 311, 445, 909, 15576, 14237, 3467, 13, 51096], "temperature": 0.0, "avg_logprob": -0.12097315710099017, "compression_ratio": 1.8063241106719368, "no_speech_prob": 0.01495325192809105}, {"id": 589, "seek": 230536, "start": 2320.0, "end": 2325.04, "text": " And they use sort of the lowest common denominator, primitive types to describe it.", "tokens": [51096, 400, 436, 764, 1333, 295, 264, 12437, 2689, 20687, 11, 28540, 3467, 281, 6786, 309, 13, 51348], "temperature": 0.0, "avg_logprob": -0.12097315710099017, "compression_ratio": 1.8063241106719368, "no_speech_prob": 0.01495325192809105}, {"id": 590, "seek": 230536, "start": 2325.04, "end": 2326.08, "text": " And that, and that's pretty nice.", "tokens": [51348, 400, 300, 11, 293, 300, 311, 1238, 1481, 13, 51400], "temperature": 0.0, "avg_logprob": -0.12097315710099017, "compression_ratio": 1.8063241106719368, "no_speech_prob": 0.01495325192809105}, {"id": 591, "seek": 230536, "start": 2326.08, "end": 2331.1200000000003, "text": " And then with, you know, with TypeScript, you have an enum and it can be five different strings.", "tokens": [51400, 400, 550, 365, 11, 291, 458, 11, 365, 15576, 14237, 11, 291, 362, 364, 465, 449, 293, 309, 393, 312, 1732, 819, 13985, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12097315710099017, "compression_ratio": 1.8063241106719368, "no_speech_prob": 0.01495325192809105}, {"id": 592, "seek": 230536, "start": 2331.1200000000003, "end": 2333.36, "text": " And then in TypeScript, you can just describe that.", "tokens": [51652, 400, 550, 294, 15576, 14237, 11, 291, 393, 445, 6786, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12097315710099017, "compression_ratio": 1.8063241106719368, "no_speech_prob": 0.01495325192809105}, {"id": 593, "seek": 233336, "start": 2333.36, "end": 2337.4, "text": " You can say, this is one of these five strings, which is really nice.", "tokens": [50364, 509, 393, 584, 11, 341, 307, 472, 295, 613, 1732, 13985, 11, 597, 307, 534, 1481, 13, 50566], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 594, "seek": 233336, "start": 2337.6, "end": 2344.28, "text": " But at the same time, it is, you know, we also like opaque types and being able to have certain", "tokens": [50576, 583, 412, 264, 912, 565, 11, 309, 307, 11, 291, 458, 11, 321, 611, 411, 42687, 3467, 293, 885, 1075, 281, 362, 1629, 50910], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 595, "seek": 233336, "start": 2344.6, "end": 2346.04, "text": " guarantees around those things.", "tokens": [50926, 32567, 926, 729, 721, 13, 50998], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 596, "seek": 233336, "start": 2346.04, "end": 2348.1200000000003, "text": " So we also like custom types.", "tokens": [50998, 407, 321, 611, 411, 2375, 3467, 13, 51102], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 597, "seek": 233336, "start": 2348.44, "end": 2355.56, "text": " And so if you want to do more nominally typed things, TypeScript isn't, doesn't give you the same", "tokens": [51118, 400, 370, 498, 291, 528, 281, 360, 544, 5369, 19801, 33941, 721, 11, 15576, 14237, 1943, 380, 11, 1177, 380, 976, 291, 264, 912, 51474], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 598, "seek": 233336, "start": 2355.56, "end": 2356.4, "text": " guarantees with that.", "tokens": [51474, 32567, 365, 300, 13, 51516], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 599, "seek": 233336, "start": 2356.4, "end": 2358.2000000000003, "text": " And it's not the path of least resistance.", "tokens": [51516, 400, 309, 311, 406, 264, 3100, 295, 1935, 7335, 13, 51606], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 600, "seek": 233336, "start": 2358.2000000000003, "end": 2363.2400000000002, "text": " And a lot of tools end up just catering to that sort of, hey, let's nicely describe what", "tokens": [51606, 400, 257, 688, 295, 3873, 917, 493, 445, 21557, 278, 281, 300, 1333, 295, 11, 4177, 11, 718, 311, 9594, 6786, 437, 51858], "temperature": 0.0, "avg_logprob": -0.13361704899714544, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.0028008364606648684}, {"id": 601, "seek": 236324, "start": 2363.2799999999997, "end": 2364.8799999999997, "text": " primitives we're using everywhere.", "tokens": [50366, 2886, 38970, 321, 434, 1228, 5315, 13, 50446], "temperature": 0.0, "avg_logprob": -0.20167715890066965, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0019264192087575793}, {"id": 602, "seek": 236324, "start": 2365.2, "end": 2369.52, "text": " But to me, the really interesting thing is when you go beyond that and you say like, okay,", "tokens": [50462, 583, 281, 385, 11, 264, 534, 1880, 551, 307, 562, 291, 352, 4399, 300, 293, 291, 584, 411, 11, 1392, 11, 50678], "temperature": 0.0, "avg_logprob": -0.20167715890066965, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0019264192087575793}, {"id": 603, "seek": 236324, "start": 2369.52, "end": 2374.04, "text": " well, it's giving us all these primitives as a starting point, but then we can actually", "tokens": [50678, 731, 11, 309, 311, 2902, 505, 439, 613, 2886, 38970, 382, 257, 2891, 935, 11, 457, 550, 321, 393, 767, 50904], "temperature": 0.0, "avg_logprob": -0.20167715890066965, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0019264192087575793}, {"id": 604, "seek": 236324, "start": 2374.04, "end": 2380.6, "text": " describe, give more semantic meaning to that set of primitives and use opaque types around them.", "tokens": [50904, 6786, 11, 976, 544, 47982, 3620, 281, 300, 992, 295, 2886, 38970, 293, 764, 42687, 3467, 926, 552, 13, 51232], "temperature": 0.0, "avg_logprob": -0.20167715890066965, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0019264192087575793}, {"id": 605, "seek": 236324, "start": 2380.9199999999996, "end": 2382.64, "text": " Yeah, I do.", "tokens": [51248, 865, 11, 286, 360, 13, 51334], "temperature": 0.0, "avg_logprob": -0.20167715890066965, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0019264192087575793}, {"id": 606, "seek": 236324, "start": 2383.04, "end": 2388.9599999999996, "text": " There absolutely is a reason for, like you pointed out, being a being very minimal in your", "tokens": [51354, 821, 3122, 307, 257, 1778, 337, 11, 411, 291, 10932, 484, 11, 885, 257, 885, 588, 13206, 294, 428, 51650], "temperature": 0.0, "avg_logprob": -0.20167715890066965, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0019264192087575793}, {"id": 607, "seek": 238896, "start": 2389.16, "end": 2390.16, "text": " definitions.", "tokens": [50374, 21988, 13, 50424], "temperature": 0.0, "avg_logprob": -0.19326579052469003, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.010649127885699272}, {"id": 608, "seek": 238896, "start": 2390.56, "end": 2395.92, "text": " We ran into that a lot at Square with our open API spec in that we were having to support, we", "tokens": [50444, 492, 5872, 666, 300, 257, 688, 412, 16463, 365, 527, 1269, 9362, 1608, 294, 300, 321, 645, 1419, 281, 1406, 11, 321, 50712], "temperature": 0.0, "avg_logprob": -0.19326579052469003, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.010649127885699272}, {"id": 609, "seek": 238896, "start": 2395.92, "end": 2398.0, "text": " support TypeScript, which was usually fine.", "tokens": [50712, 1406, 15576, 14237, 11, 597, 390, 2673, 2489, 13, 50816], "temperature": 0.0, "avg_logprob": -0.19326579052469003, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.010649127885699272}, {"id": 610, "seek": 238896, "start": 2398.84, "end": 2400.52, "text": " Ruby was usually fine.", "tokens": [50858, 19907, 390, 2673, 2489, 13, 50942], "temperature": 0.0, "avg_logprob": -0.19326579052469003, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.010649127885699272}, {"id": 611, "seek": 238896, "start": 2401.32, "end": 2407.92, "text": " Java and C sharp were always a bit frustrating, more, more due to like the generated definitions", "tokens": [50982, 10745, 293, 383, 8199, 645, 1009, 257, 857, 16522, 11, 544, 11, 544, 3462, 281, 411, 264, 10833, 21988, 51312], "temperature": 0.0, "avg_logprob": -0.19326579052469003, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.010649127885699272}, {"id": 612, "seek": 238896, "start": 2407.92, "end": 2412.88, "text": " would, you know, the, if everyone's familiar with working with, especially with Java, you would", "tokens": [51312, 576, 11, 291, 458, 11, 264, 11, 498, 1518, 311, 4963, 365, 1364, 365, 11, 2318, 365, 10745, 11, 291, 576, 51560], "temperature": 0.0, "avg_logprob": -0.19326579052469003, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.010649127885699272}, {"id": 613, "seek": 241288, "start": 2412.96, "end": 2417.88, "text": " end up with function calls that have like six nullable arguments.", "tokens": [50368, 917, 493, 365, 2445, 5498, 300, 362, 411, 2309, 18184, 712, 12869, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1582451081008054, "compression_ratio": 1.6, "no_speech_prob": 0.03731900453567505}, {"id": 614, "seek": 241288, "start": 2418.6800000000003, "end": 2423.28, "text": " And so sometimes you just have to pass in null like four, five, six times.", "tokens": [50654, 400, 370, 2171, 291, 445, 362, 281, 1320, 294, 18184, 411, 1451, 11, 1732, 11, 2309, 1413, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1582451081008054, "compression_ratio": 1.6, "no_speech_prob": 0.03731900453567505}, {"id": 615, "seek": 241288, "start": 2424.36, "end": 2425.36, "text": " That was always fun.", "tokens": [50938, 663, 390, 1009, 1019, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1582451081008054, "compression_ratio": 1.6, "no_speech_prob": 0.03731900453567505}, {"id": 616, "seek": 241288, "start": 2426.04, "end": 2432.32, "text": " But yeah, so you don't know, like you're having to potentially target many different languages", "tokens": [51022, 583, 1338, 11, 370, 291, 500, 380, 458, 11, 411, 291, 434, 1419, 281, 7263, 3779, 867, 819, 8650, 51336], "temperature": 0.0, "avg_logprob": -0.1582451081008054, "compression_ratio": 1.6, "no_speech_prob": 0.03731900453567505}, {"id": 617, "seek": 241288, "start": 2432.32, "end": 2434.04, "text": " with very different semantics.", "tokens": [51336, 365, 588, 819, 4361, 45298, 13, 51422], "temperature": 0.0, "avg_logprob": -0.1582451081008054, "compression_ratio": 1.6, "no_speech_prob": 0.03731900453567505}, {"id": 618, "seek": 241288, "start": 2434.92, "end": 2440.1600000000003, "text": " And it's hard to define something that fits all of them equally.", "tokens": [51466, 400, 309, 311, 1152, 281, 6964, 746, 300, 9001, 439, 295, 552, 12309, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1582451081008054, "compression_ratio": 1.6, "no_speech_prob": 0.03731900453567505}, {"id": 619, "seek": 244016, "start": 2440.52, "end": 2445.92, "text": " Everyone's going to be slightly disappointed in their, in their generated SDK.", "tokens": [50382, 5198, 311, 516, 281, 312, 4748, 13856, 294, 641, 11, 294, 641, 10833, 37135, 13, 50652], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 620, "seek": 244016, "start": 2445.92, "end": 2451.12, "text": " So if you, yeah, if you have your own little config, you say, cool, I like this, but I'm", "tokens": [50652, 407, 498, 291, 11, 1338, 11, 498, 291, 362, 428, 1065, 707, 6662, 11, 291, 584, 11, 1627, 11, 286, 411, 341, 11, 457, 286, 478, 50912], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 621, "seek": 244016, "start": 2451.12, "end": 2457.0, "text": " going to tweak it to fit my language or even to fit my app, like just to be able to fit", "tokens": [50912, 516, 281, 29879, 309, 281, 3318, 452, 2856, 420, 754, 281, 3318, 452, 724, 11, 411, 445, 281, 312, 1075, 281, 3318, 51206], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 622, "seek": 244016, "start": 2457.0, "end": 2458.3999999999996, "text": " your specific project.", "tokens": [51206, 428, 2685, 1716, 13, 51276], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 623, "seek": 244016, "start": 2458.8799999999997, "end": 2459.7999999999997, "text": " Very handy.", "tokens": [51300, 4372, 13239, 13, 51346], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 624, "seek": 244016, "start": 2460.48, "end": 2462.96, "text": " Very nice to have too, too many ideas now.", "tokens": [51380, 4372, 1481, 281, 362, 886, 11, 886, 867, 3487, 586, 13, 51504], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 625, "seek": 244016, "start": 2463.56, "end": 2463.92, "text": " Yeah.", "tokens": [51534, 865, 13, 51552], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 626, "seek": 244016, "start": 2464.8799999999997, "end": 2466.44, "text": " To throw another idea out there.", "tokens": [51600, 1407, 3507, 1071, 1558, 484, 456, 13, 51678], "temperature": 0.0, "avg_logprob": -0.22028460326018157, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.007935444824397564}, {"id": 627, "seek": 246644, "start": 2466.48, "end": 2471.96, "text": " It, uh, like, I, well, I'm not sure if you do this at all.", "tokens": [50366, 467, 11, 2232, 11, 411, 11, 286, 11, 731, 11, 286, 478, 406, 988, 498, 291, 360, 341, 412, 439, 13, 50640], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 628, "seek": 246644, "start": 2471.96, "end": 2477.8, "text": " I think maybe you don't, but for enums in a JSON schema, you can describe like an enum", "tokens": [50640, 286, 519, 1310, 291, 500, 380, 11, 457, 337, 465, 8099, 294, 257, 31828, 34078, 11, 291, 393, 6786, 411, 364, 465, 449, 50932], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 629, "seek": 246644, "start": 2477.8, "end": 2480.88, "text": " like these are the five possible string types for this value.", "tokens": [50932, 411, 613, 366, 264, 1732, 1944, 6798, 3467, 337, 341, 2158, 13, 51086], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 630, "seek": 246644, "start": 2480.88, "end": 2481.2400000000002, "text": " Right.", "tokens": [51086, 1779, 13, 51104], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 631, "seek": 246644, "start": 2481.52, "end": 2486.96, "text": " Do, do those get generated as string values or custom types?", "tokens": [51118, 1144, 11, 360, 729, 483, 10833, 382, 6798, 4190, 420, 2375, 3467, 30, 51390], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 632, "seek": 246644, "start": 2487.48, "end": 2490.32, "text": " Believe they get generated as custom types right now.", "tokens": [51416, 21486, 436, 483, 10833, 382, 2375, 3467, 558, 586, 13, 51558], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 633, "seek": 246644, "start": 2490.68, "end": 2491.0, "text": " They do.", "tokens": [51576, 814, 360, 13, 51592], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 634, "seek": 246644, "start": 2491.0, "end": 2492.6, "text": " It was, they do.", "tokens": [51592, 467, 390, 11, 436, 360, 13, 51672], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 635, "seek": 246644, "start": 2494.16, "end": 2495.64, "text": " I'm fairly certain they do.", "tokens": [51750, 286, 478, 6457, 1629, 436, 360, 13, 51824], "temperature": 0.0, "avg_logprob": -0.2390200949122763, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006387337576597929}, {"id": 636, "seek": 249564, "start": 2496.08, "end": 2499.2799999999997, "text": " I went back and forth on that a lot actually.", "tokens": [50386, 286, 1437, 646, 293, 5220, 322, 300, 257, 688, 767, 13, 50546], "temperature": 0.0, "avg_logprob": -0.14606577940661497, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0006460820441134274}, {"id": 637, "seek": 249564, "start": 2499.64, "end": 2502.24, "text": " Intuitively, you think that's the right way to go.", "tokens": [50564, 5681, 1983, 3413, 11, 291, 519, 300, 311, 264, 558, 636, 281, 352, 13, 50694], "temperature": 0.0, "avg_logprob": -0.14606577940661497, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0006460820441134274}, {"id": 638, "seek": 249564, "start": 2502.64, "end": 2508.64, "text": " However, there are, there are weird edge cases, um, which I only know from dealing", "tokens": [50714, 2908, 11, 456, 366, 11, 456, 366, 3657, 4691, 3331, 11, 1105, 11, 597, 286, 787, 458, 490, 6260, 51014], "temperature": 0.0, "avg_logprob": -0.14606577940661497, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0006460820441134274}, {"id": 639, "seek": 249564, "start": 2508.64, "end": 2513.56, "text": " with them at square in that people like to sit on one version of an SDK for a very", "tokens": [51014, 365, 552, 412, 3732, 294, 300, 561, 411, 281, 1394, 322, 472, 3037, 295, 364, 37135, 337, 257, 588, 51260], "temperature": 0.0, "avg_logprob": -0.14606577940661497, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0006460820441134274}, {"id": 640, "seek": 249564, "start": 2513.56, "end": 2514.96, "text": " long time, sometimes.", "tokens": [51260, 938, 565, 11, 2171, 13, 51330], "temperature": 0.0, "avg_logprob": -0.14606577940661497, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0006460820441134274}, {"id": 641, "seek": 249564, "start": 2515.56, "end": 2523.12, "text": " And so if you, you keep updating your backend to define new, new variations on", "tokens": [51360, 400, 370, 498, 291, 11, 291, 1066, 25113, 428, 38087, 281, 6964, 777, 11, 777, 17840, 322, 51738], "temperature": 0.0, "avg_logprob": -0.14606577940661497, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0006460820441134274}, {"id": 642, "seek": 252312, "start": 2523.12, "end": 2527.2, "text": " that enum, the SDK might not be able to handle that anymore.", "tokens": [50364, 300, 465, 449, 11, 264, 37135, 1062, 406, 312, 1075, 281, 4813, 300, 3602, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1349076343183758, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.002550639910623431}, {"id": 643, "seek": 252312, "start": 2527.52, "end": 2533.04, "text": " Uh, and so we would run into issues with customers on very old versions of the SDK,", "tokens": [50584, 4019, 11, 293, 370, 321, 576, 1190, 666, 2663, 365, 4581, 322, 588, 1331, 9606, 295, 264, 37135, 11, 50860], "temperature": 0.0, "avg_logprob": -0.1349076343183758, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.002550639910623431}, {"id": 644, "seek": 252312, "start": 2533.12, "end": 2535.3599999999997, "text": " not having the time, maybe even to upgrade.", "tokens": [50864, 406, 1419, 264, 565, 11, 1310, 754, 281, 11484, 13, 50976], "temperature": 0.0, "avg_logprob": -0.1349076343183758, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.002550639910623431}, {"id": 645, "seek": 252312, "start": 2535.44, "end": 2537.92, "text": " Maybe they're just swamped with other feature work.", "tokens": [50980, 2704, 436, 434, 445, 31724, 292, 365, 661, 4111, 589, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1349076343183758, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.002550639910623431}, {"id": 646, "seek": 252312, "start": 2537.92, "end": 2544.08, "text": " They can't, you know, spend a day or a week even upgrading the SDK, but you", "tokens": [51104, 814, 393, 380, 11, 291, 458, 11, 3496, 257, 786, 420, 257, 1243, 754, 36249, 264, 37135, 11, 457, 291, 51412], "temperature": 0.0, "avg_logprob": -0.1349076343183758, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.002550639910623431}, {"id": 647, "seek": 252312, "start": 2544.08, "end": 2545.88, "text": " don't want to break them either.", "tokens": [51412, 500, 380, 528, 281, 1821, 552, 2139, 13, 51502], "temperature": 0.0, "avg_logprob": -0.1349076343183758, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.002550639910623431}, {"id": 648, "seek": 252312, "start": 2546.3599999999997, "end": 2551.24, "text": " So we ended up, at some point, we ended up switching most of our SDKs back to", "tokens": [51526, 407, 321, 4590, 493, 11, 412, 512, 935, 11, 321, 4590, 493, 16493, 881, 295, 527, 37135, 82, 646, 281, 51770], "temperature": 0.0, "avg_logprob": -0.1349076343183758, "compression_ratio": 1.623574144486692, "no_speech_prob": 0.002550639910623431}, {"id": 649, "seek": 255124, "start": 2551.24, "end": 2557.12, "text": " using strings for enums for that specific reason, which is a bit disappointing, but", "tokens": [50364, 1228, 13985, 337, 465, 8099, 337, 300, 2685, 1778, 11, 597, 307, 257, 857, 25054, 11, 457, 50658], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 650, "seek": 255124, "start": 2557.12, "end": 2560.3999999999996, "text": " it also kind of makes sense from that standpoint.", "tokens": [50658, 309, 611, 733, 295, 1669, 2020, 490, 300, 15827, 13, 50822], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 651, "seek": 255124, "start": 2560.7999999999997, "end": 2563.3199999999997, "text": " I do think this did inspire me though.", "tokens": [50842, 286, 360, 519, 341, 630, 15638, 385, 1673, 13, 50968], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 652, "seek": 255124, "start": 2563.6, "end": 2567.12, "text": " So maybe that's where opaque types come in.", "tokens": [50982, 407, 1310, 300, 311, 689, 42687, 3467, 808, 294, 13, 51158], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 653, "seek": 255124, "start": 2567.64, "end": 2572.08, "text": " You could probably with Elm instead of exposing the strings directly.", "tokens": [51184, 509, 727, 1391, 365, 2699, 76, 2602, 295, 33178, 264, 13985, 3838, 13, 51406], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 654, "seek": 255124, "start": 2572.2, "end": 2573.12, "text": " Right.", "tokens": [51412, 1779, 13, 51458], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 655, "seek": 255124, "start": 2573.3199999999997, "end": 2573.8399999999997, "text": " Yes.", "tokens": [51468, 1079, 13, 51494], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 656, "seek": 255124, "start": 2574.08, "end": 2579.2799999999997, "text": " I wonder if there's some way to do something opaque or just expose them through.", "tokens": [51506, 286, 2441, 498, 456, 311, 512, 636, 281, 360, 746, 42687, 420, 445, 19219, 552, 807, 13, 51766], "temperature": 0.0, "avg_logprob": -0.21678346517134686, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.003074940759688616}, {"id": 657, "seek": 257928, "start": 2579.32, "end": 2583.0400000000004, "text": " Like if you want to create them, you just expose a function name that behind the", "tokens": [50366, 1743, 498, 291, 528, 281, 1884, 552, 11, 291, 445, 19219, 257, 2445, 1315, 300, 2261, 264, 50552], "temperature": 0.0, "avg_logprob": -0.1363022981491764, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0014101697597652674}, {"id": 658, "seek": 257928, "start": 2583.0400000000004, "end": 2587.48, "text": " scenes is still generating a string, but hidden within some opaque type.", "tokens": [50552, 8026, 307, 920, 17746, 257, 6798, 11, 457, 7633, 1951, 512, 42687, 2010, 13, 50774], "temperature": 0.0, "avg_logprob": -0.1363022981491764, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0014101697597652674}, {"id": 659, "seek": 257928, "start": 2587.88, "end": 2588.44, "text": " Yes.", "tokens": [50794, 1079, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1363022981491764, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0014101697597652674}, {"id": 660, "seek": 257928, "start": 2588.6800000000003, "end": 2590.36, "text": " So you could totally do that.", "tokens": [50834, 407, 291, 727, 3879, 360, 300, 13, 50918], "temperature": 0.0, "avg_logprob": -0.1363022981491764, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0014101697597652674}, {"id": 661, "seek": 257928, "start": 2590.4, "end": 2596.7200000000003, "text": " And, and the benefit to that would be that, uh, from an API point of view, of", "tokens": [50920, 400, 11, 293, 264, 5121, 281, 300, 576, 312, 300, 11, 2232, 11, 490, 364, 9362, 935, 295, 1910, 11, 295, 51236], "temperature": 0.0, "avg_logprob": -0.1363022981491764, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0014101697597652674}, {"id": 662, "seek": 257928, "start": 2596.7200000000003, "end": 2603.1600000000003, "text": " course, if you're not actually publishing a package with this generated SDK from", "tokens": [51236, 1164, 11, 498, 291, 434, 406, 767, 17832, 257, 7372, 365, 341, 10833, 37135, 490, 51558], "temperature": 0.0, "avg_logprob": -0.1363022981491764, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0014101697597652674}, {"id": 663, "seek": 257928, "start": 2603.1600000000003, "end": 2608.5600000000004, "text": " Elm Open API, then the, the breaking changes thing is more of a, you know, you", "tokens": [51558, 2699, 76, 7238, 9362, 11, 550, 264, 11, 264, 7697, 2962, 551, 307, 544, 295, 257, 11, 291, 458, 11, 291, 51828], "temperature": 0.0, "avg_logprob": -0.1363022981491764, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0014101697597652674}, {"id": 664, "seek": 260856, "start": 2608.56, "end": 2612.6, "text": " know, when it's broken because your code doesn't compile, but, but from the point", "tokens": [50364, 458, 11, 562, 309, 311, 5463, 570, 428, 3089, 1177, 380, 31413, 11, 457, 11, 457, 490, 264, 935, 50566], "temperature": 0.0, "avg_logprob": -0.14194939587567304, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.004904747009277344}, {"id": 665, "seek": 260856, "start": 2612.6, "end": 2619.08, "text": " of view of like publishing breaking changes, you can add an enum variant and it's", "tokens": [50566, 295, 1910, 295, 411, 17832, 7697, 2962, 11, 291, 393, 909, 364, 465, 449, 17501, 293, 309, 311, 50890], "temperature": 0.0, "avg_logprob": -0.14194939587567304, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.004904747009277344}, {"id": 666, "seek": 260856, "start": 2619.08, "end": 2623.52, "text": " not a breaking change with an opaque type because you're exposing a new function.", "tokens": [50890, 406, 257, 7697, 1319, 365, 364, 42687, 2010, 570, 291, 434, 33178, 257, 777, 2445, 13, 51112], "temperature": 0.0, "avg_logprob": -0.14194939587567304, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.004904747009277344}, {"id": 667, "seek": 260856, "start": 2623.96, "end": 2627.72, "text": " So that's a minor version bump, not a major version bump.", "tokens": [51134, 407, 300, 311, 257, 6696, 3037, 9961, 11, 406, 257, 2563, 3037, 9961, 13, 51322], "temperature": 0.0, "avg_logprob": -0.14194939587567304, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.004904747009277344}, {"id": 668, "seek": 260856, "start": 2627.88, "end": 2633.44, "text": " But if you remove one, then it's a major breaking change, uh, because, because it", "tokens": [51330, 583, 498, 291, 4159, 472, 11, 550, 309, 311, 257, 2563, 7697, 1319, 11, 2232, 11, 570, 11, 570, 309, 51608], "temperature": 0.0, "avg_logprob": -0.14194939587567304, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.004904747009277344}, {"id": 669, "seek": 260856, "start": 2633.44, "end": 2634.96, "text": " is, because it could break someone's code.", "tokens": [51608, 307, 11, 570, 309, 727, 1821, 1580, 311, 3089, 13, 51684], "temperature": 0.0, "avg_logprob": -0.14194939587567304, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.004904747009277344}, {"id": 670, "seek": 263496, "start": 2635.28, "end": 2640.32, "text": " Uh, whereas with a non opaque custom type, you're in, we should, uh, we should lobby", "tokens": [50380, 4019, 11, 9735, 365, 257, 2107, 42687, 2375, 2010, 11, 291, 434, 294, 11, 321, 820, 11, 2232, 11, 321, 820, 21067, 50632], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 671, "seek": 263496, "start": 2640.32, "end": 2642.76, "text": " to, to call them explicitly non opaque.", "tokens": [50632, 281, 11, 281, 818, 552, 20803, 2107, 42687, 13, 50754], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 672, "seek": 263496, "start": 2643.08, "end": 2645.2, "text": " We should lobby to call them non opaque types.", "tokens": [50770, 492, 820, 21067, 281, 818, 552, 2107, 42687, 3467, 13, 50876], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 673, "seek": 263496, "start": 2648.36, "end": 2649.36, "text": " Transparent types.", "tokens": [51034, 6531, 38321, 3467, 13, 51084], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 674, "seek": 263496, "start": 2651.08, "end": 2652.0, "text": " Bad types.", "tokens": [51170, 11523, 3467, 13, 51216], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 675, "seek": 263496, "start": 2652.4, "end": 2652.7200000000003, "text": " Yeah.", "tokens": [51236, 865, 13, 51252], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 676, "seek": 263496, "start": 2652.76, "end": 2654.36, "text": " Bad types, the good types and bad types.", "tokens": [51254, 11523, 3467, 11, 264, 665, 3467, 293, 1578, 3467, 13, 51334], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 677, "seek": 263496, "start": 2654.56, "end": 2658.48, "text": " It is considered in a published API, a breaking change.", "tokens": [51344, 467, 307, 4888, 294, 257, 6572, 9362, 11, 257, 7697, 1319, 13, 51540], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 678, "seek": 263496, "start": 2658.48, "end": 2662.84, "text": " If you, um, if you add a variant, because, you know, you could have a case", "tokens": [51540, 759, 291, 11, 1105, 11, 498, 291, 909, 257, 17501, 11, 570, 11, 291, 458, 11, 291, 727, 362, 257, 1389, 51758], "temperature": 0.0, "avg_logprob": -0.24842572845188918, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.00669147539883852}, {"id": 679, "seek": 266284, "start": 2662.88, "end": 2665.6400000000003, "text": " expression that you now have to handle that one additional thing.", "tokens": [50366, 6114, 300, 291, 586, 362, 281, 4813, 300, 472, 4497, 551, 13, 50504], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 680, "seek": 266284, "start": 2665.88, "end": 2670.2400000000002, "text": " Now, on the other hand, so, so I'm able to do case expressions is handy.", "tokens": [50516, 823, 11, 322, 264, 661, 1011, 11, 370, 11, 370, 286, 478, 1075, 281, 360, 1389, 15277, 307, 13239, 13, 50734], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 681, "seek": 266284, "start": 2670.2400000000002, "end": 2671.2000000000003, "text": " So that's a trade off too.", "tokens": [50734, 407, 300, 311, 257, 4923, 766, 886, 13, 50782], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 682, "seek": 266284, "start": 2671.6000000000004, "end": 2671.76, "text": " Yeah.", "tokens": [50802, 865, 13, 50810], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 683, "seek": 266284, "start": 2671.76, "end": 2678.2400000000002, "text": " But, uh, you mentioned if you remove, uh, a possibility in the API, then that is", "tokens": [50810, 583, 11, 2232, 11, 291, 2835, 498, 291, 4159, 11, 2232, 11, 257, 7959, 294, 264, 9362, 11, 550, 300, 307, 51134], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 684, "seek": 266284, "start": 2678.2400000000002, "end": 2681.08, "text": " also a breaking change for the backend, right?", "tokens": [51134, 611, 257, 7697, 1319, 337, 264, 38087, 11, 558, 30, 51276], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 685, "seek": 266284, "start": 2681.52, "end": 2686.48, "text": " Because you're not, not, you're now not able to call it to call, get", "tokens": [51298, 1436, 291, 434, 406, 11, 406, 11, 291, 434, 586, 406, 1075, 281, 818, 309, 281, 818, 11, 483, 51546], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 686, "seek": 266284, "start": 2686.48, "end": 2690.96, "text": " article with type, uh, some vegetable.", "tokens": [51546, 7222, 365, 2010, 11, 2232, 11, 512, 16356, 13, 51770], "temperature": 0.0, "avg_logprob": -0.23464561911190257, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.009554460644721985}, {"id": 687, "seek": 269096, "start": 2690.96, "end": 2693.52, "text": " I don't know, like vegetables are not sold anymore.", "tokens": [50364, 286, 500, 380, 458, 11, 411, 9320, 366, 406, 3718, 3602, 13, 50492], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 688, "seek": 269096, "start": 2693.52, "end": 2694.84, "text": " Vegetables don't exist anymore.", "tokens": [50492, 28092, 2965, 500, 380, 2514, 3602, 13, 50558], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 689, "seek": 269096, "start": 2695.2, "end": 2697.28, "text": " Well, now you can't send that anymore.", "tokens": [50576, 1042, 11, 586, 291, 393, 380, 2845, 300, 3602, 13, 50680], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 690, "seek": 269096, "start": 2697.36, "end": 2699.16, "text": " And that's a breaking change also.", "tokens": [50684, 400, 300, 311, 257, 7697, 1319, 611, 13, 50774], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 691, "seek": 269096, "start": 2699.16, "end": 2701.8, "text": " So that shouldn't happen in that case.", "tokens": [50774, 407, 300, 4659, 380, 1051, 294, 300, 1389, 13, 50906], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 692, "seek": 269096, "start": 2701.8, "end": 2708.44, "text": " Usually the people go to a V two or V three of that same endpoint, right?", "tokens": [50906, 11419, 264, 561, 352, 281, 257, 691, 732, 420, 691, 1045, 295, 300, 912, 35795, 11, 558, 30, 51238], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 693, "seek": 269096, "start": 2708.76, "end": 2714.36, "text": " So usually what would happen is you would add a new, new, new vegetable type.", "tokens": [51254, 407, 2673, 437, 576, 1051, 307, 291, 576, 909, 257, 777, 11, 777, 11, 777, 16356, 2010, 13, 51534], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 694, "seek": 269096, "start": 2715.16, "end": 2719.44, "text": " Uh, so carrots for whatever reason, you forgot to add carrots in, in your", "tokens": [51574, 4019, 11, 370, 21005, 337, 2035, 1778, 11, 291, 5298, 281, 909, 21005, 294, 11, 294, 428, 51788], "temperature": 0.0, "avg_logprob": -0.2045898926563752, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.0011877950746566057}, {"id": 695, "seek": 271944, "start": 2719.44, "end": 2723.36, "text": " initial release of your backend and you go and you're like, shoot, I forgot those.", "tokens": [50364, 5883, 4374, 295, 428, 38087, 293, 291, 352, 293, 291, 434, 411, 11, 3076, 11, 286, 5298, 729, 13, 50560], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 696, "seek": 271944, "start": 2723.52, "end": 2724.64, "text": " You add carrots.", "tokens": [50568, 509, 909, 21005, 13, 50624], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 697, "seek": 271944, "start": 2724.96, "end": 2727.52, "text": " That's, that's stupid because that's the best vegetable.", "tokens": [50640, 663, 311, 11, 300, 311, 6631, 570, 300, 311, 264, 1151, 16356, 13, 50768], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 698, "seek": 271944, "start": 2727.88, "end": 2729.7200000000003, "text": " I, I is a fantastic vegetable.", "tokens": [50786, 286, 11, 286, 307, 257, 5456, 16356, 13, 50878], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 699, "seek": 271944, "start": 2729.7200000000003, "end": 2731.28, "text": " It's probably up in my top three.", "tokens": [50878, 467, 311, 1391, 493, 294, 452, 1192, 1045, 13, 50956], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 700, "seek": 271944, "start": 2734.32, "end": 2736.2400000000002, "text": " But hey, you know, you were busy.", "tokens": [51108, 583, 4177, 11, 291, 458, 11, 291, 645, 5856, 13, 51204], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 701, "seek": 271944, "start": 2736.28, "end": 2739.76, "text": " You, you were working 12 hour days for whatever reason to get this shipped.", "tokens": [51206, 509, 11, 291, 645, 1364, 2272, 1773, 1708, 337, 2035, 1778, 281, 483, 341, 25312, 13, 51380], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 702, "seek": 271944, "start": 2740.68, "end": 2741.8, "text": " Cause you're full and work.", "tokens": [51426, 10865, 291, 434, 1577, 293, 589, 13, 51482], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 703, "seek": 271944, "start": 2741.8, "end": 2743.16, "text": " Carrots before hands.", "tokens": [51482, 17715, 1971, 949, 2377, 13, 51550], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 704, "seek": 271944, "start": 2743.2000000000003, "end": 2743.7200000000003, "text": " Yup.", "tokens": [51552, 13593, 13, 51578], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 705, "seek": 271944, "start": 2743.76, "end": 2744.48, "text": " Exactly.", "tokens": [51580, 7587, 13, 51616], "temperature": 0.0, "avg_logprob": -0.33228285973813354, "compression_ratio": 1.58, "no_speech_prob": 0.0018099478911608458}, {"id": 706, "seek": 274448, "start": 2744.6, "end": 2752.28, "text": " So you ship it, a customer goes and installs version 1.0.0 of your SDK and they", "tokens": [50370, 407, 291, 5374, 309, 11, 257, 5474, 1709, 293, 3625, 82, 3037, 502, 13, 15, 13, 15, 295, 428, 37135, 293, 436, 50754], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 707, "seek": 274448, "start": 2752.28, "end": 2754.2, "text": " go and they use it and they're like, great.", "tokens": [50754, 352, 293, 436, 764, 309, 293, 436, 434, 411, 11, 869, 13, 50850], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 708, "seek": 274448, "start": 2754.56, "end": 2756.8, "text": " And they don't actually use that vegetable type.", "tokens": [50868, 400, 436, 500, 380, 767, 764, 300, 16356, 2010, 13, 50980], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 709, "seek": 274448, "start": 2756.88, "end": 2760.44, "text": " They don't care about it, but it's still part of the response.", "tokens": [50984, 814, 500, 380, 1127, 466, 309, 11, 457, 309, 311, 920, 644, 295, 264, 4134, 13, 51162], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 710, "seek": 274448, "start": 2760.48, "end": 2762.84, "text": " So the response decoder has to decode it.", "tokens": [51164, 407, 264, 4134, 979, 19866, 575, 281, 979, 1429, 309, 13, 51282], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 711, "seek": 274448, "start": 2763.4, "end": 2766.16, "text": " They don't care about it where they don't care it.", "tokens": [51310, 814, 500, 380, 1127, 466, 309, 689, 436, 500, 380, 1127, 309, 13, 51448], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 712, "seek": 274448, "start": 2767.2400000000002, "end": 2767.56, "text": " Sorry.", "tokens": [51502, 4919, 13, 51518], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 713, "seek": 274448, "start": 2770.04, "end": 2770.4, "text": " Nice.", "tokens": [51642, 5490, 13, 51660], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 714, "seek": 274448, "start": 2770.4, "end": 2770.8, "text": " Nice.", "tokens": [51660, 5490, 13, 51680], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 715, "seek": 274448, "start": 2771.48, "end": 2772.6, "text": " They don't care about it.", "tokens": [51714, 814, 500, 380, 1127, 466, 309, 13, 51770], "temperature": 0.0, "avg_logprob": -0.26144705700273274, "compression_ratio": 1.8465346534653466, "no_speech_prob": 0.0015008237678557634}, {"id": 716, "seek": 277260, "start": 2773.16, "end": 2775.7999999999997, "text": " So they, you go, you add carrots.", "tokens": [50392, 407, 436, 11, 291, 352, 11, 291, 909, 21005, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 717, "seek": 277260, "start": 2776.36, "end": 2778.0, "text": " It's a non-breaking change.", "tokens": [50552, 467, 311, 257, 2107, 12, 20602, 1319, 13, 50634], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 718, "seek": 277260, "start": 2778.0, "end": 2778.2799999999997, "text": " It is.", "tokens": [50634, 467, 307, 13, 50648], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 719, "seek": 277260, "start": 2778.2799999999997, "end": 2779.8399999999997, "text": " It's a minor, minor thing.", "tokens": [50648, 467, 311, 257, 6696, 11, 6696, 551, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 720, "seek": 277260, "start": 2779.8399999999997, "end": 2781.12, "text": " You've added a new type.", "tokens": [50726, 509, 600, 3869, 257, 777, 2010, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 721, "seek": 277260, "start": 2781.44, "end": 2783.24, "text": " Their decoder doesn't support it.", "tokens": [50806, 6710, 979, 19866, 1177, 380, 1406, 309, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 722, "seek": 277260, "start": 2783.6, "end": 2790.44, "text": " So their decoder, if it got carrots would fail because it doesn't recognize it, but", "tokens": [50914, 407, 641, 979, 19866, 11, 498, 309, 658, 21005, 576, 3061, 570, 309, 1177, 380, 5521, 309, 11, 457, 51256], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 723, "seek": 277260, "start": 2790.44, "end": 2791.68, "text": " they're not actually using it.", "tokens": [51256, 436, 434, 406, 767, 1228, 309, 13, 51318], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 724, "seek": 277260, "start": 2791.72, "end": 2799.12, "text": " So now you've broken their, their, their SDK for no, no apparent reason.", "tokens": [51320, 407, 586, 291, 600, 5463, 641, 11, 641, 11, 641, 37135, 337, 572, 11, 572, 18335, 1778, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1583061041655364, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.0005526618333533406}, {"id": 725, "seek": 279912, "start": 2799.68, "end": 2805.2, "text": " But if you wrapped it in just a very small, slim, opaque type that took it as just", "tokens": [50392, 583, 498, 291, 14226, 309, 294, 445, 257, 588, 1359, 11, 25357, 11, 42687, 2010, 300, 1890, 309, 382, 445, 50668], "temperature": 0.0, "avg_logprob": -0.12270772700407068, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.002251573372632265}, {"id": 726, "seek": 279912, "start": 2805.2, "end": 2809.7599999999998, "text": " like unknown, unsupported type, they can keep working.", "tokens": [50668, 411, 9841, 11, 2693, 10504, 14813, 2010, 11, 436, 393, 1066, 1364, 13, 50896], "temperature": 0.0, "avg_logprob": -0.12270772700407068, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.002251573372632265}, {"id": 727, "seek": 279912, "start": 2809.7999999999997, "end": 2810.68, "text": " They're fine.", "tokens": [50898, 814, 434, 2489, 13, 50942], "temperature": 0.0, "avg_logprob": -0.12270772700407068, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.002251573372632265}, {"id": 728, "seek": 279912, "start": 2810.7999999999997, "end": 2814.4, "text": " And everyone else who wants carrots, they still have access to carrots.", "tokens": [50948, 400, 1518, 1646, 567, 2738, 21005, 11, 436, 920, 362, 2105, 281, 21005, 13, 51128], "temperature": 0.0, "avg_logprob": -0.12270772700407068, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.002251573372632265}, {"id": 729, "seek": 279912, "start": 2814.72, "end": 2821.44, "text": " The end user might still be defining a custom type that, you know, they're", "tokens": [51144, 440, 917, 4195, 1062, 920, 312, 17827, 257, 2375, 2010, 300, 11, 291, 458, 11, 436, 434, 51480], "temperature": 0.0, "avg_logprob": -0.12270772700407068, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.002251573372632265}, {"id": 730, "seek": 279912, "start": 2821.44, "end": 2826.0, "text": " going to be adding carrots to and handling that in their own code, right?", "tokens": [51480, 516, 281, 312, 5127, 21005, 281, 293, 13175, 300, 294, 641, 1065, 3089, 11, 558, 30, 51708], "temperature": 0.0, "avg_logprob": -0.12270772700407068, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.002251573372632265}, {"id": 731, "seek": 282600, "start": 2826.0, "end": 2829.84, "text": " So in a way it, it passes the buck downstream.", "tokens": [50364, 407, 294, 257, 636, 309, 11, 309, 11335, 264, 14894, 30621, 13, 50556], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 732, "seek": 282600, "start": 2830.12, "end": 2834.08, "text": " So it's, it's an, it's an inherently challenging question.", "tokens": [50570, 407, 309, 311, 11, 309, 311, 364, 11, 309, 311, 364, 27993, 7595, 1168, 13, 50768], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 733, "seek": 282600, "start": 2834.64, "end": 2837.12, "text": " There's no easy magic bullet to it.", "tokens": [50796, 821, 311, 572, 1858, 5585, 11632, 281, 309, 13, 50920], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 734, "seek": 282600, "start": 2837.32, "end": 2837.52, "text": " Yeah.", "tokens": [50930, 865, 13, 50940], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 735, "seek": 282600, "start": 2837.68, "end": 2843.8, "text": " If a new possibility arises, like a carrot gets introduced by the back end, then", "tokens": [50948, 759, 257, 777, 7959, 27388, 11, 411, 257, 22767, 2170, 7268, 538, 264, 646, 917, 11, 550, 51254], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 736, "seek": 282600, "start": 2843.8, "end": 2848.36, "text": " your whole front end doesn't compile anymore because you've upgraded this back.", "tokens": [51254, 428, 1379, 1868, 917, 1177, 380, 31413, 3602, 570, 291, 600, 24133, 341, 646, 13, 51482], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 737, "seek": 282600, "start": 2848.88, "end": 2853.16, "text": " And now the front end needs to real quick handle that case.", "tokens": [51508, 400, 586, 264, 1868, 917, 2203, 281, 957, 1702, 4813, 300, 1389, 13, 51722], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 738, "seek": 282600, "start": 2853.56, "end": 2855.72, "text": " I think there's, so there is an edge case here.", "tokens": [51742, 286, 519, 456, 311, 11, 370, 456, 307, 364, 4691, 1389, 510, 13, 51850], "temperature": 0.0, "avg_logprob": -0.183265753551922, "compression_ratio": 1.5938697318007662, "no_speech_prob": 0.005300865042954683}, {"id": 739, "seek": 285600, "start": 2856.24, "end": 2862.24, "text": " If you are working with open APIs internally within your company, odds are you", "tokens": [50376, 759, 291, 366, 1364, 365, 1269, 21445, 19501, 1951, 428, 2237, 11, 17439, 366, 291, 50676], "temperature": 0.0, "avg_logprob": -0.1433246750192544, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00034595123725011945}, {"id": 740, "seek": 285600, "start": 2862.24, "end": 2867.0, "text": " do want the custom type, regardless if it's opaque or not, you do want, you", "tokens": [50676, 360, 528, 264, 2375, 2010, 11, 10060, 498, 309, 311, 42687, 420, 406, 11, 291, 360, 528, 11, 291, 50914], "temperature": 0.0, "avg_logprob": -0.1433246750192544, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00034595123725011945}, {"id": 741, "seek": 285600, "start": 2867.0, "end": 2869.24, "text": " don't want to use stringly typed enums.", "tokens": [50914, 500, 380, 528, 281, 764, 6798, 356, 33941, 465, 8099, 13, 51026], "temperature": 0.0, "avg_logprob": -0.1433246750192544, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00034595123725011945}, {"id": 742, "seek": 285600, "start": 2869.32, "end": 2874.6, "text": " You want the custom type because if, if your back end does change something, you", "tokens": [51030, 509, 528, 264, 2375, 2010, 570, 498, 11, 498, 428, 646, 917, 775, 1319, 746, 11, 291, 51294], "temperature": 0.0, "avg_logprob": -0.1433246750192544, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00034595123725011945}, {"id": 743, "seek": 285600, "start": 2874.6, "end": 2876.28, "text": " want that reflected in your front end.", "tokens": [51294, 528, 300, 15502, 294, 428, 1868, 917, 13, 51378], "temperature": 0.0, "avg_logprob": -0.1433246750192544, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00034595123725011945}, {"id": 744, "seek": 285600, "start": 2876.56, "end": 2882.48, "text": " However, if you are using this between companies, possibly between multiple", "tokens": [51392, 2908, 11, 498, 291, 366, 1228, 341, 1296, 3431, 11, 6264, 1296, 3866, 51688], "temperature": 0.0, "avg_logprob": -0.1433246750192544, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.00034595123725011945}, {"id": 745, "seek": 288248, "start": 2882.48, "end": 2888.92, "text": " companies like, like Spotify or something like that, then the custom type isn't, I", "tokens": [50364, 3431, 411, 11, 411, 29036, 420, 746, 411, 300, 11, 550, 264, 2375, 2010, 1943, 380, 11, 286, 50686], "temperature": 0.0, "avg_logprob": -0.1629815588192064, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008829289581626654}, {"id": 746, "seek": 288248, "start": 2888.92, "end": 2893.16, "text": " think there's more risk involved with the custom type in terms of breaking the", "tokens": [50686, 519, 456, 311, 544, 3148, 3288, 365, 264, 2375, 2010, 294, 2115, 295, 7697, 264, 50898], "temperature": 0.0, "avg_logprob": -0.1629815588192064, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008829289581626654}, {"id": 747, "seek": 288248, "start": 2893.16, "end": 2897.32, "text": " customers, the customer's application, which you don't want to do.", "tokens": [50898, 4581, 11, 264, 5474, 311, 3861, 11, 597, 291, 500, 380, 528, 281, 360, 13, 51106], "temperature": 0.0, "avg_logprob": -0.1629815588192064, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008829289581626654}, {"id": 748, "seek": 288248, "start": 2897.8, "end": 2902.64, "text": " There is still the edge case that they could forget to or customly handle", "tokens": [51130, 821, 307, 920, 264, 4691, 1389, 300, 436, 727, 2870, 281, 420, 2375, 356, 4813, 51372], "temperature": 0.0, "avg_logprob": -0.1629815588192064, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008829289581626654}, {"id": 749, "seek": 288248, "start": 2902.96, "end": 2907.92, "text": " carrots or rutabagas or whatever else they decide they need to support or you", "tokens": [51388, 21005, 420, 41324, 455, 559, 296, 420, 2035, 1646, 436, 4536, 436, 643, 281, 1406, 420, 291, 51636], "temperature": 0.0, "avg_logprob": -0.1629815588192064, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008829289581626654}, {"id": 750, "seek": 288248, "start": 2907.92, "end": 2909.0, "text": " forgot to support.", "tokens": [51636, 5298, 281, 1406, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1629815588192064, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008829289581626654}, {"id": 751, "seek": 290900, "start": 2909.44, "end": 2912.08, "text": " But that's like one of the worst vegetables.", "tokens": [50386, 583, 300, 311, 411, 472, 295, 264, 5855, 9320, 13, 50518], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 752, "seek": 290900, "start": 2914.56, "end": 2916.4, "text": " Oh, poor Vegas.", "tokens": [50642, 876, 11, 4716, 15841, 13, 50734], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 753, "seek": 290900, "start": 2917.84, "end": 2918.6, "text": " They deserve it.", "tokens": [50806, 814, 9948, 309, 13, 50844], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 754, "seek": 290900, "start": 2920.12, "end": 2924.12, "text": " But yeah, you don't want to, you don't want to break your customers and point, and", "tokens": [50920, 583, 1338, 11, 291, 500, 380, 528, 281, 11, 291, 500, 380, 528, 281, 1821, 428, 4581, 293, 935, 11, 293, 51120], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 755, "seek": 290900, "start": 2924.12, "end": 2925.32, "text": " they might not have the time.", "tokens": [51120, 436, 1062, 406, 362, 264, 565, 13, 51180], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 756, "seek": 290900, "start": 2925.32, "end": 2930.0, "text": " There isn't the same level of communication that you have between teams within a company.", "tokens": [51180, 821, 1943, 380, 264, 912, 1496, 295, 6101, 300, 291, 362, 1296, 5491, 1951, 257, 2237, 13, 51414], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 757, "seek": 290900, "start": 2930.12, "end": 2935.76, "text": " So maybe that's something that is like a flag maybe on or in your config for when", "tokens": [51420, 407, 1310, 300, 311, 746, 300, 307, 411, 257, 7166, 1310, 322, 420, 294, 428, 6662, 337, 562, 51702], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 758, "seek": 290900, "start": 2935.76, "end": 2937.68, "text": " you generate your own SDK from this.", "tokens": [51702, 291, 8460, 428, 1065, 37135, 490, 341, 13, 51798], "temperature": 0.0, "avg_logprob": -0.27151317949648257, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004330750554800034}, {"id": 759, "seek": 293768, "start": 2938.3199999999997, "end": 2942.96, "text": " Maybe I have something that says, I want custom types for enums or I want strings", "tokens": [50396, 2704, 286, 362, 746, 300, 1619, 11, 286, 528, 2375, 3467, 337, 465, 8099, 420, 286, 528, 13985, 50628], "temperature": 0.0, "avg_logprob": -0.208615072842302, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.0013248492032289505}, {"id": 760, "seek": 293768, "start": 2942.96, "end": 2950.24, "text": " for enums and let, let them choose or let them override to whatever fits their product", "tokens": [50628, 337, 465, 8099, 293, 718, 11, 718, 552, 2826, 420, 718, 552, 42321, 281, 2035, 9001, 641, 1674, 50992], "temperature": 0.0, "avg_logprob": -0.208615072842302, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.0013248492032289505}, {"id": 761, "seek": 293768, "start": 2950.24, "end": 2950.9199999999996, "text": " the best.", "tokens": [50992, 264, 1151, 13, 51026], "temperature": 0.0, "avg_logprob": -0.208615072842302, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.0013248492032289505}, {"id": 762, "seek": 293768, "start": 2951.24, "end": 2956.3999999999996, "text": " At work, we've had some, we use Elm GraphQL Dillon's version.", "tokens": [51042, 1711, 589, 11, 321, 600, 632, 512, 11, 321, 764, 2699, 76, 21884, 13695, 28160, 311, 3037, 13, 51300], "temperature": 0.0, "avg_logprob": -0.208615072842302, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.0013248492032289505}, {"id": 763, "seek": 293768, "start": 2956.9199999999996, "end": 2960.2, "text": " And some of the things are using custom types.", "tokens": [51326, 400, 512, 295, 264, 721, 366, 1228, 2375, 3467, 13, 51490], "temperature": 0.0, "avg_logprob": -0.208615072842302, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.0013248492032289505}, {"id": 764, "seek": 293768, "start": 2960.7999999999997, "end": 2962.7599999999998, "text": " And that creates some problems.", "tokens": [51520, 400, 300, 7829, 512, 2740, 13, 51618], "temperature": 0.0, "avg_logprob": -0.208615072842302, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.0013248492032289505}, {"id": 765, "seek": 296276, "start": 2963.4, "end": 2969.28, "text": " For instance, when we do migrations, like the backend migrates to a new version where", "tokens": [50396, 1171, 5197, 11, 562, 321, 360, 6186, 12154, 11, 411, 264, 38087, 6186, 12507, 281, 257, 777, 3037, 689, 50690], "temperature": 0.0, "avg_logprob": -0.21732096152730507, "compression_ratio": 1.7119341563786008, "no_speech_prob": 0.02160351537168026}, {"id": 766, "seek": 296276, "start": 2969.28, "end": 2974.6000000000004, "text": " it needs to support having older versions or newer versions of the front end talking", "tokens": [50690, 309, 2203, 281, 1406, 1419, 4906, 9606, 420, 17628, 9606, 295, 264, 1868, 917, 1417, 50956], "temperature": 0.0, "avg_logprob": -0.21732096152730507, "compression_ratio": 1.7119341563786008, "no_speech_prob": 0.02160351537168026}, {"id": 767, "seek": 296276, "start": 2974.6000000000004, "end": 2975.96, "text": " to it or the other way around.", "tokens": [50956, 281, 309, 420, 264, 661, 636, 926, 13, 51024], "temperature": 0.0, "avg_logprob": -0.21732096152730507, "compression_ratio": 1.7119341563786008, "no_speech_prob": 0.02160351537168026}, {"id": 768, "seek": 296276, "start": 2975.96, "end": 2980.6000000000004, "text": " Like you've seen Mario Rogers talk like all those variations.", "tokens": [51024, 1743, 291, 600, 1612, 9343, 29877, 751, 411, 439, 729, 17840, 13, 51256], "temperature": 0.0, "avg_logprob": -0.21732096152730507, "compression_ratio": 1.7119341563786008, "no_speech_prob": 0.02160351537168026}, {"id": 769, "seek": 296276, "start": 2981.5200000000004, "end": 2983.2400000000002, "text": " Well, we need to support those.", "tokens": [51302, 1042, 11, 321, 643, 281, 1406, 729, 13, 51388], "temperature": 0.0, "avg_logprob": -0.21732096152730507, "compression_ratio": 1.7119341563786008, "no_speech_prob": 0.02160351537168026}, {"id": 770, "seek": 296276, "start": 2983.5600000000004, "end": 2989.88, "text": " And therefore we are thinking of like moving those custom types into stringified, stringified", "tokens": [51404, 400, 4412, 321, 366, 1953, 295, 411, 2684, 729, 2375, 3467, 666, 6798, 2587, 11, 6798, 2587, 51720], "temperature": 0.0, "avg_logprob": -0.21732096152730507, "compression_ratio": 1.7119341563786008, "no_speech_prob": 0.02160351537168026}, {"id": 771, "seek": 296276, "start": 2990.7200000000003, "end": 2992.44, "text": " types, which is not great.", "tokens": [51762, 3467, 11, 597, 307, 406, 869, 13, 51848], "temperature": 0.0, "avg_logprob": -0.21732096152730507, "compression_ratio": 1.7119341563786008, "no_speech_prob": 0.02160351537168026}, {"id": 772, "seek": 299276, "start": 2992.88, "end": 2995.2000000000003, "text": " But it is a lot more flexible.", "tokens": [50370, 583, 309, 307, 257, 688, 544, 11358, 13, 50486], "temperature": 0.0, "avg_logprob": -0.16066895931138905, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0006164361839182675}, {"id": 773, "seek": 299276, "start": 2995.4, "end": 2998.88, "text": " It is a lot easier to ignore some things that are unknown.", "tokens": [50496, 467, 307, 257, 688, 3571, 281, 11200, 512, 721, 300, 366, 9841, 13, 50670], "temperature": 0.0, "avg_logprob": -0.16066895931138905, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0006164361839182675}, {"id": 774, "seek": 299276, "start": 2999.32, "end": 3003.2400000000002, "text": " And you just say, okay, well, the front end doesn't know about carrots.", "tokens": [50692, 400, 291, 445, 584, 11, 1392, 11, 731, 11, 264, 1868, 917, 1177, 380, 458, 466, 21005, 13, 50888], "temperature": 0.0, "avg_logprob": -0.16066895931138905, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0006164361839182675}, {"id": 775, "seek": 299276, "start": 3003.6800000000003, "end": 3007.76, "text": " But if it gets a carrot, what should, what should we do?", "tokens": [50910, 583, 498, 309, 2170, 257, 22767, 11, 437, 820, 11, 437, 820, 321, 360, 30, 51114], "temperature": 0.0, "avg_logprob": -0.16066895931138905, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0006164361839182675}, {"id": 776, "seek": 299276, "start": 3007.76, "end": 3012.8, "text": " Should we just ignore the message or should we crash hard?", "tokens": [51114, 6454, 321, 445, 11200, 264, 3636, 420, 820, 321, 8252, 1152, 30, 51366], "temperature": 0.0, "avg_logprob": -0.16066895931138905, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0006164361839182675}, {"id": 777, "seek": 299276, "start": 3013.2400000000002, "end": 3016.2400000000002, "text": " And in some cases, it's fine to just ignore it.", "tokens": [51388, 400, 294, 512, 3331, 11, 309, 311, 2489, 281, 445, 11200, 309, 13, 51538], "temperature": 0.0, "avg_logprob": -0.16066895931138905, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0006164361839182675}, {"id": 778, "seek": 299276, "start": 3017.1200000000003, "end": 3019.8, "text": " It really depends on the use case, obviously, or the situation.", "tokens": [51582, 467, 534, 5946, 322, 264, 764, 1389, 11, 2745, 11, 420, 264, 2590, 13, 51716], "temperature": 0.0, "avg_logprob": -0.16066895931138905, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.0006164361839182675}, {"id": 779, "seek": 301980, "start": 3020.2400000000002, "end": 3023.2000000000003, "text": " So we also use GraphQL at work.", "tokens": [50386, 407, 321, 611, 764, 21884, 13695, 412, 589, 13, 50534], "temperature": 0.0, "avg_logprob": -0.15259067311006433, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.0005032730987295508}, {"id": 780, "seek": 301980, "start": 3023.6400000000003, "end": 3031.48, "text": " And I think the way we handle that is we generate a hash, essentially, of our GraphQL", "tokens": [50556, 400, 286, 519, 264, 636, 321, 4813, 300, 307, 321, 8460, 257, 22019, 11, 4476, 11, 295, 527, 21884, 13695, 50948], "temperature": 0.0, "avg_logprob": -0.15259067311006433, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.0005032730987295508}, {"id": 781, "seek": 301980, "start": 3031.48, "end": 3038.2000000000003, "text": " schema or whole GraphQL schema and just upload that as a single file and then use", "tokens": [50948, 34078, 420, 1379, 21884, 13695, 34078, 293, 445, 6580, 300, 382, 257, 2167, 3991, 293, 550, 764, 51284], "temperature": 0.0, "avg_logprob": -0.15259067311006433, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.0005032730987295508}, {"id": 782, "seek": 301980, "start": 3038.2000000000003, "end": 3039.52, "text": " that as a versioning thing.", "tokens": [51284, 300, 382, 257, 3037, 278, 551, 13, 51350], "temperature": 0.0, "avg_logprob": -0.15259067311006433, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.0005032730987295508}, {"id": 783, "seek": 301980, "start": 3039.52, "end": 3047.6400000000003, "text": " And if the front end sees that that has changed, then it will refresh the page, essentially.", "tokens": [51350, 400, 498, 264, 1868, 917, 8194, 300, 300, 575, 3105, 11, 550, 309, 486, 15134, 264, 3028, 11, 4476, 13, 51756], "temperature": 0.0, "avg_logprob": -0.15259067311006433, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.0005032730987295508}, {"id": 784, "seek": 304764, "start": 3048.0, "end": 3054.08, "text": " Kind of like if you're on Slack on the browser or Discord or many other messaging apps, and", "tokens": [50382, 9242, 295, 411, 498, 291, 434, 322, 37211, 322, 264, 11185, 420, 32623, 420, 867, 661, 21812, 7733, 11, 293, 50686], "temperature": 0.0, "avg_logprob": -0.18092559587837445, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.02296958863735199}, {"id": 785, "seek": 304764, "start": 3054.08, "end": 3055.52, "text": " they're always like, a new version is available.", "tokens": [50686, 436, 434, 1009, 411, 11, 257, 777, 3037, 307, 2435, 13, 50758], "temperature": 0.0, "avg_logprob": -0.18092559587837445, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.02296958863735199}, {"id": 786, "seek": 304764, "start": 3055.52, "end": 3059.04, "text": " Click this button to update when it just reloads the page.", "tokens": [50758, 8230, 341, 2960, 281, 5623, 562, 309, 445, 25628, 82, 264, 3028, 13, 50934], "temperature": 0.0, "avg_logprob": -0.18092559587837445, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.02296958863735199}, {"id": 787, "seek": 304764, "start": 3059.8799999999997, "end": 3069.04, "text": " Or Discord, which will just not let you look at anything if it's outdated, which I get.", "tokens": [50976, 1610, 32623, 11, 597, 486, 445, 406, 718, 291, 574, 412, 1340, 498, 309, 311, 36313, 11, 597, 286, 483, 13, 51434], "temperature": 0.0, "avg_logprob": -0.18092559587837445, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.02296958863735199}, {"id": 788, "seek": 304764, "start": 3069.12, "end": 3070.24, "text": " I totally get that.", "tokens": [51438, 286, 3879, 483, 300, 13, 51494], "temperature": 0.0, "avg_logprob": -0.18092559587837445, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.02296958863735199}, {"id": 789, "seek": 304764, "start": 3070.3199999999997, "end": 3071.2, "text": " That's fair.", "tokens": [51498, 663, 311, 3143, 13, 51542], "temperature": 0.0, "avg_logprob": -0.18092559587837445, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.02296958863735199}, {"id": 790, "seek": 304764, "start": 3071.6, "end": 3074.04, "text": " Well, it really depends on the situation, right?", "tokens": [51562, 1042, 11, 309, 534, 5946, 322, 264, 2590, 11, 558, 30, 51684], "temperature": 0.0, "avg_logprob": -0.18092559587837445, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.02296958863735199}, {"id": 791, "seek": 307404, "start": 3074.04, "end": 3079.32, "text": " Because if you're on Discord, you're typing a message and it says, oh, I need to update.", "tokens": [50364, 1436, 498, 291, 434, 322, 32623, 11, 291, 434, 18444, 257, 3636, 293, 309, 1619, 11, 1954, 11, 286, 643, 281, 5623, 13, 50628], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 792, "seek": 307404, "start": 3079.36, "end": 3081.16, "text": " Okay, well, let's refresh.", "tokens": [50630, 1033, 11, 731, 11, 718, 311, 15134, 13, 50720], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 793, "seek": 307404, "start": 3081.2, "end": 3084.32, "text": " Your message is still in the box.", "tokens": [50722, 2260, 3636, 307, 920, 294, 264, 2424, 13, 50878], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 794, "seek": 307404, "start": 3084.96, "end": 3085.84, "text": " It's fine.", "tokens": [50910, 467, 311, 2489, 13, 50954], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 795, "seek": 307404, "start": 3086.48, "end": 3092.6, "text": " But if you're editing a very complex form, you don't want to lose your things.", "tokens": [50986, 583, 498, 291, 434, 10000, 257, 588, 3997, 1254, 11, 291, 500, 380, 528, 281, 3624, 428, 721, 13, 51292], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 796, "seek": 307404, "start": 3092.6, "end": 3099.0, "text": " If you're doing something that takes a long time to set up somehow, then you don't want to lose that.", "tokens": [51292, 759, 291, 434, 884, 746, 300, 2516, 257, 938, 565, 281, 992, 493, 6063, 11, 550, 291, 500, 380, 528, 281, 3624, 300, 13, 51612], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 797, "seek": 307404, "start": 3099.24, "end": 3099.48, "text": " Right?", "tokens": [51624, 1779, 30, 51636], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 798, "seek": 307404, "start": 3099.84, "end": 3101.72, "text": " I think it depends on the size of the company, too.", "tokens": [51654, 286, 519, 309, 5946, 322, 264, 2744, 295, 264, 2237, 11, 886, 13, 51748], "temperature": 0.0, "avg_logprob": -0.18216048168534993, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.004753472283482552}, {"id": 799, "seek": 310172, "start": 3101.7599999999998, "end": 3104.3599999999997, "text": " I don't know how big you're at CrowdStrike, right?", "tokens": [50366, 286, 500, 380, 458, 577, 955, 291, 434, 412, 40110, 4520, 40669, 11, 558, 30, 50496], "temperature": 0.0, "avg_logprob": -0.1629989091740098, "compression_ratio": 1.7890625, "no_speech_prob": 0.001476639648899436}, {"id": 800, "seek": 310172, "start": 3104.56, "end": 3109.04, "text": " And I don't know how big CrowdStrike is, but if you get to the point where you have multiple", "tokens": [50506, 400, 286, 500, 380, 458, 577, 955, 40110, 4520, 40669, 307, 11, 457, 498, 291, 483, 281, 264, 935, 689, 291, 362, 3866, 50730], "temperature": 0.0, "avg_logprob": -0.1629989091740098, "compression_ratio": 1.7890625, "no_speech_prob": 0.001476639648899436}, {"id": 801, "seek": 310172, "start": 3109.04, "end": 3115.2799999999997, "text": " back-end teams interfacing with multiple front-end apps and one of them wants to change an enum", "tokens": [50730, 646, 12, 521, 5491, 14510, 5615, 365, 3866, 1868, 12, 521, 7733, 293, 472, 295, 552, 2738, 281, 1319, 364, 465, 449, 51042], "temperature": 0.0, "avg_logprob": -0.1629989091740098, "compression_ratio": 1.7890625, "no_speech_prob": 0.001476639648899436}, {"id": 802, "seek": 310172, "start": 3115.64, "end": 3120.56, "text": " and add or remove that enum, that can be very difficult to coordinate across teams.", "tokens": [51060, 293, 909, 420, 4159, 300, 465, 449, 11, 300, 393, 312, 588, 2252, 281, 15670, 2108, 5491, 13, 51306], "temperature": 0.0, "avg_logprob": -0.1629989091740098, "compression_ratio": 1.7890625, "no_speech_prob": 0.001476639648899436}, {"id": 803, "seek": 310172, "start": 3121.7599999999998, "end": 3125.6, "text": " A lot more difficult than one back-end team and one front-end team, essentially.", "tokens": [51366, 316, 688, 544, 2252, 813, 472, 646, 12, 521, 1469, 293, 472, 1868, 12, 521, 1469, 11, 4476, 13, 51558], "temperature": 0.0, "avg_logprob": -0.1629989091740098, "compression_ratio": 1.7890625, "no_speech_prob": 0.001476639648899436}, {"id": 804, "seek": 310172, "start": 3125.6, "end": 3128.7999999999997, "text": " Or like one front-end app and one back-end app.", "tokens": [51558, 1610, 411, 472, 1868, 12, 521, 724, 293, 472, 646, 12, 521, 724, 13, 51718], "temperature": 0.0, "avg_logprob": -0.1629989091740098, "compression_ratio": 1.7890625, "no_speech_prob": 0.001476639648899436}, {"id": 805, "seek": 310172, "start": 3129.0, "end": 3129.3599999999997, "text": " Yeah.", "tokens": [51728, 865, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1629989091740098, "compression_ratio": 1.7890625, "no_speech_prob": 0.001476639648899436}, {"id": 806, "seek": 312936, "start": 3129.48, "end": 3132.4, "text": " So it mostly depends on who is your customer.", "tokens": [50370, 407, 309, 5240, 5946, 322, 567, 307, 428, 5474, 13, 50516], "temperature": 0.0, "avg_logprob": -0.275148868560791, "compression_ratio": 1.8376068376068375, "no_speech_prob": 0.009526053443551064}, {"id": 807, "seek": 312936, "start": 3132.4, "end": 3139.84, "text": " Is your customer the same mono repo application with one back-end, one front-end, and then they're", "tokens": [50516, 1119, 428, 5474, 264, 912, 35624, 49040, 3861, 365, 472, 646, 12, 521, 11, 472, 1868, 12, 521, 11, 293, 550, 436, 434, 50888], "temperature": 0.0, "avg_logprob": -0.275148868560791, "compression_ratio": 1.8376068376068375, "no_speech_prob": 0.009526053443551064}, {"id": 808, "seek": 312936, "start": 3139.84, "end": 3146.28, "text": " always in sync because there's always the same person who makes the back-end changes knows how", "tokens": [50888, 1009, 294, 20271, 570, 456, 311, 1009, 264, 912, 954, 567, 1669, 264, 646, 12, 521, 2962, 3255, 577, 51210], "temperature": 0.0, "avg_logprob": -0.275148868560791, "compression_ratio": 1.8376068376068375, "no_speech_prob": 0.009526053443551064}, {"id": 809, "seek": 312936, "start": 3146.28, "end": 3148.04, "text": " to do the front-end changes as well.", "tokens": [51210, 281, 360, 264, 1868, 12, 521, 2962, 382, 731, 13, 51298], "temperature": 0.0, "avg_logprob": -0.275148868560791, "compression_ratio": 1.8376068376068375, "no_speech_prob": 0.009526053443551064}, {"id": 810, "seek": 312936, "start": 3148.6400000000003, "end": 3155.32, "text": " Or do you have back-end that is not always in sync with front-end or you use web components", "tokens": [51328, 1610, 360, 291, 362, 646, 12, 521, 300, 307, 406, 1009, 294, 20271, 365, 1868, 12, 521, 420, 291, 764, 3670, 6677, 51662], "temperature": 0.0, "avg_logprob": -0.275148868560791, "compression_ratio": 1.8376068376068375, "no_speech_prob": 0.009526053443551064}, {"id": 811, "seek": 312936, "start": 3155.32, "end": 3159.28, "text": " or you're shipping part of your products in not the same way?", "tokens": [51662, 420, 291, 434, 14122, 644, 295, 428, 3383, 294, 406, 264, 912, 636, 30, 51860], "temperature": 0.0, "avg_logprob": -0.275148868560791, "compression_ratio": 1.8376068376068375, "no_speech_prob": 0.009526053443551064}, {"id": 812, "seek": 315928, "start": 3159.4, "end": 3159.84, "text": " So yeah.", "tokens": [50370, 407, 1338, 13, 50392], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 813, "seek": 315928, "start": 3160.1200000000003, "end": 3162.2400000000002, "text": " So the flexibility, right?", "tokens": [50406, 407, 264, 12635, 11, 558, 30, 50512], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 814, "seek": 315928, "start": 3163.2400000000002, "end": 3165.84, "text": " Yeah, it's the hard part.", "tokens": [50562, 865, 11, 309, 311, 264, 1152, 644, 13, 50692], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 815, "seek": 315928, "start": 3165.84, "end": 3170.0400000000004, "text": " I always like to view it as the hard part is communication.", "tokens": [50692, 286, 1009, 411, 281, 1910, 309, 382, 264, 1152, 644, 307, 6101, 13, 50902], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 816, "seek": 315928, "start": 3170.0800000000004, "end": 3174.0, "text": " And if the communication is hard, maybe strings are easier.", "tokens": [50904, 400, 498, 264, 6101, 307, 1152, 11, 1310, 13985, 366, 3571, 13, 51100], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 817, "seek": 315928, "start": 3174.48, "end": 3175.8, "text": " Communication is easy.", "tokens": [51124, 34930, 307, 1858, 13, 51190], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 818, "seek": 315928, "start": 3175.92, "end": 3177.5600000000004, "text": " Then go with custom types when you can.", "tokens": [51196, 1396, 352, 365, 2375, 3467, 562, 291, 393, 13, 51278], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 819, "seek": 315928, "start": 3177.7200000000003, "end": 3181.48, "text": " I mean, you communicate with chat GPT only with text.", "tokens": [51286, 286, 914, 11, 291, 7890, 365, 5081, 26039, 51, 787, 365, 2487, 13, 51474], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 820, "seek": 315928, "start": 3181.8, "end": 3182.2000000000003, "text": " Text.", "tokens": [51490, 18643, 13, 51510], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 821, "seek": 315928, "start": 3183.2400000000002, "end": 3184.8, "text": " Image is nowadays as well, right?", "tokens": [51562, 29903, 307, 13434, 382, 731, 11, 558, 30, 51640], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 822, "seek": 315928, "start": 3185.0400000000004, "end": 3185.6000000000004, "text": " But yeah.", "tokens": [51652, 583, 1338, 13, 51680], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 823, "seek": 315928, "start": 3185.6400000000003, "end": 3185.92, "text": " Yeah.", "tokens": [51682, 865, 13, 51696], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 824, "seek": 315928, "start": 3185.92, "end": 3186.2400000000002, "text": " Yeah.", "tokens": [51696, 865, 13, 51712], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 825, "seek": 315928, "start": 3186.32, "end": 3186.6000000000004, "text": " Yeah.", "tokens": [51716, 865, 13, 51730], "temperature": 0.0, "avg_logprob": -0.4117437509390024, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0016998385544866323}, {"id": 826, "seek": 318660, "start": 3187.08, "end": 3194.44, "text": " I was also thinking too, other things I've gotten to look at using this with, I know", "tokens": [50388, 286, 390, 611, 1953, 886, 11, 661, 721, 286, 600, 5768, 281, 574, 412, 1228, 341, 365, 11, 286, 458, 50756], "temperature": 0.0, "avg_logprob": -0.1903231143951416, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.00780670577660203}, {"id": 827, "seek": 318660, "start": 3194.44, "end": 3200.24, "text": " super base is a back-end service like database slash back-end as a service.", "tokens": [50756, 1687, 3096, 307, 257, 646, 12, 521, 2643, 411, 8149, 17330, 646, 12, 521, 382, 257, 2643, 13, 51046], "temperature": 0.0, "avg_logprob": -0.1903231143951416, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.00780670577660203}, {"id": 828, "seek": 318660, "start": 3200.7999999999997, "end": 3205.96, "text": " You can also, I believe generate, I think they're swagger specs, but you can generate", "tokens": [51074, 509, 393, 611, 11, 286, 1697, 8460, 11, 286, 519, 436, 434, 1693, 11062, 27911, 11, 457, 291, 393, 8460, 51332], "temperature": 0.0, "avg_logprob": -0.1903231143951416, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.00780670577660203}, {"id": 829, "seek": 318660, "start": 3205.96, "end": 3212.6, "text": " those from it, which means you could build an L map on top of super base with generating", "tokens": [51332, 729, 490, 309, 11, 597, 1355, 291, 727, 1322, 364, 441, 4471, 322, 1192, 295, 1687, 3096, 365, 17746, 51664], "temperature": 0.0, "avg_logprob": -0.1903231143951416, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.00780670577660203}, {"id": 830, "seek": 318660, "start": 3212.6, "end": 3216.2, "text": " all the rest end points or the rest SDKs for your front-end.", "tokens": [51664, 439, 264, 1472, 917, 2793, 420, 264, 1472, 37135, 82, 337, 428, 1868, 12, 521, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1903231143951416, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.00780670577660203}, {"id": 831, "seek": 321660, "start": 3216.6, "end": 3218.72, "text": " I started experimenting with it, but yeah.", "tokens": [50364, 286, 1409, 29070, 365, 309, 11, 457, 1338, 13, 50470], "temperature": 0.0, "avg_logprob": -0.1914611745763708, "compression_ratio": 1.576, "no_speech_prob": 0.000269425509031862}, {"id": 832, "seek": 321660, "start": 3219.3199999999997, "end": 3222.64, "text": " Again, have no business business ideas to actually build with it.", "tokens": [50500, 3764, 11, 362, 572, 1606, 1606, 3487, 281, 767, 1322, 365, 309, 13, 50666], "temperature": 0.0, "avg_logprob": -0.1914611745763708, "compression_ratio": 1.576, "no_speech_prob": 0.000269425509031862}, {"id": 833, "seek": 321660, "start": 3223.92, "end": 3231.08, "text": " Is your Elm open API the first time that a production ready version has been shipped?", "tokens": [50730, 1119, 428, 2699, 76, 1269, 9362, 264, 700, 565, 300, 257, 4265, 1919, 3037, 575, 668, 25312, 30, 51088], "temperature": 0.0, "avg_logprob": -0.1914611745763708, "compression_ratio": 1.576, "no_speech_prob": 0.000269425509031862}, {"id": 834, "seek": 321660, "start": 3231.3199999999997, "end": 3231.64, "text": " Like that.", "tokens": [51100, 1743, 300, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1914611745763708, "compression_ratio": 1.576, "no_speech_prob": 0.000269425509031862}, {"id": 835, "seek": 321660, "start": 3231.68, "end": 3235.6, "text": " I feel like there have been murmurings of this for a long time.", "tokens": [51118, 286, 841, 411, 456, 362, 668, 39729, 374, 1109, 295, 341, 337, 257, 938, 565, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1914611745763708, "compression_ratio": 1.576, "no_speech_prob": 0.000269425509031862}, {"id": 836, "seek": 321660, "start": 3235.64, "end": 3239.68, "text": " I mean, it's like an obvious fit for Elm because of its types.", "tokens": [51316, 286, 914, 11, 309, 311, 411, 364, 6322, 3318, 337, 2699, 76, 570, 295, 1080, 3467, 13, 51518], "temperature": 0.0, "avg_logprob": -0.1914611745763708, "compression_ratio": 1.576, "no_speech_prob": 0.000269425509031862}, {"id": 837, "seek": 321660, "start": 3239.88, "end": 3246.44, "text": " There is a, if you're talking open API and Elm, specifically,", "tokens": [51528, 821, 307, 257, 11, 498, 291, 434, 1417, 1269, 9362, 293, 2699, 76, 11, 4682, 11, 51856], "temperature": 0.0, "avg_logprob": -0.1914611745763708, "compression_ratio": 1.576, "no_speech_prob": 0.000269425509031862}, {"id": 838, "seek": 324644, "start": 3246.48, "end": 3248.6, "text": " there is a Java based one.", "tokens": [50366, 456, 307, 257, 10745, 2361, 472, 13, 50472], "temperature": 0.0, "avg_logprob": -0.2394651867094494, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0005614060210064054}, {"id": 839, "seek": 324644, "start": 3249.88, "end": 3254.48, "text": " Trying to remember, it was pointed out, I might have come across it a while ago,", "tokens": [50536, 20180, 281, 1604, 11, 309, 390, 10932, 484, 11, 286, 1062, 362, 808, 2108, 309, 257, 1339, 2057, 11, 50766], "temperature": 0.0, "avg_logprob": -0.2394651867094494, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0005614060210064054}, {"id": 840, "seek": 324644, "start": 3254.56, "end": 3257.84, "text": " but someone pointed it out to me recently.", "tokens": [50770, 457, 1580, 10932, 309, 484, 281, 385, 3938, 13, 50934], "temperature": 0.0, "avg_logprob": -0.2394651867094494, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0005614060210064054}, {"id": 841, "seek": 324644, "start": 3258.52, "end": 3263.92, "text": " That one, I think that one is, it's from open API, general or open API tools.", "tokens": [50968, 663, 472, 11, 286, 519, 300, 472, 307, 11, 309, 311, 490, 1269, 9362, 11, 2674, 420, 1269, 9362, 3873, 13, 51238], "temperature": 0.0, "avg_logprob": -0.2394651867094494, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0005614060210064054}, {"id": 842, "seek": 324644, "start": 3264.4, "end": 3268.16, "text": " They release an open API generator and it can generate Elm.", "tokens": [51262, 814, 4374, 364, 1269, 9362, 19265, 293, 309, 393, 8460, 2699, 76, 13, 51450], "temperature": 0.0, "avg_logprob": -0.2394651867094494, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0005614060210064054}, {"id": 843, "seek": 324644, "start": 3268.7200000000003, "end": 3275.92, "text": " There's also a swagger decoder Elm package, but not, it doesn't generate code as far", "tokens": [51478, 821, 311, 611, 257, 1693, 11062, 979, 19866, 2699, 76, 7372, 11, 457, 406, 11, 309, 1177, 380, 8460, 3089, 382, 1400, 51838], "temperature": 0.0, "avg_logprob": -0.2394651867094494, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.0005614060210064054}, {"id": 844, "seek": 327592, "start": 3276.0, "end": 3276.64, "text": " as I can sell.", "tokens": [50368, 382, 286, 393, 3607, 13, 50400], "temperature": 0.0, "avg_logprob": -0.2565335046677362, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.0018671689322218299}, {"id": 845, "seek": 327592, "start": 3276.84, "end": 3278.76, "text": " Do you think everyone ever seen that at one point?", "tokens": [50410, 1144, 291, 519, 1518, 1562, 1612, 300, 412, 472, 935, 30, 50506], "temperature": 0.0, "avg_logprob": -0.2565335046677362, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.0018671689322218299}, {"id": 846, "seek": 327592, "start": 3279.12, "end": 3283.6800000000003, "text": " I probably did and then promptly forgot about it while looking at other packages.", "tokens": [50524, 286, 1391, 630, 293, 550, 48594, 5298, 466, 309, 1339, 1237, 412, 661, 17401, 13, 50752], "temperature": 0.0, "avg_logprob": -0.2565335046677362, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.0018671689322218299}, {"id": 847, "seek": 327592, "start": 3286.36, "end": 3292.16, "text": " The, so when the open API tools one got brought up to me, I was very curious to see", "tokens": [50886, 440, 11, 370, 562, 264, 1269, 9362, 3873, 472, 658, 3038, 493, 281, 385, 11, 286, 390, 588, 6369, 281, 536, 51176], "temperature": 0.0, "avg_logprob": -0.2565335046677362, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.0018671689322218299}, {"id": 848, "seek": 327592, "start": 3292.16, "end": 3296.44, "text": " how ours differed because I didn't look at it at all while working on my stuff.", "tokens": [51176, 577, 11896, 743, 292, 570, 286, 994, 380, 574, 412, 309, 412, 439, 1339, 1364, 322, 452, 1507, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2565335046677362, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.0018671689322218299}, {"id": 849, "seek": 327592, "start": 3297.56, "end": 3305.16, "text": " It is one thing I do like that it does is it splits out into multiple Elm modules,", "tokens": [51446, 467, 307, 472, 551, 286, 360, 411, 300, 309, 775, 307, 309, 37741, 484, 666, 3866, 2699, 76, 16679, 11, 51826], "temperature": 0.0, "avg_logprob": -0.2565335046677362, "compression_ratio": 1.5634920634920635, "no_speech_prob": 0.0018671689322218299}, {"id": 850, "seek": 330516, "start": 3305.2799999999997, "end": 3307.3199999999997, "text": " kind of around types a little bit.", "tokens": [50370, 733, 295, 926, 3467, 257, 707, 857, 13, 50472], "temperature": 0.0, "avg_logprob": -0.1811979866027832, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0023223450407385826}, {"id": 851, "seek": 330516, "start": 3308.04, "end": 3314.04, "text": " It splits it out based on, so it splits out your endpoints into one module, your", "tokens": [50508, 467, 37741, 309, 484, 2361, 322, 11, 370, 309, 37741, 484, 428, 917, 20552, 666, 472, 10088, 11, 428, 50808], "temperature": 0.0, "avg_logprob": -0.1811979866027832, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0023223450407385826}, {"id": 852, "seek": 330516, "start": 3314.6, "end": 3318.6, "text": " request body, response body objects into another file.", "tokens": [50836, 5308, 1772, 11, 4134, 1772, 6565, 666, 1071, 3991, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1811979866027832, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0023223450407385826}, {"id": 853, "seek": 330516, "start": 3318.6, "end": 3323.08, "text": " And I think there might be a third one it generates as well, which is kind of nice", "tokens": [51036, 400, 286, 519, 456, 1062, 312, 257, 2636, 472, 309, 23815, 382, 731, 11, 597, 307, 733, 295, 1481, 51260], "temperature": 0.0, "avg_logprob": -0.1811979866027832, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0023223450407385826}, {"id": 854, "seek": 330516, "start": 3323.08, "end": 3325.04, "text": " because it breaks things up a bit.", "tokens": [51260, 570, 309, 9857, 721, 493, 257, 857, 13, 51358], "temperature": 0.0, "avg_logprob": -0.1811979866027832, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0023223450407385826}, {"id": 855, "seek": 330516, "start": 3325.48, "end": 3331.04, "text": " I would like to do the same though, I think more around types themselves instead", "tokens": [51380, 286, 576, 411, 281, 360, 264, 912, 1673, 11, 286, 519, 544, 926, 3467, 2969, 2602, 51658], "temperature": 0.0, "avg_logprob": -0.1811979866027832, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0023223450407385826}, {"id": 856, "seek": 330516, "start": 3331.04, "end": 3333.08, "text": " of around functions.", "tokens": [51658, 295, 926, 6828, 13, 51760], "temperature": 0.0, "avg_logprob": -0.1811979866027832, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0023223450407385826}, {"id": 857, "seek": 333308, "start": 3333.44, "end": 3335.48, "text": " I think that feels a little bit more Elm like to me.", "tokens": [50382, 286, 519, 300, 3417, 257, 707, 857, 544, 2699, 76, 411, 281, 385, 13, 50484], "temperature": 0.0, "avg_logprob": -0.24933405620295826, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0014099294785410166}, {"id": 858, "seek": 333308, "start": 3335.48, "end": 3340.24, "text": " I think the open API stool feels a little bit more Java to me.", "tokens": [50484, 286, 519, 264, 1269, 9362, 35086, 3417, 257, 707, 857, 544, 10745, 281, 385, 13, 50722], "temperature": 0.0, "avg_logprob": -0.24933405620295826, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0014099294785410166}, {"id": 859, "seek": 333308, "start": 3341.16, "end": 3346.56, "text": " And then same with, I noticed too, with how you call functions feels a little bit", "tokens": [50768, 400, 550, 912, 365, 11, 286, 5694, 886, 11, 365, 577, 291, 818, 6828, 3417, 257, 707, 857, 51038], "temperature": 0.0, "avg_logprob": -0.24933405620295826, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0014099294785410166}, {"id": 860, "seek": 333308, "start": 3346.56, "end": 3354.68, "text": " more Java like and how they're written for it, bringing back a little bit to off", "tokens": [51038, 544, 10745, 411, 293, 577, 436, 434, 3720, 337, 309, 11, 5062, 646, 257, 707, 857, 281, 766, 51444], "temperature": 0.0, "avg_logprob": -0.24933405620295826, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0014099294785410166}, {"id": 861, "seek": 333308, "start": 3354.84, "end": 3356.36, "text": " off stuff and tokens.", "tokens": [51452, 766, 1507, 293, 22667, 13, 51528], "temperature": 0.0, "avg_logprob": -0.24933405620295826, "compression_ratio": 1.7543859649122806, "no_speech_prob": 0.0014099294785410166}, {"id": 862, "seek": 335636, "start": 3357.0, "end": 3365.1200000000003, "text": " The API tools generation provide like a with authentication function to allow", "tokens": [50396, 440, 9362, 3873, 5125, 2893, 411, 257, 365, 26643, 2445, 281, 2089, 50802], "temperature": 0.0, "avg_logprob": -0.1869252665659015, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.02160835638642311}, {"id": 863, "seek": 335636, "start": 3365.1200000000003, "end": 3370.32, "text": " you to add that bearer token or JWT or whatever it is to your request.", "tokens": [50802, 291, 281, 909, 300, 6155, 260, 14862, 420, 49885, 51, 420, 2035, 309, 307, 281, 428, 5308, 13, 51062], "temperature": 0.0, "avg_logprob": -0.1869252665659015, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.02160835638642311}, {"id": 864, "seek": 335636, "start": 3371.04, "end": 3374.96, "text": " But it's always optional and you don't know if you need it or not for your", "tokens": [51098, 583, 309, 311, 1009, 17312, 293, 291, 500, 380, 458, 498, 291, 643, 309, 420, 406, 337, 428, 51294], "temperature": 0.0, "avg_logprob": -0.1869252665659015, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.02160835638642311}, {"id": 865, "seek": 335636, "start": 3374.96, "end": 3378.76, "text": " endpoint, which doesn't feel Elm like to me.", "tokens": [51294, 35795, 11, 597, 1177, 380, 841, 2699, 76, 411, 281, 385, 13, 51484], "temperature": 0.0, "avg_logprob": -0.1869252665659015, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.02160835638642311}, {"id": 866, "seek": 335636, "start": 3378.76, "end": 3384.2400000000002, "text": " Like if this function requires, if this endpoint requires a token, then it's,", "tokens": [51484, 1743, 498, 341, 2445, 7029, 11, 498, 341, 35795, 7029, 257, 14862, 11, 550, 309, 311, 11, 51758], "temperature": 0.0, "avg_logprob": -0.1869252665659015, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.02160835638642311}, {"id": 867, "seek": 338424, "start": 3384.2799999999997, "end": 3389.0, "text": " you should be forced to pass it in, which is what mine does.", "tokens": [50366, 291, 820, 312, 7579, 281, 1320, 309, 294, 11, 597, 307, 437, 3892, 775, 13, 50602], "temperature": 0.0, "avg_logprob": -0.16535164226185192, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005058659706264734}, {"id": 868, "seek": 338424, "start": 3389.0, "end": 3392.52, "text": " If for every endpoint that requires a token, you're forced to pass in that token.", "tokens": [50602, 759, 337, 633, 35795, 300, 7029, 257, 14862, 11, 291, 434, 7579, 281, 1320, 294, 300, 14862, 13, 50778], "temperature": 0.0, "avg_logprob": -0.16535164226185192, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005058659706264734}, {"id": 869, "seek": 338424, "start": 3392.56, "end": 3397.16, "text": " Otherwise, what's the point in requesting the data because it's just going to fail?", "tokens": [50780, 10328, 11, 437, 311, 264, 935, 294, 31937, 264, 1412, 570, 309, 311, 445, 516, 281, 3061, 30, 51010], "temperature": 0.0, "avg_logprob": -0.16535164226185192, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005058659706264734}, {"id": 870, "seek": 338424, "start": 3397.8399999999997, "end": 3400.3599999999997, "text": " So I think, yeah.", "tokens": [51044, 407, 286, 519, 11, 1338, 13, 51170], "temperature": 0.0, "avg_logprob": -0.16535164226185192, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005058659706264734}, {"id": 871, "seek": 338424, "start": 3400.4399999999996, "end": 3404.8799999999997, "text": " So I think there's something there where maybe I definitely need to break mine", "tokens": [51174, 407, 286, 519, 456, 311, 746, 456, 689, 1310, 286, 2138, 643, 281, 1821, 3892, 51396], "temperature": 0.0, "avg_logprob": -0.16535164226185192, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005058659706264734}, {"id": 872, "seek": 338424, "start": 3404.8799999999997, "end": 3410.2, "text": " out so that you don't have an API module that's 5,000 lines long, 10,000 lines long.", "tokens": [51396, 484, 370, 300, 291, 500, 380, 362, 364, 9362, 10088, 300, 311, 1025, 11, 1360, 3876, 938, 11, 1266, 11, 1360, 3876, 938, 13, 51662], "temperature": 0.0, "avg_logprob": -0.16535164226185192, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005058659706264734}, {"id": 873, "seek": 341020, "start": 3410.4399999999996, "end": 3419.6, "text": " How do you manage like staging API URLs versus dev URLs versus production URLs?", "tokens": [50376, 1012, 360, 291, 3067, 411, 41085, 9362, 43267, 5717, 1905, 43267, 5717, 4265, 43267, 30, 50834], "temperature": 0.0, "avg_logprob": -0.18795273520729758, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.014498790726065636}, {"id": 874, "seek": 341020, "start": 3419.6, "end": 3421.4399999999996, "text": " Is there a way to handle that?", "tokens": [50834, 1119, 456, 257, 636, 281, 4813, 300, 30, 50926], "temperature": 0.0, "avg_logprob": -0.18795273520729758, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.014498790726065636}, {"id": 875, "seek": 341020, "start": 3422.2, "end": 3423.96, "text": " I think there is.", "tokens": [50964, 286, 519, 456, 307, 13, 51052], "temperature": 0.0, "avg_logprob": -0.18795273520729758, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.014498790726065636}, {"id": 876, "seek": 341020, "start": 3424.52, "end": 3426.56, "text": " I hadn't actually thought about it too much.", "tokens": [51080, 286, 8782, 380, 767, 1194, 466, 309, 886, 709, 13, 51182], "temperature": 0.0, "avg_logprob": -0.18795273520729758, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.014498790726065636}, {"id": 877, "seek": 341020, "start": 3426.7599999999998, "end": 3432.68, "text": " So the way the schema works is there is like a top level URL defined.", "tokens": [51192, 407, 264, 636, 264, 34078, 1985, 307, 456, 307, 411, 257, 1192, 1496, 12905, 7642, 13, 51488], "temperature": 0.0, "avg_logprob": -0.18795273520729758, "compression_ratio": 1.4294117647058824, "no_speech_prob": 0.014498790726065636}, {"id": 878, "seek": 343268, "start": 3433.24, "end": 3441.24, "text": " So Spotify's might be like HTTPS colon slash slash Spotify.com slash API.", "tokens": [50392, 407, 29036, 311, 1062, 312, 411, 11751, 51, 6273, 8255, 17330, 17330, 29036, 13, 1112, 17330, 9362, 13, 50792], "temperature": 0.0, "avg_logprob": -0.23817591349283854, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.025169337168335915}, {"id": 879, "seek": 343268, "start": 3442.24, "end": 3447.64, "text": " That is then used for all the endpoints and the endpoints are then just slash", "tokens": [50842, 663, 307, 550, 1143, 337, 439, 264, 917, 20552, 293, 264, 917, 20552, 366, 550, 445, 17330, 51112], "temperature": 0.0, "avg_logprob": -0.23817591349283854, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.025169337168335915}, {"id": 880, "seek": 343268, "start": 3447.9199999999996, "end": 3451.0, "text": " playlist or slash artists.", "tokens": [51126, 16788, 420, 17330, 6910, 13, 51280], "temperature": 0.0, "avg_logprob": -0.23817591349283854, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.025169337168335915}, {"id": 881, "seek": 343268, "start": 3451.52, "end": 3454.04, "text": " And it all gets joined together.", "tokens": [51306, 400, 309, 439, 2170, 6869, 1214, 13, 51432], "temperature": 0.0, "avg_logprob": -0.23817591349283854, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.025169337168335915}, {"id": 882, "seek": 343268, "start": 3454.6, "end": 3461.24, "text": " I believe you can define multiple top level API or yeah, top level URLs.", "tokens": [51460, 286, 1697, 291, 393, 6964, 3866, 1192, 1496, 9362, 420, 1338, 11, 1192, 1496, 43267, 13, 51792], "temperature": 0.0, "avg_logprob": -0.23817591349283854, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.025169337168335915}, {"id": 883, "seek": 346124, "start": 3461.4799999999996, "end": 3465.8399999999997, "text": " So I guess I'm not sure what that looks like, but that probably should be something", "tokens": [50376, 407, 286, 2041, 286, 478, 406, 988, 437, 300, 1542, 411, 11, 457, 300, 1391, 820, 312, 746, 50594], "temperature": 0.0, "avg_logprob": -0.25055384395098446, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.001048366422764957}, {"id": 884, "seek": 346124, "start": 3465.8399999999997, "end": 3466.56, "text": " that's handled.", "tokens": [50594, 300, 311, 18033, 13, 50630], "temperature": 0.0, "avg_logprob": -0.25055384395098446, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.001048366422764957}, {"id": 885, "seek": 346124, "start": 3468.3599999999997, "end": 3473.12, "text": " Yeah, because you could talk to like a real world backend.", "tokens": [50720, 865, 11, 570, 291, 727, 751, 281, 411, 257, 957, 1002, 38087, 13, 50958], "temperature": 0.0, "avg_logprob": -0.25055384395098446, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.001048366422764957}, {"id": 886, "seek": 346124, "start": 3473.52, "end": 3478.16, "text": " But you can just say, well, whatever URL, whatever backend I'm targeting,", "tokens": [50978, 583, 291, 393, 445, 584, 11, 731, 11, 2035, 12905, 11, 2035, 38087, 286, 478, 17918, 11, 51210], "temperature": 0.0, "avg_logprob": -0.25055384395098446, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.001048366422764957}, {"id": 887, "seek": 346124, "start": 3478.68, "end": 3484.24, "text": " the URL is defined not at code generation time, but at runtime or right.", "tokens": [51236, 264, 12905, 307, 7642, 406, 412, 3089, 5125, 565, 11, 457, 412, 34474, 420, 558, 13, 51514], "temperature": 0.0, "avg_logprob": -0.25055384395098446, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.001048366422764957}, {"id": 888, "seek": 346124, "start": 3484.52, "end": 3489.3199999999997, "text": " But that doesn't work with your approach as far as I can tell because you generate", "tokens": [51528, 583, 300, 1177, 380, 589, 365, 428, 3109, 382, 1400, 382, 286, 393, 980, 570, 291, 8460, 51768], "temperature": 0.0, "avg_logprob": -0.25055384395098446, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.001048366422764957}, {"id": 889, "seek": 348932, "start": 3489.4, "end": 3493.96, "text": " that URL inside the you put that URL inside the generated code.", "tokens": [50368, 300, 12905, 1854, 264, 291, 829, 300, 12905, 1854, 264, 10833, 3089, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2183702095695164, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0006664158427156508}, {"id": 890, "seek": 348932, "start": 3494.1200000000003, "end": 3497.52, "text": " But that could be an argument rights as well.", "tokens": [50604, 583, 300, 727, 312, 364, 6770, 4601, 382, 731, 13, 50774], "temperature": 0.0, "avg_logprob": -0.2183702095695164, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0006664158427156508}, {"id": 891, "seek": 348932, "start": 3497.84, "end": 3502.2000000000003, "text": " Yeah, there there likely are a lot of additional arguments that need to be", "tokens": [50790, 865, 11, 456, 456, 3700, 366, 257, 688, 295, 4497, 12869, 300, 643, 281, 312, 51008], "temperature": 0.0, "avg_logprob": -0.2183702095695164, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0006664158427156508}, {"id": 892, "seek": 348932, "start": 3502.2000000000003, "end": 3504.4, "text": " optional ones, like completely optional.", "tokens": [51008, 17312, 2306, 11, 411, 2584, 17312, 13, 51118], "temperature": 0.0, "avg_logprob": -0.2183702095695164, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0006664158427156508}, {"id": 893, "seek": 348932, "start": 3504.92, "end": 3510.6800000000003, "text": " The real world server might be behind a proxy of some kind or something like that.", "tokens": [51144, 440, 957, 1002, 7154, 1062, 312, 2261, 257, 29690, 295, 512, 733, 420, 746, 411, 300, 13, 51432], "temperature": 0.0, "avg_logprob": -0.2183702095695164, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0006664158427156508}, {"id": 894, "seek": 348932, "start": 3510.6800000000003, "end": 3516.6000000000004, "text": " And so being able to say with proxy URL or something along those lines would be", "tokens": [51432, 400, 370, 885, 1075, 281, 584, 365, 29690, 12905, 420, 746, 2051, 729, 3876, 576, 312, 51728], "temperature": 0.0, "avg_logprob": -0.2183702095695164, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0006664158427156508}, {"id": 895, "seek": 351660, "start": 3516.6, "end": 3520.88, "text": " a very nice thing to prepend every URL requests that you make with that.", "tokens": [50364, 257, 588, 1481, 551, 281, 2666, 521, 633, 12905, 12475, 300, 291, 652, 365, 300, 13, 50578], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 896, "seek": 351660, "start": 3521.6, "end": 3525.3199999999997, "text": " There could be other things as well, maybe cuss, maybe for whatever reason", "tokens": [50614, 821, 727, 312, 661, 721, 382, 731, 11, 1310, 269, 2023, 11, 1310, 337, 2035, 1778, 50800], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 897, "seek": 351660, "start": 3525.3199999999997, "end": 3528.96, "text": " you want to handle custom decoding just locally in your app.", "tokens": [50800, 291, 528, 281, 4813, 2375, 979, 8616, 445, 16143, 294, 428, 724, 13, 50982], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 898, "seek": 351660, "start": 3528.96, "end": 3532.6, "text": " You're like, the generated coding is great, but I want a little tweak on it.", "tokens": [50982, 509, 434, 411, 11, 264, 10833, 17720, 307, 869, 11, 457, 286, 528, 257, 707, 29879, 322, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 899, "seek": 351660, "start": 3533.2, "end": 3538.72, "text": " Maybe in the special case, so like with decode map or something along those lines", "tokens": [51194, 2704, 294, 264, 2121, 1389, 11, 370, 411, 365, 979, 1429, 4471, 420, 746, 2051, 729, 3876, 51470], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 900, "seek": 351660, "start": 3539.12, "end": 3541.4, "text": " might be might be a nice thing to have.", "tokens": [51490, 1062, 312, 1062, 312, 257, 1481, 551, 281, 362, 13, 51604], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 901, "seek": 351660, "start": 3541.68, "end": 3542.48, "text": " There are other ones too.", "tokens": [51618, 821, 366, 661, 2306, 886, 13, 51658], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 902, "seek": 351660, "start": 3542.48, "end": 3544.3199999999997, "text": " I know I want to add retry at some point.", "tokens": [51658, 286, 458, 286, 528, 281, 909, 1533, 627, 412, 512, 935, 13, 51750], "temperature": 0.0, "avg_logprob": -0.17173109279842827, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.0031719107646495104}, {"id": 903, "seek": 354432, "start": 3544.6800000000003, "end": 3548.2000000000003, "text": " Retry is something that's usually usually nice to have.", "tokens": [50382, 11495, 627, 307, 746, 300, 311, 2673, 2673, 1481, 281, 362, 13, 50558], "temperature": 0.0, "avg_logprob": -0.1920836336977847, "compression_ratio": 1.6159695817490494, "no_speech_prob": 0.0014100685948505998}, {"id": 904, "seek": 354432, "start": 3548.88, "end": 3554.48, "text": " For request fails, maybe retry every 30 seconds incrementing by 20 seconds", "tokens": [50592, 1171, 5308, 18199, 11, 1310, 1533, 627, 633, 2217, 3949, 26200, 278, 538, 945, 3949, 50872], "temperature": 0.0, "avg_logprob": -0.1920836336977847, "compression_ratio": 1.6159695817490494, "no_speech_prob": 0.0014100685948505998}, {"id": 905, "seek": 354432, "start": 3554.48, "end": 3556.84, "text": " or something along those lines for 15 times.", "tokens": [50872, 420, 746, 2051, 729, 3876, 337, 2119, 1413, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1920836336977847, "compression_ratio": 1.6159695817490494, "no_speech_prob": 0.0014100685948505998}, {"id": 906, "seek": 354432, "start": 3557.1600000000003, "end": 3562.1200000000003, "text": " Wouldn't you be able to do that through the task API, the Elm task API anyway?", "tokens": [51006, 26291, 380, 291, 312, 1075, 281, 360, 300, 807, 264, 5633, 9362, 11, 264, 2699, 76, 5633, 9362, 4033, 30, 51254], "temperature": 0.0, "avg_logprob": -0.1920836336977847, "compression_ratio": 1.6159695817490494, "no_speech_prob": 0.0014100685948505998}, {"id": 907, "seek": 354432, "start": 3562.52, "end": 3564.1200000000003, "text": " You could write your own retry.", "tokens": [51274, 509, 727, 2464, 428, 1065, 1533, 627, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1920836336977847, "compression_ratio": 1.6159695817490494, "no_speech_prob": 0.0014100685948505998}, {"id": 908, "seek": 354432, "start": 3564.1200000000003, "end": 3568.84, "text": " Yeah, but it'd be nice to this is mostly a lot of these things are things", "tokens": [51354, 865, 11, 457, 309, 1116, 312, 1481, 281, 341, 307, 5240, 257, 688, 295, 613, 721, 366, 721, 51590], "temperature": 0.0, "avg_logprob": -0.1920836336977847, "compression_ratio": 1.6159695817490494, "no_speech_prob": 0.0014100685948505998}, {"id": 909, "seek": 354432, "start": 3568.84, "end": 3572.4, "text": " that I knew were request when I was working at Square from users", "tokens": [51590, 300, 286, 2586, 645, 5308, 562, 286, 390, 1364, 412, 16463, 490, 5022, 51768], "temperature": 0.0, "avg_logprob": -0.1920836336977847, "compression_ratio": 1.6159695817490494, "no_speech_prob": 0.0014100685948505998}, {"id": 910, "seek": 357240, "start": 3573.2000000000003, "end": 3578.48, "text": " that were just really nice to have built in for the language.", "tokens": [50404, 300, 645, 445, 534, 1481, 281, 362, 3094, 294, 337, 264, 2856, 13, 50668], "temperature": 0.0, "avg_logprob": -0.17973459436652367, "compression_ratio": 1.4497816593886463, "no_speech_prob": 0.0015482822200283408}, {"id": 911, "seek": 357240, "start": 3578.56, "end": 3583.6, "text": " So why make you write your own retry logic when I can just generate it for you?", "tokens": [50672, 407, 983, 652, 291, 2464, 428, 1065, 1533, 627, 9952, 562, 286, 393, 445, 8460, 309, 337, 291, 30, 50924], "temperature": 0.0, "avg_logprob": -0.17973459436652367, "compression_ratio": 1.4497816593886463, "no_speech_prob": 0.0015482822200283408}, {"id": 912, "seek": 357240, "start": 3583.92, "end": 3585.8, "text": " Because it's never exactly what I want.", "tokens": [50940, 1436, 309, 311, 1128, 2293, 437, 286, 528, 13, 51034], "temperature": 0.0, "avg_logprob": -0.17973459436652367, "compression_ratio": 1.4497816593886463, "no_speech_prob": 0.0015482822200283408}, {"id": 913, "seek": 357240, "start": 3588.1600000000003, "end": 3593.28, "text": " Yeah, I get the feeling too that you're talking Wolfgang about a sort of", "tokens": [51152, 865, 11, 286, 483, 264, 2633, 886, 300, 291, 434, 1417, 16634, 19619, 466, 257, 1333, 295, 51408], "temperature": 0.0, "avg_logprob": -0.17973459436652367, "compression_ratio": 1.4497816593886463, "no_speech_prob": 0.0015482822200283408}, {"id": 914, "seek": 357240, "start": 3593.28, "end": 3601.1600000000003, "text": " pre-packaged SDK, something, the kind of thing that would be like NPM install", "tokens": [51408, 659, 12, 9539, 2980, 37135, 11, 746, 11, 264, 733, 295, 551, 300, 576, 312, 411, 426, 18819, 3625, 51802], "temperature": 0.0, "avg_logprob": -0.17973459436652367, "compression_ratio": 1.4497816593886463, "no_speech_prob": 0.0015482822200283408}, {"id": 915, "seek": 360116, "start": 3601.3199999999997, "end": 3605.2, "text": " Spotify API or GitHub API or something.", "tokens": [50372, 29036, 9362, 420, 23331, 9362, 420, 746, 13, 50566], "temperature": 0.0, "avg_logprob": -0.20028071933322483, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019868826493620872}, {"id": 916, "seek": 360116, "start": 3605.2799999999997, "end": 3610.3199999999997, "text": " And then it installs some JavaScript functions that let you use the API.", "tokens": [50570, 400, 550, 309, 3625, 82, 512, 15778, 6828, 300, 718, 291, 764, 264, 9362, 13, 50822], "temperature": 0.0, "avg_logprob": -0.20028071933322483, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019868826493620872}, {"id": 917, "seek": 360116, "start": 3610.3199999999997, "end": 3613.3999999999996, "text": " So you're talking about that sort of pre-packaged SDK, right?", "tokens": [50822, 407, 291, 434, 1417, 466, 300, 1333, 295, 659, 12, 9539, 2980, 37135, 11, 558, 30, 50976], "temperature": 0.0, "avg_logprob": -0.20028071933322483, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019868826493620872}, {"id": 918, "seek": 360116, "start": 3614.0, "end": 3615.08, "text": " Yes, yes.", "tokens": [51006, 1079, 11, 2086, 13, 51060], "temperature": 0.0, "avg_logprob": -0.20028071933322483, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019868826493620872}, {"id": 919, "seek": 360116, "start": 3615.8399999999997, "end": 3621.44, "text": " Because that's, I find it very useful when I when I'm doing JavaScript", "tokens": [51098, 1436, 300, 311, 11, 286, 915, 309, 588, 4420, 562, 286, 562, 286, 478, 884, 15778, 51378], "temperature": 0.0, "avg_logprob": -0.20028071933322483, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019868826493620872}, {"id": 920, "seek": 360116, "start": 3621.44, "end": 3623.64, "text": " and I'm like, I need to talk to the service.", "tokens": [51378, 293, 286, 478, 411, 11, 286, 643, 281, 751, 281, 264, 2643, 13, 51488], "temperature": 0.0, "avg_logprob": -0.20028071933322483, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019868826493620872}, {"id": 921, "seek": 360116, "start": 3624.3199999999997, "end": 3628.56, "text": " Oh, hey, there's a package I can talk to the service and it's no setup.", "tokens": [51522, 876, 11, 4177, 11, 456, 311, 257, 7372, 286, 393, 751, 281, 264, 2643, 293, 309, 311, 572, 8657, 13, 51734], "temperature": 0.0, "avg_logprob": -0.20028071933322483, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.0019868826493620872}, {"id": 922, "seek": 362856, "start": 3628.7599999999998, "end": 3633.2, "text": " You there's a lot like you usually have to pass on a token once your API token", "tokens": [50374, 509, 456, 311, 257, 688, 411, 291, 2673, 362, 281, 1320, 322, 257, 14862, 1564, 428, 9362, 14862, 50596], "temperature": 0.0, "avg_logprob": -0.1911779419193423, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.002251182682812214}, {"id": 923, "seek": 362856, "start": 3633.2, "end": 3639.4, "text": " for that service once and then you get some type of pre-pre-setup request.", "tokens": [50596, 337, 300, 2643, 1564, 293, 550, 291, 483, 512, 2010, 295, 659, 12, 3712, 12, 3854, 1010, 5308, 13, 50906], "temperature": 0.0, "avg_logprob": -0.1911779419193423, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.002251182682812214}, {"id": 924, "seek": 362856, "start": 3639.88, "end": 3643.32, "text": " And then you just say, OK, go get me this data and you're done.", "tokens": [50930, 400, 550, 291, 445, 584, 11, 2264, 11, 352, 483, 385, 341, 1412, 293, 291, 434, 1096, 13, 51102], "temperature": 0.0, "avg_logprob": -0.1911779419193423, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.002251182682812214}, {"id": 925, "seek": 362856, "start": 3643.92, "end": 3649.7599999999998, "text": " And then if you look at I'm trying to think of some but with those pre-packaged ones,", "tokens": [51132, 400, 550, 498, 291, 574, 412, 286, 478, 1382, 281, 519, 295, 512, 457, 365, 729, 659, 12, 9539, 2980, 2306, 11, 51424], "temperature": 0.0, "avg_logprob": -0.1911779419193423, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.002251182682812214}, {"id": 926, "seek": 362856, "start": 3650.48, "end": 3653.7999999999997, "text": " if you look at their REST APIs and you go to the rest docs,", "tokens": [51460, 498, 291, 574, 412, 641, 497, 14497, 21445, 293, 291, 352, 281, 264, 1472, 45623, 11, 51626], "temperature": 0.0, "avg_logprob": -0.1911779419193423, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.002251182682812214}, {"id": 927, "seek": 362856, "start": 3654.48, "end": 3657.96, "text": " it's it's a fair amount of work to get back to that same point.", "tokens": [51660, 309, 311, 309, 311, 257, 3143, 2372, 295, 589, 281, 483, 646, 281, 300, 912, 935, 13, 51834], "temperature": 0.0, "avg_logprob": -0.1911779419193423, "compression_ratio": 1.6550387596899225, "no_speech_prob": 0.002251182682812214}, {"id": 928, "seek": 365856, "start": 3658.68, "end": 3661.7999999999997, "text": " And you're not sure, like, oh, shoot, did I type that wrong?", "tokens": [50370, 400, 291, 434, 406, 988, 11, 411, 11, 1954, 11, 3076, 11, 630, 286, 2010, 300, 2085, 30, 50526], "temperature": 0.0, "avg_logprob": -0.1655074048925329, "compression_ratio": 1.625514403292181, "no_speech_prob": 0.00034591855364851654}, {"id": 929, "seek": 365856, "start": 3661.7999999999997, "end": 3663.6, "text": " Did I type the URL correctly?", "tokens": [50526, 2589, 286, 2010, 264, 12905, 8944, 30, 50616], "temperature": 0.0, "avg_logprob": -0.1655074048925329, "compression_ratio": 1.625514403292181, "no_speech_prob": 0.00034591855364851654}, {"id": 930, "seek": 365856, "start": 3663.7599999999998, "end": 3666.08, "text": " Did I remember to pass in the off token?", "tokens": [50624, 2589, 286, 1604, 281, 1320, 294, 264, 766, 14862, 30, 50740], "temperature": 0.0, "avg_logprob": -0.1655074048925329, "compression_ratio": 1.625514403292181, "no_speech_prob": 0.00034591855364851654}, {"id": 931, "seek": 365856, "start": 3667.16, "end": 3672.0, "text": " Did did they change their their timeout like or some limits?", "tokens": [50794, 2589, 630, 436, 1319, 641, 641, 565, 346, 411, 420, 512, 10406, 30, 51036], "temperature": 0.0, "avg_logprob": -0.1655074048925329, "compression_ratio": 1.625514403292181, "no_speech_prob": 0.00034591855364851654}, {"id": 932, "seek": 365856, "start": 3672.0, "end": 3676.48, "text": " Like maybe the API only allows you to make a request every 15 seconds.", "tokens": [51036, 1743, 1310, 264, 9362, 787, 4045, 291, 281, 652, 257, 5308, 633, 2119, 3949, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1655074048925329, "compression_ratio": 1.625514403292181, "no_speech_prob": 0.00034591855364851654}, {"id": 933, "seek": 365856, "start": 3676.96, "end": 3680.4, "text": " Did I actually set that to 15 seconds or did I set it to 10 seconds", "tokens": [51284, 2589, 286, 767, 992, 300, 281, 2119, 3949, 420, 630, 286, 992, 309, 281, 1266, 3949, 51456], "temperature": 0.0, "avg_logprob": -0.1655074048925329, "compression_ratio": 1.625514403292181, "no_speech_prob": 0.00034591855364851654}, {"id": 934, "seek": 365856, "start": 3680.4, "end": 3683.64, "text": " because I was looking at something else and got a number wrong.", "tokens": [51456, 570, 286, 390, 1237, 412, 746, 1646, 293, 658, 257, 1230, 2085, 13, 51618], "temperature": 0.0, "avg_logprob": -0.1655074048925329, "compression_ratio": 1.625514403292181, "no_speech_prob": 0.00034591855364851654}, {"id": 935, "seek": 368364, "start": 3684.2799999999997, "end": 3688.68, "text": " So having those types of experiences is very nice.", "tokens": [50396, 407, 1419, 729, 3467, 295, 5235, 307, 588, 1481, 13, 50616], "temperature": 0.0, "avg_logprob": -0.18660527652071923, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.011130320839583874}, {"id": 936, "seek": 368364, "start": 3688.8799999999997, "end": 3694.04, "text": " And and I don't expect companies to go make Elm SDKs.", "tokens": [50626, 400, 293, 286, 500, 380, 2066, 3431, 281, 352, 652, 2699, 76, 37135, 82, 13, 50884], "temperature": 0.0, "avg_logprob": -0.18660527652071923, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.011130320839583874}, {"id": 937, "seek": 368364, "start": 3694.56, "end": 3698.7599999999998, "text": " Like that's a lot of work to go and make SDKs for other languages.", "tokens": [50910, 1743, 300, 311, 257, 688, 295, 589, 281, 352, 293, 652, 37135, 82, 337, 661, 8650, 13, 51120], "temperature": 0.0, "avg_logprob": -0.18660527652071923, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.011130320839583874}, {"id": 938, "seek": 368364, "start": 3699.3599999999997, "end": 3704.3599999999997, "text": " So maybe I can help bring that experience, that nice quick.", "tokens": [51150, 407, 1310, 286, 393, 854, 1565, 300, 1752, 11, 300, 1481, 1702, 13, 51400], "temperature": 0.0, "avg_logprob": -0.18660527652071923, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.011130320839583874}, {"id": 939, "seek": 368364, "start": 3704.3599999999997, "end": 3709.3199999999997, "text": " I want to use a service with minimal effort, that experience to Elm.", "tokens": [51400, 286, 528, 281, 764, 257, 2643, 365, 13206, 4630, 11, 300, 1752, 281, 2699, 76, 13, 51648], "temperature": 0.0, "avg_logprob": -0.18660527652071923, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.011130320839583874}, {"id": 940, "seek": 368364, "start": 3709.52, "end": 3713.2, "text": " Would you so let's say Spotify got big into Elm?", "tokens": [51658, 6068, 291, 370, 718, 311, 584, 29036, 658, 955, 666, 2699, 76, 30, 51842], "temperature": 0.0, "avg_logprob": -0.18660527652071923, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.011130320839583874}, {"id": 941, "seek": 371364, "start": 3713.68, "end": 3718.56, "text": " And they wanted to make it easy for people to use their SDK.", "tokens": [50366, 400, 436, 1415, 281, 652, 309, 1858, 337, 561, 281, 764, 641, 37135, 13, 50610], "temperature": 0.0, "avg_logprob": -0.2236453925862032, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.00018226998508907855}, {"id": 942, "seek": 371364, "start": 3719.04, "end": 3722.6, "text": " Would you recommend that they would generate an Elm package", "tokens": [50634, 6068, 291, 2748, 300, 436, 576, 8460, 364, 2699, 76, 7372, 50812], "temperature": 0.0, "avg_logprob": -0.2236453925862032, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.00018226998508907855}, {"id": 943, "seek": 371364, "start": 3723.12, "end": 3728.3199999999997, "text": " or that they make their open API spec", "tokens": [50838, 420, 300, 436, 652, 641, 1269, 9362, 1608, 51098], "temperature": 0.0, "avg_logprob": -0.2236453925862032, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.00018226998508907855}, {"id": 944, "seek": 371364, "start": 3728.7999999999997, "end": 3733.56, "text": " readily available and give good instructions on how to generate", "tokens": [51122, 26336, 2435, 293, 976, 665, 9415, 322, 577, 281, 8460, 51360], "temperature": 0.0, "avg_logprob": -0.2236453925862032, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.00018226998508907855}, {"id": 945, "seek": 371364, "start": 3734.0, "end": 3739.24, "text": " it's using your package, like especially with the idea of having", "tokens": [51382, 309, 311, 1228, 428, 7372, 11, 411, 2318, 365, 264, 1558, 295, 1419, 51644], "temperature": 0.0, "avg_logprob": -0.2236453925862032, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.00018226998508907855}, {"id": 946, "seek": 373924, "start": 3739.6, "end": 3741.64, "text": " custom opaque types and all that.", "tokens": [50382, 2375, 42687, 3467, 293, 439, 300, 13, 50484], "temperature": 0.0, "avg_logprob": -0.21453475952148438, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0017817866755649447}, {"id": 947, "seek": 373924, "start": 3741.64, "end": 3746.0, "text": " Maybe it's better to have it to generate it yourself so you can edit", "tokens": [50484, 2704, 309, 311, 1101, 281, 362, 309, 281, 8460, 309, 1803, 370, 291, 393, 8129, 50702], "temperature": 0.0, "avg_logprob": -0.21453475952148438, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0017817866755649447}, {"id": 948, "seek": 373924, "start": 3746.64, "end": 3752.7999999999997, "text": " a bit instead of having a readily but fixed Elm package.", "tokens": [50734, 257, 857, 2602, 295, 1419, 257, 26336, 457, 6806, 2699, 76, 7372, 13, 51042], "temperature": 0.0, "avg_logprob": -0.21453475952148438, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0017817866755649447}, {"id": 949, "seek": 373924, "start": 3753.04, "end": 3759.24, "text": " I think handwriting your own SDK for especially a larger API can be a lot of work.", "tokens": [51054, 286, 519, 39179, 428, 1065, 37135, 337, 2318, 257, 4833, 9362, 393, 312, 257, 688, 295, 589, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21453475952148438, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0017817866755649447}, {"id": 950, "seek": 373924, "start": 3759.8799999999997, "end": 3763.6, "text": " Oh, no, in both cases, it would be using your your package,", "tokens": [51396, 876, 11, 572, 11, 294, 1293, 3331, 11, 309, 576, 312, 1228, 428, 428, 7372, 11, 51582], "temperature": 0.0, "avg_logprob": -0.21453475952148438, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0017817866755649447}, {"id": 951, "seek": 373924, "start": 3763.6, "end": 3768.16, "text": " but it would be publish publishing the results or telling people to", "tokens": [51582, 457, 309, 576, 312, 11374, 17832, 264, 3542, 420, 3585, 561, 281, 51810], "temperature": 0.0, "avg_logprob": -0.21453475952148438, "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.0017817866755649447}, {"id": 952, "seek": 376816, "start": 3768.6, "end": 3770.7599999999998, "text": " generate themselves using your tool.", "tokens": [50386, 8460, 2969, 1228, 428, 2290, 13, 50494], "temperature": 0.0, "avg_logprob": -0.16596346485371494, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0007434721337631345}, {"id": 953, "seek": 376816, "start": 3771.04, "end": 3774.7999999999997, "text": " I think it would be I think it'd be publishing would be my guess.", "tokens": [50508, 286, 519, 309, 576, 312, 286, 519, 309, 1116, 312, 17832, 576, 312, 452, 2041, 13, 50696], "temperature": 0.0, "avg_logprob": -0.16596346485371494, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0007434721337631345}, {"id": 954, "seek": 376816, "start": 3776.12, "end": 3778.92, "text": " I think that if Spotify were really big into Elm,", "tokens": [50762, 286, 519, 300, 498, 29036, 645, 534, 955, 666, 2699, 76, 11, 50902], "temperature": 0.0, "avg_logprob": -0.16596346485371494, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0007434721337631345}, {"id": 955, "seek": 376816, "start": 3779.2, "end": 3782.7599999999998, "text": " my my thought would be you would go to their developer page.", "tokens": [50916, 452, 452, 1194, 576, 312, 291, 576, 352, 281, 641, 10754, 3028, 13, 51094], "temperature": 0.0, "avg_logprob": -0.16596346485371494, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0007434721337631345}, {"id": 956, "seek": 376816, "start": 3783.12, "end": 3786.7599999999998, "text": " They would have their first listed out their SDKs.", "tokens": [51112, 814, 576, 362, 641, 700, 10052, 484, 641, 37135, 82, 13, 51294], "temperature": 0.0, "avg_logprob": -0.16596346485371494, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0007434721337631345}, {"id": 957, "seek": 376816, "start": 3787.2799999999997, "end": 3792.12, "text": " And those would be say then Elm and JavaScript and maybe Python.", "tokens": [51320, 400, 729, 576, 312, 584, 550, 2699, 76, 293, 15778, 293, 1310, 15329, 13, 51562], "temperature": 0.0, "avg_logprob": -0.16596346485371494, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0007434721337631345}, {"id": 958, "seek": 376816, "start": 3792.48, "end": 3795.8799999999997, "text": " And then they would after that list their API spec", "tokens": [51580, 400, 550, 436, 576, 934, 300, 1329, 641, 9362, 1608, 51750], "temperature": 0.0, "avg_logprob": -0.16596346485371494, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0007434721337631345}, {"id": 959, "seek": 379588, "start": 3795.92, "end": 3798.4, "text": " and their rest endpoint documentation.", "tokens": [50366, 293, 641, 1472, 35795, 14333, 13, 50490], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 960, "seek": 379588, "start": 3798.96, "end": 3800.6800000000003, "text": " You'd probably still want the rest.", "tokens": [50518, 509, 1116, 1391, 920, 528, 264, 1472, 13, 50604], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 961, "seek": 379588, "start": 3800.6800000000003, "end": 3802.96, "text": " I mean, you'd still want the rest documentation regardless", "tokens": [50604, 286, 914, 11, 291, 1116, 920, 528, 264, 1472, 14333, 10060, 50718], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 962, "seek": 379588, "start": 3802.96, "end": 3806.0, "text": " because it's helpful for understanding the SDKs.", "tokens": [50718, 570, 309, 311, 4961, 337, 3701, 264, 37135, 82, 13, 50870], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 963, "seek": 379588, "start": 3806.6, "end": 3810.44, "text": " But you would list the SDKs for saying like, hey, we support these.", "tokens": [50900, 583, 291, 576, 1329, 264, 37135, 82, 337, 1566, 411, 11, 4177, 11, 321, 1406, 613, 13, 51092], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 964, "seek": 379588, "start": 3810.44, "end": 3815.08, "text": " We put time into these because we want you to quickly get up to speed", "tokens": [51092, 492, 829, 565, 666, 613, 570, 321, 528, 291, 281, 2661, 483, 493, 281, 3073, 51324], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 965, "seek": 379588, "start": 3815.08, "end": 3817.52, "text": " and build an app with our stuff.", "tokens": [51324, 293, 1322, 364, 724, 365, 527, 1507, 13, 51446], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 966, "seek": 379588, "start": 3817.52, "end": 3822.4, "text": " If you're using a language that doesn't isn't listed here.", "tokens": [51446, 759, 291, 434, 1228, 257, 2856, 300, 1177, 380, 1943, 380, 10052, 510, 13, 51690], "temperature": 0.0, "avg_logprob": -0.17306388427163952, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.001895955647341907}, {"id": 967, "seek": 382240, "start": 3822.64, "end": 3826.08, "text": " Here's another way to go about an interface with our with our API.", "tokens": [50376, 1692, 311, 1071, 636, 281, 352, 466, 364, 9226, 365, 527, 365, 527, 9362, 13, 50548], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 968, "seek": 382240, "start": 3826.36, "end": 3829.6, "text": " Also, if I had one once I have some way to like", "tokens": [50562, 2743, 11, 498, 286, 632, 472, 1564, 286, 362, 512, 636, 281, 411, 50724], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 969, "seek": 382240, "start": 3830.04, "end": 3833.52, "text": " customize the generated SDK with it like extra config,", "tokens": [50746, 19734, 264, 10833, 37135, 365, 309, 411, 2857, 6662, 11, 50920], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 970, "seek": 382240, "start": 3834.12, "end": 3838.64, "text": " they could go into and like define their own extra stuff that they want.", "tokens": [50950, 436, 727, 352, 666, 293, 411, 6964, 641, 1065, 2857, 1507, 300, 436, 528, 13, 51176], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 971, "seek": 382240, "start": 3839.04, "end": 3841.44, "text": " Like, like, you know, a custom.", "tokens": [51196, 1743, 11, 411, 11, 291, 458, 11, 257, 2375, 13, 51316], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 972, "seek": 382240, "start": 3841.7200000000003, "end": 3843.52, "text": " I'm trying to think what would be Spotify.", "tokens": [51330, 286, 478, 1382, 281, 519, 437, 576, 312, 29036, 13, 51420], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 973, "seek": 382240, "start": 3843.52, "end": 3846.36, "text": " I don't know what Spotify I was going to say track length always", "tokens": [51420, 286, 500, 380, 458, 437, 29036, 286, 390, 516, 281, 584, 2837, 4641, 1009, 51562], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 974, "seek": 382240, "start": 3846.88, "end": 3850.84, "text": " a positive number, but you can have tracks that are negative have negative time as well.", "tokens": [51588, 257, 3353, 1230, 11, 457, 291, 393, 362, 10218, 300, 366, 3671, 362, 3671, 565, 382, 731, 13, 51786], "temperature": 0.0, "avg_logprob": -0.23025380886667143, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0013248006580397487}, {"id": 975, "seek": 385084, "start": 3851.56, "end": 3855.32, "text": " Oh, yeah, great way to hide hide audio in tracks is to go negative.", "tokens": [50400, 876, 11, 1338, 11, 869, 636, 281, 6479, 6479, 6278, 294, 10218, 307, 281, 352, 3671, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 976, "seek": 385084, "start": 3855.76, "end": 3858.4, "text": " Yeah. And you can't if I remember.", "tokens": [50610, 865, 13, 400, 291, 393, 380, 498, 286, 1604, 13, 50742], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 977, "seek": 385084, "start": 3858.4, "end": 3862.04, "text": " So I know this from back to listening to CDs", "tokens": [50742, 407, 286, 458, 341, 490, 646, 281, 4764, 281, 45257, 50924], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 978, "seek": 385084, "start": 3862.44, "end": 3863.8, "text": " because you can't really do that on a tape.", "tokens": [50944, 570, 291, 393, 380, 534, 360, 300, 322, 257, 7314, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 979, "seek": 385084, "start": 3863.8, "end": 3865.76, "text": " There's tape has a start and an end.", "tokens": [51012, 821, 311, 7314, 575, 257, 722, 293, 364, 917, 13, 51110], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 980, "seek": 385084, "start": 3865.76, "end": 3869.48, "text": " But in CDs, you have tracks that have a start and stop time.", "tokens": [51110, 583, 294, 45257, 11, 291, 362, 10218, 300, 362, 257, 722, 293, 1590, 565, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 981, "seek": 385084, "start": 3870.0, "end": 3874.1200000000003, "text": " If you have negative time, the only easy way to get there", "tokens": [51322, 759, 291, 362, 3671, 565, 11, 264, 787, 1858, 636, 281, 483, 456, 51528], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 982, "seek": 385084, "start": 3874.1200000000003, "end": 3879.48, "text": " is to finish the previous song and it will start the next song at the negative time stamp.", "tokens": [51528, 307, 281, 2413, 264, 3894, 2153, 293, 309, 486, 722, 264, 958, 2153, 412, 264, 3671, 565, 9921, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1541174291595211, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0005031273467466235}, {"id": 983, "seek": 387948, "start": 3880.16, "end": 3882.8, "text": " If you skip to the next song, it starts at zero.", "tokens": [50398, 759, 291, 10023, 281, 264, 958, 2153, 11, 309, 3719, 412, 4018, 13, 50530], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 984, "seek": 387948, "start": 3883.08, "end": 3885.8, "text": " And so you would completely miss that like hidden audio.", "tokens": [50544, 400, 370, 291, 576, 2584, 1713, 300, 411, 7633, 6278, 13, 50680], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 985, "seek": 387948, "start": 3886.04, "end": 3891.08, "text": " Oh, is that for instance, like to make the transition between songs nicer?", "tokens": [50692, 876, 11, 307, 300, 337, 5197, 11, 411, 281, 652, 264, 6034, 1296, 5781, 22842, 30, 50944], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 986, "seek": 387948, "start": 3891.72, "end": 3892.84, "text": " I don't know.", "tokens": [50976, 286, 500, 380, 458, 13, 51032], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 987, "seek": 387948, "start": 3892.84, "end": 3896.32, "text": " I maybe but maybe like a translate type thing.", "tokens": [51032, 286, 1310, 457, 1310, 411, 257, 13799, 2010, 551, 13, 51206], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 988, "seek": 387948, "start": 3896.36, "end": 3898.28, "text": " Yeah. Oh, you can hide.", "tokens": [51208, 865, 13, 876, 11, 291, 393, 6479, 13, 51304], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 989, "seek": 387948, "start": 3898.28, "end": 3899.88, "text": " You could hide an entire song there.", "tokens": [51304, 509, 727, 6479, 364, 2302, 2153, 456, 13, 51384], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 990, "seek": 387948, "start": 3899.88, "end": 3904.2400000000002, "text": " I've had CDs back in the day where they had like negative four minutes", "tokens": [51384, 286, 600, 632, 45257, 646, 294, 264, 786, 689, 436, 632, 411, 3671, 1451, 2077, 51602], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 991, "seek": 387948, "start": 3904.2400000000002, "end": 3906.52, "text": " and would hide entire songs in a track.", "tokens": [51602, 293, 576, 6479, 2302, 5781, 294, 257, 2837, 13, 51716], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 992, "seek": 387948, "start": 3906.76, "end": 3908.44, "text": " What the hell? Yeah.", "tokens": [51728, 708, 264, 4921, 30, 865, 13, 51812], "temperature": 0.0, "avg_logprob": -0.24949700270241837, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.001866665086708963}, {"id": 993, "seek": 390844, "start": 3908.44, "end": 3909.44, "text": " Oh, yeah.", "tokens": [50364, 876, 11, 1338, 13, 50414], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 994, "seek": 390844, "start": 3911.32, "end": 3916.8, "text": " I mean, there are songs where the play song, then they have like two minutes", "tokens": [50508, 286, 914, 11, 456, 366, 5781, 689, 264, 862, 2153, 11, 550, 436, 362, 411, 732, 2077, 50782], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 995, "seek": 390844, "start": 3916.8, "end": 3920.4, "text": " of of nothing and then a little thing at the end.", "tokens": [50782, 295, 295, 1825, 293, 550, 257, 707, 551, 412, 264, 917, 13, 50962], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 996, "seek": 390844, "start": 3920.4, "end": 3921.56, "text": " Is that it as well?", "tokens": [50962, 1119, 300, 309, 382, 731, 30, 51020], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 997, "seek": 390844, "start": 3921.56, "end": 3923.84, "text": " Like and that's somehow good.", "tokens": [51020, 1743, 293, 300, 311, 6063, 665, 13, 51134], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 998, "seek": 390844, "start": 3923.84, "end": 3925.92, "text": " That's that's kind of.", "tokens": [51134, 663, 311, 300, 311, 733, 295, 13, 51238], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 999, "seek": 390844, "start": 3925.92, "end": 3929.7200000000003, "text": " But that's not the same because you can like you can easily tell.", "tokens": [51238, 583, 300, 311, 406, 264, 912, 570, 291, 393, 411, 291, 393, 3612, 980, 13, 51428], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 1000, "seek": 390844, "start": 3929.7200000000003, "end": 3934.48, "text": " Oh, my my track length is listed as six minutes and you get to four minutes.", "tokens": [51428, 876, 11, 452, 452, 2837, 4641, 307, 10052, 382, 2309, 2077, 293, 291, 483, 281, 1451, 2077, 13, 51666], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 1001, "seek": 390844, "start": 3934.48, "end": 3936.28, "text": " And then it's just two minutes of white noise.", "tokens": [51666, 400, 550, 309, 311, 445, 732, 2077, 295, 2418, 5658, 13, 51756], "temperature": 0.0, "avg_logprob": -0.24695489345452723, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.0007786384667269886}, {"id": 1002, "seek": 393628, "start": 3936.32, "end": 3938.7200000000003, "text": " And you're like, why is it listed six minutes?", "tokens": [50366, 400, 291, 434, 411, 11, 983, 307, 309, 10052, 2309, 2077, 30, 50486], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1003, "seek": 393628, "start": 3938.7200000000003, "end": 3940.44, "text": " Let me like skip forward to it.", "tokens": [50486, 961, 385, 411, 10023, 2128, 281, 309, 13, 50572], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1004, "seek": 393628, "start": 3940.44, "end": 3942.84, "text": " But if it's negative, that's not listed.", "tokens": [50572, 583, 498, 309, 311, 3671, 11, 300, 311, 406, 10052, 13, 50692], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1005, "seek": 393628, "start": 3943.2400000000002, "end": 3947.6000000000004, "text": " So the only way to really know it's there is to either like rewind.", "tokens": [50712, 407, 264, 787, 636, 281, 534, 458, 309, 311, 456, 307, 281, 2139, 411, 41458, 13, 50930], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1006, "seek": 393628, "start": 3947.8, "end": 3950.0800000000004, "text": " Maybe I don't even know if that works.", "tokens": [50940, 2704, 286, 500, 380, 754, 458, 498, 300, 1985, 13, 51054], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1007, "seek": 393628, "start": 3950.0800000000004, "end": 3955.6400000000003, "text": " I think you have to like finish the previous song and let it auto go to the next section.", "tokens": [51054, 286, 519, 291, 362, 281, 411, 2413, 264, 3894, 2153, 293, 718, 309, 8399, 352, 281, 264, 958, 3541, 13, 51332], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1008, "seek": 393628, "start": 3956.0400000000004, "end": 3960.76, "text": " So so if you're if you're a sucker for shuffling songs in an album,", "tokens": [51352, 407, 370, 498, 291, 434, 498, 291, 434, 257, 43259, 337, 402, 1245, 1688, 5781, 294, 364, 6030, 11, 51588], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1009, "seek": 393628, "start": 3961.2000000000003, "end": 3963.4, "text": " like, no, you're not going to have it.", "tokens": [51610, 411, 11, 572, 11, 291, 434, 406, 516, 281, 362, 309, 13, 51720], "temperature": 0.0, "avg_logprob": -0.14066155751546225, "compression_ratio": 1.6588235294117648, "no_speech_prob": 0.001366797019727528}, {"id": 1010, "seek": 396340, "start": 3963.7200000000003, "end": 3966.76, "text": " I knew shuffle as long as you finish the previous song.", "tokens": [50380, 286, 2586, 39426, 382, 938, 382, 291, 2413, 264, 3894, 2153, 13, 50532], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1011, "seek": 396340, "start": 3967.12, "end": 3969.48, "text": " But I don't know if that works in web with MP3s.", "tokens": [50550, 583, 286, 500, 380, 458, 498, 300, 1985, 294, 3670, 365, 14146, 18, 82, 13, 50668], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1012, "seek": 396340, "start": 3969.48, "end": 3971.56, "text": " I don't know if MP3s can go negative.", "tokens": [50668, 286, 500, 380, 458, 498, 14146, 18, 82, 393, 352, 3671, 13, 50772], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1013, "seek": 396340, "start": 3971.56, "end": 3975.96, "text": " It might just be a it might be a forgotten thing, a lost thing with CDs.", "tokens": [50772, 467, 1062, 445, 312, 257, 309, 1062, 312, 257, 11832, 551, 11, 257, 2731, 551, 365, 45257, 13, 50992], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1014, "seek": 396340, "start": 3976.36, "end": 3978.2000000000003, "text": " I'm very curious now.", "tokens": [51012, 286, 478, 588, 6369, 586, 13, 51104], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1015, "seek": 396340, "start": 3978.2000000000003, "end": 3980.76, "text": " I might have to after this, I'm going to look all this up now.", "tokens": [51104, 286, 1062, 362, 281, 934, 341, 11, 286, 478, 516, 281, 574, 439, 341, 493, 586, 13, 51232], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1016, "seek": 396340, "start": 3980.76, "end": 3982.6800000000003, "text": " I'm really curious because I forgot.", "tokens": [51232, 286, 478, 534, 6369, 570, 286, 5298, 13, 51328], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1017, "seek": 396340, "start": 3982.6800000000003, "end": 3984.44, "text": " You just play this.", "tokens": [51328, 509, 445, 862, 341, 13, 51416], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1018, "seek": 396340, "start": 3984.44, "end": 3987.36, "text": " Yeah. So yeah, no big type for that one.", "tokens": [51416, 865, 13, 407, 1338, 11, 572, 955, 2010, 337, 300, 472, 13, 51562], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1019, "seek": 396340, "start": 3988.0, "end": 3992.8, "text": " No, in less in less than MP3s, you can't have negative time.", "tokens": [51594, 883, 11, 294, 1570, 294, 1570, 813, 14146, 18, 82, 11, 291, 393, 380, 362, 3671, 565, 13, 51834], "temperature": 0.0, "avg_logprob": -0.29282608032226565, "compression_ratio": 1.7, "no_speech_prob": 0.005381718277931213}, {"id": 1020, "seek": 399280, "start": 3992.88, "end": 3996.1200000000003, "text": " I don't know. Now I have to kind of hope it doesn't.", "tokens": [50368, 286, 500, 380, 458, 13, 823, 286, 362, 281, 733, 295, 1454, 309, 1177, 380, 13, 50530], "temperature": 0.0, "avg_logprob": -0.24937327221186475, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0027115223929286003}, {"id": 1021, "seek": 399280, "start": 3998.36, "end": 4000.7200000000003, "text": " It is pretty cool to be able to do that.", "tokens": [50642, 467, 307, 1238, 1627, 281, 312, 1075, 281, 360, 300, 13, 50760], "temperature": 0.0, "avg_logprob": -0.24937327221186475, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0027115223929286003}, {"id": 1022, "seek": 399280, "start": 4000.7200000000003, "end": 4005.36, "text": " So yeah, so they Spotify could go and add their own flavor", "tokens": [50760, 407, 1338, 11, 370, 436, 29036, 727, 352, 293, 909, 641, 1065, 6813, 50992], "temperature": 0.0, "avg_logprob": -0.24937327221186475, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0027115223929286003}, {"id": 1023, "seek": 399280, "start": 4005.36, "end": 4009.32, "text": " onto the SDK generation that really makes it shine.", "tokens": [50992, 3911, 264, 37135, 5125, 300, 534, 1669, 309, 12207, 13, 51190], "temperature": 0.0, "avg_logprob": -0.24937327221186475, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0027115223929286003}, {"id": 1024, "seek": 399280, "start": 4009.32, "end": 4015.2400000000002, "text": " That if you or I went and generated their their SDK ourselves, we wouldn't have.", "tokens": [51190, 663, 498, 291, 420, 286, 1437, 293, 10833, 641, 641, 37135, 4175, 11, 321, 2759, 380, 362, 13, 51486], "temperature": 0.0, "avg_logprob": -0.24937327221186475, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0027115223929286003}, {"id": 1025, "seek": 399280, "start": 4016.6000000000004, "end": 4022.52, "text": " So I know we've talked about it during one of many Elm Cogen episodes.", "tokens": [51554, 407, 286, 458, 321, 600, 2825, 466, 309, 1830, 472, 295, 867, 2699, 76, 383, 8799, 9313, 13, 51850], "temperature": 0.0, "avg_logprob": -0.24937327221186475, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0027115223929286003}, {"id": 1026, "seek": 402280, "start": 4022.8, "end": 4027.0, "text": " But it's still worth pointing out that it's really cool that in Elm,", "tokens": [50364, 583, 309, 311, 920, 3163, 12166, 484, 300, 309, 311, 534, 1627, 300, 294, 2699, 76, 11, 50574], "temperature": 0.0, "avg_logprob": -0.21190460797013908, "compression_ratio": 1.6611570247933884, "no_speech_prob": 0.00021649924747180194}, {"id": 1027, "seek": 402280, "start": 4027.0, "end": 4034.36, "text": " when you generate these huge API files, you only get in a ship, whatever you use.", "tokens": [50574, 562, 291, 8460, 613, 2603, 9362, 7098, 11, 291, 787, 483, 294, 257, 5374, 11, 2035, 291, 764, 13, 50942], "temperature": 0.0, "avg_logprob": -0.21190460797013908, "compression_ratio": 1.6611570247933884, "no_speech_prob": 0.00021649924747180194}, {"id": 1028, "seek": 402280, "start": 4034.8, "end": 4036.48, "text": " And I find that to be so cool.", "tokens": [50964, 400, 286, 915, 300, 281, 312, 370, 1627, 13, 51048], "temperature": 0.0, "avg_logprob": -0.21190460797013908, "compression_ratio": 1.6611570247933884, "no_speech_prob": 0.00021649924747180194}, {"id": 1029, "seek": 402280, "start": 4036.48, "end": 4040.84, "text": " But like you just said, like, oh, yeah, like you forgot about it", "tokens": [51048, 583, 411, 291, 445, 848, 11, 411, 11, 1954, 11, 1338, 11, 411, 291, 5298, 466, 309, 51266], "temperature": 0.0, "avg_logprob": -0.21190460797013908, "compression_ratio": 1.6611570247933884, "no_speech_prob": 0.00021649924747180194}, {"id": 1030, "seek": 402280, "start": 4040.84, "end": 4042.52, "text": " because you don't have to think about it anymore.", "tokens": [51266, 570, 291, 500, 380, 362, 281, 519, 466, 309, 3602, 13, 51350], "temperature": 0.0, "avg_logprob": -0.21190460797013908, "compression_ratio": 1.6611570247933884, "no_speech_prob": 0.00021649924747180194}, {"id": 1031, "seek": 402280, "start": 4042.52, "end": 4044.32, "text": " It's just fun. It's so cool.", "tokens": [51350, 467, 311, 445, 1019, 13, 467, 311, 370, 1627, 13, 51440], "temperature": 0.0, "avg_logprob": -0.21190460797013908, "compression_ratio": 1.6611570247933884, "no_speech_prob": 0.00021649924747180194}, {"id": 1032, "seek": 402280, "start": 4044.32, "end": 4048.52, "text": " Yeah, I was when I initially started this out, I was playing with the GitHub", "tokens": [51440, 865, 11, 286, 390, 562, 286, 9105, 1409, 341, 484, 11, 286, 390, 2433, 365, 264, 23331, 51650], "temperature": 0.0, "avg_logprob": -0.21190460797013908, "compression_ratio": 1.6611570247933884, "no_speech_prob": 0.00021649924747180194}, {"id": 1033, "seek": 404852, "start": 4049.04, "end": 4050.92, "text": " Open API spec.", "tokens": [50390, 7238, 9362, 1608, 13, 50484], "temperature": 0.0, "avg_logprob": -0.18278916676839194, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.007575363852083683}, {"id": 1034, "seek": 404852, "start": 4050.92, "end": 4055.36, "text": " And I think at one point I was getting during some of the experimentation,", "tokens": [50484, 400, 286, 519, 412, 472, 935, 286, 390, 1242, 1830, 512, 295, 264, 37142, 11, 50706], "temperature": 0.0, "avg_logprob": -0.18278916676839194, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.007575363852083683}, {"id": 1035, "seek": 404852, "start": 4055.36, "end": 4060.04, "text": " we were getting up around like 10 or 15,000 lines of code, I want to say.", "tokens": [50706, 321, 645, 1242, 493, 926, 411, 1266, 420, 2119, 11, 1360, 3876, 295, 3089, 11, 286, 528, 281, 584, 13, 50940], "temperature": 0.0, "avg_logprob": -0.18278916676839194, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.007575363852083683}, {"id": 1036, "seek": 404852, "start": 4060.6, "end": 4062.8, "text": " Maybe more than it was probably more than that.", "tokens": [50968, 2704, 544, 813, 309, 390, 1391, 544, 813, 300, 13, 51078], "temperature": 0.0, "avg_logprob": -0.18278916676839194, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.007575363852083683}, {"id": 1037, "seek": 404852, "start": 4063.32, "end": 4066.12, "text": " Yeah, that's actually not as bad as I would have imagined.", "tokens": [51104, 865, 11, 300, 311, 767, 406, 382, 1578, 382, 286, 576, 362, 16590, 13, 51244], "temperature": 0.0, "avg_logprob": -0.18278916676839194, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.007575363852083683}, {"id": 1038, "seek": 404852, "start": 4066.52, "end": 4071.2, "text": " It's not it's definitely not the biggest Elm module by far.", "tokens": [51264, 467, 311, 406, 309, 311, 2138, 406, 264, 3880, 2699, 76, 10088, 538, 1400, 13, 51498], "temperature": 0.0, "avg_logprob": -0.18278916676839194, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.007575363852083683}, {"id": 1039, "seek": 404852, "start": 4071.92, "end": 4075.44, "text": " But I don't think GitHub's API is the largest API either.", "tokens": [51534, 583, 286, 500, 380, 519, 23331, 311, 9362, 307, 264, 6443, 9362, 2139, 13, 51710], "temperature": 0.0, "avg_logprob": -0.18278916676839194, "compression_ratio": 1.5275590551181102, "no_speech_prob": 0.007575363852083683}, {"id": 1040, "seek": 407544, "start": 4075.8, "end": 4080.16, "text": " So I do wonder if there are APIs out there.", "tokens": [50382, 407, 286, 360, 2441, 498, 456, 366, 21445, 484, 456, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1041, "seek": 407544, "start": 4080.16, "end": 4084.08, "text": " There probably are that would that would break Elm if I generated it right now.", "tokens": [50600, 821, 1391, 366, 300, 576, 300, 576, 1821, 2699, 76, 498, 286, 10833, 309, 558, 586, 13, 50796], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1042, "seek": 407544, "start": 4084.2400000000002, "end": 4085.88, "text": " I would imagine there are.", "tokens": [50804, 286, 576, 3811, 456, 366, 13, 50886], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1043, "seek": 407544, "start": 4085.88, "end": 4087.64, "text": " I should try and find one.", "tokens": [50886, 286, 820, 853, 293, 915, 472, 13, 50974], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1044, "seek": 407544, "start": 4087.64, "end": 4090.92, "text": " I wonder if there are any listeners who know of one that is big enough", "tokens": [50974, 286, 2441, 498, 456, 366, 604, 23274, 567, 458, 295, 472, 300, 307, 955, 1547, 51138], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1045, "seek": 407544, "start": 4090.92, "end": 4095.56, "text": " that would that would break the Elm compiler because the generated code is just too large.", "tokens": [51138, 300, 576, 300, 576, 1821, 264, 2699, 76, 31958, 570, 264, 10833, 3089, 307, 445, 886, 2416, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1046, "seek": 407544, "start": 4095.7200000000003, "end": 4099.68, "text": " Yeah, if you generate you generate your SDK, you only use the commands.", "tokens": [51378, 865, 11, 498, 291, 8460, 291, 8460, 428, 37135, 11, 291, 787, 764, 264, 16901, 13, 51576], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1047, "seek": 407544, "start": 4100.0, "end": 4102.0, "text": " You never use a task.", "tokens": [51592, 509, 1128, 764, 257, 5633, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1312949113678514, "compression_ratio": 1.8504273504273505, "no_speech_prob": 0.001000415999442339}, {"id": 1048, "seek": 410200, "start": 4102.04, "end": 4106.76, "text": " That's a third, at least, if not half of the Elm module that is just completely", "tokens": [50366, 663, 311, 257, 2636, 11, 412, 1935, 11, 498, 406, 1922, 295, 264, 2699, 76, 10088, 300, 307, 445, 2584, 50602], "temperature": 0.0, "avg_logprob": -0.2063600398876049, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0011333434376865625}, {"id": 1049, "seek": 410200, "start": 4106.76, "end": 4108.36, "text": " ignored and never shipped.", "tokens": [50602, 19735, 293, 1128, 25312, 13, 50682], "temperature": 0.0, "avg_logprob": -0.2063600398876049, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0011333434376865625}, {"id": 1050, "seek": 410200, "start": 4108.36, "end": 4110.04, "text": " Yeah, in your code.", "tokens": [50682, 865, 11, 294, 428, 3089, 13, 50766], "temperature": 0.0, "avg_logprob": -0.2063600398876049, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0011333434376865625}, {"id": 1051, "seek": 410200, "start": 4110.04, "end": 4110.24, "text": " Yeah.", "tokens": [50766, 865, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2063600398876049, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0011333434376865625}, {"id": 1052, "seek": 410200, "start": 4110.24, "end": 4115.6, "text": " And if you expose all the decoders and you don't use them, that's fine.", "tokens": [50776, 400, 498, 291, 19219, 439, 264, 979, 378, 433, 293, 291, 500, 380, 764, 552, 11, 300, 311, 2489, 13, 51044], "temperature": 0.0, "avg_logprob": -0.2063600398876049, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0011333434376865625}, {"id": 1053, "seek": 410200, "start": 4115.6, "end": 4119.44, "text": " And therefore, that's why it's fine for you to expose all those things as well.", "tokens": [51044, 400, 4412, 11, 300, 311, 983, 309, 311, 2489, 337, 291, 281, 19219, 439, 729, 721, 382, 731, 13, 51236], "temperature": 0.0, "avg_logprob": -0.2063600398876049, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0011333434376865625}, {"id": 1054, "seek": 410200, "start": 4120.68, "end": 4126.68, "text": " And I can't imagine like if you do this in TypeScript, like how how do you avoid", "tokens": [51298, 400, 286, 393, 380, 3811, 411, 498, 291, 360, 341, 294, 15576, 14237, 11, 411, 577, 577, 360, 291, 5042, 51598], "temperature": 0.0, "avg_logprob": -0.2063600398876049, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0011333434376865625}, {"id": 1055, "seek": 412668, "start": 4127.400000000001, "end": 4132.64, "text": " bundling all that code with with your production bundle?", "tokens": [50400, 13882, 1688, 439, 300, 3089, 365, 365, 428, 4265, 24438, 30, 50662], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1056, "seek": 412668, "start": 4132.64, "end": 4133.04, "text": " Right.", "tokens": [50662, 1779, 13, 50682], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1057, "seek": 412668, "start": 4133.4800000000005, "end": 4134.04, "text": " Yeah.", "tokens": [50704, 865, 13, 50732], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1058, "seek": 412668, "start": 4134.400000000001, "end": 4137.72, "text": " So there's like module that code in a donation.", "tokens": [50750, 407, 456, 311, 411, 10088, 300, 3089, 294, 257, 19724, 13, 50916], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1059, "seek": 412668, "start": 4137.72, "end": 4140.12, "text": " Like if you don't import a module, it won't be imported.", "tokens": [50916, 1743, 498, 291, 500, 380, 974, 257, 10088, 11, 309, 1582, 380, 312, 25524, 13, 51036], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1060, "seek": 412668, "start": 4140.4400000000005, "end": 4145.76, "text": " So you need to split things up by module, but it also uses objects, right?", "tokens": [51052, 407, 291, 643, 281, 7472, 721, 493, 538, 10088, 11, 457, 309, 611, 4960, 6565, 11, 558, 30, 51318], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1061, "seek": 412668, "start": 4146.04, "end": 4149.360000000001, "text": " So anything or classes.", "tokens": [51332, 407, 1340, 420, 5359, 13, 51498], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1062, "seek": 412668, "start": 4149.56, "end": 4153.84, "text": " So if you do anything with a product, then that for sure, all the things", "tokens": [51508, 407, 498, 291, 360, 1340, 365, 257, 1674, 11, 550, 300, 337, 988, 11, 439, 264, 721, 51722], "temperature": 0.0, "avg_logprob": -0.21462900951655225, "compression_ratio": 1.6878048780487804, "no_speech_prob": 0.024036092683672905}, {"id": 1063, "seek": 415384, "start": 4153.88, "end": 4156.6, "text": " that are related to products are going to get shipped.", "tokens": [50366, 300, 366, 4077, 281, 3383, 366, 516, 281, 483, 25312, 13, 50502], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1064, "seek": 415384, "start": 4157.6, "end": 4158.04, "text": " Yeah.", "tokens": [50552, 865, 13, 50574], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1065, "seek": 415384, "start": 4158.24, "end": 4161.88, "text": " Like if you're if you're using Spotify, for example, maybe you never hit", "tokens": [50584, 1743, 498, 291, 434, 498, 291, 434, 1228, 29036, 11, 337, 1365, 11, 1310, 291, 1128, 2045, 50766], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1066, "seek": 415384, "start": 4161.88, "end": 4167.04, "text": " put or artists, you never hit the artist endpoint, but every song has an artist.", "tokens": [50766, 829, 420, 6910, 11, 291, 1128, 2045, 264, 5748, 35795, 11, 457, 633, 2153, 575, 364, 5748, 13, 51024], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1067, "seek": 415384, "start": 4167.04, "end": 4169.76, "text": " So you need the artist decoder for every song.", "tokens": [51024, 407, 291, 643, 264, 5748, 979, 19866, 337, 633, 2153, 13, 51160], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1068, "seek": 415384, "start": 4169.96, "end": 4170.360000000001, "text": " Yes.", "tokens": [51170, 1079, 13, 51190], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1069, "seek": 415384, "start": 4170.360000000001, "end": 4172.400000000001, "text": " So regardless, that's going to get shipped.", "tokens": [51190, 407, 10060, 11, 300, 311, 516, 281, 483, 25312, 13, 51292], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1070, "seek": 415384, "start": 4172.4400000000005, "end": 4175.88, "text": " So you want the banana, you get the gorilla and the giant.", "tokens": [51294, 407, 291, 528, 264, 14194, 11, 291, 483, 264, 45066, 293, 264, 7410, 13, 51466], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1071, "seek": 415384, "start": 4176.88, "end": 4177.2, "text": " Yeah.", "tokens": [51516, 865, 13, 51532], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1072, "seek": 415384, "start": 4177.32, "end": 4177.56, "text": " Yeah.", "tokens": [51538, 865, 13, 51550], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1073, "seek": 415384, "start": 4179.400000000001, "end": 4179.84, "text": " Yeah.", "tokens": [51642, 865, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3481244957965353, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.003026978811249137}, {"id": 1074, "seek": 417984, "start": 4179.84, "end": 4183.72, "text": " I do wonder how that works from language to language.", "tokens": [50364, 286, 360, 2441, 577, 300, 1985, 490, 2856, 281, 2856, 13, 50558], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1075, "seek": 417984, "start": 4183.92, "end": 4190.04, "text": " I should I should generate trying to generate the square square SDK for", "tokens": [50568, 286, 820, 286, 820, 8460, 1382, 281, 8460, 264, 3732, 3732, 37135, 337, 50874], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1076, "seek": 417984, "start": 4190.04, "end": 4193.08, "text": " Elm and see how big it ends up compared to the other languages.", "tokens": [50874, 2699, 76, 293, 536, 577, 955, 309, 5314, 493, 5347, 281, 264, 661, 8650, 13, 51026], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1077, "seek": 417984, "start": 4193.08, "end": 4196.88, "text": " Because I know they ship like six to eight languages, I think it is.", "tokens": [51026, 1436, 286, 458, 436, 5374, 411, 2309, 281, 3180, 8650, 11, 286, 519, 309, 307, 13, 51216], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1078, "seek": 417984, "start": 4198.08, "end": 4200.92, "text": " I'm guessing it's on NPM and you can find it there.", "tokens": [51276, 286, 478, 17939, 309, 311, 322, 426, 18819, 293, 291, 393, 915, 309, 456, 13, 51418], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1079, "seek": 417984, "start": 4200.96, "end": 4202.88, "text": " See how big the API is.", "tokens": [51420, 3008, 577, 955, 264, 9362, 307, 13, 51516], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1080, "seek": 417984, "start": 4203.4400000000005, "end": 4205.4400000000005, "text": " It is MPM.", "tokens": [51544, 467, 307, 14146, 44, 13, 51644], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1081, "seek": 417984, "start": 4205.4400000000005, "end": 4207.4400000000005, "text": " They ship a square.", "tokens": [51644, 814, 5374, 257, 3732, 13, 51744], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1082, "seek": 417984, "start": 4207.8, "end": 4208.72, "text": " It's just square.", "tokens": [51762, 467, 311, 445, 3732, 13, 51808], "temperature": 0.0, "avg_logprob": -0.330160243170602, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.008702321909368038}, {"id": 1083, "seek": 420872, "start": 4208.72, "end": 4212.4800000000005, "text": " I think square square up square.", "tokens": [50364, 286, 519, 3732, 3732, 493, 3732, 13, 50552], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1084, "seek": 420872, "start": 4213.2, "end": 4216.400000000001, "text": " It is unpacked.", "tokens": [50588, 467, 307, 26699, 292, 13, 50748], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1085, "seek": 420872, "start": 4216.4400000000005, "end": 4219.52, "text": " It is six just shy of seven megabytes.", "tokens": [50750, 467, 307, 2309, 445, 12685, 295, 3407, 10816, 24538, 13, 50904], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1086, "seek": 420872, "start": 4220.320000000001, "end": 4221.88, "text": " That's for the node SDK.", "tokens": [50944, 663, 311, 337, 264, 9984, 37135, 13, 51022], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1087, "seek": 420872, "start": 4222.4800000000005, "end": 4227.12, "text": " I don't know how big they they also have the Java one on Maven.", "tokens": [51052, 286, 500, 380, 458, 577, 955, 436, 436, 611, 362, 264, 10745, 472, 322, 4042, 553, 13, 51284], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1088, "seek": 420872, "start": 4227.56, "end": 4231.68, "text": " There's a PHP one Rails C sharp.", "tokens": [51306, 821, 311, 257, 47298, 472, 48526, 383, 8199, 13, 51512], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1089, "seek": 420872, "start": 4232.08, "end": 4232.320000000001, "text": " Right.", "tokens": [51532, 1779, 13, 51544], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1090, "seek": 420872, "start": 4232.64, "end": 4232.92, "text": " Okay.", "tokens": [51560, 1033, 13, 51574], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1091, "seek": 420872, "start": 4232.92, "end": 4234.96, "text": " So this is really specifically for node.", "tokens": [51574, 407, 341, 307, 534, 4682, 337, 9984, 13, 51676], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1092, "seek": 420872, "start": 4234.96, "end": 4237.56, "text": " It's not for front end JavaScript.", "tokens": [51676, 467, 311, 406, 337, 1868, 917, 15778, 13, 51806], "temperature": 0.0, "avg_logprob": -0.27648308350867834, "compression_ratio": 1.4679802955665024, "no_speech_prob": 0.0005439756787382066}, {"id": 1093, "seek": 423756, "start": 4238.04, "end": 4241.0, "text": " That would definitely be a problem for your bundle size.", "tokens": [50388, 663, 576, 2138, 312, 257, 1154, 337, 428, 24438, 2744, 13, 50536], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1094, "seek": 423756, "start": 4241.0, "end": 4242.52, "text": " That's that's a lot.", "tokens": [50536, 663, 311, 300, 311, 257, 688, 13, 50612], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1095, "seek": 423756, "start": 4242.68, "end": 4243.080000000001, "text": " Yeah.", "tokens": [50620, 865, 13, 50640], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1096, "seek": 423756, "start": 4244.320000000001, "end": 4248.68, "text": " That does I wonder though, like does that affect I don't know enough about various", "tokens": [50702, 663, 775, 286, 2441, 1673, 11, 411, 775, 300, 3345, 286, 500, 380, 458, 1547, 466, 3683, 50920], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1097, "seek": 423756, "start": 4248.68, "end": 4254.84, "text": " languages that have start like like Java has a startup time and does the size of", "tokens": [50920, 8650, 300, 362, 722, 411, 411, 10745, 575, 257, 18578, 565, 293, 775, 264, 2744, 295, 51228], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1098, "seek": 423756, "start": 4254.84, "end": 4259.120000000001, "text": " your dependencies drastically affect the startup time of your server then?", "tokens": [51228, 428, 36606, 29673, 3345, 264, 18578, 565, 295, 428, 7154, 550, 30, 51442], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1099, "seek": 423756, "start": 4259.92, "end": 4263.68, "text": " It's a problem for for cold starts for serverless functions.", "tokens": [51482, 467, 311, 257, 1154, 337, 337, 3554, 3719, 337, 7154, 1832, 6828, 13, 51670], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1100, "seek": 423756, "start": 4263.68, "end": 4264.400000000001, "text": " That's for sure.", "tokens": [51670, 663, 311, 337, 988, 13, 51706], "temperature": 0.0, "avg_logprob": -0.2521321891558052, "compression_ratio": 1.7467248908296944, "no_speech_prob": 0.00028677767841145396}, {"id": 1101, "seek": 426440, "start": 4264.48, "end": 4269.04, "text": " It there's a there's a hard limit and it slows it down.", "tokens": [50368, 467, 456, 311, 257, 456, 311, 257, 1152, 4948, 293, 309, 35789, 309, 760, 13, 50596], "temperature": 0.0, "avg_logprob": -0.23764179529768698, "compression_ratio": 1.617801047120419, "no_speech_prob": 0.0008036658400669694}, {"id": 1102, "seek": 426440, "start": 4269.4, "end": 4272.639999999999, "text": " So yeah, so that is an interesting thing.", "tokens": [50614, 407, 1338, 11, 370, 300, 307, 364, 1880, 551, 13, 50776], "temperature": 0.0, "avg_logprob": -0.23764179529768698, "compression_ratio": 1.617801047120419, "no_speech_prob": 0.0008036658400669694}, {"id": 1103, "seek": 426440, "start": 4272.639999999999, "end": 4274.0, "text": " I never considered that.", "tokens": [50776, 286, 1128, 4888, 300, 13, 50844], "temperature": 0.0, "avg_logprob": -0.23764179529768698, "compression_ratio": 1.617801047120419, "no_speech_prob": 0.0008036658400669694}, {"id": 1104, "seek": 426440, "start": 4275.4, "end": 4279.04, "text": " So we talked about cogeneration for open API specs.", "tokens": [50914, 407, 321, 2825, 466, 598, 30372, 337, 1269, 9362, 27911, 13, 51096], "temperature": 0.0, "avg_logprob": -0.23764179529768698, "compression_ratio": 1.617801047120419, "no_speech_prob": 0.0008036658400669694}, {"id": 1105, "seek": 426440, "start": 4279.04, "end": 4281.44, "text": " We talked about cogeneration for GraphQL.", "tokens": [51096, 492, 2825, 466, 598, 30372, 337, 21884, 13695, 13, 51216], "temperature": 0.0, "avg_logprob": -0.23764179529768698, "compression_ratio": 1.617801047120419, "no_speech_prob": 0.0008036658400669694}, {"id": 1106, "seek": 426440, "start": 4282.28, "end": 4285.12, "text": " What else is missing in that realm?", "tokens": [51258, 708, 1646, 307, 5361, 294, 300, 15355, 30, 51400], "temperature": 0.0, "avg_logprob": -0.23764179529768698, "compression_ratio": 1.617801047120419, "no_speech_prob": 0.0008036658400669694}, {"id": 1107, "seek": 426440, "start": 4285.28, "end": 4289.28, "text": " Like do we have protobufs, cogeneration that is missing?", "tokens": [51408, 1743, 360, 321, 362, 1742, 996, 2947, 82, 11, 598, 30372, 300, 307, 5361, 30, 51608], "temperature": 0.0, "avg_logprob": -0.23764179529768698, "compression_ratio": 1.617801047120419, "no_speech_prob": 0.0008036658400669694}, {"id": 1108, "seek": 428928, "start": 4289.5199999999995, "end": 4294.719999999999, "text": " Do we have other applications that that are not even related to SDKs?", "tokens": [50376, 1144, 321, 362, 661, 5821, 300, 300, 366, 406, 754, 4077, 281, 37135, 82, 30, 50636], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1109, "seek": 428928, "start": 4295.28, "end": 4295.88, "text": " There are.", "tokens": [50664, 821, 366, 13, 50694], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1110, "seek": 428928, "start": 4295.88, "end": 4296.639999999999, "text": " I think there is.", "tokens": [50694, 286, 519, 456, 307, 13, 50732], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1111, "seek": 428928, "start": 4297.639999999999, "end": 4301.5599999999995, "text": " I want to say there's a protobuf some some Elmstaffer protobuf.", "tokens": [50782, 286, 528, 281, 584, 456, 311, 257, 1742, 996, 2947, 512, 512, 2699, 76, 372, 2518, 260, 1742, 996, 2947, 13, 50978], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1112, "seek": 428928, "start": 4301.92, "end": 4303.5199999999995, "text": " Yeah, there is actually.", "tokens": [50996, 865, 11, 456, 307, 767, 13, 51076], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1113, "seek": 428928, "start": 4304.28, "end": 4304.88, "text": " What is it?", "tokens": [51114, 708, 307, 309, 30, 51144], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1114, "seek": 428928, "start": 4305.2, "end": 4310.639999999999, "text": " There is one for NATS that was just released this week.", "tokens": [51160, 821, 307, 472, 337, 14500, 50, 300, 390, 445, 4736, 341, 1243, 13, 51432], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1115, "seek": 428928, "start": 4311.44, "end": 4315.24, "text": " I don't know what NATS is yet, but I am kind of curious.", "tokens": [51472, 286, 500, 380, 458, 437, 14500, 50, 307, 1939, 11, 457, 286, 669, 733, 295, 6369, 13, 51662], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1116, "seek": 428928, "start": 4316.08, "end": 4318.16, "text": " I think there's gRPC is another one.", "tokens": [51704, 286, 519, 456, 311, 290, 49, 12986, 307, 1071, 472, 13, 51808], "temperature": 0.0, "avg_logprob": -0.26308880681576935, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.007694232277572155}, {"id": 1117, "seek": 431816, "start": 4318.16, "end": 4320.4, "text": " I think I've seen something for that realm.", "tokens": [50364, 286, 519, 286, 600, 1612, 746, 337, 300, 15355, 13, 50476], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1118, "seek": 431816, "start": 4321.04, "end": 4323.28, "text": " There's always a few new ones here and there.", "tokens": [50508, 821, 311, 1009, 257, 1326, 777, 2306, 510, 293, 456, 13, 50620], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1119, "seek": 431816, "start": 4323.639999999999, "end": 4326.28, "text": " There's one that's message something.", "tokens": [50638, 821, 311, 472, 300, 311, 3636, 746, 13, 50770], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1120, "seek": 431816, "start": 4327.32, "end": 4330.4, "text": " Web message or something like along those lines.", "tokens": [50822, 9573, 3636, 420, 746, 411, 2051, 729, 3876, 13, 50976], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1121, "seek": 431816, "start": 4330.599999999999, "end": 4333.8, "text": " I think most have at least a minimal implementation in Elm.", "tokens": [50986, 286, 519, 881, 362, 412, 1935, 257, 13206, 11420, 294, 2699, 76, 13, 51146], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1122, "seek": 431816, "start": 4334.32, "end": 4334.72, "text": " I do.", "tokens": [51172, 286, 360, 13, 51192], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1123, "seek": 431816, "start": 4334.72, "end": 4339.12, "text": " So the other code gen aspect that I would like to explore someday.", "tokens": [51192, 407, 264, 661, 3089, 1049, 4171, 300, 286, 576, 411, 281, 6839, 19412, 13, 51412], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1124, "seek": 431816, "start": 4340.32, "end": 4345.04, "text": " Maybe maybe I'm not sure if I'm going to go to it before or after forms is testing.", "tokens": [51472, 2704, 1310, 286, 478, 406, 988, 498, 286, 478, 516, 281, 352, 281, 309, 949, 420, 934, 6422, 307, 4997, 13, 51708], "temperature": 0.0, "avg_logprob": -0.19774873680043442, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0006262128590606153}, {"id": 1125, "seek": 434504, "start": 4346.0, "end": 4352.16, "text": " I figure if I can generate the endpoints, can I not generate Elm test code as well", "tokens": [50412, 286, 2573, 498, 286, 393, 8460, 264, 917, 20552, 11, 393, 286, 406, 8460, 2699, 76, 1500, 3089, 382, 731, 50720], "temperature": 0.0, "avg_logprob": -0.26789188385009766, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.002756123198196292}, {"id": 1126, "seek": 434504, "start": 4352.16, "end": 4358.0, "text": " for those endpoints so that like if you were right, say you were using Elm program test,", "tokens": [50720, 337, 729, 917, 20552, 370, 300, 411, 498, 291, 645, 558, 11, 584, 291, 645, 1228, 2699, 76, 1461, 1500, 11, 51012], "temperature": 0.0, "avg_logprob": -0.26789188385009766, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.002756123198196292}, {"id": 1127, "seek": 434504, "start": 4358.72, "end": 4366.16, "text": " you could send out your requests and have it respond with fake data essentially that is generated.", "tokens": [51048, 291, 727, 2845, 484, 428, 12475, 293, 362, 309, 4196, 365, 7592, 1412, 4476, 300, 307, 10833, 13, 51420], "temperature": 0.0, "avg_logprob": -0.26789188385009766, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.002756123198196292}, {"id": 1128, "seek": 434504, "start": 4366.16, "end": 4367.2, "text": " Yes.", "tokens": [51420, 1079, 13, 51472], "temperature": 0.0, "avg_logprob": -0.26789188385009766, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.002756123198196292}, {"id": 1129, "seek": 434504, "start": 4367.2, "end": 4367.5199999999995, "text": " Yeah.", "tokens": [51472, 865, 13, 51488], "temperature": 0.0, "avg_logprob": -0.26789188385009766, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.002756123198196292}, {"id": 1130, "seek": 434504, "start": 4368.24, "end": 4370.8, "text": " I've thought about this for Elm GraphQL as well.", "tokens": [51524, 286, 600, 1194, 466, 341, 337, 2699, 76, 21884, 13695, 382, 731, 13, 51652], "temperature": 0.0, "avg_logprob": -0.26789188385009766, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.002756123198196292}, {"id": 1131, "seek": 437080, "start": 4370.8, "end": 4380.4800000000005, "text": " Like there's this factory girl Ruby gem where you're using these testing factories to basically say", "tokens": [50364, 1743, 456, 311, 341, 9265, 2013, 19907, 7173, 689, 291, 434, 1228, 613, 4997, 24813, 281, 1936, 584, 50848], "temperature": 0.0, "avg_logprob": -0.12867086361616087, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0023573520593345165}, {"id": 1132, "seek": 437080, "start": 4381.12, "end": 4383.68, "text": " it knows what kind of data it's generating.", "tokens": [50880, 309, 3255, 437, 733, 295, 1412, 309, 311, 17746, 13, 51008], "temperature": 0.0, "avg_logprob": -0.12867086361616087, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0023573520593345165}, {"id": 1133, "seek": 437080, "start": 4383.68, "end": 4388.8, "text": " You can let it generate some random things or give specific data in some of those pieces.", "tokens": [51008, 509, 393, 718, 309, 8460, 512, 4974, 721, 420, 976, 2685, 1412, 294, 512, 295, 729, 3755, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12867086361616087, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0023573520593345165}, {"id": 1134, "seek": 437080, "start": 4388.8, "end": 4391.92, "text": " But that would be very interesting for sure.", "tokens": [51264, 583, 300, 576, 312, 588, 1880, 337, 988, 13, 51420], "temperature": 0.0, "avg_logprob": -0.12867086361616087, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0023573520593345165}, {"id": 1135, "seek": 437080, "start": 4391.92, "end": 4395.28, "text": " But generating a nice API for it is a challenge.", "tokens": [51420, 583, 17746, 257, 1481, 9362, 337, 309, 307, 257, 3430, 13, 51588], "temperature": 0.0, "avg_logprob": -0.12867086361616087, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0023573520593345165}, {"id": 1136, "seek": 439528, "start": 4395.36, "end": 4395.84, "text": " Yeah.", "tokens": [50368, 865, 13, 50392], "temperature": 0.0, "avg_logprob": -0.24559307098388672, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.003704747883602977}, {"id": 1137, "seek": 439528, "start": 4395.84, "end": 4400.8, "text": " Also like generating results that make sense can be difficult, I think.", "tokens": [50392, 2743, 411, 17746, 3542, 300, 652, 2020, 393, 312, 2252, 11, 286, 519, 13, 50640], "temperature": 0.0, "avg_logprob": -0.24559307098388672, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.003704747883602977}, {"id": 1138, "seek": 439528, "start": 4400.8, "end": 4411.04, "text": " For instance, if you like create a user, then you need to return the same data that you gave to generate the user.", "tokens": [50640, 1171, 5197, 11, 498, 291, 411, 1884, 257, 4195, 11, 550, 291, 643, 281, 2736, 264, 912, 1412, 300, 291, 2729, 281, 8460, 264, 4195, 13, 51152], "temperature": 0.0, "avg_logprob": -0.24559307098388672, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.003704747883602977}, {"id": 1139, "seek": 439528, "start": 4411.04, "end": 4418.639999999999, "text": " So the same name as the input, the same age as the input, but a new ID and that ID has to be different", "tokens": [51152, 407, 264, 912, 1315, 382, 264, 4846, 11, 264, 912, 3205, 382, 264, 4846, 11, 457, 257, 777, 7348, 293, 300, 7348, 575, 281, 312, 819, 51532], "temperature": 0.0, "avg_logprob": -0.24559307098388672, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.003704747883602977}, {"id": 1140, "seek": 439528, "start": 4418.639999999999, "end": 4421.92, "text": " from all the ones that you created previously probably, right?", "tokens": [51532, 490, 439, 264, 2306, 300, 291, 2942, 8046, 1391, 11, 558, 30, 51696], "temperature": 0.0, "avg_logprob": -0.24559307098388672, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.003704747883602977}, {"id": 1141, "seek": 442192, "start": 4422.64, "end": 4433.36, "text": " So like because with Swagger back in the day, there was the ability to generate test code like this.", "tokens": [50400, 407, 411, 570, 365, 3926, 11062, 646, 294, 264, 786, 11, 456, 390, 264, 3485, 281, 8460, 1500, 3089, 411, 341, 13, 50936], "temperature": 0.0, "avg_logprob": -0.211033752986363, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.006792553700506687}, {"id": 1142, "seek": 442192, "start": 4433.36, "end": 4441.6, "text": " But in some cases, it's not exactly what you want, especially if you want things to be connected in some shape or form through IDs or something.", "tokens": [50936, 583, 294, 512, 3331, 11, 309, 311, 406, 2293, 437, 291, 528, 11, 2318, 498, 291, 528, 721, 281, 312, 4582, 294, 512, 3909, 420, 1254, 807, 48212, 420, 746, 13, 51348], "temperature": 0.0, "avg_logprob": -0.211033752986363, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.006792553700506687}, {"id": 1143, "seek": 442192, "start": 4442.16, "end": 4443.68, "text": " It gets tricky.", "tokens": [51376, 467, 2170, 12414, 13, 51452], "temperature": 0.0, "avg_logprob": -0.211033752986363, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.006792553700506687}, {"id": 1144, "seek": 442192, "start": 4443.68, "end": 4446.4, "text": " So very challenging, I think.", "tokens": [51452, 407, 588, 7595, 11, 286, 519, 13, 51588], "temperature": 0.0, "avg_logprob": -0.211033752986363, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.006792553700506687}, {"id": 1145, "seek": 442192, "start": 4446.4, "end": 4448.96, "text": " But if it works, super interesting.", "tokens": [51588, 583, 498, 309, 1985, 11, 1687, 1880, 13, 51716], "temperature": 0.0, "avg_logprob": -0.211033752986363, "compression_ratio": 1.5352112676056338, "no_speech_prob": 0.006792553700506687}, {"id": 1146, "seek": 444896, "start": 4449.92, "end": 4452.16, "text": " Yeah, maybe I'll do that after form generation.", "tokens": [50412, 865, 11, 1310, 286, 603, 360, 300, 934, 1254, 5125, 13, 50524], "temperature": 0.0, "avg_logprob": -0.2679293991683365, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.003941483795642853}, {"id": 1147, "seek": 444896, "start": 4458.72, "end": 4460.72, "text": " No, no, no, I'm still motivated to do it.", "tokens": [50852, 883, 11, 572, 11, 572, 11, 286, 478, 920, 14515, 281, 360, 309, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2679293991683365, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.003941483795642853}, {"id": 1148, "seek": 444896, "start": 4460.72, "end": 4469.84, "text": " It's more what given a year, what can I do in a year and what will be the most beneficial?", "tokens": [50952, 467, 311, 544, 437, 2212, 257, 1064, 11, 437, 393, 286, 360, 294, 257, 1064, 293, 437, 486, 312, 264, 881, 14072, 30, 51408], "temperature": 0.0, "avg_logprob": -0.2679293991683365, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.003941483795642853}, {"id": 1149, "seek": 444896, "start": 4471.36, "end": 4478.08, "text": " Testing is still very beneficial, but I think more people benefit from forms than tests is my guess.", "tokens": [51484, 45517, 307, 920, 588, 14072, 11, 457, 286, 519, 544, 561, 5121, 490, 6422, 813, 6921, 307, 452, 2041, 13, 51820], "temperature": 0.0, "avg_logprob": -0.2679293991683365, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.003941483795642853}, {"id": 1150, "seek": 447896, "start": 4479.12, "end": 4485.92, "text": " I think the testing side is more, I think there's an interesting challenge there and also provides a benefit, whereas forms,", "tokens": [50372, 286, 519, 264, 4997, 1252, 307, 544, 11, 286, 519, 456, 311, 364, 1880, 3430, 456, 293, 611, 6417, 257, 5121, 11, 9735, 6422, 11, 50712], "temperature": 0.0, "avg_logprob": -0.15163261374247441, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.0005192143144086003}, {"id": 1151, "seek": 447896, "start": 4486.8, "end": 4488.24, "text": " it's mostly benefit.", "tokens": [50756, 309, 311, 5240, 5121, 13, 50828], "temperature": 0.0, "avg_logprob": -0.15163261374247441, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.0005192143144086003}, {"id": 1152, "seek": 447896, "start": 4488.24, "end": 4498.08, "text": " Like the challenge I don't think is enormous or insurmountable, but the gains that people have from it are significantly higher.", "tokens": [50828, 1743, 264, 3430, 286, 500, 380, 519, 307, 11322, 420, 1028, 26717, 792, 712, 11, 457, 264, 16823, 300, 561, 362, 490, 309, 366, 10591, 2946, 13, 51320], "temperature": 0.0, "avg_logprob": -0.15163261374247441, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.0005192143144086003}, {"id": 1153, "seek": 447896, "start": 4498.8, "end": 4507.68, "text": " Also, if you wanted to make it work for an program test, you would need to generate a command, a task, and some kind of effect", "tokens": [51356, 2743, 11, 498, 291, 1415, 281, 652, 309, 589, 337, 364, 1461, 1500, 11, 291, 576, 643, 281, 8460, 257, 5622, 11, 257, 5633, 11, 293, 512, 733, 295, 1802, 51800], "temperature": 0.0, "avg_logprob": -0.15163261374247441, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.0005192143144086003}, {"id": 1154, "seek": 450768, "start": 4508.400000000001, "end": 4512.0, "text": " that goes well with the effects that the user defined.", "tokens": [50400, 300, 1709, 731, 365, 264, 5065, 300, 264, 4195, 7642, 13, 50580], "temperature": 0.0, "avg_logprob": -0.2138025577251728, "compression_ratio": 1.6010362694300517, "no_speech_prob": 0.007809656206518412}, {"id": 1155, "seek": 450768, "start": 4512.56, "end": 4513.360000000001, "text": " Yeah, yeah.", "tokens": [50608, 865, 11, 1338, 13, 50648], "temperature": 0.0, "avg_logprob": -0.2138025577251728, "compression_ratio": 1.6010362694300517, "no_speech_prob": 0.007809656206518412}, {"id": 1156, "seek": 450768, "start": 4514.16, "end": 4517.360000000001, "text": " I think you might just consider having like a", "tokens": [50688, 286, 519, 291, 1062, 445, 1949, 1419, 411, 257, 50848], "temperature": 0.0, "avg_logprob": -0.2138025577251728, "compression_ratio": 1.6010362694300517, "no_speech_prob": 0.007809656206518412}, {"id": 1157, "seek": 450768, "start": 4518.16, "end": 4529.76, "text": " wrapper function where every endpoint calls that function to generate a whatever, whether it's a command, a task, a backend task, an elm concurrent task,", "tokens": [50888, 46906, 2445, 689, 633, 35795, 5498, 300, 2445, 281, 8460, 257, 2035, 11, 1968, 309, 311, 257, 5622, 11, 257, 5633, 11, 257, 38087, 5633, 11, 364, 806, 76, 37702, 5633, 11, 51468], "temperature": 0.0, "avg_logprob": -0.2138025577251728, "compression_ratio": 1.6010362694300517, "no_speech_prob": 0.007809656206518412}, {"id": 1158, "seek": 450768, "start": 4530.56, "end": 4533.68, "text": " an effect, whatever it might be, like just", "tokens": [51508, 364, 1802, 11, 2035, 309, 1062, 312, 11, 411, 445, 51664], "temperature": 0.0, "avg_logprob": -0.2138025577251728, "compression_ratio": 1.6010362694300517, "no_speech_prob": 0.007809656206518412}, {"id": 1159, "seek": 453368, "start": 4534.400000000001, "end": 4538.16, "text": " let the user define that's using the code gen part,", "tokens": [50400, 718, 264, 4195, 6964, 300, 311, 1228, 264, 3089, 1049, 644, 11, 50588], "temperature": 0.0, "avg_logprob": -0.15813237877302272, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.013216745108366013}, {"id": 1160, "seek": 453368, "start": 4538.72, "end": 4542.96, "text": " define a function that takes the decoder and headers and", "tokens": [50616, 6964, 257, 2445, 300, 2516, 264, 979, 19866, 293, 45101, 293, 50828], "temperature": 0.0, "avg_logprob": -0.15813237877302272, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.013216745108366013}, {"id": 1161, "seek": 453368, "start": 4543.52, "end": 4548.240000000001, "text": " HTTP method and all that as input, and then turns that into a something,", "tokens": [50856, 33283, 3170, 293, 439, 300, 382, 4846, 11, 293, 550, 4523, 300, 666, 257, 746, 11, 51092], "temperature": 0.0, "avg_logprob": -0.15813237877302272, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.013216745108366013}, {"id": 1162, "seek": 453368, "start": 4548.8, "end": 4554.4800000000005, "text": " and then you let them do that. The hard part is in the generated code, then having the appropriate", "tokens": [51120, 293, 550, 291, 718, 552, 360, 300, 13, 440, 1152, 644, 307, 294, 264, 10833, 3089, 11, 550, 1419, 264, 6854, 51404], "temperature": 0.0, "avg_logprob": -0.15813237877302272, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.013216745108366013}, {"id": 1163, "seek": 453368, "start": 4554.96, "end": 4559.4400000000005, "text": " type signature for all of that generated code, which you kind of want, and that becomes a pain.", "tokens": [51428, 2010, 13397, 337, 439, 295, 300, 10833, 3089, 11, 597, 291, 733, 295, 528, 11, 293, 300, 3643, 257, 1822, 13, 51652], "temperature": 0.0, "avg_logprob": -0.15813237877302272, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.013216745108366013}, {"id": 1164, "seek": 455944, "start": 4560.4, "end": 4563.5199999999995, "text": " It is, it's a type gymnastics, basically.", "tokens": [50412, 467, 307, 11, 309, 311, 257, 2010, 48461, 11, 1936, 13, 50568], "temperature": 0.0, "avg_logprob": -0.27372041533264935, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.030658312141895294}, {"id": 1165, "seek": 455944, "start": 4564.5599999999995, "end": 4565.679999999999, "text": " Absolutely.", "tokens": [50620, 7021, 13, 50676], "temperature": 0.0, "avg_logprob": -0.27372041533264935, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.030658312141895294}, {"id": 1166, "seek": 455944, "start": 4565.679999999999, "end": 4569.839999999999, "text": " You have four hours and you are not allowed to use more than three type variables.", "tokens": [50676, 509, 362, 1451, 2496, 293, 291, 366, 406, 4350, 281, 764, 544, 813, 1045, 2010, 9102, 13, 50884], "temperature": 0.0, "avg_logprob": -0.27372041533264935, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.030658312141895294}, {"id": 1167, "seek": 455944, "start": 4574.32, "end": 4575.759999999999, "text": " Well, the exam starts now.", "tokens": [51108, 1042, 11, 264, 1139, 3719, 586, 13, 51180], "temperature": 0.0, "avg_logprob": -0.27372041533264935, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.030658312141895294}, {"id": 1168, "seek": 455944, "start": 4576.799999999999, "end": 4584.4, "text": " Well, Wolfgang, this is a really great asset for the community. So, thank you for pushing across the finish line. I really do believe", "tokens": [51232, 1042, 11, 16634, 19619, 11, 341, 307, 257, 534, 869, 11999, 337, 264, 1768, 13, 407, 11, 1309, 291, 337, 7380, 2108, 264, 2413, 1622, 13, 286, 534, 360, 1697, 51612], "temperature": 0.0, "avg_logprob": -0.27372041533264935, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.030658312141895294}, {"id": 1169, "seek": 458440, "start": 4585.36, "end": 4588.96, "text": " the more pieces like this we have in the ecosystem,", "tokens": [50412, 264, 544, 3755, 411, 341, 321, 362, 294, 264, 11311, 11, 50592], "temperature": 0.0, "avg_logprob": -0.2034360458110941, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.060039080679416656}, {"id": 1170, "seek": 458440, "start": 4589.679999999999, "end": 4591.44, "text": " the", "tokens": [50628, 264, 50716], "temperature": 0.0, "avg_logprob": -0.2034360458110941, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.060039080679416656}, {"id": 1171, "seek": 458440, "start": 4591.44, "end": 4597.36, "text": " fewer caveats there are to like, oh, but does the Elm ecosystem, does it have good GraphQL support, does it have good", "tokens": [50716, 13366, 11730, 1720, 456, 366, 281, 411, 11, 1954, 11, 457, 775, 264, 2699, 76, 11311, 11, 775, 309, 362, 665, 21884, 13695, 1406, 11, 775, 309, 362, 665, 51012], "temperature": 0.0, "avg_logprob": -0.2034360458110941, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.060039080679416656}, {"id": 1172, "seek": 458440, "start": 4597.759999999999, "end": 4600.24, "text": " OpenAPI support? So, thank you.", "tokens": [51032, 7238, 4715, 40, 1406, 30, 407, 11, 1309, 291, 13, 51156], "temperature": 0.0, "avg_logprob": -0.2034360458110941, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.060039080679416656}, {"id": 1173, "seek": 458440, "start": 4601.04, "end": 4605.36, "text": " And if someone wants to get started, what's a good place to start, where can they learn more?", "tokens": [51196, 400, 498, 1580, 2738, 281, 483, 1409, 11, 437, 311, 257, 665, 1081, 281, 722, 11, 689, 393, 436, 1466, 544, 30, 51412], "temperature": 0.0, "avg_logprob": -0.2034360458110941, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.060039080679416656}, {"id": 1174, "seek": 458440, "start": 4605.92, "end": 4606.96, "text": " They're completely,", "tokens": [51440, 814, 434, 2584, 11, 51492], "temperature": 0.0, "avg_logprob": -0.2034360458110941, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.060039080679416656}, {"id": 1175, "seek": 458440, "start": 4607.92, "end": 4612.5599999999995, "text": " if they want to go down the road of learning OpenAPI, I would say just Google OpenAPI.", "tokens": [51540, 498, 436, 528, 281, 352, 760, 264, 3060, 295, 2539, 7238, 4715, 40, 11, 286, 576, 584, 445, 3329, 7238, 4715, 40, 13, 51772], "temperature": 0.0, "avg_logprob": -0.2034360458110941, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.060039080679416656}, {"id": 1176, "seek": 461256, "start": 4613.04, "end": 4616.56, "text": " Don't confuse it with OpenAI. You and I have been doing it lately.", "tokens": [50388, 1468, 380, 28584, 309, 365, 7238, 48698, 13, 509, 293, 286, 362, 668, 884, 309, 12881, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19129295349121095, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.007933677174150944}, {"id": 1177, "seek": 461256, "start": 4617.200000000001, "end": 4619.200000000001, "text": " Yeah, OpenAPI.", "tokens": [50596, 865, 11, 7238, 4715, 40, 13, 50696], "temperature": 0.0, "avg_logprob": -0.19129295349121095, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.007933677174150944}, {"id": 1178, "seek": 461256, "start": 4619.52, "end": 4621.360000000001, "text": " If you are looking to use it,", "tokens": [50712, 759, 291, 366, 1237, 281, 764, 309, 11, 50804], "temperature": 0.0, "avg_logprob": -0.19129295349121095, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.007933677174150944}, {"id": 1179, "seek": 461256, "start": 4622.56, "end": 4624.72, "text": " check out the NPM package.", "tokens": [50864, 1520, 484, 264, 426, 18819, 7372, 13, 50972], "temperature": 0.0, "avg_logprob": -0.19129295349121095, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.007933677174150944}, {"id": 1180, "seek": 461256, "start": 4625.52, "end": 4633.68, "text": " Most likely, if you're looking to generate, and then whatever API endpoints you're, whatever service you're looking to communicate with,", "tokens": [51012, 4534, 3700, 11, 498, 291, 434, 1237, 281, 8460, 11, 293, 550, 2035, 9362, 917, 20552, 291, 434, 11, 2035, 2643, 291, 434, 1237, 281, 7890, 365, 11, 51420], "temperature": 0.0, "avg_logprob": -0.19129295349121095, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.007933677174150944}, {"id": 1181, "seek": 461256, "start": 4634.160000000001, "end": 4635.6, "text": " generate something.", "tokens": [51444, 8460, 746, 13, 51516], "temperature": 0.0, "avg_logprob": -0.19129295349121095, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.007933677174150944}, {"id": 1182, "seek": 461256, "start": 4635.6, "end": 4641.76, "text": " If you're looking to contribute to new features like form generation or testing, if you're feeling ambitious,", "tokens": [51516, 759, 291, 434, 1237, 281, 10586, 281, 777, 4122, 411, 1254, 5125, 420, 4997, 11, 498, 291, 434, 2633, 20239, 11, 51824], "temperature": 0.0, "avg_logprob": -0.19129295349121095, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.007933677174150944}, {"id": 1183, "seek": 464256, "start": 4643.04, "end": 4646.0, "text": " feel free to reach out to me on Slack or Discord,", "tokens": [50388, 841, 1737, 281, 2524, 484, 281, 385, 322, 37211, 420, 32623, 11, 50536], "temperature": 0.0, "avg_logprob": -0.17979299105130708, "compression_ratio": 1.6856060606060606, "no_speech_prob": 0.001206322107464075}, {"id": 1184, "seek": 464256, "start": 4646.64, "end": 4651.6, "text": " or however else you know how to reach me. Amazing. And the real world", "tokens": [50568, 420, 4461, 1646, 291, 458, 577, 281, 2524, 385, 13, 14165, 13, 400, 264, 957, 1002, 50816], "temperature": 0.0, "avg_logprob": -0.17979299105130708, "compression_ratio": 1.6856060606060606, "no_speech_prob": 0.001206322107464075}, {"id": 1185, "seek": 464256, "start": 4652.080000000001, "end": 4655.92, "text": " example that you have where you're hitting the API using", "tokens": [50840, 1365, 300, 291, 362, 689, 291, 434, 8850, 264, 9362, 1228, 51032], "temperature": 0.0, "avg_logprob": -0.17979299105130708, "compression_ratio": 1.6856060606060606, "no_speech_prob": 0.001206322107464075}, {"id": 1186, "seek": 464256, "start": 4656.64, "end": 4662.96, "text": " using OpenAPI is also very handy. So, worth taking a look if you want to see what it looks like calling these", "tokens": [51068, 1228, 7238, 4715, 40, 307, 611, 588, 13239, 13, 407, 11, 3163, 1940, 257, 574, 498, 291, 528, 281, 536, 437, 309, 1542, 411, 5141, 613, 51384], "temperature": 0.0, "avg_logprob": -0.17979299105130708, "compression_ratio": 1.6856060606060606, "no_speech_prob": 0.001206322107464075}, {"id": 1187, "seek": 464256, "start": 4663.4400000000005, "end": 4668.160000000001, "text": " generated functions. Thank you so much. Wolfgang, great having you on. Thank you for having me. It was great.", "tokens": [51408, 10833, 6828, 13, 1044, 291, 370, 709, 13, 16634, 19619, 11, 869, 1419, 291, 322, 13, 1044, 291, 337, 1419, 385, 13, 467, 390, 869, 13, 51644], "temperature": 0.0, "avg_logprob": -0.17979299105130708, "compression_ratio": 1.6856060606060606, "no_speech_prob": 0.001206322107464075}, {"id": 1188, "seek": 464256, "start": 4668.64, "end": 4671.280000000001, "text": " And you're in. Until next time. Until next time.", "tokens": [51668, 400, 291, 434, 294, 13, 9088, 958, 565, 13, 9088, 958, 565, 13, 51800], "temperature": 0.0, "avg_logprob": -0.17979299105130708, "compression_ratio": 1.6856060606060606, "no_speech_prob": 0.001206322107464075}, {"id": 1189, "seek": 467256, "start": 4672.56, "end": 4674.56, "text": " Bye.", "tokens": [50364, 4621, 13, 50464], "temperature": 0.0, "avg_logprob": -0.9797157287597656, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.7705263495445251}], "language": "en"}