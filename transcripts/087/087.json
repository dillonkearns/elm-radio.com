{"text": " Hello Jeroen. Hello Dillon. Well Jeroen, there are some topics that are hot off the presses and you gotta hit them while they're hot. And some topics never get old. You might even call them evergreen. And I'd like to think that our topic today is a bit of an evergreen topic. So, Mario Rojic, welcome back to the show. Hey guys, thanks for having me. And what are we talking about today? So yeah, I guess we're talking about the idea of evergreen elm. So, type safety between versions of your application. Or at least that's my interest in evergreen elm. Maybe there's other kinds of evergreen elm that people are interested in. That's a nice tagline for it. So, evergreen elm. And we were talking before the recording here about different species, different variants of elm tree. And apparently there are some variants that are deciduous elm trees and some that are evergreen. So, perhaps lamdara is an evergreen variant of an elm tree. Yeah, I'm down for that. I'm really keen to dig into the philosophical implications and ideas behind evergreen migrations. And what makes it such a core part of lamdara and the guarantees that it provides. So, I'm kind of curious, Mario, could you tell us about the motivations and contrast it to deciduous migrations in other contexts? Maybe non-elm contexts. What was the pain point and problem you were trying to solve and the guarantees you were trying to provide? Yeah, sure. I think the driver for evergreen was a practical one and the name came afterwards as I kind of expanded that practical problem to the front end. So, where the concept arose from and kind of what led to my original presentation called Evergreen Elm at Elm Europe in 2019. I had started working on the ideas behind early versions of lamdara. So, this idea of full stack elm and what that would look like if the elm architecture was on the back end as well. As opposed to previous incarnations of back end elm and some current incarnations of back end elm. I kind of like, well, what if we just use elm as a language instead of JavaScript as a language, but otherwise have all the same concepts. Back ends obviously have a server with HTTP endpoints. And so, one of the approaches was like, okay, let's model all of that in elm and see what that looks like. Whereas, the projects that I'd been working on and the thing that I was kind of reaching for was, I wanted an experience on the back end that was the same as the experience on the front end. And so, part of that crazy idea initially was, well, what if the database wasn't like a separate thing? Like, I like the elm model. It's nice that it just, you know, while you're operating in the front end as a front end developer, you don't think about the persistence of the front end model. As long as the user's browser is open, it's quote unquote persistent, right? You're not thinking about it. And if the user closes their tab, well, then it's gone. And so, for the back end, I was like, okay, that's great. But, you know, if the server crashes and all your state disappears, like that sucks. And so, step one was like, okay, can we solve that problem? But then step two was several of them being like, okay, well, you know, now there's a new thing that we don't really have. Or at least I hadn't heard talked about a lot in elm or just in front end in general, which is what happens when we have a new version of our app and the types have changed. Now, I think the general approach on the front end is like, I don't know. Cross your fingers and yeah. Maybe they'll click a link and the page will refresh. We'll be lucky. Or, you know, like, I suppose if you're, I suppose most companies would be like, I don't know. We'll just, hopefully it doesn't happen that much. And if something goes wrong and they contact support, we'll tell them, hey, have you tried refreshing the page? Oh, the issues have gone away. That's cool. Great. Glad we solved your problem. And no one thinks about it again. I guess the problem happens more with single page applications rather than. Yeah, definitely. More traditional way. And I had heard, I can't remember who I heard this from, but I had heard anecdotally that like someone was, told me, I was like, oh, they once received like their company used like versioning or something. Right. So for the front end comes. So they're like, oh, you know, I don't think we have this problem because we use versioning. And I was like, okay, cool. What's the oldest event version that you've gotten? They're like, oh yeah, we got like, someone must have kept a tab open for like a year or something because we got events from like a year ago. I was like, right. And what do you do with those? And they're like, I don't know if you drop them on the floor. What can you do? You can't do anything right. In the normal thinking. So I was like, what would be a really elm way to solve this? And yeah, the only thing I could think of was, well, it should just upgrade. Like it should just work. It should just continue. And even more so than that, it's like, I want it to type check. That's what I want. Like I want to go from one version of app to a new version of an app. And I want the compiler to be like, yeah, you, this is good. Like however you're going between these two, that type checks, that makes sense. So that was the first motivation. It was like that, that thought of like, okay, well, if we have data that's alive in one app and we're releasing new app with new types, how do we get from one app to the second app? And the reason name came about. I feel like you're intuitively going to the more like subtle edge case of it, but there's also like the core meat of what most people would think of as a migration that people wouldn't gloss over how to handle these edge cases in their application and just be like, Matt, I hope it works, which is migrating your backend, migrating your database. Right. And so of course that's a less subtle case, but evergreen migrations also provide a type safe way to migrate from one version of your backend model to a new version of your backend model. Yeah, absolutely. So that was the practical driver. It was like, yeah, obviously the concept of migrations of your backend is already a thing and it's important. So I wanted to facilitate that and for it to be type safe. And then where the name came from is I realized like, oh, now that like if we have this, well, we could do this on the front end as well. And so instead of the front end effectively like having to drop it state every time there was a new app or you hope the user clicks, which for me had that notion of like, you know, the leaves falling off the tree, kind of being deciduous. Then I thought, oh, well, if the front end can always stay alive, then I was suddenly like, well, that's kind of like how browsers now, they call them like evergreen, right? Like browsers that continually update themselves. And so, yeah, that's, you know, elm and trees and evergreen. I was like, oh, this is nice. This is, yeah, this is the way I'm going. So, yeah, I'm not sure how well though that term translates. A lot of people get confused. I had imagined evergreen would like perfectly like encapsulate and explain everything. I'd be like, yeah, it's evergreen, but it doesn't, I'm not sure that in practice that's translated. So I've definitely found that, that, yeah, people, it feels to me like, or at least the way that I experienced it, I'd be curious how you guys have experienced it. My experience is that people go, oh, that's some black magic. Like there's something crazy going on with evergreen. There's something magical happening. When in reality, it's actually really dumb. Like it's, it's the dumbest possible thing. It's like, how do you migrate with a function? Like it takes value A and it returns value B. But I think, yeah, the, the grokking, like the, the, the details of what that actually means has interestingly been, yeah, a stumbling point, which I found interesting. Yeah, I was there. Well, we were both there actually with them. Churchalk in Europe, 2019. So yeah, we, we got the explanation right off the bat. So you've been biased. Yeah, very biased. So for me, it's clear. It's, it's a function. And I've also played with them there a little bit. So yeah, for me, it's pretty simple, but I can understand why people would be confused or scared of this because yeah. Like imagine doing this in any other system than Elm or Lamdara and like, yeah, no, like that has to be magic. Yeah, yeah, yeah. So maybe, and I think it was also funny because like at, I was devising this idea for Lamdara, which didn't quite exist yet. And so I kind of wanted to do a talk on evergreen Elm, but I couldn't really mention the backend. So I had to reframe my whole Elm Europe talk as, well, yeah, like this concept on the front end. And I think for some people, maybe it was like, well, why would you go to all that effort in just refresh? So it was kind of like this idea was there and I was like, this idea is so cool, but then I couldn't talk about it in the context where I thought it was super cool. So yeah, I think I kind of demonstrated it backwards, but yeah, maybe, maybe if we just went very briefly for those listening to understand or people who may not have a concept or maybe aren't tracking, we could go through like a couple of types of state and what a migration would look like verbally. I think that might give an indicator. What do you reckon? I just want to correct something because, which is my fault because I induced you into giving an error, saying something wrong. Evergreen Elm was from 2018. Oh, I see. Lamdera. Lamdera was 2019. That's the one I was in the audience for. But I really enjoyed watching the Evergreen one on YouTube. Yeah. Okay. So let's take a model. So let's say you have a, I think explaining it on the front end is probably the easiest. So let's say on your front end, we have the counter app. Right. And I think I did this in my talk in France. So say the counter app starts with like counter of, it's a record with one field, field name counter, and the field type is int. Right. And let's say for some silly reason, we say, okay, well, we now want to release a new version of the app, but we've changed the counter field to type float. And the question would be like, how do we migrate from one to the other? And so, yeah, when we say it's just a function, what we're saying is like, yeah, we want a function that takes a record with a field counter type int and returns a record with a field counter type float. So returning the new record, you basically just construct the record. But for the value of counter, right, you've got an integer and now you need a float. So you kind of go off and you find a function, you know, what does like what function takes an integer and returns a float. And that's the function that you would use. So I can't remember off the top of my head. Is that from int or to float? One of those two. Probably have to look that up. To float, I'm pretty sure. Is what I would try. And then if I get a compiler error. This is a guess driven development is how we operate. Yeah, so to float, to float, right from basics. So it's in the call. So yeah, that is in an essence it, like that's the end of Evergreen migrations at like a simplified level. So basically any model that you have in your back end or front end in Lambda, it's both. But if you're just thinking of Elm, like in the Elm architecture, whatever your model type is, if you change that type, the question is, well, what is a sensible transition? What is a sensible transformation between whatever it used to be and whatever the new value is? And on the front end, there is some interesting stuff, like some of the problems I posed in my talk was like, you know, do you need to tell a user that things are upgrading? Like sometimes for a minor change or maybe for new features, additive stuff, you don't need to. But, you know, maybe it would be kind of weird if the user was halfway filling out a form and then suddenly a form field like disappears. But maybe it's not, you know, maybe it's not a big deal. Like is it any more or less confusing than the page breaking and them having to refresh? But yeah, the cool thing is that like once you have this idea and then that evergreen setup is possible, it becomes a bit magical. And it's something I love about Lambda and I really love it. You know, occasionally people will come and tell me, so someone at Elm camp came and told me like, oh yeah, you know, I was making a game and some friends were testing and they were like, oh, this thing doesn't work. And he was like, hold on, I'll fix that. And they're like, all right, that's deployed. And like all of their apps like updated, you know, without them doing anything. And he's like, cool, that's done. And they were like, wow. And yeah, that's that. I'm like, yeah, that that feels right to me. I feel like that's how it should be. That feels awesome. So yeah, I think that's the joy of it. That is very unique. So you're doing a hot reload on the client and applying the new code changes with that safe migration that migrates the front end model and then migrates any incoming front end messages. So it hot swaps that in. Is that correct? Yeah, that's right. So basically we take the concept of it. So the theoretical concept of evergreen is really just a function, right? It's saying we've got this old type with an actual value of that old type that's live. And then, you know, we want to go to this new type in the new app. So we write a function that helps us migrate that live value from one to the other. And then once you have that tool, it's like, what do you do with it? And so in Lambdaera, what we do with it is we apply it to every value primitive that you have in Lambdaera. And there's six of them. There's the front end model, there's the back end model, there's a front end message, the back end message. And then the two messages that connect the two, two back end and two front end. And so we apply this in Lambdaera, this idea that any time you change a type in any of those six core types, we go, cool, we can see you've changed this type. You know, please write this migration. You know, here's the harness for it. And, you know, if you do and it type checks, then we go, great. If you deploy this, then, yeah, we can do exactly what you've said. On both the front end and the back end, load the new version, take the model from the old app, migrate it into the new app. And now you've got the new app running and it was all kind of smooth and hot and live and delightful. So, yeah, that's the deal. Yeah, that's amazing. And I mean, if you wanted to, like, so you have, you know, as you said, it's a function from one version of a model to another. And we should probably just to paint a picture here more explicitly, you have your types module in a Lambdaera app, which defines, as you described, those six core types, back end model, front end model, back end message, front end message, and then the two back end and two front end custom types, which define sending data back and forth. But then those essentially get versioned under these namespaces V1, V2, V3, as you change your types. So you have that module with all those six core types defined and any supporting types that you might have that are used in your back end model, in your front end model. Yeah, the subsumed types. Right. And that all, so those are all versioned. So now instead of just types, being some fuzzy thing that you're like, Oh, yeah, I think it was different in a different version. You actually have every version of your types for your Lambdaera application that you've ever deployed. Yeah, absolutely. So the, it's funny, it's like the fundamental idea is really, really simple. But then like the practicality of the tooling to implement that simple idea ended up being quite complicated. And I think this is kind of where, and I talked about this as well in the talk, like part of the pitch for Lambdaera was that this idea behind this kind of live reload migrations. I think it was, it seems to me that it is most compelling when your entire system has this philosophy behind it. Because if you can, like if you're tracking your entire system, your front end and your back end and everything between, it's then that the tooling is really nice. Right. Without that setup, you would have kind of what you've just described, which is like, you know, there's no good way to know what the authority is on these types. So you can't, like, it's not very easy for you to, you'd have to use discipline to be like, okay, you know, I've changed the type, therefore let's go and copy paste our types and let's try and name space. You know, you'd have to do this. Whereas with Lambdaera, because everything's integrated, like the local tooling, you know, when you run a check, like a pre-check for a deploy, it knows to talk to your production instances and go, okay, well that's what's deployed currently. That's what the type hashes are. Do we have a change? Okay, we do. Let's automate the snapshotting. Let's automate, you know, giving you some feedback about what's happening, what types have changed so that, you know, the developer experience that, you know, I've been chasing in Lambdaera is that you don't have to think about it. Right. So in the early versions of Lambdaera, you had to think about it, you had to track it. And yeah, as of the latest release, we are now trying to make that as seamless and kind of carefree as possible so that you can, with confidence, be like, you know what, I'm going to re-architect my whole backend. I'm going to change the whole backend model. I'm going to move things around. I'm going to change dictionaries to sets and nest things into custom. I'll do whatever I want. And the idea is that you can kind of do that and then Lambdaera will be like, cool, that's cool. Here's what changed. All right. Like this is now you need to tell me how to get from where you were to where you are. And if you can figure that out, if you can make it compile, then cool. Like probably this is going to, well, from a type perspective, it's going to migrate. But obviously we can't guarantee that you haven't put dic.empty or set.empty where you shouldn't have and that you'll lose some data. So you can still write the wrong migration in the business sense, but you can't forget to migrate a column. You can't delete a column and still have code that's referring to it. Right. So in that sense, I think it's a step up from the traditional transactional database migrations that I think most of us are used to. Have you heard of people writing unit tests for their migrations or is it usually straightforward enough where people are? Yeah, no, that's a good question. I think the question's been expressed before. I don't think there's any reason you couldn't. There's a slightly awkward technical reason in Lambdaera why it would be a bit weird. It's because Lambdaera, yes, actually it's not specific to Lambdaera. There's a question of, you know, we've got these type snapshots, right? So we snapshot our types every time things change. But we want to snapshot anything that's changed and we deploy. We want to snapshot that. However, because of the way that Elm's namespacing works, we want in production for our migrations to result in values of the type of our current types file, not of values of the type of the last snapshot file, if that makes sense. So maybe to put this another way, if I define a custom type called ice cream with the variants chocolate, vanilla and strawberry, and I put that in my types.elm, and then in Lambdaera I go to deploy, Lambdaera will go, ooh, let's say it's our first deploy. Lambdaera goes, ooh, it's your first deploy. I'm going to snapshot those types into evergreen slash v1 slash types.elm. So now in this v1 types.elm, there's an evergreen.v1.icecream type that has the exact same variants. That is what's going to be deployed, the ones with v1. Yeah, so both actually get deployed, but the v1s there, the v1s on the first deploy only exist to be referenced in future deploys when the types change. But in the current version, we still want ice cream values of the types.elm, right? Because that's what our whole application uses. But in the future, when we now deploy version two, and let's say for ice cream, we had chocolate, vanilla, strawberry, and let's say we add mango, right? So now we're going, okay, this type is now different. It has an additional variant. The migration that gets generated is a migration from version one to version two, right? And so we decide what we do with those values. Probably we do nothing, or maybe we say, hey, you know, actually, everybody that told us that they put strawberry in the first version, they all emailed us and they said, you know, we hate strawberry, we actually want mango. So can you please add a mango option, but also migrate any of our strawberry choices to mango, right? So that would be an example where that function when you migrate the value, you might case the old type, you know, on chocolate, vanilla, strawberry, and chocolate and vanilla, you would map to chocolate and vanilla in the new type. But maybe strawberry variants, we would map to mango in the new type, right? So that's how you could do data changes or data transformations in your migrations. But yeah, we're getting a bit into the weeds here. But the trick is that that new type will become a snapshot version two, right? And your migration has to type check between version one and version two. But actually, when we deploy, we get sneaky. And we don't map it to version two, we actually map it to types dot elm, because that's what you need in your application. So we do this little we do this little swapsy, so that you're always getting the types that your application actually uses. But at the same time, we're preparing that snapshot, that's that snapshot in time to go, okay, when we deployed version two, this is what the types were. So that's ready and committed and in your repository, such that if you change it in the future, we don't have to try to go back and remember what they were, we already snapshotted what they were when you deployed. Is that making any sense? I realize that's a bit that's a bit squirrelly. It makes sense to me. Nice. So you said that every time there's a change in the the types in those six core types, you need to write a migration. One, it's only when they change, right? If nothing has changed, and you don't need to write a to write a migration? Yeah, that's correct. So that's a that's a little optimization that okay, yeah, lambda has, which is that even though in elm, those types aren't the same. If when we snapshot the types, the type tree that gets generated, generates identical encoders and decoders. So we can keep, you know, if, if you have a version two snapshot, right, and then you deploy version 34567, and nothing's changed, we can keep decoding straight into your types dot elm type. Because we know that those encoders and decoders are identical from the binary format, right? So it's only when it's only when the types do change that we know, okay, that will mean that the encoders and decoders will become different, which means we need a function to get between the old value and the new value to keep everything sane. And the second question is, could you write a migration, even if there's no type change, for instance, everyone hates strawberries. So let's write a migration that goes to to mango without having a type change. Yeah, so you can't. But with an asterisk, you can't with an asterisk, you can. So you can't change nothing and ask, like evergreen and lambda to be like, please let me do a migration anyway. But you can just add a dummy field. And then evergreen will happily be like, Oh, you need a migration, right? And so the trick is that when this is a technical, maybe a shortfall that we might tighten in the future. But right now, whenever you get asked to write a migration, you actually get placeholders for all six types. But by default, it'll tag the migrations for the types that haven't changed as unchanged. So you could if you wanted to still apply a migration whenever there is a migration, if that makes sense. If there's a migration to any type, there's a migration to all six types. That's just the way that it's implemented in lambda right now. So you could do data transformations like across the board. But yeah, yeah, it's you have to get lambda lambda is evergreen implementation to think that something's changed in order for it to be like, fine, okay, let's get let's get some migrations involved. If the current version is v6, and I manually create a v7 folder, would that trick lambda into generating a migration? No. So if you manually create a lambda right now, what would happen if you manually created a version seven, and then you try to deploy it in production, it would say, Hey, I wasn't expecting there to be a migration, but I see a v7 migration file, I don't know what's going on. Like something something's bad. So it'll it'll try bail out. So let's say that you wanted to do the the strawberry to mango migration in a more polite opt in way. So the migrations files, they give you the opportunity to I mean, to arbitrarily change the model, the back end model, the front end model. So you could, you could make whatever changes, you know, as you're in is describing, you know, changing a model, when the data hasn't really changed. And you need to sort of get lambda to create a migration file for you. But once you do that, you can make any data change, it doesn't necessarily have to be to get the types to fit together for the new format. That's correct. Yeah. And you also get a command for your for your back end and front end, you can trigger a command as part of that migration. So so let's talk through that a little bit. If we wanted to do the polite version of the strawberry to mango, where we're saying, hey, unfortunately, strawberry is no longer an option. But so we are inviting you to opt into this. And now when you log in to the ice cream shop, you're going to see a, an announcement banner that says, we need you to choose a flavor. So how would you model that with a with an evergreen migration? Yeah, absolutely. So I think 90% of the answer to that question has got nothing to do with evergreen. The question first is how would we model this in an L map? So kind of what if I reflect back at you, what I'm hearing is in terms of state. So there's something now on the user profile, right, which I would say a Boolean, which is something like requested, what would we call it? Like, please, revalidate ice cream preferences question, right? Or show revalidate ice cream preference question, right? And that's a Boolean true or false. And so we add this to our front end model. Actually, we add it to our back end model. Let's see, because we've got accounts here. We've got user accounts, right? We have only saved preferences for users that have logged in, right? Otherwise, how will we know who they are when they come back? So we say, okay, every user profile has this new flag, right? And we add that to the back end model. And then we say, okay, in the front end model. Now, suddenly, we get type errors, right in the front end model, because in this case, it's lambda error, our types are shared. You know, so when a user logs in on the front end, the session hydrates their account. And so that value will come into the front end. And we go into the view and we say, okay, cool. If user.showRevalidateIceCreamPreference is equal to true, then show them this little bit of UI that asks them for this question. Else, let's just show nothing. We'll just leave it, right? And then maybe as part of that UI, we say, hey, like, it looks like you've chosen, you know, strawberry in the past. And, you know, you told us that you wanted to choose a different flavor when we had more available. That's now available. Like, click here to go to your profile page and edit your preferences. And I think, is that it from an app perspective? I think that's probably it. Yeah. And I guess you could decide whether you want your custom type to include strawberry and have it be deprecated or remove strawberry and then set it as mango. But then you're going to need to have some application logic in the appropriate places so you don't accidentally ship them their monthly flavor of mango before they've chosen. And maybe, you know, the monthly shipment process, instead of automatically sending it, is going to send an email and say, hey, you need to log in to change this because we no longer have this flavor. If you want your shipments to resume, then please log in and select something. So, you know, as you say, a lot of it is just modeling the problem in a sort of high-level way. Yeah. So let's say you wanted to, so we're still, we're nowhere near evergreen yet, right? We're still in our application realm. So if you wanted to apply, like, make impossible states impossible, you know, if you wanted to be like, actually, you know what, like, what's the business rationale? Like, so let's invent some. Say the business rationale is enough people told us that they hate strawberry, that we're discontinuing strawberry. Right? So maybe we go, okay, well, we need to remove the strawberry variant, but we need somewhere to migrate these people to. Right? So maybe we turn, you know, ice cream preference into a custom type itself. Well, maybe we turn it into a maybe, right? And nothing means they haven't selected. So we can't send them any shipments. Right? Because we don't know what flavor. Well, maybe we change it into a custom type where we say, you know, not selected or ice cream selected in this new type that only has the variants that we offer. And then a third state, which is like needs revalidation or, you know, ex, ex strawberry lover or something like that. Right? Yeah. I thought there was no one who likes strawberries. Yeah. It's an impossible state. Yeah, absolutely. So yeah, we, we, we kind of do whatever modeling and I think this is the nice bit. And this is a nice bit that we go, we at this point, I think, unlike the way that you do it in, in kind of traditional full stack applications is like you're, you're simultaneously thinking about your data structure and the migration at the same time. Whereas in, in Elm, like via Lambda or specifically in that context, the idea is like, well, just forget about it. Just, just model what new state of affairs you would love to have in your app. Like what is the actual value set up? That makes sense. And then we go great. Now that we've done, we've chosen whatever one of those it was. Now we go, okay, Lambda or deploy or Lambda or check like Lambda or deploy invokes Lambda or check first. And Lambera goes, Ooh, I can see your types have changed. Okay. I'm going to go and attempt to make some migrations for you. I'm going to do my best. So that in the latest version, it goes, not only am I going to generate the scaffold, I'm going to do a best effort generation for you. I'm going to look at the tree of your types in the old version, the tree of your types in the new version. I'm going to try diff them, like zip them together and you know where they don't look like they've changed. I'm going to try and just intelligently make all those, those choices for you to keep everything the same. Mainly the concern is migrating custom types. Cause as we know in Elm, two equally semantically the same custom types in two different modules aren't actually equal. Right. So unlike two, two semantically identical record types in two different modules are equal. Right. So evergreen goes, okay, I'm going to automate all of those kind of crud transformations for you. But in the bits where things have definitely changed and I can't, you know, I can't do anything reasonable. I'm going to put placeholders for you and go, okay, here, I've gotten to ice cream preference. This has changed. You know, it used to be this custom type from version one. Now it's this different custom type from version two. How do you want to do with this? You know, how do you want to get this value across? And so you write the code that says, great, I've done all my business logic. Everything makes sense. And you know, I'll, I guess we've, we've invented some sort of ice cream subscription store here in this, in this, in this analogy. Sounds like a great business. Yeah. Except for the free frozen shipments. That could be. Yeah, that's tough. Tough. But yeah, I want, I definitely want ice cream now. But anyway, yeah. So we've done it. We've done all the business logic. And then now it's kind of like, cool, how do we want to get from one value to the other? And the nice thing is once we've done all of this, you know, it type checks. So it goes, yeah, cool. Okay. That makes sense. You've, you've successfully done the transition. And I suppose going back to the, to the unit test question, it's like, yeah, if, if that was a very complicated transition and you wanted to have certain invariants that hold, yeah, there's no reason why that function that you wrote in there for that particular part of the migration, you can put that function to the side. You could put it elsewhere. You could include it from the tests and then you could do all your scenarios and tests. You know, I put in these old versions. I expect these new versions is my migration kind of making sense. And I think that's nice as well, because you get to stay in Elm, right? If you think of that in any other system, now you're mocking perhaps the database. You have to, you have to, if you're not mocking the database, you actually have to boot a database in your test setup. You have to set the first versions. You have to do the migration of the schema. Then you have to load the value. You know, there'd be a lot to get that working. Whereas in Elm, we get like a lot of confidence from the type checking and then you can cover the rest of the ground in tests if you need to. Whenever there's a type change, you copy everything into like a V2. You copy all the types or do you copy all the code? No, just the types. So what Lambda, so the implementation currently is Lambda kind of recursively trolls through your types from their known. So in Lambda you have to put the core types in a specific location, right? So Lambda goes, okay, I know where they have to be. I start from there and I kind of recursively troll through the type definition and any type definitions it references. And it kind of progressively sucks those out. And as it goes, it namespaces them and it writes them into the snapshots file. So you get, yeah, like an extracted copy of your types tree for every single core type. Okay. Now there's a really good feature in Elm, which I don't know if you've ever heard us talk about, which is opaque types. How does it work for opaque types? Can you migrate them? Can you not? Yeah, this is a great question. So sadly right now, there is a way around this. We could add compiler support and do some magic. But right now you sadly need to open up the constructors within your code base. So there is a risk. Do you mean only in migrations or always? Always. So the reason for that is if you consider like, so let's go back to our ice cream flavors. Say you made that an opaque type, all right? Obviously. Yeah, obviously. Nobody is allowed to specifically say what flavor they like. They have to go through the flavor constructor function, right? Okay, so maybe that's what you love. You're like, yes, this is the best way to do this. So in this maybe convoluted example, think about now how you express a migration for this, right? So I'm asking you, I've written the function signature. So the function signature is from v1.icecream to v2.icecream, right? And into the value, into the migration, you get a value called old. That's what I call it by default, right? Old is the name of the old ice cream flavor. So this is going to be a specific instance, like a specific value instance of that type, right? The first thing you would normally do in migrations of a custom type is you would be case old of, and then you would pattern match on all the variants, right? And then you would return new variants. But in your case, if you've made an opaque type, what do you do now? Yeah, exactly. Yeah, that's why I was wondering, like, I think there's some problem with the opaque types. And I seem to remember that, yeah, opaque types didn't work well with Dundara or with the migration system. If you really, really wanted to, you could write specific code to try. Yeah, so you would have to write like special functions that were like basically constructor functions for your new types. And then maybe you would inside your actual normal elm code, like within that module that has access to the constructors, you might write like a deconstructing or put the migration file directly in there, right? So you could try and keep all of your types opaque and hidden and put the actual migration function in the file somehow, right? The thing that gets really weird and what I discourage with Lambda, although it is possible, is that obviously migrations are like a point in time thing. And so part of the reason we do snapshots is as your app continues to evolve, your code is going to change, right? So the weird thing is, if you put the migration function into the file that defines those constructors, in the next version, you're going to have to change the type that's in that file. And suddenly those migration functions that are referring to the types that are in the same file are wrong. So the question is like, okay, well, now where do I put them? You want to put them into the history, but the history can't access the constructors, right? So the problem today is, okay, with those types, Lambda forces you to open it up. That's kind of the easy way. Long-term, is there a way around it? Yes, the way around it would be at the compiler level for us to go when we're compiling, when we see that we're compiling a module that is within the Evergreen namespace, magically the compiler unhinges its export restrictions and pretends like as if everything is exported. So then only in the migrations context, you can reference a opaque type constructor and you won't get a type error saying, oh, this is, you know, unexposed or hidden. But then in your main code base, any way you referenced one, you would get the type error as usual. So that's the idea. I don't know how difficult that would be. I haven't delved into it yet. But at least in theory, I think we could improve those ergonomics and, you know, get back the same kind of opaque type protections that we have today. So yeah, to lovers of opaque types, I'm sorry, Jeroen, there's a little bit of a compromise there from, you know, ergonomics and language restrictions. But yeah, I think for now it's probably not the end of the world, but we have a way to improve that in the future. Not too many people have complained about it so far. And by not too many people, I mean zero people. I'm complaining. Here's my official complaint, Mario. Okay. So far we have officially had one person complaining. So there is one people demanding. First the strawberry, now opaque types. What next? Yeah, but also like with opaque types, what we tend to represent with the opaque types is invariance, right? So there's the actual data and there's the invariance. So like, even if the types matched, if we have an opaque type that makes sure you have selected three types of ice cream, and then in another version, well, it's only supposed to be two now. Like, yeah, there's, you have some kind of revalidation to do anyway, or which, yeah, not sure how you would do it. Yeah. So there's one interesting part of this where this actually has come up. So there's some kind of, there's some a little bit undocumented auto-generation support for specific package types that are opaque and people kind of commonly use. So for example, something like non-empty. Now, the thing I don't think we've spoken about yet is when I say that LambdaEra extracts the type hierarchy, we only do that to the extent of user-defined types. Right? So if you're referencing like non-empty string, or like the string type from the non-empty package, we won't... Which is an opaque type? Yes. Yeah. So you have a constructor where you have to give it a string that has something in it, and it'll only return a non-empty string value if you give it a non-empty string. So once you have a value of that type, you know that it's definitely non-empty. So yeah, in that case, LambdaEra isn't snapshotting the package type. There's a few reasons for that, that I'm not sure entirely worth going into, but long story short, it focuses just on the user types now. Mainly, actually, the best reason for it is the opaque type discussion that we're having. The best reason for it is we can't really do migrations on package types, because if it's something like non-empty, they don't offer us the internals of that package. Right? So what I do is I generate some code that basically does what the sensible default would be if you knew you had a... Well, the other thing is that package types don't change, at least if the package version hasn't changed. So if we already have a value of non-empty, we don't need to migrate it. But there has been some cases where... Okay, so a better type than non-empty string, which is kind of monomorphic, or one that isn't, one that would be polymorphic, would be like, say, the anydict. One of the anydict packages, right? Like, so packages that let you define a custom type as your key, and then it lets you do lookups and inserts on this dictionary that you can't do with the vanilla dictionary implementation, because it requires keys to be comparable. So if you use one of these types, now you've got like a parametric type, right? You are putting your own user-defined custom type into this third-party package. So if you've changed that type, when it comes to a migration, you may have anydict icecream version 1, and now you need to go to anydict icecream version 2. So you need to extract all of the values from that first dictionary, and then do the migration, and then create a new dictionary. So for some of these types, it's kind of like, that's annoying, and it's mechanical. And we know pretty much what people are going to do, especially if that custom type hasn't changed. So lamdara in some cases will detect certain common library types, and it'll try and do the sensible migration for you if your custom type that's being used there hasn't changed. Is that making sense? Am I tracking? Yeah, does that mean that if I were to make a new package with a new data structure, like anydict, that it would be better if I contacted you to add support for that? I definitely wouldn't want to encourage a perspective in the ARM community that everyone should be thinking of lamdara concerns in their packages. I'd rather tackle it when it came up. But yeah, what would make it easier for a lamdara user to use your type if it's a type that doesn't contain user types, then I don't think it matters. Because they can just put the old ones in the new one. But if you are publishing a package that does contain the ability for users to put in custom types, then putting in the ability to migrate. If there was a map function that sensibly made sense, where they could give you a function of type A to B, and that let them migrate your type A to your type B in the package, then that would definitely make it easier for them to construct migrations. But otherwise, yeah, an ability to somehow exhaustively deconstruct values in a meaningful way and exhaustively construct values in a meaningful way of your package type, I think would be the key primitives that people would need to be able to express this idea of going from an old type to a new type for any conceivable type change, if that makes sense. Yeah, I could also imagine for some use cases, instead of directly having an opaque type, if you really wanted to model your application logic with an opaque type, especially in the front end, with the back end, there might be more performance concerns in some instances. But you could have your raw type stored, let's say, in your front end model. And then you could have a wrapper that takes it from the raw type to your opaque types. So you could have a function that transforms the raw types to opaque types. So immediately, as soon as you're actually working with those types, it's turning it into some opaque type that you can work with those guarantees. But then, as you're modifying it, you also need a way to transform that too. So it's a challenging problem. But that idea you have of being able to reach into the opaque types in the context of a migration is very intriguing. And it does, I mean, it's a, it's a, an interesting philosophical place to be. But overall, the feeling that I'm getting what I'm realizing with evergreen migrations is that so much of what happens with, you know, the traditional way that many people may have worked with migrating, you know, between a some sort of JavaScript front end and a Rails back end, or whatever it might be, the traditional experience that I've had doing conceptually migrations, maybe there are some, you know, there are probably some back end migrations involved. And then you need to handle that with different versions of the front end is you, you very carefully queue up the changes you're going to make, you very carefully, you know, test your actual migrations on some test data. And maybe you sort of hope that that in between state works out okay, and don't think too carefully about it. Or maybe you think very carefully about it. But even so, even if you're thinking very carefully about it, you're thinking very carefully about an implicit contract. Whereas what I'm realizing is that what lambda gives you is an explicit contract for all of these pieces, and they're all living in the same ecosystem. And, you know, if you, you could make something implicit in a lambda app by depending on something in the outside world, but for the things that are self contained within a lambda back end and lambda front end code base, it's an explicit contract. And it has the elm type system and all the guarantees that come with that of type safety and purity and no escape patches and immutability and all these things. So you put all those pieces together and what you get is an explicit contract for managing the entire migration, which is a really interesting feature to have. I mean, that's kind of a game changer. Yeah, absolutely. So for people thinking about like their traditional migration setup, I think it's kind of the pitch of evergreen is similar to the elm pitch against JavaScript. It comes up in a lot of different areas, but let's take like JavaScript, you know, decoders, right? So the idea is that you go, okay, well, there's like a whole, you know, we know, that first name is always going to be there. Like we know this, in quotes, know this, right? Then at some point in the future, someone changes it and now our code's broken. Right? So like an elm, it's like, yeah, yeah, I know, you know, this, but no, like, you've got to, you got to tell me how does this decode? I'm going to, I'm going to validate it. Okay. We validate it with the decoder. Now we know, now we know for sure. Right? Like, it's not just a, like you said, we've turned that implicit thing into the explicit. And so I think there's a lot of stuff with migrations. And part of why I was really excited about this is like, there's all this, there's all this cognitive overhead that you have to keep. Right? Because there's nothing snapshotting, nothing keeping your types for you. You have to remember when you're building this new thing, like exactly what you've changed, exactly what you've added, exactly what you've removed to the point where, like, I think it's, it's kind of nuts that the industry response seems to me, the industry standard response is like, oh, well, obviously the sensible solution is the one that's going to be the most effective. The sensible solution is to never remove anything ever again. And that's like, I get it. Like, that's, that's probably the only way that we can cut that, at least that part of the problem out. Right. In like the traditional kind of stack, or if you're working at scale, right, you go, okay, we never remove everything, never deprecate it. That's why you have stuff like, you know, protobufs. So it's like, yeah, you've allocated a field. Well, that's there forever in your payloads, every future payload, even if you never use it, that byte range is now allocated. Right? You'd have to do a major version deprecation or shift your, your schema to a new endpoint to change that or to optimize it. And so the other nice thing that we get there is that all of that stuff becomes explicit. But also, on the other hand, something I think we haven't talked about is that gap problem. Right? So I talk about in my talk, which is what if you have, if you don't have a system where everything's integrated together, and especially if you work at a company where potentially different teams can deploy different things at different times, right, you merge this change in for your backend, maybe if you're lucky, I mean, not if you're lucky, that's the wrong word. Let's say your company has chosen chosen to have a mono repo, maybe you've got one pull request, right? So the thing you merge is the front end and the backend changes at the same time. So at least that step is synchronized. But worst case, you have two different repositories, right? So those things could merge at different times. In both cases, regardless of whether you merge at the same time, or whether you merged at two different times, the deploy could still happen at two different times, right? And in most systems, they're not necessarily synchronized from a user perspective. So it is very possible that you have a situation where you have version one front end and version one backend in production. And you get a version two front end, that's trying to talk to a version one backend for a moment in time, or you could have the version two backend launches faster. So you have version one front end that's trying to talk to a future version two backend. And that inverses as well, right? So you could have a version two backend that's trying to respond to a version one front end or a version two front end that's trying to respond back to a version one backend. So there's four variations there, right? There's the outbound payloads, and there's the inbound payloads, and you could have failures on both sides. And I think generally, the thing that like, we kind of think about, okay, what tools or techniques can we apply to solve that? And I think generally, the answer is none. Right? Like there isn't really anything we can do to get explicit guarantees. One thing we can do is say we never remove fields, right? But we can still get the wrong semantics, right? Like it may be compatible, that a version two front end message gets ingested by the version one backend, but it may process the wrong business logic, you know, stuff that we've said, or should now change, right? It should be version two. Version two should be handling, you know, strawberry responses coming in and becoming mango, but we've actually gotten, you know, a mango response in the future, or we've gotten a strawberry response in the future, and a version one backend has created yet another subscription to strawberry, which shouldn't be possible anymore. So yeah, one thing. This is why I mentioned this before about, I think evergreen works really well in a system where you have full and total control of both the front end and the backend migration synchronicity, synchronicity, I'm not sure the correct pronunciation of that word. So that, you know, we apply this evergreen concepts to everything in lockstep. Right? So we know that you only there's never going to be a scenario where we're getting events from the future to older versions. Everything's always being pushed forward. And so yeah, lambda has like, like a migration, kind of like a staging thing where it basically hot loads everything and everything's prepped and ready, all the new versions are ready, everything's live. And then there's like a sudden lockstep where everything in the same instant as much as possible, kind of all slides into the future. But if we've missed anything, you know, if there's any old front end that comes online later, or there's a backend that lags for some reason, or whatever, I mean, that's technically not possible. Let's say in the future, we had, you know, distributed setup, and it was possible, it was a backend that was lagging, because that all those new versions have this migration thing set up, the first thing they do be like, Oh, I'm receiving a version one, but I'm actually on version two, let's run that through the migration first. And so now you always have consistent, like that's consistently being executed in the latest version, regardless of kind of what's at play in that synchronicity. So I think that that is a really difficult problem to solve outside of the context of a type safe, pure, immutable and exhaustive language. And I think that's what makes this thing so nice. And I'm like evergreen in Elm, I think is super delightful. It just sheds all these delightful properties. And so yeah, we leverage that as much as possible as we can. I don't think you would need the type safe parts, but it definitely helps. Yeah, it definitely you could with discipline get the same effect. But yeah, you could. And it makes the contract more explicit. I find that like thinking about migrations in this paradigm, like it, it's making me think of the data modeling in a different way, as you said, like more, more focused on the ideal data modeling, which often just a vanilla LMAP makes us do this too, I think, right? Just think about the data modeling in a, I don't know, less hacky way, like just what would be the ideal way to model this? And then you just kind of do that and, and let everything flow from there. And like, for the, you know, strawberry ice cream deprecation, like I was talking about that strategy, you could use of, you know, leaving the users flavor, selected flavors as mango, and then having some separate data that tracks that it's actually, it's actually not mango, right? And if you're doing, if you're doing a sort of migration in this more old school way, where you have these implicit contracts, then that that's not really any worse than modeling, modeling it explicitly. But if you, if you have the ability to have all these pieces, connects together very explicitly, I would tend to think of it differently, I would tend to think of it as I want my data to reflect exactly what it is. So I would tend to want to say, you know, maybe it's like a variant of flavors, or maybe it's like a wrapper around that. So you have, you either have like a selected flavor, or you have a legacy selection, in which case, shipments can't proceed or whatever. But I would want to model that state very explicitly, because otherwise, you might end up with a code path where you aren't considering that case. And what ends up happening all the time with sort of supporting these, these migrations, and these conceptual changes to the domain, in a code base is you don't consider how a change affects the entire system. You end up with all these sort of conditionals scattered around where you could easily forget something. But as with any Elm custom type, you have, you know, when some when an Elm custom type changes, it forces you to consider the impact of that change everywhere, even if it's trivial, it forces you to explicitly, you know, recognize that it's changed. And so instead of just scattering some conditionals around and saying, Oh, if it's this weird, extra Boolean that I need to check for that it's this weird case, like that ends up being really unpleasant to maintain and a huge source of bugs and also a huge source of like, you know, the more veteran programmers on that team who know this code base are like, Oh, like, talk to talk to this one programmer, you know, they know all the ins and outs of this code base, they know all the conditionals you need to be sure to check for, they know all the strange Booleans in the system that you need to check for anytime you do something. But if you model it as a custom type where you're saying exactly what it is, like a user doesn't necessarily have a flavor, they may have some legacy thing. And why does that happen? And maybe, maybe you get to a point where you can drop that at a certain point, maybe you, you have, you are able to confirm that everybody has migrated off of that. And then you can migrate off of that and, and reflect that. But it really allows you to think in terms of your ideal domain modeling instead of hacking something together and throwing some conditionals in there. Yeah, absolutely. So this is this is slightly a tangent away from evergreen. And I kind of see this as more a benefit of Lambdaera itself, or at least a benefit of the idea of like, what if we didn't have this disconnection between the way that we choose to store our data and the way that we choose to model our data? Right. And so like, if you're already familiar with Elm, and you have experienced like the delight of Elm's type system in, in the large, right, I reckon in most, cases, you can model, like your view of the world really nicely, right, especially like with custom types. And so there's this, I think, when you're in a more traditional code base, I think, at least the way that I'm thinking of more traditional code base is like, I might model those invariants with a new custom type on the front end. But usually I start to, like, especially for something like this, right, where we go, okay, we're discontinuing this product, right? The marketing team has said, like, could we, you know, could we do this, like, it would make our lives easier. Could you do this stuff in the UI, but you know, like, if it's going to take more than a day, and it's going to impact our backlog, like, we just don't, like, just don't bother, we'll just manually send some emails, or we'll do something, right? Like, because engineering time, I think, becomes scarce and precious in organizations, usually. So, you know, if you sit down, you're like, okay, well, yeah, you know, we could try and manage this for you. Okay, front end, easy, that's fine. We'll just put this new state and we'll do that. But, okay, so we're going to change the profile system in the backend, we're gonna have to add this new field, we're gonna have to do that migration. But this other system uses that way to make sure, you know, like, and you're thinking about all these layers and the different systems that you're gonna have to coordinate. Whereas in Lambdaera, you can kind of go, okay, let's add this field to the thing. And bam, we forced all these failures across the, you know, we can see immediately what the impact is, like, we're driving, what is our to do list in terms of implementing this feature. And we can see straight away, like, oh, yeah, you know what, we forgot, this is used in this module, and it causes heaps of, you know what, guys, this isn't going to be worth it. Or we can say, oh, I got two type errors, like, no, I reckon we can do this, you know, and you're not thinking about all these extra steps about how that change or set of changes is going to be translated into these kind of primitives, like into this primitive obsession that we talk about, or like, you know, Boolean blindness, or, you know, just this generally this idea where we have to dumb down to types, which we kind of, I mean, until we are graced with this much anticipated, you know, anticipated future release of Evan's work and whatever's happening in the database side, right, but at the moment, there isn't a really great way to put custom types natively into Postgres, for example, right, so on the project, so I work on those elements, the front end, your custom types become something entirely different, there's a lot of glue around modeling that, and modeling those changes. And so I think a lot of the time, doing silly little things like that, it's kind of like, ah, too hard basket, let's not bother trying to, let's not mess with the stack. Whereas I think in the Lambda situation, it's like, yeah, let's mess with the stack. Let's actively mess with it, because, you know, later on, the compiler is going to be like, all right, cool. I saw you messed with XYZ, can you please fix that? Tell me what you want to do with the migration. So I think that, yeah, that's a cool part. I'm excited for that on my own projects, because that's what I want. You know, when I'm dealing with my hobby projects, or I'm picking up a project I haven't touched for months. And I'm like, I want to add all this and change this and do that. I want to have that joy of not now worrying like, oh, is my modeling correct? Do I need to fix how I've set my stuff up in Postgres? Am I going to have SQL queries somewhere that are now wrong? Yeah, I really like what that gives you from whatever green gives you, whatever green ideology kind of gives you in that kind of full stack Elm context. So yeah, and Lambdaera pitch rant at this point. No, it's amazing. And Jeroen mentioned the idea of testing before, like that also strikes me as something that, you know, I mean, for a team that really, really wants to robustly manage their migrations and data integrity and all these sorts of things, just having evergreen migrations is huge. And, like, you know, makes the whole process so much more explicit. And you could probably write some sort of tests manually for that. But I could imagine some sort of automated things around even like testing the UI after a migration with, you know, Lambdaera program tests or, you know, testing the hot swapping. There are so many things you could imagine conceptually when you just have these pieces fitting together in this way. So it's really intriguing, really exciting stuff. Yeah, definitely. So that kind of thing, like being able to test migrations is not something that's easily done today in Lambdaera. Like if you're in Lambdaera live, like the live development environment locally, and you've changed all your types, like you'll be working on an app with the latest version. Right. But there's not a super easy way. You can with some effort do it manually. And I've helped people try and figure that out. Like the pieces are there, but it's not ergonomic. But yeah, I think that would be really, really cool in future for you to be like, hey, I want to pull down my production model and run it through the migration. And then I want to play with that result locally and see what that looks like. So that would be one cool improvement. Another one that, and so this is kind of like a full disclosure on a downside of this approach. So something that's kind of come up recently, there's been more, increasingly more teams using Lambdaera. And so when you have a team of people using or trying to develop an app together, we end up with this problem of, you know, it's kind of fairly common if you're working on a backlog and you have a few features and these teams are like kind of in an agency setting. Right. So they're working on behalf of a customer. So they go, okay, well, you know, I've got a pull request for this particular feature. Can I deploy like a preview version of this app so we can kind of show the customer we can do some QA, right. Or some kind of review so they can take a look at this. Now this has caused an interesting problem with Evergreen or some confusion, because if we think about the Evergreen assumption, right, the assumption is that you've got a straight linear change version one to two to three to four. And the idea is that when you're doing a check, what you're checking against is the production app. The being the operative word there as in the singular the, because if you have multiple production apps, suddenly everything starts to not make sense. You know, if you've changed your types, you've changed it relative to which production app. So we kind of had this issue where teams would go to create, they would just manually create another app. They would manually call it, you know, my app dash preview one, some feature. And then first things first, they would try to deploy their Lambda app that was only previously going to production. Let's say it's at version seven. So they tried to deploy to this new app and the app goes, well, hold on, you're deploying version one, but I'm seeing like version seven snapshots, like what's going on? Right? So the compulsion there, I think the natural thing is to be like, oh, that's weird. I don't know what to do about this. I'm just going to delete Evergreen folder. And now it goes, okay, great, cool. I can deploy version one for you. Right? So now you've got this code base that's deployed to version seven and to version one in two different apps. Now, a Lambdaera doesn't have a preview app concept right now. So as far as Evergreen is concerned, there's two production apps. Right? So let's say throughout the course of your pull request, you change types. And whenever you do, and you're checking against your preview deployment, Evergreen is going to be like, okay, well, this is your production app. Looks like your types have changed. You need to write a migration. Right? There is a way in Lambdaera currently to be like, do you ignore the migration? Or like, I'm happy for you to just drop it on the floor and reset my backend model to init. So they might do that because they're like, oh, this isn't important. You know, my state isn't important on this preview app. And so they do that a few times. And then if they're unlucky, what's happened a couple of times was you get everything's good and they're like, okay, great. Let's merge. And now you merge that in and you've clobbered kind of your Evergreen history. And now you're on your main app, the one that's already at version seven, you're trying to deploy and it's looking at maybe like an app version three. And so in that context, it goes, okay, cool. You've got app version three, app version seven's improved. And it tries to figure out what's going on and it doesn't really make sense. And then we end up in a really confusing position. So the way that I'm thinking to fix this, and maybe there's multiple ways to address it, but at least the Lambdaera way that I think we're going to do the first version is going to be, okay, let's have the concept of preview apps. And in a preview app, like it's a first class concept, your main production application has this thing called preview apps. So maybe somewhat similar to kind of like what people might be familiar with, like Netlify, or I think Vercel might do the same thing. And so the idea is that when you deploy to a preview app, Lambdaera will be like, ah, okay, this is a preview app. We're going to completely ignore anything to do with migrations. The assumption is that we're always deploying a version one, and we're always going to re-initialize. And actually, we might be cheeky and be like, we'll try restore the existing state into the new version. But if it happens that you've changed your types, we're just going to reset it. If the decoder fails, and it's a strict decoder, it's slightly variant from Elm's decoder. So if all the bytes aren't consumed perfectly, then it will fail. So it has to be a perfect decoding from bytes into the value. Then we've got this separate chain. So that would restore back to Lambdaera's assumptions, which is if you have an app, there is only one production, and there is only one evergreen story. There is only one chain history of changes into production. And then we've got this separate mechanism, which is preview apps, no migrations, no nothing there. So the idea is that you do your pull request, you go through all the changes, evergreen doesn't bother you at all. And then once it's approved, you merge that into main. And now when you're trying to deploy main, now you've got that. That's when that check comes in and goes, oh, okay, you're trying to deploy. Let's consistently look at what's in production now. What have you been up to locally? And what's changed? Let's get you to do that migration. So yeah, that's coming soon. TM, trademark. Yeah, what I would have imagined to be maybe slightly easier, or well, the previous thing sounds awesome. But like, if you had a v7 in production, and you're trying to deploy and it's a v10, because you tried to do some migrations in the meantime. I feel like maybe accepting unseen versions would have been easier. So there's another scenario that I can see, which could be problematic when your team grows, when you're working with teams, is that, like, I'm adding a new feature, or I'm changing a feature and the requires to write a migration, maybe complex, maybe not. And two other people on my team do the same thing. And then we all leave on vacation, and someone else tries to deploy and has to write those migrations, because we didn't in the meantime, right? But I could imagine like, well, I'm done with my work, let me write a migration that will make a v8, which we're never gonna ship. But we'll merge it anyway. And then someone else does the same thing v9 and v10. And we ship that. I just say this, like, not just, I'm just throwing this out, but I'm sure that there's some problems that you have in mind. Like, nope, that's not gonna work. Yeah. So the first thing that would happen, remember that the snapshot only happens when you deploy. Right? So let's say we have three pull requests, and every single pull, let's, so the worst case scenario is this. Worst case scenario is you have three separate pull requests, all three team members have all changed the backend model. All three team members independently on their branch have done a Lambda check against production, they've generated migrations, and they've implemented them. Right? And then now they start merging. So first person merges, they get in first. Hunky-dory, no problems. Second person now is probably gonna have GitHub conflicts on their pull request. Right? Let's say even worst case version, let's say it's a clean merge for some reason. Right? So they don't have any conflicts. Right? You mean that both would have generated a v8 migration and that would... But it would have been identical except for the differences. So let's say they changed very different parts of the model. Right? All of the snapshots were almost identical except for these deep changes. Right? So maybe the changes ended up in different snapshot files. Right? So maybe Git's like clever and it's like, ah, yeah, you're merging the same thing except for this. And I can figure out the merge. I'll merge it for you. And let's say the third person does the same. Right? What's gonna happen now is because there's only one deploy that's possible, whoever gets to the deploy is gonna have to go through that Lambda check process. And potentially, if Git has tried to be too clever, you're gonna get type errors. Right? Because something as part of those mergers might not have fully carefully covered things. So there, if you think of like, you know, like what kind of burden are they stuck with? You know, all three of you have conveniently gone on holiday. So it's the worst, worst, worst case. Like what's the absolute worst that happens? The absolute worst that happens is, let's say it's me. I'm stuck with it. I go, ah, you know, Dillon and Jeroen have left me with this. So I go, you know what? I'm gonna delete the migration. I'm gonna do a Lambda check again. Lambda always does the type snapshots. Right? So say you did type snapshots, you didn't deploy, you committed them, and then you changed more types and you did a check again. Lambda is gonna replace those version snapshots. Right? Because you haven't deployed yet. So if you haven't deployed version five, if that's the next version, but you've been changing stuff, it's just gonna keep replacing the snapshots until you deployed. Once you've deployed, it's gonna be like, okay, well fine. Next one's version six. Right? So if I delete the migration file, I do a Lambda check. I'm gonna get now consistent snapshots with everybody's changes together. If their changes together, the mergers didn't type check, like if there was an Elm compiler error, I wouldn't even be able to generate the snapshots. I would just get an Elm compiler error first. So let's say it's like worst, worst, worst, worst case. The code is broken. It's been merge broken. So A, I fix all the Elm types. Great. Now Elm's compiling. B, I run Lambda check. It redoes the snapshots. Cool. C, Lambda has now done the snapshots and it sees the migration's not there. So it goes, cool, I'm gonna generate the whole migration file for you. Right? With the placeholders for the bits that I can't migrate. And so now my job is to go, okay, let me see if I can go to those individual pull requests and slice out the individual specific migration implementations that everyone's already done. If I can, and they fit in, and then it type checks, then I go, great. I've gone through pretty much a mechanical process just following the types and getting stuff done in. If instead it was, hey, two people have actually changed stuff in the same thing, like somebody's both added and removed variants on the same custom type on two different PRs all at the same time, that's a really great stopping point for me to look at this and be like, this is nuts. We need them to come back from holiday and explain what should happen, or I need to go to a product person, or I need to figure out what's actually going on here. And I think that's cool because if that happened without this setup, there wouldn't necessarily be an indicator there that something's gone wrong. You might merge these migrations together, or actually even worse, if I think of Rails, I have a lot of experience with Rails as a counterpoint. In Rails, each developer does their own migrations as a separate file. So you wouldn't necessarily even be aware that someone else was adding and removing stuff to the same model because that would all be in different migrations. There'd be no natural mechanism to see a conflict. So you could end up in a situation where you deploy these non-commutative migrations that end you up in a weird schema state, but that nothing catches. Whereas at least in the Elm Lambdaera world, there'd be warning signs there. There's things that in the worst case would make you be like, huh, what's going on? So I think that's pretty cool. That's a decent outcome, I think, even though there might be some pain. So to summarize, your recommendation is for everyone to run Lambdaera check and write a migration, and then those migrations get merged somehow by someone at some point? No. So my recommendation would be that you do the migration part separately on the main branch if you've got lots of people editing the same stuff. Or that would be a point that you communicate together as a team. It brings that kind of idea of continuous deployment back a step. So it's not as freeform as some companies may practice, like, oh, everybody can deploy and we deploy all the time, we just don't think about it kind of thing. So it makes that a little bit more centralized. But I think what you get as a result is you get a much greater ability to model your business logic directly. And it means that you don't have to think as carefully about the consistency or the invariance that you're holding in that migration. So it trades off, I suppose, some of that kind of maybe, I don't know if you'd call it decentralized deployment model to something that's a bit more centralized, but that gives you back a bunch of guarantees in return, if that makes sense. So I remember that at one point, someone told me that Arm Review was kind of slow with their projects. If I recall correctly, that was James Carlson, who's a fervent user of Landera. And I checked it out and was like, yeah, this is a bit slow. And I figured out why. I think I know why. There was an evergreen folder with over 600 versions, meaning over six, you don't have a migration for every version, but a few hundreds migrations and snapshots. And there was code that remained in the project and that Arm Review had to run to go through, which is now a bit faster, so I'm not getting those issues anymore. But yeah, I was wondering, when should you remove those migrations? Should you? And can you remove v1? Is there a way to tell, oh, well, no one is using v1 anymore because Landera knows which client applications are running? Do you have the knowledge or not at all? Yeah, so the reason, maybe I'll answer your question backwards. So here's why you wouldn't want to remove all the migrations. You wouldn't want to remove all the migrations if you wanted to preserve a full stack, full time history, time traveling debugger. And this is a feature that doesn't exist yet. But if you wanted to use this feature when it does exist, you can imagine a slider and if you can slide back like 17 versions to a point in time backup or like a log stream restoration in Landera, you could do that. Because it could go through the migration chain to get you to the right data source. So that's why you wouldn't maybe want to remove those. But saying that... Can you migrate backwards? No, but it would be being able to get to the exact state that you were in for any given version at any point in time in history. So if you had the old migration chains, you can make sure that if the last snapshot was in version 10 and you were trying to get to it, well, this wouldn't happen because we take snapshots anyway. So it's a bit of a moot point. But this idea that you could slide a piece of value through, like the scenario that somebody told me where they had a customer that came back like a year later and they started seeing events from a year before. That person could get a hot reload a year later. Like their app could go from version 10, in Jim's case, to version like 1500 and it could slide that value like all the way up all the versions and if they were halfway through filling in a form, in theory, the form would upgrade and all their stuff would still be there. That's far-fetched. In theory, that would be possible. From a practical point of view, if you weren't concerned about that, there's no really good reason to keep more than a few versions. The only reason you want to keep a few versions is if you wanted to roll back your data. To say you've done a migration and you've done the wrong thing in the migration. You set a dict.empty or a set.empty somewhere where you were being lazy for the moment because you were like, I'll do the migration later and you did it and suddenly you destroyed all your users. If you wanted to go back in that case or if you had a customer that had been like, oh my god, it's Wednesday but we just found out on Tuesday that Mario went in and deleted 600 blog posts, can you please roll back to our state on Sunday? That happened to be four versions ago. Keeping those migrations around lets us go, yeah, sure, we can load up that old version and bam, it'll upgrade through the last four migration functions into your current app. We wouldn't have to roll your whole app back, you can have all the new features we can just roll the data back in a safe way. I guess you could use Git to restore those migrations again. Yeah, so you've got it. Which is a good feature of Git, right? Yes, you've got it exactly. From a technical perspective, there's no reason given that the snapshots would have always had to exist when you deploy, there's no reason you couldn't clean that up or that Lambda Era couldn't clean that up for you. The actual reason that everything is there is I decided early on that I wanted to keep everything really explicit and visible because in my head that would demystify it. People could go and actually look at the Elm code and be like, there's no actual magic here, it's literally just the Elm code and it's literally just there. The fact that it's kind of there and you can see it, yes, we're doing code generation but we're not hiding it away somewhere and it's not going to break in really weird ways. If it breaks, it should break with Elm type errors pointing to files that you can go and look at and be like, oh yeah, that looks wrong or right or whatever it is. It doesn't seem to happen often to my absolute bewilderment but I've always got this terror that I've implemented something wrong and someone's going to do a migration and it's just going to horrendously generate the wrong stuff. So I kind of want that also to be visible so that the user can see and if I've done the wrong thing, they could fix it. You could fix the type snapshot yourself or you could modify stuff. Thankfully, there's some gen issues in the latest migration generation but so far there haven't been many or maybe only one or two a long time ago issues with the type snapshotting. So far the type snapshots extract correctly it seems. Knock on wood, obviously. So yeah, it's more a social reality thing. I think it's a bit of a shame that Elm is not a big part of the Elm community so yeah, it's more a social reason for them being there than a technical necessity, so to speak. So yeah, we might make that more magical in future and then Elm review won't have problems, side effects as it were. So the Evergreen migration auto generation which we haven't really explicitly talked about but I understand that was a big pain point that was addressed by this latest release which is v1.1. Is there much to say about that besides that it does most of the tedious work for you? That's kind of the headline of it. Yeah, that's the headline. So the thing that people would run into that I think people would find confusing is like, say you had a custom type. Let's say we had the Ice Cream custom type but with lots and lots and lots of flavors. Let's say we had 200 flavors. And say you've changed a field somewhere else and you have to write migrations. So like, not sadly, but as a trade-off of Elm's current equality model, right? You couldn't just take that old custom type and cram it into the new one, right? Like Elm would be like, well, these are different. They're in different files, right? They're different namespaces. These are different values, even though they're structurally the same. So in prior to version 1.1, the migration would only generate the placeholders. Like, so the top level function types and names, and it'd be like, okay, here's the six migration functions I need, but you have to go to all the work of implementing what's inside them, including writing a massive function, migrate ice cream flavor, case old of every single old variant matches to every single new variant. The only difference being they're in different namespaces, but otherwise it's like the same text over and over and over and over, right? So I think this was frustrating as a user experience because it was like, if I'd only changed one field, now I'm writing migrations for all fields and all custom types everywhere, and I have to do this every time. So it wasn't the end of the world. Some people found like, okay, once I've done it once, I can pretty much kind of copy paste a lot of my migration implementation, but I wasn't happy with it. If you've done it 600 times. Yeah, Jim got really, really, really good at doing these migrations, clearly. But for newcomers as well, it was really confusing, right? Like it made them confused, extra confused. Cause it's like, I'm like, oh yeah, you generate migrations for your change types and also for these types that haven't changed at all. And it's like, once you get through it and once you think about it, you're like, oh yeah, okay. I can understand now if I understand Elm why this is necessary, but yeah, it was getting you to have to think about something else. So yeah, long story short now, Evergreen, where those types haven't changed, it does a pretty good job at basically generating a bunch of that for you. And so it tries to, as deeply as possible, I mentioned it zips effectively these two types. It starts at the top and keeps going through them. So if it's a record, it tries to pair the record fields by name and so on and so forth. And then yeah, anything that's been added, it gives you like a little notice to be like, hey, this variant has been added. It's just a reminder in case you wanted some old variants to map to this new variant that doesn't exist yet. And also, hey, this variant has been removed, right? Like what do you wanna do with this old value? Cause it has nowhere to go. So yeah, that now tries to be a lot more kind of automatic. So yeah, the feedback so far is pretty good. And Jim's happy at least. He's my, I think he suffered the pain point the most of anybody categorically. So he's told me he's enjoying it. And he says migrations only take him like, you know, a minute or two now to sort out. So yeah, that was the call. Yeah. That's great. So one thing I've been curious about, so this new release also ships with the Elm PKG's JS spec. And I've been curious like how, so from what I understand before this, with a Lambda app, you couldn't just add a.js file and ship that and arbitrarily add ports and JavaScript behavior on the page, right? So I was really curious to understand like, how does that design decision and how does Elm PKG JS fit into the concept of evergreen migrations with the front end and the guarantees you're trying to give in a front end Lambda application or the front end part of a Lambda application? Yeah, absolutely. That's a great question. So in short, there's a few features in Lambda that have actually been there for a few versions. And I've been kind of trialing it out with some customers who've ran into certain kind of limitations and they needed solutions for. And so what I announced in the last version, I think was this idea of labs. So Lambda Labs is like a set of features that are in Lambda that you can use in production, but they're marked labs because it's kind of like buyer beware, right? It's like, there's a reason this isn't recommended yet or isn't part of the mainline platform. So Elm PKG JS, which kind of, yeah, kind of started from the spec, which I was hoping maybe it would take off, but it hasn't yet, but maybe there's still time. But the idea was to be like, I'd noticed this problem where a lot of the JavaScript that people wanted to use is this. And I think as an Elm community, we've talked about this problem a few times where we've got like, there's certain Elm packages that are like, hey, this Elm package requires some ports and some JavaScript set up. Here's a bunch of long-winded instructions of varying consistency between packages of how to do that. And it feels like, I don't think it's a massive issue, but it's kind of like, it's just a bit, it's a bit painful. I was always like, oh, how do I do this? And you paste this and where do I paste it? And should I put that on this file? And what bundler do I use? I was kind of thinking about that experience with Lambda being like, what would be a nicer way to do this? And with Evergreen in mind, right? And also this restriction where, we don't want this on the backend at the moment. So in the front end, it was like, okay, a great example and a package that kind of got native, quote unquote, a support for Lambda era is Martin's Elm audio package, right? So he added a specific, like Lambda era front end with audio. So it's a function where you put your Lambda era app in that particular wrapper. And then he has an app wrapper that depends on certain ports and add some extra functionality to support like loading audio and playing audio and managing like the various, the state bits of that, right? So as a user, you can kind of be like, yeah, I have a normal app and then I wrap it in this Lambda era front end with audio and then I get like some extra bits, right? So I can manage audio and that requires some JavaScript. So the idea was to say, okay, well, there should be some way, like what effectively does this slimline JavaScript need? Effectively, it needs a way to hook into init, right? So when the Elm app is being initialized, we want to get an instance of that Elm app so that we can bind our subscriptions, like our port, our inbound and our outbound ports. So Elm package.js was being like, okay, how could we, what would it look like to have like a really delightful standard for shipping a bit of extra JavaScript and some ports with an Elm app in a way where ideally it was type safe, right? Like it was very clear what ports are there, what things go in, what things go out, how should they be used and for it to be able to check this, right? And there was some bigger grand ideas about like, you know, should we do like community verification that the JavaScript isn't gonna launch a blockchain client, you know, or something like that, you know, like maybe, you know, introduce like some safety stuff to be like, you know, if you do like Elm package.js install some package, it's gonna do an Elm install of the package and it's gonna pull the JavaScript down and it's gonna set everything up in a consistent way. And maybe that would make it really nice, you know, for the use cases. So there's, you know, there's like a copy to clipboard example in the Elm package.js spec and a few other examples of like, you know, what would it look like to have these little bits, you know, little port bindings to web APIs usually that aren't available natively in Elm. So yeah, the second concern there was when I was, so there's a proof of concept implementation of this spec. The spec has got nothing to do with Lamdera, but Lamdera implements that on package.js spec, at least in its first version, as far as I'm aware, it's the only consumer or implementer of the spec, which was also written by me, so maybe that's why. But I haven't pushed it too hard, I guess. And so yeah, in the Lamdera implementation, we only have this init, but in the spec I was also considering like an upgrade, right? So what happens when a front end is upgrading, maybe rather than init being re-invoked and you having to carefully think about, well, what happens if init gets invoked multiple times? You know, yes, you want to rebind your ports to the new app, but maybe you don't wanna re-initialize like the audio context, right? Because the user's browser hasn't reloaded. So there was an idea of like, well, could we just do all that in init and say to people, you have to think about init as being kind of like item potent, I guess, you know, like it can be run multiple times and you have to do what's sensible when that happens, or should we explicitly have, okay, this is an init thing, and then here's an upgrade. So that in upgrade, you could just be like, okay, you know, I know I don't have to do any of the init stuff, it's already there, I can only do the code that needs to happen for upgrade, which is probably rebinding ports. So that's kind of an open question. And that's why on package JS implementation, Lambda is still in labs, because that's not fully handled. So there is a way, currently the process is you contact me, but there is a way to opt out of the hot reload stuff in production. And some people have chosen to do that on their apps where they're like, you know, I don't have that many users, or I don't worry about that, but I've got like some, you know, complicated JavaScript setup that I haven't figured out how to handle this evergreen concept. And so they might opt out of the live reload. And then instead what happens is that when the app deploys and then your front end version happens it forces a hard refresh. So it forces a full page reload and reinitialization, you know, which will lose some front end state for people, but gets back the, you know, things aren't, you don't end up in that state where, you know, things are broken or you're sending old versions and you don't have the new app version. So we still get the consistency at the loss of some front end state. Does that make sense? Right, yeah, so how does that fit in with like the guarantee? So like what would happen if it was just a free for all, you can run JavaScript on the front end of a Lambda app, like, or what do you gain by the design decision to not allow that? What's the motivation behind that? Yeah, so there is no constraint actually, like you can do anything in that JavaScript on the front end. What I suppose, like the guarantee that gets made is that that JavaScript will only run in the context of an app being initialized, which isn't a huge guarantee, but it means that you, your code doesn't have to worry about the initialization setup and Lambdaera can continue to control, like there's a harness, right? Like that does the evergreen magic and some other stuff and does special bindings to make the front end back end stuff work, right? So Lambdaera platform has a bunch of harness things that it does for you to make everything seamless and everything work. So yeah, the thing that we get from a Lambdaera perspective is that we let the user kind of plug into that into a sensible way, but yeah, 100%, like a user can throw exceptions in that JavaScript code, the user could crash the app, like that all the normal JavaScript stuff comes back into play. So the user has to be careful, but yeah, I suppose the only other guarantee they get is that that in it will be called again when the upgrade happens. So it gives them a mechanism if they wanted to try to make the evergreen philosophy work with their JavaScript stuff. If the JavaScript simple enough, you don't really have to do anything, but yeah, if you had something a bit more complex, like an audio context, then yeah, you could be like, okay, cool, I know and it's gonna get called again. I'll put in some guards to check whether I'm doing the first initialization or maybe I'm already on my second one. Mm-hmm, got it, so conceptually, like if you really wanted to model what happens when you invoke a port, I guess you could just say, if an exception happens, we wanna model that explicitly and you could, but I mean, since an outgoing port doesn't get anything back, couldn't you just like let a port call happen and then say, if it crashes, we just catch the exception and log something, something like that. But I guess having the Elm package JS spec gives you a box to make those, to organize things and to make those safety guarantees within a port, is that sort of the motivation? Yeah, so the Elm package JS spec does kind of philosophize about what would be a good overall mechanism there, which could include those safety guards, but the Lambda implementation currently, it just goes to the, at least, it just kind of goes to the concept of being like, okay, you put your JavaScript for your individual port use cases in this folder in this way, and then anything that's in that folder, I'm looking for an init in each file and I'm gonna run it for you, and that's as far as it goes. So none of the spec stuff about like the types and the safety of that, like you still have to implement that yourself. So it's not like a full automatic implementation of the spec, but I would like to have that eventually, like it would be nice if there, like if that was a feature where you, if you did like a Lambda install of a package that did have JavaScript, that Lambda would be like, hey, this package is JavaScript, would you like me to set this up and create the packageports.elm file and put all the types in there for you? And I'm like, then it's just ready to go. I think that would be really nice. And then it's just a question of, would people find that interesting as a standalone tool? And would the community find that interesting as a way to be like, hey, this is how we bundle small bits of JavaScript, utility JavaScript with our packages. It's definitely an anti-goal for it to be like, this is how you drag in 17 NPM dependencies into your project like it's absolutely not for that use case. There's an, I mean, there's an example of how you would use Elm package.js, but at this stage, you'd have to bundle some of that stuff yourself, right? Like you'd have to pre-package things. Like it's not really for that. The idea was how do we like in a nice way get like this ancillary JavaScript for Elm packages. Yeah. Well, amazing stuff, Mario. If people wanna learn more about the latest release, more about Evergreen, more about Lemdira, what should they look at? Yeah, absolutely. So lemdira.com, probably the easiest starting point and easiest to remember. With a B, right? Oh, you're just ruining all of my naming. It's definitely not with a B, lemdira.com. But yeah, that'll get you through some of the pitch and pretty much straight through to the documentation. So you can take a look at that. Yeah, there's some example apps there that are really quite small and contained. So I'd recommend people take a look at those. Evergreen, I wouldn't, I mean, I would say it probably makes more sense to look at Evergreen when you need to look at Evergreen. If people are interested, you can read the Evergreen docs, but I think it probably makes more sense in practice when you're actually trying to get from one model to another in a specific use case for your specific app, but then going through that process, I think you can be like, oh yeah, that makes sense. I've done these changes and this migration that I want out of that. Reading it as a high level, I'm not sure how well that lands. But yeah, anyway, all the docs are there. And yeah, as always, we've got a Discord full of lots of lovely and helpful people. So if you want to ask any questions or ponder anything, you're always very welcome in there. Yeah, that's pretty much it. Wonderful, thanks so much for coming on, Mario. Yeah, thank you. No, it's been my absolute pleasure. Thanks for having me as always. And you're in, until next time. Until next time. Music", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " Hello Jeroen.", "tokens": [2425, 508, 2032, 268, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 1, "seek": 0, "start": 2.0, "end": 4.0, "text": " Hello Dillon.", "tokens": [2425, 28160, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 2, "seek": 0, "start": 4.0, "end": 9.0, "text": " Well Jeroen, there are some topics that are hot off the presses and you gotta hit them while they're hot.", "tokens": [1042, 508, 2032, 268, 11, 456, 366, 512, 8378, 300, 366, 2368, 766, 264, 40892, 293, 291, 3428, 2045, 552, 1339, 436, 434, 2368, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 3, "seek": 0, "start": 9.0, "end": 11.0, "text": " And some topics never get old.", "tokens": [400, 512, 8378, 1128, 483, 1331, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 4, "seek": 0, "start": 11.0, "end": 13.0, "text": " You might even call them evergreen.", "tokens": [509, 1062, 754, 818, 552, 1562, 27399, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 5, "seek": 0, "start": 13.0, "end": 17.0, "text": " And I'd like to think that our topic today is a bit of an evergreen topic.", "tokens": [400, 286, 1116, 411, 281, 519, 300, 527, 4829, 965, 307, 257, 857, 295, 364, 1562, 27399, 4829, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 6, "seek": 0, "start": 17.0, "end": 20.0, "text": " So, Mario Rojic, welcome back to the show.", "tokens": [407, 11, 9343, 3101, 73, 299, 11, 2928, 646, 281, 264, 855, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 7, "seek": 0, "start": 20.0, "end": 22.0, "text": " Hey guys, thanks for having me.", "tokens": [1911, 1074, 11, 3231, 337, 1419, 385, 13], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 8, "seek": 0, "start": 22.0, "end": 25.0, "text": " And what are we talking about today?", "tokens": [400, 437, 366, 321, 1417, 466, 965, 30], "temperature": 0.0, "avg_logprob": -0.27224953421230974, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.23225818574428558}, {"id": 9, "seek": 2500, "start": 25.0, "end": 30.0, "text": " So yeah, I guess we're talking about the idea of evergreen elm.", "tokens": [407, 1338, 11, 286, 2041, 321, 434, 1417, 466, 264, 1558, 295, 1562, 27399, 806, 76, 13], "temperature": 0.0, "avg_logprob": -0.16794561225677204, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.0038561897817999125}, {"id": 10, "seek": 2500, "start": 30.0, "end": 35.0, "text": " So, type safety between versions of your application.", "tokens": [407, 11, 2010, 4514, 1296, 9606, 295, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.16794561225677204, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.0038561897817999125}, {"id": 11, "seek": 2500, "start": 35.0, "end": 38.0, "text": " Or at least that's my interest in evergreen elm.", "tokens": [1610, 412, 1935, 300, 311, 452, 1179, 294, 1562, 27399, 806, 76, 13], "temperature": 0.0, "avg_logprob": -0.16794561225677204, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.0038561897817999125}, {"id": 12, "seek": 2500, "start": 38.0, "end": 41.0, "text": " Maybe there's other kinds of evergreen elm that people are interested in.", "tokens": [2704, 456, 311, 661, 3685, 295, 1562, 27399, 806, 76, 300, 561, 366, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.16794561225677204, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.0038561897817999125}, {"id": 13, "seek": 2500, "start": 41.0, "end": 43.0, "text": " That's a nice tagline for it.", "tokens": [663, 311, 257, 1481, 6162, 1889, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.16794561225677204, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.0038561897817999125}, {"id": 14, "seek": 2500, "start": 43.0, "end": 45.0, "text": " So, evergreen elm.", "tokens": [407, 11, 1562, 27399, 806, 76, 13], "temperature": 0.0, "avg_logprob": -0.16794561225677204, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.0038561897817999125}, {"id": 15, "seek": 2500, "start": 45.0, "end": 53.0, "text": " And we were talking before the recording here about different species, different variants of elm tree.", "tokens": [400, 321, 645, 1417, 949, 264, 6613, 510, 466, 819, 6172, 11, 819, 21669, 295, 806, 76, 4230, 13], "temperature": 0.0, "avg_logprob": -0.16794561225677204, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.0038561897817999125}, {"id": 16, "seek": 5300, "start": 53.0, "end": 59.0, "text": " And apparently there are some variants that are deciduous elm trees and some that are evergreen.", "tokens": [400, 7970, 456, 366, 512, 21669, 300, 366, 21937, 12549, 806, 76, 5852, 293, 512, 300, 366, 1562, 27399, 13], "temperature": 0.0, "avg_logprob": -0.18975888921859416, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.001476553617976606}, {"id": 17, "seek": 5300, "start": 59.0, "end": 64.0, "text": " So, perhaps lamdara is an evergreen variant of an elm tree.", "tokens": [407, 11, 4317, 24688, 67, 2419, 307, 364, 1562, 27399, 17501, 295, 364, 806, 76, 4230, 13], "temperature": 0.0, "avg_logprob": -0.18975888921859416, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.001476553617976606}, {"id": 18, "seek": 5300, "start": 64.0, "end": 67.0, "text": " Yeah, I'm down for that.", "tokens": [865, 11, 286, 478, 760, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.18975888921859416, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.001476553617976606}, {"id": 19, "seek": 5300, "start": 67.0, "end": 76.0, "text": " I'm really keen to dig into the philosophical implications and ideas behind evergreen migrations.", "tokens": [286, 478, 534, 20297, 281, 2528, 666, 264, 25066, 16602, 293, 3487, 2261, 1562, 27399, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.18975888921859416, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.001476553617976606}, {"id": 20, "seek": 5300, "start": 76.0, "end": 82.0, "text": " And what makes it such a core part of lamdara and the guarantees that it provides.", "tokens": [400, 437, 1669, 309, 1270, 257, 4965, 644, 295, 24688, 67, 2419, 293, 264, 32567, 300, 309, 6417, 13], "temperature": 0.0, "avg_logprob": -0.18975888921859416, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.001476553617976606}, {"id": 21, "seek": 8200, "start": 82.0, "end": 94.0, "text": " So, I'm kind of curious, Mario, could you tell us about the motivations and contrast it to deciduous migrations in other contexts?", "tokens": [407, 11, 286, 478, 733, 295, 6369, 11, 9343, 11, 727, 291, 980, 505, 466, 264, 39034, 293, 8712, 309, 281, 21937, 12549, 6186, 12154, 294, 661, 30628, 30], "temperature": 0.0, "avg_logprob": -0.2548404210050341, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.0003387665201444179}, {"id": 22, "seek": 8200, "start": 94.0, "end": 96.0, "text": " Maybe non-elm contexts.", "tokens": [2704, 2107, 12, 338, 76, 30628, 13], "temperature": 0.0, "avg_logprob": -0.2548404210050341, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.0003387665201444179}, {"id": 23, "seek": 8200, "start": 96.0, "end": 101.0, "text": " What was the pain point and problem you were trying to solve and the guarantees you were trying to provide?", "tokens": [708, 390, 264, 1822, 935, 293, 1154, 291, 645, 1382, 281, 5039, 293, 264, 32567, 291, 645, 1382, 281, 2893, 30], "temperature": 0.0, "avg_logprob": -0.2548404210050341, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.0003387665201444179}, {"id": 24, "seek": 8200, "start": 101.0, "end": 102.0, "text": " Yeah, sure.", "tokens": [865, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.2548404210050341, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.0003387665201444179}, {"id": 25, "seek": 10200, "start": 102.0, "end": 115.0, "text": " I think the driver for evergreen was a practical one and the name came afterwards as I kind of expanded that practical problem to the front end.", "tokens": [286, 519, 264, 6787, 337, 1562, 27399, 390, 257, 8496, 472, 293, 264, 1315, 1361, 10543, 382, 286, 733, 295, 14342, 300, 8496, 1154, 281, 264, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.24069153316437253, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00034221081295982003}, {"id": 26, "seek": 10200, "start": 115.0, "end": 128.0, "text": " So, where the concept arose from and kind of what led to my original presentation called Evergreen Elm at Elm Europe in 2019.", "tokens": [407, 11, 689, 264, 3410, 37192, 490, 293, 733, 295, 437, 4684, 281, 452, 3380, 5860, 1219, 12123, 27399, 2699, 76, 412, 2699, 76, 3315, 294, 6071, 13], "temperature": 0.0, "avg_logprob": -0.24069153316437253, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00034221081295982003}, {"id": 27, "seek": 12800, "start": 128.0, "end": 134.0, "text": " I had started working on the ideas behind early versions of lamdara.", "tokens": [286, 632, 1409, 1364, 322, 264, 3487, 2261, 2440, 9606, 295, 24688, 67, 2419, 13], "temperature": 0.0, "avg_logprob": -0.21686279439480505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.00046625419054180384}, {"id": 28, "seek": 12800, "start": 134.0, "end": 141.0, "text": " So, this idea of full stack elm and what that would look like if the elm architecture was on the back end as well.", "tokens": [407, 11, 341, 1558, 295, 1577, 8630, 806, 76, 293, 437, 300, 576, 574, 411, 498, 264, 806, 76, 9482, 390, 322, 264, 646, 917, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21686279439480505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.00046625419054180384}, {"id": 29, "seek": 12800, "start": 141.0, "end": 148.0, "text": " As opposed to previous incarnations of back end elm and some current incarnations of back end elm.", "tokens": [1018, 8851, 281, 3894, 30938, 763, 295, 646, 917, 806, 76, 293, 512, 2190, 30938, 763, 295, 646, 917, 806, 76, 13], "temperature": 0.0, "avg_logprob": -0.21686279439480505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.00046625419054180384}, {"id": 30, "seek": 12800, "start": 148.0, "end": 156.0, "text": " I kind of like, well, what if we just use elm as a language instead of JavaScript as a language, but otherwise have all the same concepts.", "tokens": [286, 733, 295, 411, 11, 731, 11, 437, 498, 321, 445, 764, 806, 76, 382, 257, 2856, 2602, 295, 15778, 382, 257, 2856, 11, 457, 5911, 362, 439, 264, 912, 10392, 13], "temperature": 0.0, "avg_logprob": -0.21686279439480505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 0.00046625419054180384}, {"id": 31, "seek": 15600, "start": 156.0, "end": 161.0, "text": " Back ends obviously have a server with HTTP endpoints.", "tokens": [5833, 5314, 2745, 362, 257, 7154, 365, 33283, 917, 20552, 13], "temperature": 0.0, "avg_logprob": -0.25882796402815933, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00026881968369707465}, {"id": 32, "seek": 15600, "start": 161.0, "end": 167.0, "text": " And so, one of the approaches was like, okay, let's model all of that in elm and see what that looks like.", "tokens": [400, 370, 11, 472, 295, 264, 11587, 390, 411, 11, 1392, 11, 718, 311, 2316, 439, 295, 300, 294, 806, 76, 293, 536, 437, 300, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.25882796402815933, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00026881968369707465}, {"id": 33, "seek": 15600, "start": 167.0, "end": 172.0, "text": " Whereas, the projects that I'd been working on and the thing that I was kind of reaching for was,", "tokens": [13813, 11, 264, 4455, 300, 286, 1116, 668, 1364, 322, 293, 264, 551, 300, 286, 390, 733, 295, 9906, 337, 390, 11], "temperature": 0.0, "avg_logprob": -0.25882796402815933, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00026881968369707465}, {"id": 34, "seek": 15600, "start": 172.0, "end": 178.0, "text": " I wanted an experience on the back end that was the same as the experience on the front end.", "tokens": [286, 1415, 364, 1752, 322, 264, 646, 917, 300, 390, 264, 912, 382, 264, 1752, 322, 264, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.25882796402815933, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.00026881968369707465}, {"id": 35, "seek": 17800, "start": 178.0, "end": 186.0, "text": " And so, part of that crazy idea initially was, well, what if the database wasn't like a separate thing?", "tokens": [400, 370, 11, 644, 295, 300, 3219, 1558, 9105, 390, 11, 731, 11, 437, 498, 264, 8149, 2067, 380, 411, 257, 4994, 551, 30], "temperature": 0.0, "avg_logprob": -0.22559755356585393, "compression_ratio": 1.7272727272727273, "no_speech_prob": 8.33686426631175e-05}, {"id": 36, "seek": 17800, "start": 186.0, "end": 192.0, "text": " Like, I like the elm model. It's nice that it just, you know, while you're operating in the front end as a front end developer,", "tokens": [1743, 11, 286, 411, 264, 806, 76, 2316, 13, 467, 311, 1481, 300, 309, 445, 11, 291, 458, 11, 1339, 291, 434, 7447, 294, 264, 1868, 917, 382, 257, 1868, 917, 10754, 11], "temperature": 0.0, "avg_logprob": -0.22559755356585393, "compression_ratio": 1.7272727272727273, "no_speech_prob": 8.33686426631175e-05}, {"id": 37, "seek": 17800, "start": 192.0, "end": 196.0, "text": " you don't think about the persistence of the front end model.", "tokens": [291, 500, 380, 519, 466, 264, 37617, 295, 264, 1868, 917, 2316, 13], "temperature": 0.0, "avg_logprob": -0.22559755356585393, "compression_ratio": 1.7272727272727273, "no_speech_prob": 8.33686426631175e-05}, {"id": 38, "seek": 17800, "start": 196.0, "end": 202.0, "text": " As long as the user's browser is open, it's quote unquote persistent, right? You're not thinking about it.", "tokens": [1018, 938, 382, 264, 4195, 311, 11185, 307, 1269, 11, 309, 311, 6513, 37557, 24315, 11, 558, 30, 509, 434, 406, 1953, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.22559755356585393, "compression_ratio": 1.7272727272727273, "no_speech_prob": 8.33686426631175e-05}, {"id": 39, "seek": 17800, "start": 202.0, "end": 206.0, "text": " And if the user closes their tab, well, then it's gone.", "tokens": [400, 498, 264, 4195, 24157, 641, 4421, 11, 731, 11, 550, 309, 311, 2780, 13], "temperature": 0.0, "avg_logprob": -0.22559755356585393, "compression_ratio": 1.7272727272727273, "no_speech_prob": 8.33686426631175e-05}, {"id": 40, "seek": 20600, "start": 206.0, "end": 212.0, "text": " And so, for the back end, I was like, okay, that's great. But, you know, if the server crashes", "tokens": [400, 370, 11, 337, 264, 646, 917, 11, 286, 390, 411, 11, 1392, 11, 300, 311, 869, 13, 583, 11, 291, 458, 11, 498, 264, 7154, 28642], "temperature": 0.0, "avg_logprob": -0.21328935784808659, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.683202496380545e-05}, {"id": 41, "seek": 20600, "start": 212.0, "end": 216.0, "text": " and all your state disappears, like that sucks.", "tokens": [293, 439, 428, 1785, 25527, 11, 411, 300, 15846, 13], "temperature": 0.0, "avg_logprob": -0.21328935784808659, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.683202496380545e-05}, {"id": 42, "seek": 20600, "start": 216.0, "end": 220.0, "text": " And so, step one was like, okay, can we solve that problem?", "tokens": [400, 370, 11, 1823, 472, 390, 411, 11, 1392, 11, 393, 321, 5039, 300, 1154, 30], "temperature": 0.0, "avg_logprob": -0.21328935784808659, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.683202496380545e-05}, {"id": 43, "seek": 20600, "start": 220.0, "end": 226.0, "text": " But then step two was several of them being like, okay, well, you know, now there's a new thing that we don't really have.", "tokens": [583, 550, 1823, 732, 390, 2940, 295, 552, 885, 411, 11, 1392, 11, 731, 11, 291, 458, 11, 586, 456, 311, 257, 777, 551, 300, 321, 500, 380, 534, 362, 13], "temperature": 0.0, "avg_logprob": -0.21328935784808659, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.683202496380545e-05}, {"id": 44, "seek": 20600, "start": 226.0, "end": 231.0, "text": " Or at least I hadn't heard talked about a lot in elm or just in front end in general,", "tokens": [1610, 412, 1935, 286, 8782, 380, 2198, 2825, 466, 257, 688, 294, 806, 76, 420, 445, 294, 1868, 917, 294, 2674, 11], "temperature": 0.0, "avg_logprob": -0.21328935784808659, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.683202496380545e-05}, {"id": 45, "seek": 23100, "start": 231.0, "end": 237.0, "text": " which is what happens when we have a new version of our app and the types have changed.", "tokens": [597, 307, 437, 2314, 562, 321, 362, 257, 777, 3037, 295, 527, 724, 293, 264, 3467, 362, 3105, 13], "temperature": 0.0, "avg_logprob": -0.21166481634583137, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.182860827015247e-06}, {"id": 46, "seek": 23100, "start": 237.0, "end": 243.0, "text": " Now, I think the general approach on the front end is like, I don't know.", "tokens": [823, 11, 286, 519, 264, 2674, 3109, 322, 264, 1868, 917, 307, 411, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.21166481634583137, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.182860827015247e-06}, {"id": 47, "seek": 23100, "start": 243.0, "end": 247.0, "text": " Cross your fingers and yeah.", "tokens": [11623, 428, 7350, 293, 1338, 13], "temperature": 0.0, "avg_logprob": -0.21166481634583137, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.182860827015247e-06}, {"id": 48, "seek": 23100, "start": 247.0, "end": 253.0, "text": " Maybe they'll click a link and the page will refresh. We'll be lucky.", "tokens": [2704, 436, 603, 2052, 257, 2113, 293, 264, 3028, 486, 15134, 13, 492, 603, 312, 6356, 13], "temperature": 0.0, "avg_logprob": -0.21166481634583137, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.182860827015247e-06}, {"id": 49, "seek": 23100, "start": 253.0, "end": 258.0, "text": " Or, you know, like, I suppose if you're, I suppose most companies would be like, I don't know.", "tokens": [1610, 11, 291, 458, 11, 411, 11, 286, 7297, 498, 291, 434, 11, 286, 7297, 881, 3431, 576, 312, 411, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.21166481634583137, "compression_ratio": 1.6210045662100456, "no_speech_prob": 7.182860827015247e-06}, {"id": 50, "seek": 25800, "start": 258.0, "end": 263.0, "text": " We'll just, hopefully it doesn't happen that much. And if something goes wrong and they contact support,", "tokens": [492, 603, 445, 11, 4696, 309, 1177, 380, 1051, 300, 709, 13, 400, 498, 746, 1709, 2085, 293, 436, 3385, 1406, 11], "temperature": 0.0, "avg_logprob": -0.217513769865036, "compression_ratio": 1.607717041800643, "no_speech_prob": 1.4509252650896087e-05}, {"id": 51, "seek": 25800, "start": 263.0, "end": 267.0, "text": " we'll tell them, hey, have you tried refreshing the page? Oh, the issues have gone away. That's cool.", "tokens": [321, 603, 980, 552, 11, 4177, 11, 362, 291, 3031, 19772, 264, 3028, 30, 876, 11, 264, 2663, 362, 2780, 1314, 13, 663, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.217513769865036, "compression_ratio": 1.607717041800643, "no_speech_prob": 1.4509252650896087e-05}, {"id": 52, "seek": 25800, "start": 267.0, "end": 270.0, "text": " Great. Glad we solved your problem. And no one thinks about it again.", "tokens": [3769, 13, 28301, 321, 13041, 428, 1154, 13, 400, 572, 472, 7309, 466, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.217513769865036, "compression_ratio": 1.607717041800643, "no_speech_prob": 1.4509252650896087e-05}, {"id": 53, "seek": 25800, "start": 270.0, "end": 275.0, "text": " I guess the problem happens more with single page applications rather than.", "tokens": [286, 2041, 264, 1154, 2314, 544, 365, 2167, 3028, 5821, 2831, 813, 13], "temperature": 0.0, "avg_logprob": -0.217513769865036, "compression_ratio": 1.607717041800643, "no_speech_prob": 1.4509252650896087e-05}, {"id": 54, "seek": 25800, "start": 275.0, "end": 276.0, "text": " Yeah, definitely.", "tokens": [865, 11, 2138, 13], "temperature": 0.0, "avg_logprob": -0.217513769865036, "compression_ratio": 1.607717041800643, "no_speech_prob": 1.4509252650896087e-05}, {"id": 55, "seek": 25800, "start": 276.0, "end": 278.0, "text": " More traditional way.", "tokens": [5048, 5164, 636, 13], "temperature": 0.0, "avg_logprob": -0.217513769865036, "compression_ratio": 1.607717041800643, "no_speech_prob": 1.4509252650896087e-05}, {"id": 56, "seek": 25800, "start": 278.0, "end": 283.0, "text": " And I had heard, I can't remember who I heard this from, but I had heard anecdotally that like someone was,", "tokens": [400, 286, 632, 2198, 11, 286, 393, 380, 1604, 567, 286, 2198, 341, 490, 11, 457, 286, 632, 2198, 26652, 310, 379, 300, 411, 1580, 390, 11], "temperature": 0.0, "avg_logprob": -0.217513769865036, "compression_ratio": 1.607717041800643, "no_speech_prob": 1.4509252650896087e-05}, {"id": 57, "seek": 28300, "start": 283.0, "end": 288.0, "text": " told me, I was like, oh, they once received like their company used like versioning or something.", "tokens": [1907, 385, 11, 286, 390, 411, 11, 1954, 11, 436, 1564, 4613, 411, 641, 2237, 1143, 411, 3037, 278, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.2537870128252353, "compression_ratio": 1.9626168224299065, "no_speech_prob": 1.982924914045725e-05}, {"id": 58, "seek": 28300, "start": 288.0, "end": 293.0, "text": " Right. So for the front end comes. So they're like, oh, you know, I don't think we have this problem because we use versioning.", "tokens": [1779, 13, 407, 337, 264, 1868, 917, 1487, 13, 407, 436, 434, 411, 11, 1954, 11, 291, 458, 11, 286, 500, 380, 519, 321, 362, 341, 1154, 570, 321, 764, 3037, 278, 13], "temperature": 0.0, "avg_logprob": -0.2537870128252353, "compression_ratio": 1.9626168224299065, "no_speech_prob": 1.982924914045725e-05}, {"id": 59, "seek": 28300, "start": 293.0, "end": 298.0, "text": " And I was like, okay, cool. What's the oldest event version that you've gotten?", "tokens": [400, 286, 390, 411, 11, 1392, 11, 1627, 13, 708, 311, 264, 14026, 2280, 3037, 300, 291, 600, 5768, 30], "temperature": 0.0, "avg_logprob": -0.2537870128252353, "compression_ratio": 1.9626168224299065, "no_speech_prob": 1.982924914045725e-05}, {"id": 60, "seek": 28300, "start": 298.0, "end": 304.0, "text": " They're like, oh yeah, we got like, someone must have kept a tab open for like a year or something because we got events from like a year ago.", "tokens": [814, 434, 411, 11, 1954, 1338, 11, 321, 658, 411, 11, 1580, 1633, 362, 4305, 257, 4421, 1269, 337, 411, 257, 1064, 420, 746, 570, 321, 658, 3931, 490, 411, 257, 1064, 2057, 13], "temperature": 0.0, "avg_logprob": -0.2537870128252353, "compression_ratio": 1.9626168224299065, "no_speech_prob": 1.982924914045725e-05}, {"id": 61, "seek": 28300, "start": 304.0, "end": 308.0, "text": " I was like, right. And what do you do with those? And they're like, I don't know if you drop them on the floor.", "tokens": [286, 390, 411, 11, 558, 13, 400, 437, 360, 291, 360, 365, 729, 30, 400, 436, 434, 411, 11, 286, 500, 380, 458, 498, 291, 3270, 552, 322, 264, 4123, 13], "temperature": 0.0, "avg_logprob": -0.2537870128252353, "compression_ratio": 1.9626168224299065, "no_speech_prob": 1.982924914045725e-05}, {"id": 62, "seek": 28300, "start": 308.0, "end": 312.0, "text": " What can you do? You can't do anything right. In the normal thinking.", "tokens": [708, 393, 291, 360, 30, 509, 393, 380, 360, 1340, 558, 13, 682, 264, 2710, 1953, 13], "temperature": 0.0, "avg_logprob": -0.2537870128252353, "compression_ratio": 1.9626168224299065, "no_speech_prob": 1.982924914045725e-05}, {"id": 63, "seek": 31200, "start": 312.0, "end": 316.0, "text": " So I was like, what would be a really elm way to solve this?", "tokens": [407, 286, 390, 411, 11, 437, 576, 312, 257, 534, 806, 76, 636, 281, 5039, 341, 30], "temperature": 0.0, "avg_logprob": -0.1947302235116204, "compression_ratio": 1.7925925925925925, "no_speech_prob": 7.60196999181062e-05}, {"id": 64, "seek": 31200, "start": 316.0, "end": 320.0, "text": " And yeah, the only thing I could think of was, well, it should just upgrade.", "tokens": [400, 1338, 11, 264, 787, 551, 286, 727, 519, 295, 390, 11, 731, 11, 309, 820, 445, 11484, 13], "temperature": 0.0, "avg_logprob": -0.1947302235116204, "compression_ratio": 1.7925925925925925, "no_speech_prob": 7.60196999181062e-05}, {"id": 65, "seek": 31200, "start": 320.0, "end": 324.0, "text": " Like it should just work. It should just continue.", "tokens": [1743, 309, 820, 445, 589, 13, 467, 820, 445, 2354, 13], "temperature": 0.0, "avg_logprob": -0.1947302235116204, "compression_ratio": 1.7925925925925925, "no_speech_prob": 7.60196999181062e-05}, {"id": 66, "seek": 31200, "start": 324.0, "end": 327.0, "text": " And even more so than that, it's like, I want it to type check. That's what I want.", "tokens": [400, 754, 544, 370, 813, 300, 11, 309, 311, 411, 11, 286, 528, 309, 281, 2010, 1520, 13, 663, 311, 437, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.1947302235116204, "compression_ratio": 1.7925925925925925, "no_speech_prob": 7.60196999181062e-05}, {"id": 67, "seek": 31200, "start": 327.0, "end": 331.0, "text": " Like I want to go from one version of app to a new version of an app.", "tokens": [1743, 286, 528, 281, 352, 490, 472, 3037, 295, 724, 281, 257, 777, 3037, 295, 364, 724, 13], "temperature": 0.0, "avg_logprob": -0.1947302235116204, "compression_ratio": 1.7925925925925925, "no_speech_prob": 7.60196999181062e-05}, {"id": 68, "seek": 31200, "start": 331.0, "end": 335.0, "text": " And I want the compiler to be like, yeah, you, this is good.", "tokens": [400, 286, 528, 264, 31958, 281, 312, 411, 11, 1338, 11, 291, 11, 341, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.1947302235116204, "compression_ratio": 1.7925925925925925, "no_speech_prob": 7.60196999181062e-05}, {"id": 69, "seek": 31200, "start": 335.0, "end": 340.0, "text": " Like however you're going between these two, that type checks, that makes sense.", "tokens": [1743, 4461, 291, 434, 516, 1296, 613, 732, 11, 300, 2010, 13834, 11, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1947302235116204, "compression_ratio": 1.7925925925925925, "no_speech_prob": 7.60196999181062e-05}, {"id": 70, "seek": 34000, "start": 340.0, "end": 353.0, "text": " So that was the first motivation. It was like that, that thought of like, okay, well, if we have data that's alive in one app and we're releasing new app with new types, how do we get from one app to the second app?", "tokens": [407, 300, 390, 264, 700, 12335, 13, 467, 390, 411, 300, 11, 300, 1194, 295, 411, 11, 1392, 11, 731, 11, 498, 321, 362, 1412, 300, 311, 5465, 294, 472, 724, 293, 321, 434, 16327, 777, 724, 365, 777, 3467, 11, 577, 360, 321, 483, 490, 472, 724, 281, 264, 1150, 724, 30], "temperature": 0.0, "avg_logprob": -0.2412139574686686, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.00012533327389974147}, {"id": 71, "seek": 34000, "start": 353.0, "end": 355.0, "text": " And the reason name came about.", "tokens": [400, 264, 1778, 1315, 1361, 466, 13], "temperature": 0.0, "avg_logprob": -0.2412139574686686, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.00012533327389974147}, {"id": 72, "seek": 35500, "start": 355.0, "end": 380.0, "text": " I feel like you're intuitively going to the more like subtle edge case of it, but there's also like the core meat of what most people would think of as a migration that people wouldn't gloss over how to handle these edge cases in their application and just be like, Matt, I hope it works, which is migrating your backend, migrating your database.", "tokens": [286, 841, 411, 291, 434, 46506, 516, 281, 264, 544, 411, 13743, 4691, 1389, 295, 309, 11, 457, 456, 311, 611, 411, 264, 4965, 4615, 295, 437, 881, 561, 576, 519, 295, 382, 257, 17011, 300, 561, 2759, 380, 19574, 670, 577, 281, 4813, 613, 4691, 3331, 294, 641, 3861, 293, 445, 312, 411, 11, 7397, 11, 286, 1454, 309, 1985, 11, 597, 307, 6186, 8754, 428, 38087, 11, 6186, 8754, 428, 8149, 13], "temperature": 0.0, "avg_logprob": -0.1989594483986879, "compression_ratio": 1.6398104265402844, "no_speech_prob": 6.20463615632616e-05}, {"id": 73, "seek": 38000, "start": 380.0, "end": 395.0, "text": " Right. And so of course that's a less subtle case, but evergreen migrations also provide a type safe way to migrate from one version of your backend model to a new version of your backend model.", "tokens": [1779, 13, 400, 370, 295, 1164, 300, 311, 257, 1570, 13743, 1389, 11, 457, 1562, 27399, 6186, 12154, 611, 2893, 257, 2010, 3273, 636, 281, 31821, 490, 472, 3037, 295, 428, 38087, 2316, 281, 257, 777, 3037, 295, 428, 38087, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18408873529717473, "compression_ratio": 1.7436974789915967, "no_speech_prob": 2.8856273274868727e-05}, {"id": 74, "seek": 38000, "start": 395.0, "end": 398.0, "text": " Yeah, absolutely. So that was the practical driver.", "tokens": [865, 11, 3122, 13, 407, 300, 390, 264, 8496, 6787, 13], "temperature": 0.0, "avg_logprob": -0.18408873529717473, "compression_ratio": 1.7436974789915967, "no_speech_prob": 2.8856273274868727e-05}, {"id": 75, "seek": 38000, "start": 398.0, "end": 403.0, "text": " It was like, yeah, obviously the concept of migrations of your backend is already a thing and it's important.", "tokens": [467, 390, 411, 11, 1338, 11, 2745, 264, 3410, 295, 6186, 12154, 295, 428, 38087, 307, 1217, 257, 551, 293, 309, 311, 1021, 13], "temperature": 0.0, "avg_logprob": -0.18408873529717473, "compression_ratio": 1.7436974789915967, "no_speech_prob": 2.8856273274868727e-05}, {"id": 76, "seek": 38000, "start": 403.0, "end": 406.0, "text": " So I wanted to facilitate that and for it to be type safe.", "tokens": [407, 286, 1415, 281, 20207, 300, 293, 337, 309, 281, 312, 2010, 3273, 13], "temperature": 0.0, "avg_logprob": -0.18408873529717473, "compression_ratio": 1.7436974789915967, "no_speech_prob": 2.8856273274868727e-05}, {"id": 77, "seek": 40600, "start": 406.0, "end": 413.0, "text": " And then where the name came from is I realized like, oh, now that like if we have this, well, we could do this on the front end as well.", "tokens": [400, 550, 689, 264, 1315, 1361, 490, 307, 286, 5334, 411, 11, 1954, 11, 586, 300, 411, 498, 321, 362, 341, 11, 731, 11, 321, 727, 360, 341, 322, 264, 1868, 917, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.24987762532335647, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.0002913484931923449}, {"id": 78, "seek": 40600, "start": 413.0, "end": 427.0, "text": " And so instead of the front end effectively like having to drop it state every time there was a new app or you hope the user clicks, which for me had that notion of like, you know, the leaves falling off the tree, kind of being deciduous.", "tokens": [400, 370, 2602, 295, 264, 1868, 917, 8659, 411, 1419, 281, 3270, 309, 1785, 633, 565, 456, 390, 257, 777, 724, 420, 291, 1454, 264, 4195, 18521, 11, 597, 337, 385, 632, 300, 10710, 295, 411, 11, 291, 458, 11, 264, 5510, 7440, 766, 264, 4230, 11, 733, 295, 885, 21937, 12549, 13], "temperature": 0.0, "avg_logprob": -0.24987762532335647, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.0002913484931923449}, {"id": 79, "seek": 42700, "start": 427.0, "end": 436.0, "text": " Then I thought, oh, well, if the front end can always stay alive, then I was suddenly like, well, that's kind of like how browsers now, they call them like evergreen, right?", "tokens": [1396, 286, 1194, 11, 1954, 11, 731, 11, 498, 264, 1868, 917, 393, 1009, 1754, 5465, 11, 550, 286, 390, 5800, 411, 11, 731, 11, 300, 311, 733, 295, 411, 577, 36069, 586, 11, 436, 818, 552, 411, 1562, 27399, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2177540826015785, "compression_ratio": 1.7392996108949417, "no_speech_prob": 1.1478396118036471e-05}, {"id": 80, "seek": 42700, "start": 436.0, "end": 445.0, "text": " Like browsers that continually update themselves. And so, yeah, that's, you know, elm and trees and evergreen. I was like, oh, this is nice. This is, yeah, this is the way I'm going.", "tokens": [1743, 36069, 300, 22277, 5623, 2969, 13, 400, 370, 11, 1338, 11, 300, 311, 11, 291, 458, 11, 806, 76, 293, 5852, 293, 1562, 27399, 13, 286, 390, 411, 11, 1954, 11, 341, 307, 1481, 13, 639, 307, 11, 1338, 11, 341, 307, 264, 636, 286, 478, 516, 13], "temperature": 0.0, "avg_logprob": -0.2177540826015785, "compression_ratio": 1.7392996108949417, "no_speech_prob": 1.1478396118036471e-05}, {"id": 81, "seek": 42700, "start": 445.0, "end": 451.0, "text": " So, yeah, I'm not sure how well though that term translates. A lot of people get confused.", "tokens": [407, 11, 1338, 11, 286, 478, 406, 988, 577, 731, 1673, 300, 1433, 28468, 13, 316, 688, 295, 561, 483, 9019, 13], "temperature": 0.0, "avg_logprob": -0.2177540826015785, "compression_ratio": 1.7392996108949417, "no_speech_prob": 1.1478396118036471e-05}, {"id": 82, "seek": 45100, "start": 451.0, "end": 460.0, "text": " I had imagined evergreen would like perfectly like encapsulate and explain everything. I'd be like, yeah, it's evergreen, but it doesn't, I'm not sure that in practice that's translated.", "tokens": [286, 632, 16590, 1562, 27399, 576, 411, 6239, 411, 38745, 5256, 293, 2903, 1203, 13, 286, 1116, 312, 411, 11, 1338, 11, 309, 311, 1562, 27399, 11, 457, 309, 1177, 380, 11, 286, 478, 406, 988, 300, 294, 3124, 300, 311, 16805, 13], "temperature": 0.0, "avg_logprob": -0.19668597709841845, "compression_ratio": 1.7711267605633803, "no_speech_prob": 9.60568359005265e-05}, {"id": 83, "seek": 45100, "start": 460.0, "end": 470.0, "text": " So I've definitely found that, that, yeah, people, it feels to me like, or at least the way that I experienced it, I'd be curious how you guys have experienced it.", "tokens": [407, 286, 600, 2138, 1352, 300, 11, 300, 11, 1338, 11, 561, 11, 309, 3417, 281, 385, 411, 11, 420, 412, 1935, 264, 636, 300, 286, 6751, 309, 11, 286, 1116, 312, 6369, 577, 291, 1074, 362, 6751, 309, 13], "temperature": 0.0, "avg_logprob": -0.19668597709841845, "compression_ratio": 1.7711267605633803, "no_speech_prob": 9.60568359005265e-05}, {"id": 84, "seek": 45100, "start": 470.0, "end": 478.0, "text": " My experience is that people go, oh, that's some black magic. Like there's something crazy going on with evergreen. There's something magical happening.", "tokens": [1222, 1752, 307, 300, 561, 352, 11, 1954, 11, 300, 311, 512, 2211, 5585, 13, 1743, 456, 311, 746, 3219, 516, 322, 365, 1562, 27399, 13, 821, 311, 746, 12066, 2737, 13], "temperature": 0.0, "avg_logprob": -0.19668597709841845, "compression_ratio": 1.7711267605633803, "no_speech_prob": 9.60568359005265e-05}, {"id": 85, "seek": 47800, "start": 478.0, "end": 486.0, "text": " When in reality, it's actually really dumb. Like it's, it's the dumbest possible thing. It's like, how do you migrate with a function?", "tokens": [1133, 294, 4103, 11, 309, 311, 767, 534, 10316, 13, 1743, 309, 311, 11, 309, 311, 264, 10316, 377, 1944, 551, 13, 467, 311, 411, 11, 577, 360, 291, 31821, 365, 257, 2445, 30], "temperature": 0.0, "avg_logprob": -0.22667899339095407, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.3844693057762925e-05}, {"id": 86, "seek": 47800, "start": 486.0, "end": 498.0, "text": " Like it takes value A and it returns value B. But I think, yeah, the, the grokking, like the, the, the details of what that actually means has interestingly been, yeah, a stumbling point, which I found interesting.", "tokens": [1743, 309, 2516, 2158, 316, 293, 309, 11247, 2158, 363, 13, 583, 286, 519, 11, 1338, 11, 264, 11, 264, 4634, 74, 5092, 11, 411, 264, 11, 264, 11, 264, 4365, 295, 437, 300, 767, 1355, 575, 25873, 668, 11, 1338, 11, 257, 342, 14188, 935, 11, 597, 286, 1352, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22667899339095407, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.3844693057762925e-05}, {"id": 87, "seek": 49800, "start": 498.0, "end": 508.0, "text": " Yeah, I was there. Well, we were both there actually with them. Churchalk in Europe, 2019. So yeah, we, we got the explanation right off the bat.", "tokens": [865, 11, 286, 390, 456, 13, 1042, 11, 321, 645, 1293, 456, 767, 365, 552, 13, 7882, 667, 294, 3315, 11, 6071, 13, 407, 1338, 11, 321, 11, 321, 658, 264, 10835, 558, 766, 264, 7362, 13], "temperature": 0.0, "avg_logprob": -0.3825606599860235, "compression_ratio": 1.612, "no_speech_prob": 1.3203674825490452e-05}, {"id": 88, "seek": 49800, "start": 508.0, "end": 526.0, "text": " So you've been biased. Yeah, very biased. So for me, it's clear. It's, it's a function. And I've also played with them there a little bit. So yeah, for me, it's pretty simple, but I can understand why people would be confused or scared of this because yeah.", "tokens": [407, 291, 600, 668, 28035, 13, 865, 11, 588, 28035, 13, 407, 337, 385, 11, 309, 311, 1850, 13, 467, 311, 11, 309, 311, 257, 2445, 13, 400, 286, 600, 611, 3737, 365, 552, 456, 257, 707, 857, 13, 407, 1338, 11, 337, 385, 11, 309, 311, 1238, 2199, 11, 457, 286, 393, 1223, 983, 561, 576, 312, 9019, 420, 5338, 295, 341, 570, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3825606599860235, "compression_ratio": 1.612, "no_speech_prob": 1.3203674825490452e-05}, {"id": 89, "seek": 52600, "start": 526.0, "end": 534.0, "text": " Like imagine doing this in any other system than Elm or Lamdara and like, yeah, no, like that has to be magic.", "tokens": [1743, 3811, 884, 341, 294, 604, 661, 1185, 813, 2699, 76, 420, 18825, 67, 2419, 293, 411, 11, 1338, 11, 572, 11, 411, 300, 575, 281, 312, 5585, 13], "temperature": 0.0, "avg_logprob": -0.21315901620047434, "compression_ratio": 1.5530973451327434, "no_speech_prob": 4.1325554775539786e-05}, {"id": 90, "seek": 52600, "start": 534.0, "end": 546.0, "text": " Yeah, yeah, yeah. So maybe, and I think it was also funny because like at, I was devising this idea for Lamdara, which didn't quite exist yet. And so I kind of wanted to do a talk on evergreen Elm, but I couldn't really mention the backend.", "tokens": [865, 11, 1338, 11, 1338, 13, 407, 1310, 11, 293, 286, 519, 309, 390, 611, 4074, 570, 411, 412, 11, 286, 390, 1905, 3436, 341, 1558, 337, 18825, 67, 2419, 11, 597, 994, 380, 1596, 2514, 1939, 13, 400, 370, 286, 733, 295, 1415, 281, 360, 257, 751, 322, 1562, 27399, 2699, 76, 11, 457, 286, 2809, 380, 534, 2152, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.21315901620047434, "compression_ratio": 1.5530973451327434, "no_speech_prob": 4.1325554775539786e-05}, {"id": 91, "seek": 54600, "start": 546.0, "end": 558.0, "text": " So I had to reframe my whole Elm Europe talk as, well, yeah, like this concept on the front end. And I think for some people, maybe it was like, well, why would you go to all that effort in just refresh?", "tokens": [407, 286, 632, 281, 13334, 529, 452, 1379, 2699, 76, 3315, 751, 382, 11, 731, 11, 1338, 11, 411, 341, 3410, 322, 264, 1868, 917, 13, 400, 286, 519, 337, 512, 561, 11, 1310, 309, 390, 411, 11, 731, 11, 983, 576, 291, 352, 281, 439, 300, 4630, 294, 445, 15134, 30], "temperature": 0.0, "avg_logprob": -0.2035851965145189, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.7609090213663876e-05}, {"id": 92, "seek": 54600, "start": 558.0, "end": 566.0, "text": " So it was kind of like this idea was there and I was like, this idea is so cool, but then I couldn't talk about it in the context where I thought it was super cool.", "tokens": [407, 309, 390, 733, 295, 411, 341, 1558, 390, 456, 293, 286, 390, 411, 11, 341, 1558, 307, 370, 1627, 11, 457, 550, 286, 2809, 380, 751, 466, 309, 294, 264, 4319, 689, 286, 1194, 309, 390, 1687, 1627, 13], "temperature": 0.0, "avg_logprob": -0.2035851965145189, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.7609090213663876e-05}, {"id": 93, "seek": 56600, "start": 566.0, "end": 576.0, "text": " So yeah, I think I kind of demonstrated it backwards, but yeah, maybe, maybe if we just went very briefly for those listening to understand or people who may not have a concept or maybe aren't tracking,", "tokens": [407, 1338, 11, 286, 519, 286, 733, 295, 18772, 309, 12204, 11, 457, 1338, 11, 1310, 11, 1310, 498, 321, 445, 1437, 588, 10515, 337, 729, 4764, 281, 1223, 420, 561, 567, 815, 406, 362, 257, 3410, 420, 1310, 3212, 380, 11603, 11], "temperature": 0.0, "avg_logprob": -0.20224550035264757, "compression_ratio": 1.6329113924050633, "no_speech_prob": 2.8394595574354753e-05}, {"id": 94, "seek": 56600, "start": 576.0, "end": 584.0, "text": " we could go through like a couple of types of state and what a migration would look like verbally. I think that might give an indicator. What do you reckon?", "tokens": [321, 727, 352, 807, 411, 257, 1916, 295, 3467, 295, 1785, 293, 437, 257, 17011, 576, 574, 411, 48162, 13, 286, 519, 300, 1062, 976, 364, 16961, 13, 708, 360, 291, 29548, 30], "temperature": 0.0, "avg_logprob": -0.20224550035264757, "compression_ratio": 1.6329113924050633, "no_speech_prob": 2.8394595574354753e-05}, {"id": 95, "seek": 56600, "start": 584.0, "end": 594.0, "text": " I just want to correct something because, which is my fault because I induced you into giving an error, saying something wrong. Evergreen Elm was from 2018.", "tokens": [286, 445, 528, 281, 3006, 746, 570, 11, 597, 307, 452, 7441, 570, 286, 33991, 291, 666, 2902, 364, 6713, 11, 1566, 746, 2085, 13, 12123, 27399, 2699, 76, 390, 490, 6096, 13], "temperature": 0.0, "avg_logprob": -0.20224550035264757, "compression_ratio": 1.6329113924050633, "no_speech_prob": 2.8394595574354753e-05}, {"id": 96, "seek": 59400, "start": 594.0, "end": 596.0, "text": " Oh, I see.", "tokens": [876, 11, 286, 536, 13], "temperature": 0.0, "avg_logprob": -0.2581117225415779, "compression_ratio": 1.5138888888888888, "no_speech_prob": 3.426521288929507e-05}, {"id": 97, "seek": 59400, "start": 596.0, "end": 598.0, "text": " Lamdera.", "tokens": [18825, 1068, 64, 13], "temperature": 0.0, "avg_logprob": -0.2581117225415779, "compression_ratio": 1.5138888888888888, "no_speech_prob": 3.426521288929507e-05}, {"id": 98, "seek": 59400, "start": 598.0, "end": 600.0, "text": " Lamdera was 2019.", "tokens": [18825, 1068, 64, 390, 6071, 13], "temperature": 0.0, "avg_logprob": -0.2581117225415779, "compression_ratio": 1.5138888888888888, "no_speech_prob": 3.426521288929507e-05}, {"id": 99, "seek": 59400, "start": 600.0, "end": 608.0, "text": " That's the one I was in the audience for. But I really enjoyed watching the Evergreen one on YouTube.", "tokens": [663, 311, 264, 472, 286, 390, 294, 264, 4034, 337, 13, 583, 286, 534, 4626, 1976, 264, 12123, 27399, 472, 322, 3088, 13], "temperature": 0.0, "avg_logprob": -0.2581117225415779, "compression_ratio": 1.5138888888888888, "no_speech_prob": 3.426521288929507e-05}, {"id": 100, "seek": 59400, "start": 608.0, "end": 620.0, "text": " Yeah. Okay. So let's take a model. So let's say you have a, I think explaining it on the front end is probably the easiest. So let's say on your front end, we have the counter app. Right.", "tokens": [865, 13, 1033, 13, 407, 718, 311, 747, 257, 2316, 13, 407, 718, 311, 584, 291, 362, 257, 11, 286, 519, 13468, 309, 322, 264, 1868, 917, 307, 1391, 264, 12889, 13, 407, 718, 311, 584, 322, 428, 1868, 917, 11, 321, 362, 264, 5682, 724, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.2581117225415779, "compression_ratio": 1.5138888888888888, "no_speech_prob": 3.426521288929507e-05}, {"id": 101, "seek": 62000, "start": 620.0, "end": 633.0, "text": " And I think I did this in my talk in France. So say the counter app starts with like counter of, it's a record with one field, field name counter, and the field type is int. Right.", "tokens": [400, 286, 519, 286, 630, 341, 294, 452, 751, 294, 6190, 13, 407, 584, 264, 5682, 724, 3719, 365, 411, 5682, 295, 11, 309, 311, 257, 2136, 365, 472, 2519, 11, 2519, 1315, 5682, 11, 293, 264, 2519, 2010, 307, 560, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.20664562268203565, "compression_ratio": 1.61244019138756, "no_speech_prob": 2.885345384129323e-05}, {"id": 102, "seek": 62000, "start": 633.0, "end": 644.0, "text": " And let's say for some silly reason, we say, okay, well, we now want to release a new version of the app, but we've changed the counter field to type float.", "tokens": [400, 718, 311, 584, 337, 512, 11774, 1778, 11, 321, 584, 11, 1392, 11, 731, 11, 321, 586, 528, 281, 4374, 257, 777, 3037, 295, 264, 724, 11, 457, 321, 600, 3105, 264, 5682, 2519, 281, 2010, 15706, 13], "temperature": 0.0, "avg_logprob": -0.20664562268203565, "compression_ratio": 1.61244019138756, "no_speech_prob": 2.885345384129323e-05}, {"id": 103, "seek": 64400, "start": 644.0, "end": 659.0, "text": " And the question would be like, how do we migrate from one to the other? And so, yeah, when we say it's just a function, what we're saying is like, yeah, we want a function that takes a record with a field counter type int and returns a record with a field counter type float.", "tokens": [400, 264, 1168, 576, 312, 411, 11, 577, 360, 321, 31821, 490, 472, 281, 264, 661, 30, 400, 370, 11, 1338, 11, 562, 321, 584, 309, 311, 445, 257, 2445, 11, 437, 321, 434, 1566, 307, 411, 11, 1338, 11, 321, 528, 257, 2445, 300, 2516, 257, 2136, 365, 257, 2519, 5682, 2010, 560, 293, 11247, 257, 2136, 365, 257, 2519, 5682, 2010, 15706, 13], "temperature": 0.0, "avg_logprob": -0.1745200929187593, "compression_ratio": 1.7883817427385893, "no_speech_prob": 2.429882988508325e-05}, {"id": 104, "seek": 64400, "start": 659.0, "end": 668.0, "text": " So returning the new record, you basically just construct the record. But for the value of counter, right, you've got an integer and now you need a float.", "tokens": [407, 12678, 264, 777, 2136, 11, 291, 1936, 445, 7690, 264, 2136, 13, 583, 337, 264, 2158, 295, 5682, 11, 558, 11, 291, 600, 658, 364, 24922, 293, 586, 291, 643, 257, 15706, 13], "temperature": 0.0, "avg_logprob": -0.1745200929187593, "compression_ratio": 1.7883817427385893, "no_speech_prob": 2.429882988508325e-05}, {"id": 105, "seek": 66800, "start": 668.0, "end": 680.0, "text": " So you kind of go off and you find a function, you know, what does like what function takes an integer and returns a float. And that's the function that you would use. So I can't remember off the top of my head. Is that from int or to float?", "tokens": [407, 291, 733, 295, 352, 766, 293, 291, 915, 257, 2445, 11, 291, 458, 11, 437, 775, 411, 437, 2445, 2516, 364, 24922, 293, 11247, 257, 15706, 13, 400, 300, 311, 264, 2445, 300, 291, 576, 764, 13, 407, 286, 393, 380, 1604, 766, 264, 1192, 295, 452, 1378, 13, 1119, 300, 490, 560, 420, 281, 15706, 30], "temperature": 0.0, "avg_logprob": -0.2550000483446782, "compression_ratio": 1.626086956521739, "no_speech_prob": 1.0289361853210721e-05}, {"id": 106, "seek": 66800, "start": 680.0, "end": 683.0, "text": " One of those two. Probably have to look that up.", "tokens": [1485, 295, 729, 732, 13, 9210, 362, 281, 574, 300, 493, 13], "temperature": 0.0, "avg_logprob": -0.2550000483446782, "compression_ratio": 1.626086956521739, "no_speech_prob": 1.0289361853210721e-05}, {"id": 107, "seek": 66800, "start": 683.0, "end": 691.0, "text": " To float, I'm pretty sure. Is what I would try. And then if I get a compiler error.", "tokens": [1407, 15706, 11, 286, 478, 1238, 988, 13, 1119, 437, 286, 576, 853, 13, 400, 550, 498, 286, 483, 257, 31958, 6713, 13], "temperature": 0.0, "avg_logprob": -0.2550000483446782, "compression_ratio": 1.626086956521739, "no_speech_prob": 1.0289361853210721e-05}, {"id": 108, "seek": 69100, "start": 691.0, "end": 700.0, "text": " This is a guess driven development is how we operate. Yeah, so to float, to float, right from basics. So it's in the call.", "tokens": [639, 307, 257, 2041, 9555, 3250, 307, 577, 321, 9651, 13, 865, 11, 370, 281, 15706, 11, 281, 15706, 11, 558, 490, 14688, 13, 407, 309, 311, 294, 264, 818, 13], "temperature": 0.0, "avg_logprob": -0.2930465577140687, "compression_ratio": 1.440251572327044, "no_speech_prob": 9.60927500273101e-05}, {"id": 109, "seek": 69100, "start": 700.0, "end": 710.0, "text": " So yeah, that is in an essence it, like that's the end of Evergreen migrations at like a simplified level.", "tokens": [407, 1338, 11, 300, 307, 294, 364, 12801, 309, 11, 411, 300, 311, 264, 917, 295, 12123, 27399, 6186, 12154, 412, 411, 257, 26335, 1496, 13], "temperature": 0.0, "avg_logprob": -0.2930465577140687, "compression_ratio": 1.440251572327044, "no_speech_prob": 9.60927500273101e-05}, {"id": 110, "seek": 71000, "start": 710.0, "end": 727.0, "text": " So basically any model that you have in your back end or front end in Lambda, it's both. But if you're just thinking of Elm, like in the Elm architecture, whatever your model type is, if you change that type, the question is, well, what is a sensible transition?", "tokens": [407, 1936, 604, 2316, 300, 291, 362, 294, 428, 646, 917, 420, 1868, 917, 294, 45691, 11, 309, 311, 1293, 13, 583, 498, 291, 434, 445, 1953, 295, 2699, 76, 11, 411, 294, 264, 2699, 76, 9482, 11, 2035, 428, 2316, 2010, 307, 11, 498, 291, 1319, 300, 2010, 11, 264, 1168, 307, 11, 731, 11, 437, 307, 257, 25380, 6034, 30], "temperature": 0.0, "avg_logprob": -0.2492782237917878, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0001879959017969668}, {"id": 111, "seek": 71000, "start": 727.0, "end": 732.0, "text": " What is a sensible transformation between whatever it used to be and whatever the new value is?", "tokens": [708, 307, 257, 25380, 9887, 1296, 2035, 309, 1143, 281, 312, 293, 2035, 264, 777, 2158, 307, 30], "temperature": 0.0, "avg_logprob": -0.2492782237917878, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0001879959017969668}, {"id": 112, "seek": 73200, "start": 732.0, "end": 741.0, "text": " And on the front end, there is some interesting stuff, like some of the problems I posed in my talk was like, you know, do you need to tell a user that things are upgrading?", "tokens": [400, 322, 264, 1868, 917, 11, 456, 307, 512, 1880, 1507, 11, 411, 512, 295, 264, 2740, 286, 31399, 294, 452, 751, 390, 411, 11, 291, 458, 11, 360, 291, 643, 281, 980, 257, 4195, 300, 721, 366, 36249, 30], "temperature": 0.0, "avg_logprob": -0.20937577415915096, "compression_ratio": 1.7781350482315113, "no_speech_prob": 0.00016076191968750209}, {"id": 113, "seek": 73200, "start": 741.0, "end": 746.0, "text": " Like sometimes for a minor change or maybe for new features, additive stuff, you don't need to.", "tokens": [1743, 2171, 337, 257, 6696, 1319, 420, 1310, 337, 777, 4122, 11, 45558, 1507, 11, 291, 500, 380, 643, 281, 13], "temperature": 0.0, "avg_logprob": -0.20937577415915096, "compression_ratio": 1.7781350482315113, "no_speech_prob": 0.00016076191968750209}, {"id": 114, "seek": 73200, "start": 746.0, "end": 753.0, "text": " But, you know, maybe it would be kind of weird if the user was halfway filling out a form and then suddenly a form field like disappears.", "tokens": [583, 11, 291, 458, 11, 1310, 309, 576, 312, 733, 295, 3657, 498, 264, 4195, 390, 15461, 10623, 484, 257, 1254, 293, 550, 5800, 257, 1254, 2519, 411, 25527, 13], "temperature": 0.0, "avg_logprob": -0.20937577415915096, "compression_ratio": 1.7781350482315113, "no_speech_prob": 0.00016076191968750209}, {"id": 115, "seek": 73200, "start": 753.0, "end": 760.0, "text": " But maybe it's not, you know, maybe it's not a big deal. Like is it any more or less confusing than the page breaking and them having to refresh?", "tokens": [583, 1310, 309, 311, 406, 11, 291, 458, 11, 1310, 309, 311, 406, 257, 955, 2028, 13, 1743, 307, 309, 604, 544, 420, 1570, 13181, 813, 264, 3028, 7697, 293, 552, 1419, 281, 15134, 30], "temperature": 0.0, "avg_logprob": -0.20937577415915096, "compression_ratio": 1.7781350482315113, "no_speech_prob": 0.00016076191968750209}, {"id": 116, "seek": 76000, "start": 760.0, "end": 770.0, "text": " But yeah, the cool thing is that like once you have this idea and then that evergreen setup is possible, it becomes a bit magical.", "tokens": [583, 1338, 11, 264, 1627, 551, 307, 300, 411, 1564, 291, 362, 341, 1558, 293, 550, 300, 1562, 27399, 8657, 307, 1944, 11, 309, 3643, 257, 857, 12066, 13], "temperature": 0.0, "avg_logprob": -0.22183953798734224, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.00013339825090952218}, {"id": 117, "seek": 76000, "start": 770.0, "end": 773.0, "text": " And it's something I love about Lambda and I really love it.", "tokens": [400, 309, 311, 746, 286, 959, 466, 45691, 293, 286, 534, 959, 309, 13], "temperature": 0.0, "avg_logprob": -0.22183953798734224, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.00013339825090952218}, {"id": 118, "seek": 76000, "start": 773.0, "end": 782.0, "text": " You know, occasionally people will come and tell me, so someone at Elm camp came and told me like, oh yeah, you know, I was making a game and some friends were testing and they were like, oh, this thing doesn't work.", "tokens": [509, 458, 11, 16895, 561, 486, 808, 293, 980, 385, 11, 370, 1580, 412, 2699, 76, 2255, 1361, 293, 1907, 385, 411, 11, 1954, 1338, 11, 291, 458, 11, 286, 390, 1455, 257, 1216, 293, 512, 1855, 645, 4997, 293, 436, 645, 411, 11, 1954, 11, 341, 551, 1177, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.22183953798734224, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.00013339825090952218}, {"id": 119, "seek": 76000, "start": 782.0, "end": 786.0, "text": " And he was like, hold on, I'll fix that. And they're like, all right, that's deployed.", "tokens": [400, 415, 390, 411, 11, 1797, 322, 11, 286, 603, 3191, 300, 13, 400, 436, 434, 411, 11, 439, 558, 11, 300, 311, 17826, 13], "temperature": 0.0, "avg_logprob": -0.22183953798734224, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.00013339825090952218}, {"id": 120, "seek": 78600, "start": 786.0, "end": 791.0, "text": " And like all of their apps like updated, you know, without them doing anything. And he's like, cool, that's done.", "tokens": [400, 411, 439, 295, 641, 7733, 411, 10588, 11, 291, 458, 11, 1553, 552, 884, 1340, 13, 400, 415, 311, 411, 11, 1627, 11, 300, 311, 1096, 13], "temperature": 0.0, "avg_logprob": -0.2055461969268456, "compression_ratio": 1.7062146892655368, "no_speech_prob": 4.9857462727231905e-05}, {"id": 121, "seek": 78600, "start": 791.0, "end": 796.0, "text": " And they were like, wow. And yeah, that's that. I'm like, yeah, that that feels right to me.", "tokens": [400, 436, 645, 411, 11, 6076, 13, 400, 1338, 11, 300, 311, 300, 13, 286, 478, 411, 11, 1338, 11, 300, 300, 3417, 558, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.2055461969268456, "compression_ratio": 1.7062146892655368, "no_speech_prob": 4.9857462727231905e-05}, {"id": 122, "seek": 78600, "start": 796.0, "end": 800.0, "text": " I feel like that's how it should be. That feels awesome. So yeah, I think that's the joy of it.", "tokens": [286, 841, 411, 300, 311, 577, 309, 820, 312, 13, 663, 3417, 3476, 13, 407, 1338, 11, 286, 519, 300, 311, 264, 6258, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.2055461969268456, "compression_ratio": 1.7062146892655368, "no_speech_prob": 4.9857462727231905e-05}, {"id": 123, "seek": 80000, "start": 800.0, "end": 819.0, "text": " That is very unique. So you're doing a hot reload on the client and applying the new code changes with that safe migration that migrates the front end model and then migrates any incoming front end messages.", "tokens": [663, 307, 588, 3845, 13, 407, 291, 434, 884, 257, 2368, 25628, 322, 264, 6423, 293, 9275, 264, 777, 3089, 2962, 365, 300, 3273, 17011, 300, 6186, 12507, 264, 1868, 917, 2316, 293, 550, 6186, 12507, 604, 22341, 1868, 917, 7897, 13], "temperature": 0.0, "avg_logprob": -0.18272739907969598, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00016862851043697447}, {"id": 124, "seek": 80000, "start": 819.0, "end": 822.0, "text": " So it hot swaps that in. Is that correct?", "tokens": [407, 309, 2368, 1693, 2382, 300, 294, 13, 1119, 300, 3006, 30], "temperature": 0.0, "avg_logprob": -0.18272739907969598, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00016862851043697447}, {"id": 125, "seek": 80000, "start": 822.0, "end": 828.0, "text": " Yeah, that's right. So basically we take the concept of it. So the theoretical concept of evergreen is really just a function, right?", "tokens": [865, 11, 300, 311, 558, 13, 407, 1936, 321, 747, 264, 3410, 295, 309, 13, 407, 264, 20864, 3410, 295, 1562, 27399, 307, 534, 445, 257, 2445, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18272739907969598, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.00016862851043697447}, {"id": 126, "seek": 82800, "start": 828.0, "end": 833.0, "text": " It's saying we've got this old type with an actual value of that old type that's live.", "tokens": [467, 311, 1566, 321, 600, 658, 341, 1331, 2010, 365, 364, 3539, 2158, 295, 300, 1331, 2010, 300, 311, 1621, 13], "temperature": 0.0, "avg_logprob": -0.22576622009277345, "compression_ratio": 1.9747292418772564, "no_speech_prob": 5.919132672715932e-05}, {"id": 127, "seek": 82800, "start": 833.0, "end": 836.0, "text": " And then, you know, we want to go to this new type in the new app.", "tokens": [400, 550, 11, 291, 458, 11, 321, 528, 281, 352, 281, 341, 777, 2010, 294, 264, 777, 724, 13], "temperature": 0.0, "avg_logprob": -0.22576622009277345, "compression_ratio": 1.9747292418772564, "no_speech_prob": 5.919132672715932e-05}, {"id": 128, "seek": 82800, "start": 836.0, "end": 841.0, "text": " So we write a function that helps us migrate that live value from one to the other.", "tokens": [407, 321, 2464, 257, 2445, 300, 3665, 505, 31821, 300, 1621, 2158, 490, 472, 281, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.22576622009277345, "compression_ratio": 1.9747292418772564, "no_speech_prob": 5.919132672715932e-05}, {"id": 129, "seek": 82800, "start": 841.0, "end": 845.0, "text": " And then once you have that tool, it's like, what do you do with it?", "tokens": [400, 550, 1564, 291, 362, 300, 2290, 11, 309, 311, 411, 11, 437, 360, 291, 360, 365, 309, 30], "temperature": 0.0, "avg_logprob": -0.22576622009277345, "compression_ratio": 1.9747292418772564, "no_speech_prob": 5.919132672715932e-05}, {"id": 130, "seek": 82800, "start": 845.0, "end": 850.0, "text": " And so in Lambdaera, what we do with it is we apply it to every value primitive that you have in Lambdaera.", "tokens": [400, 370, 294, 45691, 1663, 11, 437, 321, 360, 365, 309, 307, 321, 3079, 309, 281, 633, 2158, 28540, 300, 291, 362, 294, 45691, 1663, 13], "temperature": 0.0, "avg_logprob": -0.22576622009277345, "compression_ratio": 1.9747292418772564, "no_speech_prob": 5.919132672715932e-05}, {"id": 131, "seek": 82800, "start": 850.0, "end": 856.0, "text": " And there's six of them. There's the front end model, there's the back end model, there's a front end message, the back end message.", "tokens": [400, 456, 311, 2309, 295, 552, 13, 821, 311, 264, 1868, 917, 2316, 11, 456, 311, 264, 646, 917, 2316, 11, 456, 311, 257, 1868, 917, 3636, 11, 264, 646, 917, 3636, 13], "temperature": 0.0, "avg_logprob": -0.22576622009277345, "compression_ratio": 1.9747292418772564, "no_speech_prob": 5.919132672715932e-05}, {"id": 132, "seek": 85600, "start": 856.0, "end": 860.0, "text": " And then the two messages that connect the two, two back end and two front end.", "tokens": [400, 550, 264, 732, 7897, 300, 1745, 264, 732, 11, 732, 646, 917, 293, 732, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.22712139892578126, "compression_ratio": 1.7768924302788844, "no_speech_prob": 5.142156078363769e-05}, {"id": 133, "seek": 85600, "start": 860.0, "end": 870.0, "text": " And so we apply this in Lambdaera, this idea that any time you change a type in any of those six core types, we go, cool, we can see you've changed this type.", "tokens": [400, 370, 321, 3079, 341, 294, 45691, 1663, 11, 341, 1558, 300, 604, 565, 291, 1319, 257, 2010, 294, 604, 295, 729, 2309, 4965, 3467, 11, 321, 352, 11, 1627, 11, 321, 393, 536, 291, 600, 3105, 341, 2010, 13], "temperature": 0.0, "avg_logprob": -0.22712139892578126, "compression_ratio": 1.7768924302788844, "no_speech_prob": 5.142156078363769e-05}, {"id": 134, "seek": 85600, "start": 870.0, "end": 873.0, "text": " You know, please write this migration. You know, here's the harness for it.", "tokens": [509, 458, 11, 1767, 2464, 341, 17011, 13, 509, 458, 11, 510, 311, 264, 19700, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.22712139892578126, "compression_ratio": 1.7768924302788844, "no_speech_prob": 5.142156078363769e-05}, {"id": 135, "seek": 85600, "start": 873.0, "end": 877.0, "text": " And, you know, if you do and it type checks, then we go, great.", "tokens": [400, 11, 291, 458, 11, 498, 291, 360, 293, 309, 2010, 13834, 11, 550, 321, 352, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.22712139892578126, "compression_ratio": 1.7768924302788844, "no_speech_prob": 5.142156078363769e-05}, {"id": 136, "seek": 85600, "start": 877.0, "end": 880.0, "text": " If you deploy this, then, yeah, we can do exactly what you've said.", "tokens": [759, 291, 7274, 341, 11, 550, 11, 1338, 11, 321, 393, 360, 2293, 437, 291, 600, 848, 13], "temperature": 0.0, "avg_logprob": -0.22712139892578126, "compression_ratio": 1.7768924302788844, "no_speech_prob": 5.142156078363769e-05}, {"id": 137, "seek": 88000, "start": 880.0, "end": 887.0, "text": " On both the front end and the back end, load the new version, take the model from the old app, migrate it into the new app.", "tokens": [1282, 1293, 264, 1868, 917, 293, 264, 646, 917, 11, 3677, 264, 777, 3037, 11, 747, 264, 2316, 490, 264, 1331, 724, 11, 31821, 309, 666, 264, 777, 724, 13], "temperature": 0.0, "avg_logprob": -0.180744336998981, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0002098340046359226}, {"id": 138, "seek": 88000, "start": 887.0, "end": 893.0, "text": " And now you've got the new app running and it was all kind of smooth and hot and live and delightful.", "tokens": [400, 586, 291, 600, 658, 264, 777, 724, 2614, 293, 309, 390, 439, 733, 295, 5508, 293, 2368, 293, 1621, 293, 35194, 13], "temperature": 0.0, "avg_logprob": -0.180744336998981, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0002098340046359226}, {"id": 139, "seek": 88000, "start": 893.0, "end": 895.0, "text": " So, yeah, that's the deal.", "tokens": [407, 11, 1338, 11, 300, 311, 264, 2028, 13], "temperature": 0.0, "avg_logprob": -0.180744336998981, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0002098340046359226}, {"id": 140, "seek": 88000, "start": 895.0, "end": 897.0, "text": " Yeah, that's amazing.", "tokens": [865, 11, 300, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.180744336998981, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0002098340046359226}, {"id": 141, "seek": 88000, "start": 897.0, "end": 907.0, "text": " And I mean, if you wanted to, like, so you have, you know, as you said, it's a function from one version of a model to another.", "tokens": [400, 286, 914, 11, 498, 291, 1415, 281, 11, 411, 11, 370, 291, 362, 11, 291, 458, 11, 382, 291, 848, 11, 309, 311, 257, 2445, 490, 472, 3037, 295, 257, 2316, 281, 1071, 13], "temperature": 0.0, "avg_logprob": -0.180744336998981, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.0002098340046359226}, {"id": 142, "seek": 90700, "start": 907.0, "end": 918.0, "text": " And we should probably just to paint a picture here more explicitly, you have your types module in a Lambdaera app,", "tokens": [400, 321, 820, 1391, 445, 281, 4225, 257, 3036, 510, 544, 20803, 11, 291, 362, 428, 3467, 10088, 294, 257, 45691, 1663, 724, 11], "temperature": 0.0, "avg_logprob": -0.19725539360517336, "compression_ratio": 1.7346938775510203, "no_speech_prob": 1.2604369658220094e-05}, {"id": 143, "seek": 90700, "start": 918.0, "end": 927.0, "text": " which defines, as you described, those six core types, back end model, front end model, back end message, front end message,", "tokens": [597, 23122, 11, 382, 291, 7619, 11, 729, 2309, 4965, 3467, 11, 646, 917, 2316, 11, 1868, 917, 2316, 11, 646, 917, 3636, 11, 1868, 917, 3636, 11], "temperature": 0.0, "avg_logprob": -0.19725539360517336, "compression_ratio": 1.7346938775510203, "no_speech_prob": 1.2604369658220094e-05}, {"id": 144, "seek": 90700, "start": 927.0, "end": 934.0, "text": " and then the two back end and two front end custom types, which define sending data back and forth.", "tokens": [293, 550, 264, 732, 646, 917, 293, 732, 1868, 917, 2375, 3467, 11, 597, 6964, 7750, 1412, 646, 293, 5220, 13], "temperature": 0.0, "avg_logprob": -0.19725539360517336, "compression_ratio": 1.7346938775510203, "no_speech_prob": 1.2604369658220094e-05}, {"id": 145, "seek": 93400, "start": 934.0, "end": 944.0, "text": " But then those essentially get versioned under these namespaces V1, V2, V3, as you change your types.", "tokens": [583, 550, 729, 4476, 483, 3037, 292, 833, 613, 5288, 79, 2116, 691, 16, 11, 691, 17, 11, 691, 18, 11, 382, 291, 1319, 428, 3467, 13], "temperature": 0.0, "avg_logprob": -0.2507859520290209, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.372798164491542e-05}, {"id": 146, "seek": 93400, "start": 944.0, "end": 956.0, "text": " So you have that module with all those six core types defined and any supporting types that you might have that are used in your back end model, in your front end model.", "tokens": [407, 291, 362, 300, 10088, 365, 439, 729, 2309, 4965, 3467, 7642, 293, 604, 7231, 3467, 300, 291, 1062, 362, 300, 366, 1143, 294, 428, 646, 917, 2316, 11, 294, 428, 1868, 917, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2507859520290209, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.372798164491542e-05}, {"id": 147, "seek": 93400, "start": 956.0, "end": 958.0, "text": " Yeah, the subsumed types.", "tokens": [865, 11, 264, 2090, 28189, 3467, 13], "temperature": 0.0, "avg_logprob": -0.2507859520290209, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.372798164491542e-05}, {"id": 148, "seek": 93400, "start": 958.0, "end": 962.0, "text": " Right. And that all, so those are all versioned.", "tokens": [1779, 13, 400, 300, 439, 11, 370, 729, 366, 439, 3037, 292, 13], "temperature": 0.0, "avg_logprob": -0.2507859520290209, "compression_ratio": 1.6320754716981132, "no_speech_prob": 3.372798164491542e-05}, {"id": 149, "seek": 96200, "start": 962.0, "end": 969.0, "text": " So now instead of just types, being some fuzzy thing that you're like, Oh, yeah, I think it was different in a different version.", "tokens": [407, 586, 2602, 295, 445, 3467, 11, 885, 512, 34710, 551, 300, 291, 434, 411, 11, 876, 11, 1338, 11, 286, 519, 309, 390, 819, 294, 257, 819, 3037, 13], "temperature": 0.0, "avg_logprob": -0.20137139862658932, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00010226517770206556}, {"id": 150, "seek": 96200, "start": 969.0, "end": 976.0, "text": " You actually have every version of your types for your Lambdaera application that you've ever deployed.", "tokens": [509, 767, 362, 633, 3037, 295, 428, 3467, 337, 428, 45691, 1663, 3861, 300, 291, 600, 1562, 17826, 13], "temperature": 0.0, "avg_logprob": -0.20137139862658932, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00010226517770206556}, {"id": 151, "seek": 96200, "start": 976.0, "end": 983.0, "text": " Yeah, absolutely. So the, it's funny, it's like the fundamental idea is really, really simple.", "tokens": [865, 11, 3122, 13, 407, 264, 11, 309, 311, 4074, 11, 309, 311, 411, 264, 8088, 1558, 307, 534, 11, 534, 2199, 13], "temperature": 0.0, "avg_logprob": -0.20137139862658932, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00010226517770206556}, {"id": 152, "seek": 96200, "start": 983.0, "end": 991.0, "text": " But then like the practicality of the tooling to implement that simple idea ended up being quite complicated.", "tokens": [583, 550, 411, 264, 8496, 507, 295, 264, 46593, 281, 4445, 300, 2199, 1558, 4590, 493, 885, 1596, 6179, 13], "temperature": 0.0, "avg_logprob": -0.20137139862658932, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00010226517770206556}, {"id": 153, "seek": 99100, "start": 991.0, "end": 1007.0, "text": " And I think this is kind of where, and I talked about this as well in the talk, like part of the pitch for Lambdaera was that this idea behind this kind of live reload migrations.", "tokens": [400, 286, 519, 341, 307, 733, 295, 689, 11, 293, 286, 2825, 466, 341, 382, 731, 294, 264, 751, 11, 411, 644, 295, 264, 7293, 337, 45691, 1663, 390, 300, 341, 1558, 2261, 341, 733, 295, 1621, 25628, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.21196620565065197, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.00020336361194495112}, {"id": 154, "seek": 99100, "start": 1007.0, "end": 1014.0, "text": " I think it was, it seems to me that it is most compelling when your entire system has this philosophy behind it.", "tokens": [286, 519, 309, 390, 11, 309, 2544, 281, 385, 300, 309, 307, 881, 20050, 562, 428, 2302, 1185, 575, 341, 10675, 2261, 309, 13], "temperature": 0.0, "avg_logprob": -0.21196620565065197, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.00020336361194495112}, {"id": 155, "seek": 101400, "start": 1014.0, "end": 1023.0, "text": " Because if you can, like if you're tracking your entire system, your front end and your back end and everything between, it's then that the tooling is really nice.", "tokens": [1436, 498, 291, 393, 11, 411, 498, 291, 434, 11603, 428, 2302, 1185, 11, 428, 1868, 917, 293, 428, 646, 917, 293, 1203, 1296, 11, 309, 311, 550, 300, 264, 46593, 307, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.20760364532470704, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.00017116917297244072}, {"id": 156, "seek": 101400, "start": 1023.0, "end": 1033.0, "text": " Right. Without that setup, you would have kind of what you've just described, which is like, you know, there's no good way to know what the authority is on these types.", "tokens": [1779, 13, 9129, 300, 8657, 11, 291, 576, 362, 733, 295, 437, 291, 600, 445, 7619, 11, 597, 307, 411, 11, 291, 458, 11, 456, 311, 572, 665, 636, 281, 458, 437, 264, 8281, 307, 322, 613, 3467, 13], "temperature": 0.0, "avg_logprob": -0.20760364532470704, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.00017116917297244072}, {"id": 157, "seek": 103300, "start": 1033.0, "end": 1044.0, "text": " So you can't, like, it's not very easy for you to, you'd have to use discipline to be like, okay, you know, I've changed the type, therefore let's go and copy paste our types and let's try and name space.", "tokens": [407, 291, 393, 380, 11, 411, 11, 309, 311, 406, 588, 1858, 337, 291, 281, 11, 291, 1116, 362, 281, 764, 13635, 281, 312, 411, 11, 1392, 11, 291, 458, 11, 286, 600, 3105, 264, 2010, 11, 4412, 718, 311, 352, 293, 5055, 9163, 527, 3467, 293, 718, 311, 853, 293, 1315, 1901, 13], "temperature": 0.0, "avg_logprob": -0.20959949493408203, "compression_ratio": 1.7518248175182483, "no_speech_prob": 3.646332334028557e-05}, {"id": 158, "seek": 103300, "start": 1044.0, "end": 1053.0, "text": " You know, you'd have to do this. Whereas with Lambdaera, because everything's integrated, like the local tooling, you know, when you run a check, like a pre-check for a deploy,", "tokens": [509, 458, 11, 291, 1116, 362, 281, 360, 341, 13, 13813, 365, 45691, 1663, 11, 570, 1203, 311, 10919, 11, 411, 264, 2654, 46593, 11, 291, 458, 11, 562, 291, 1190, 257, 1520, 11, 411, 257, 659, 12, 15723, 337, 257, 7274, 11], "temperature": 0.0, "avg_logprob": -0.20959949493408203, "compression_ratio": 1.7518248175182483, "no_speech_prob": 3.646332334028557e-05}, {"id": 159, "seek": 103300, "start": 1053.0, "end": 1059.0, "text": " it knows to talk to your production instances and go, okay, well that's what's deployed currently.", "tokens": [309, 3255, 281, 751, 281, 428, 4265, 14519, 293, 352, 11, 1392, 11, 731, 300, 311, 437, 311, 17826, 4362, 13], "temperature": 0.0, "avg_logprob": -0.20959949493408203, "compression_ratio": 1.7518248175182483, "no_speech_prob": 3.646332334028557e-05}, {"id": 160, "seek": 105900, "start": 1059.0, "end": 1064.0, "text": " That's what the type hashes are. Do we have a change? Okay, we do. Let's automate the snapshotting.", "tokens": [663, 311, 437, 264, 2010, 575, 8076, 366, 13, 1144, 321, 362, 257, 1319, 30, 1033, 11, 321, 360, 13, 961, 311, 31605, 264, 30163, 783, 13], "temperature": 0.0, "avg_logprob": -0.19557487212859834, "compression_ratio": 1.8008474576271187, "no_speech_prob": 4.610403993865475e-05}, {"id": 161, "seek": 105900, "start": 1064.0, "end": 1075.0, "text": " Let's automate, you know, giving you some feedback about what's happening, what types have changed so that, you know, the developer experience that, you know, I've been chasing in Lambdaera is that you don't have to think about it. Right.", "tokens": [961, 311, 31605, 11, 291, 458, 11, 2902, 291, 512, 5824, 466, 437, 311, 2737, 11, 437, 3467, 362, 3105, 370, 300, 11, 291, 458, 11, 264, 10754, 1752, 300, 11, 291, 458, 11, 286, 600, 668, 17876, 294, 45691, 1663, 307, 300, 291, 500, 380, 362, 281, 519, 466, 309, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.19557487212859834, "compression_ratio": 1.8008474576271187, "no_speech_prob": 4.610403993865475e-05}, {"id": 162, "seek": 105900, "start": 1075.0, "end": 1080.0, "text": " So in the early versions of Lambdaera, you had to think about it, you had to track it.", "tokens": [407, 294, 264, 2440, 9606, 295, 45691, 1663, 11, 291, 632, 281, 519, 466, 309, 11, 291, 632, 281, 2837, 309, 13], "temperature": 0.0, "avg_logprob": -0.19557487212859834, "compression_ratio": 1.8008474576271187, "no_speech_prob": 4.610403993865475e-05}, {"id": 163, "seek": 108000, "start": 1080.0, "end": 1094.0, "text": " And yeah, as of the latest release, we are now trying to make that as seamless and kind of carefree as possible so that you can, with confidence, be like, you know what, I'm going to re-architect my whole backend.", "tokens": [400, 1338, 11, 382, 295, 264, 6792, 4374, 11, 321, 366, 586, 1382, 281, 652, 300, 382, 28677, 293, 733, 295, 1127, 10792, 382, 1944, 370, 300, 291, 393, 11, 365, 6687, 11, 312, 411, 11, 291, 458, 437, 11, 286, 478, 516, 281, 319, 12, 1178, 5739, 452, 1379, 38087, 13], "temperature": 0.0, "avg_logprob": -0.19155576277752312, "compression_ratio": 1.7706422018348624, "no_speech_prob": 4.7561847168253735e-05}, {"id": 164, "seek": 108000, "start": 1094.0, "end": 1103.0, "text": " I'm going to change the whole backend model. I'm going to move things around. I'm going to change dictionaries to sets and nest things into custom. I'll do whatever I want.", "tokens": [286, 478, 516, 281, 1319, 264, 1379, 38087, 2316, 13, 286, 478, 516, 281, 1286, 721, 926, 13, 286, 478, 516, 281, 1319, 22352, 4889, 281, 6352, 293, 15646, 721, 666, 2375, 13, 286, 603, 360, 2035, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.19155576277752312, "compression_ratio": 1.7706422018348624, "no_speech_prob": 4.7561847168253735e-05}, {"id": 165, "seek": 110300, "start": 1103.0, "end": 1114.0, "text": " And the idea is that you can kind of do that and then Lambdaera will be like, cool, that's cool. Here's what changed. All right. Like this is now you need to tell me how to get from where you were to where you are.", "tokens": [400, 264, 1558, 307, 300, 291, 393, 733, 295, 360, 300, 293, 550, 45691, 1663, 486, 312, 411, 11, 1627, 11, 300, 311, 1627, 13, 1692, 311, 437, 3105, 13, 1057, 558, 13, 1743, 341, 307, 586, 291, 643, 281, 980, 385, 577, 281, 483, 490, 689, 291, 645, 281, 689, 291, 366, 13], "temperature": 0.0, "avg_logprob": -0.18780632408297793, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.00010877692693611607}, {"id": 166, "seek": 110300, "start": 1114.0, "end": 1122.0, "text": " And if you can figure that out, if you can make it compile, then cool. Like probably this is going to, well, from a type perspective, it's going to migrate.", "tokens": [400, 498, 291, 393, 2573, 300, 484, 11, 498, 291, 393, 652, 309, 31413, 11, 550, 1627, 13, 1743, 1391, 341, 307, 516, 281, 11, 731, 11, 490, 257, 2010, 4585, 11, 309, 311, 516, 281, 31821, 13], "temperature": 0.0, "avg_logprob": -0.18780632408297793, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.00010877692693611607}, {"id": 167, "seek": 112200, "start": 1122.0, "end": 1140.0, "text": " But obviously we can't guarantee that you haven't put dic.empty or set.empty where you shouldn't have and that you'll lose some data. So you can still write the wrong migration in the business sense, but you can't forget to migrate a column.", "tokens": [583, 2745, 321, 393, 380, 10815, 300, 291, 2378, 380, 829, 14285, 13, 4543, 88, 420, 992, 13, 4543, 88, 689, 291, 4659, 380, 362, 293, 300, 291, 603, 3624, 512, 1412, 13, 407, 291, 393, 920, 2464, 264, 2085, 17011, 294, 264, 1606, 2020, 11, 457, 291, 393, 380, 2870, 281, 31821, 257, 7738, 13], "temperature": 0.0, "avg_logprob": -0.2528715928395589, "compression_ratio": 1.50625, "no_speech_prob": 0.0001292133965762332}, {"id": 168, "seek": 114000, "start": 1140.0, "end": 1152.0, "text": " You can't delete a column and still have code that's referring to it. Right. So in that sense, I think it's a step up from the traditional transactional database migrations that I think most of us are used to.", "tokens": [509, 393, 380, 12097, 257, 7738, 293, 920, 362, 3089, 300, 311, 13761, 281, 309, 13, 1779, 13, 407, 294, 300, 2020, 11, 286, 519, 309, 311, 257, 1823, 493, 490, 264, 5164, 46688, 1966, 8149, 6186, 12154, 300, 286, 519, 881, 295, 505, 366, 1143, 281, 13], "temperature": 0.0, "avg_logprob": -0.17771553993225098, "compression_ratio": 1.5660377358490567, "no_speech_prob": 2.0144567315583117e-05}, {"id": 169, "seek": 114000, "start": 1152.0, "end": 1160.0, "text": " Have you heard of people writing unit tests for their migrations or is it usually straightforward enough where people are?", "tokens": [3560, 291, 2198, 295, 561, 3579, 4985, 6921, 337, 641, 6186, 12154, 420, 307, 309, 2673, 15325, 1547, 689, 561, 366, 30], "temperature": 0.0, "avg_logprob": -0.17771553993225098, "compression_ratio": 1.5660377358490567, "no_speech_prob": 2.0144567315583117e-05}, {"id": 170, "seek": 116000, "start": 1160.0, "end": 1173.0, "text": " Yeah, no, that's a good question. I think the question's been expressed before. I don't think there's any reason you couldn't. There's a slightly awkward technical reason in Lambdaera why it would be a bit weird.", "tokens": [865, 11, 572, 11, 300, 311, 257, 665, 1168, 13, 286, 519, 264, 1168, 311, 668, 12675, 949, 13, 286, 500, 380, 519, 456, 311, 604, 1778, 291, 2809, 380, 13, 821, 311, 257, 4748, 11411, 6191, 1778, 294, 45691, 1663, 983, 309, 576, 312, 257, 857, 3657, 13], "temperature": 0.0, "avg_logprob": -0.19434417120300898, "compression_ratio": 1.6804979253112033, "no_speech_prob": 8.348419214598835e-05}, {"id": 171, "seek": 116000, "start": 1173.0, "end": 1184.0, "text": " It's because Lambdaera, yes, actually it's not specific to Lambdaera. There's a question of, you know, we've got these type snapshots, right? So we snapshot our types every time things change.", "tokens": [467, 311, 570, 45691, 1663, 11, 2086, 11, 767, 309, 311, 406, 2685, 281, 45691, 1663, 13, 821, 311, 257, 1168, 295, 11, 291, 458, 11, 321, 600, 658, 613, 2010, 19206, 27495, 11, 558, 30, 407, 321, 30163, 527, 3467, 633, 565, 721, 1319, 13], "temperature": 0.0, "avg_logprob": -0.19434417120300898, "compression_ratio": 1.6804979253112033, "no_speech_prob": 8.348419214598835e-05}, {"id": 172, "seek": 118400, "start": 1184.0, "end": 1210.0, "text": " But we want to snapshot anything that's changed and we deploy. We want to snapshot that. However, because of the way that Elm's namespacing works, we want in production for our migrations to result in values of the type of our current types file, not of values of the type of the last snapshot file, if that makes sense.", "tokens": [583, 321, 528, 281, 30163, 1340, 300, 311, 3105, 293, 321, 7274, 13, 492, 528, 281, 30163, 300, 13, 2908, 11, 570, 295, 264, 636, 300, 2699, 76, 311, 5288, 79, 5615, 1985, 11, 321, 528, 294, 4265, 337, 527, 6186, 12154, 281, 1874, 294, 4190, 295, 264, 2010, 295, 527, 2190, 3467, 3991, 11, 406, 295, 4190, 295, 264, 2010, 295, 264, 1036, 30163, 3991, 11, 498, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.2195037289669639, "compression_ratio": 1.693121693121693, "no_speech_prob": 5.1434599299682304e-05}, {"id": 173, "seek": 121000, "start": 1210.0, "end": 1226.0, "text": " So maybe to put this another way, if I define a custom type called ice cream with the variants chocolate, vanilla and strawberry, and I put that in my types.elm, and then in Lambdaera I go to deploy, Lambdaera will go, ooh, let's say it's our first deploy.", "tokens": [407, 1310, 281, 829, 341, 1071, 636, 11, 498, 286, 6964, 257, 2375, 2010, 1219, 4435, 4689, 365, 264, 21669, 6215, 11, 17528, 293, 20440, 11, 293, 286, 829, 300, 294, 452, 3467, 13, 338, 76, 11, 293, 550, 294, 45691, 1663, 286, 352, 281, 7274, 11, 45691, 1663, 486, 352, 11, 17024, 11, 718, 311, 584, 309, 311, 527, 700, 7274, 13], "temperature": 0.0, "avg_logprob": -0.25772245606379723, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.0001970855228137225}, {"id": 174, "seek": 122600, "start": 1226.0, "end": 1242.0, "text": " Lambdaera goes, ooh, it's your first deploy. I'm going to snapshot those types into evergreen slash v1 slash types.elm. So now in this v1 types.elm, there's an evergreen.v1.icecream type that has the exact same variants.", "tokens": [45691, 1663, 1709, 11, 17024, 11, 309, 311, 428, 700, 7274, 13, 286, 478, 516, 281, 30163, 729, 3467, 666, 1562, 27399, 17330, 371, 16, 17330, 3467, 13, 338, 76, 13, 407, 586, 294, 341, 371, 16, 3467, 13, 338, 76, 11, 456, 311, 364, 1562, 27399, 13, 85, 16, 13, 573, 30277, 2010, 300, 575, 264, 1900, 912, 21669, 13], "temperature": 0.0, "avg_logprob": -0.1807275051023902, "compression_ratio": 1.5193370165745856, "no_speech_prob": 9.157640306511894e-05}, {"id": 175, "seek": 122600, "start": 1242.0, "end": 1248.0, "text": " That is what's going to be deployed, the ones with v1.", "tokens": [663, 307, 437, 311, 516, 281, 312, 17826, 11, 264, 2306, 365, 371, 16, 13], "temperature": 0.0, "avg_logprob": -0.1807275051023902, "compression_ratio": 1.5193370165745856, "no_speech_prob": 9.157640306511894e-05}, {"id": 176, "seek": 124800, "start": 1248.0, "end": 1260.0, "text": " Yeah, so both actually get deployed, but the v1s there, the v1s on the first deploy only exist to be referenced in future deploys when the types change.", "tokens": [865, 11, 370, 1293, 767, 483, 17826, 11, 457, 264, 371, 16, 82, 456, 11, 264, 371, 16, 82, 322, 264, 700, 7274, 787, 2514, 281, 312, 32734, 294, 2027, 368, 49522, 562, 264, 3467, 1319, 13], "temperature": 0.0, "avg_logprob": -0.1948650830412564, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.0004171954351477325}, {"id": 177, "seek": 124800, "start": 1260.0, "end": 1268.0, "text": " But in the current version, we still want ice cream values of the types.elm, right? Because that's what our whole application uses.", "tokens": [583, 294, 264, 2190, 3037, 11, 321, 920, 528, 4435, 4689, 4190, 295, 264, 3467, 13, 338, 76, 11, 558, 30, 1436, 300, 311, 437, 527, 1379, 3861, 4960, 13], "temperature": 0.0, "avg_logprob": -0.1948650830412564, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.0004171954351477325}, {"id": 178, "seek": 126800, "start": 1268.0, "end": 1279.0, "text": " But in the future, when we now deploy version two, and let's say for ice cream, we had chocolate, vanilla, strawberry, and let's say we add mango, right? So now we're going, okay, this type is now different.", "tokens": [583, 294, 264, 2027, 11, 562, 321, 586, 7274, 3037, 732, 11, 293, 718, 311, 584, 337, 4435, 4689, 11, 321, 632, 6215, 11, 17528, 11, 20440, 11, 293, 718, 311, 584, 321, 909, 23481, 11, 558, 30, 407, 586, 321, 434, 516, 11, 1392, 11, 341, 2010, 307, 586, 819, 13], "temperature": 0.0, "avg_logprob": -0.19804850868556811, "compression_ratio": 1.6147186147186148, "no_speech_prob": 8.345681999344379e-05}, {"id": 179, "seek": 126800, "start": 1279.0, "end": 1288.0, "text": " It has an additional variant. The migration that gets generated is a migration from version one to version two, right? And so we decide what we do with those values.", "tokens": [467, 575, 364, 4497, 17501, 13, 440, 17011, 300, 2170, 10833, 307, 257, 17011, 490, 3037, 472, 281, 3037, 732, 11, 558, 30, 400, 370, 321, 4536, 437, 321, 360, 365, 729, 4190, 13], "temperature": 0.0, "avg_logprob": -0.19804850868556811, "compression_ratio": 1.6147186147186148, "no_speech_prob": 8.345681999344379e-05}, {"id": 180, "seek": 128800, "start": 1288.0, "end": 1299.0, "text": " Probably we do nothing, or maybe we say, hey, you know, actually, everybody that told us that they put strawberry in the first version, they all emailed us and they said, you know, we hate strawberry, we actually want mango.", "tokens": [9210, 321, 360, 1825, 11, 420, 1310, 321, 584, 11, 4177, 11, 291, 458, 11, 767, 11, 2201, 300, 1907, 505, 300, 436, 829, 20440, 294, 264, 700, 3037, 11, 436, 439, 45460, 505, 293, 436, 848, 11, 291, 458, 11, 321, 4700, 20440, 11, 321, 767, 528, 23481, 13], "temperature": 0.0, "avg_logprob": -0.2324191566527359, "compression_ratio": 1.9683098591549295, "no_speech_prob": 3.268998261773959e-05}, {"id": 181, "seek": 128800, "start": 1299.0, "end": 1315.0, "text": " So can you please add a mango option, but also migrate any of our strawberry choices to mango, right? So that would be an example where that function when you migrate the value, you might case the old type, you know, on chocolate, vanilla, strawberry, and chocolate and vanilla, you would map to chocolate and vanilla in the new type.", "tokens": [407, 393, 291, 1767, 909, 257, 23481, 3614, 11, 457, 611, 31821, 604, 295, 527, 20440, 7994, 281, 23481, 11, 558, 30, 407, 300, 576, 312, 364, 1365, 689, 300, 2445, 562, 291, 31821, 264, 2158, 11, 291, 1062, 1389, 264, 1331, 2010, 11, 291, 458, 11, 322, 6215, 11, 17528, 11, 20440, 11, 293, 6215, 293, 17528, 11, 291, 576, 4471, 281, 6215, 293, 17528, 294, 264, 777, 2010, 13], "temperature": 0.0, "avg_logprob": -0.2324191566527359, "compression_ratio": 1.9683098591549295, "no_speech_prob": 3.268998261773959e-05}, {"id": 182, "seek": 131500, "start": 1315.0, "end": 1324.0, "text": " But maybe strawberry variants, we would map to mango in the new type, right? So that's how you could do data changes or data transformations in your migrations.", "tokens": [583, 1310, 20440, 21669, 11, 321, 576, 4471, 281, 23481, 294, 264, 777, 2010, 11, 558, 30, 407, 300, 311, 577, 291, 727, 360, 1412, 2962, 420, 1412, 34852, 294, 428, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.1786463601248605, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.0001195834411191754}, {"id": 183, "seek": 131500, "start": 1324.0, "end": 1335.0, "text": " But yeah, we're getting a bit into the weeds here. But the trick is that that new type will become a snapshot version two, right? And your migration has to type check between version one and version two.", "tokens": [583, 1338, 11, 321, 434, 1242, 257, 857, 666, 264, 26370, 510, 13, 583, 264, 4282, 307, 300, 300, 777, 2010, 486, 1813, 257, 30163, 3037, 732, 11, 558, 30, 400, 428, 17011, 575, 281, 2010, 1520, 1296, 3037, 472, 293, 3037, 732, 13], "temperature": 0.0, "avg_logprob": -0.1786463601248605, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.0001195834411191754}, {"id": 184, "seek": 133500, "start": 1335.0, "end": 1351.0, "text": " But actually, when we deploy, we get sneaky. And we don't map it to version two, we actually map it to types dot elm, because that's what you need in your application. So we do this little we do this little swapsy, so that you're always getting the types that your application actually uses.", "tokens": [583, 767, 11, 562, 321, 7274, 11, 321, 483, 39518, 13, 400, 321, 500, 380, 4471, 309, 281, 3037, 732, 11, 321, 767, 4471, 309, 281, 3467, 5893, 806, 76, 11, 570, 300, 311, 437, 291, 643, 294, 428, 3861, 13, 407, 321, 360, 341, 707, 321, 360, 341, 707, 1693, 2382, 88, 11, 370, 300, 291, 434, 1009, 1242, 264, 3467, 300, 428, 3861, 767, 4960, 13], "temperature": 0.0, "avg_logprob": -0.22923101319207084, "compression_ratio": 1.7218934911242603, "no_speech_prob": 8.479408279526979e-05}, {"id": 185, "seek": 135100, "start": 1351.0, "end": 1370.0, "text": " But at the same time, we're preparing that snapshot, that's that snapshot in time to go, okay, when we deployed version two, this is what the types were. So that's ready and committed and in your repository, such that if you change it in the future, we don't have to try to go back and remember what they were, we already snapshotted what they were when you deployed.", "tokens": [583, 412, 264, 912, 565, 11, 321, 434, 10075, 300, 30163, 11, 300, 311, 300, 30163, 294, 565, 281, 352, 11, 1392, 11, 562, 321, 17826, 3037, 732, 11, 341, 307, 437, 264, 3467, 645, 13, 407, 300, 311, 1919, 293, 7784, 293, 294, 428, 25841, 11, 1270, 300, 498, 291, 1319, 309, 294, 264, 2027, 11, 321, 500, 380, 362, 281, 853, 281, 352, 646, 293, 1604, 437, 436, 645, 11, 321, 1217, 30163, 14727, 437, 436, 645, 562, 291, 17826, 13], "temperature": 0.0, "avg_logprob": -0.22268828407662813, "compression_ratio": 1.7969348659003832, "no_speech_prob": 3.944059062632732e-05}, {"id": 186, "seek": 135100, "start": 1370.0, "end": 1374.0, "text": " Is that making any sense? I realize that's a bit that's a bit squirrelly.", "tokens": [1119, 300, 1455, 604, 2020, 30, 286, 4325, 300, 311, 257, 857, 300, 311, 257, 857, 2339, 347, 265, 13020, 13], "temperature": 0.0, "avg_logprob": -0.22268828407662813, "compression_ratio": 1.7969348659003832, "no_speech_prob": 3.944059062632732e-05}, {"id": 187, "seek": 135100, "start": 1374.0, "end": 1375.0, "text": " It makes sense to me.", "tokens": [467, 1669, 2020, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.22268828407662813, "compression_ratio": 1.7969348659003832, "no_speech_prob": 3.944059062632732e-05}, {"id": 188, "seek": 135100, "start": 1375.0, "end": 1376.0, "text": " Nice.", "tokens": [5490, 13], "temperature": 0.0, "avg_logprob": -0.22268828407662813, "compression_ratio": 1.7969348659003832, "no_speech_prob": 3.944059062632732e-05}, {"id": 189, "seek": 137600, "start": 1376.0, "end": 1390.0, "text": " So you said that every time there's a change in the the types in those six core types, you need to write a migration. One, it's only when they change, right? If nothing has changed, and you don't need to write a to write a migration?", "tokens": [407, 291, 848, 300, 633, 565, 456, 311, 257, 1319, 294, 264, 264, 3467, 294, 729, 2309, 4965, 3467, 11, 291, 643, 281, 2464, 257, 17011, 13, 1485, 11, 309, 311, 787, 562, 436, 1319, 11, 558, 30, 759, 1825, 575, 3105, 11, 293, 291, 500, 380, 643, 281, 2464, 257, 281, 2464, 257, 17011, 30], "temperature": 0.0, "avg_logprob": -0.21335150400797526, "compression_ratio": 1.6408450704225352, "no_speech_prob": 0.00010720372665673494}, {"id": 190, "seek": 139000, "start": 1390.0, "end": 1419.0, "text": " Yeah, that's correct. So that's a that's a little optimization that okay, yeah, lambda has, which is that even though in elm, those types aren't the same. If when we snapshot the types, the type tree that gets generated, generates identical encoders and decoders. So we can keep, you know, if, if you have a version two snapshot, right, and then you deploy version 34567, and nothing's changed, we can keep decoding straight into your types dot elm type.", "tokens": [865, 11, 300, 311, 3006, 13, 407, 300, 311, 257, 300, 311, 257, 707, 19618, 300, 1392, 11, 1338, 11, 13607, 575, 11, 597, 307, 300, 754, 1673, 294, 806, 76, 11, 729, 3467, 3212, 380, 264, 912, 13, 759, 562, 321, 30163, 264, 3467, 11, 264, 2010, 4230, 300, 2170, 10833, 11, 23815, 14800, 2058, 378, 433, 293, 979, 378, 433, 13, 407, 321, 393, 1066, 11, 291, 458, 11, 498, 11, 498, 291, 362, 257, 3037, 732, 30163, 11, 558, 11, 293, 550, 291, 7274, 3037, 805, 8465, 22452, 11, 293, 1825, 311, 3105, 11, 321, 393, 1066, 979, 8616, 2997, 666, 428, 3467, 5893, 806, 76, 2010, 13], "temperature": 0.0, "avg_logprob": -0.22128766930621604, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.00013549641880672425}, {"id": 191, "seek": 141900, "start": 1419.0, "end": 1434.0, "text": " Because we know that those encoders and decoders are identical from the binary format, right? So it's only when it's only when the types do change that we know, okay, that will mean that the encoders and decoders will become different, which means we need a function to get between the old value and the new value to keep everything sane.", "tokens": [1436, 321, 458, 300, 729, 2058, 378, 433, 293, 979, 378, 433, 366, 14800, 490, 264, 17434, 7877, 11, 558, 30, 407, 309, 311, 787, 562, 309, 311, 787, 562, 264, 3467, 360, 1319, 300, 321, 458, 11, 1392, 11, 300, 486, 914, 300, 264, 2058, 378, 433, 293, 979, 378, 433, 486, 1813, 819, 11, 597, 1355, 321, 643, 257, 2445, 281, 483, 1296, 264, 1331, 2158, 293, 264, 777, 2158, 281, 1066, 1203, 45610, 13], "temperature": 0.0, "avg_logprob": -0.1529552082956573, "compression_ratio": 1.715736040609137, "no_speech_prob": 0.00022691802587360144}, {"id": 192, "seek": 143400, "start": 1434.0, "end": 1449.0, "text": " And the second question is, could you write a migration, even if there's no type change, for instance, everyone hates strawberries. So let's write a migration that goes to to mango without having a type change.", "tokens": [400, 264, 1150, 1168, 307, 11, 727, 291, 2464, 257, 17011, 11, 754, 498, 456, 311, 572, 2010, 1319, 11, 337, 5197, 11, 1518, 23000, 26873, 13, 407, 718, 311, 2464, 257, 17011, 300, 1709, 281, 281, 23481, 1553, 1419, 257, 2010, 1319, 13], "temperature": 0.0, "avg_logprob": -0.20417157808939615, "compression_ratio": 1.4583333333333333, "no_speech_prob": 5.3903349908068776e-05}, {"id": 193, "seek": 144900, "start": 1449.0, "end": 1466.0, "text": " Yeah, so you can't. But with an asterisk, you can't with an asterisk, you can. So you can't change nothing and ask, like evergreen and lambda to be like, please let me do a migration anyway. But you can just add a dummy field. And then evergreen will happily be like, Oh, you need a migration, right?", "tokens": [865, 11, 370, 291, 393, 380, 13, 583, 365, 364, 257, 3120, 7797, 11, 291, 393, 380, 365, 364, 257, 3120, 7797, 11, 291, 393, 13, 407, 291, 393, 380, 1319, 1825, 293, 1029, 11, 411, 1562, 27399, 293, 13607, 281, 312, 411, 11, 1767, 718, 385, 360, 257, 17011, 4033, 13, 583, 291, 393, 445, 909, 257, 35064, 2519, 13, 400, 550, 1562, 27399, 486, 19909, 312, 411, 11, 876, 11, 291, 643, 257, 17011, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21797046891177993, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.6440295439679176e-05}, {"id": 194, "seek": 146600, "start": 1466.0, "end": 1480.0, "text": " And so the trick is that when this is a technical, maybe a shortfall that we might tighten in the future. But right now, whenever you get asked to write a migration, you actually get placeholders for all six types.", "tokens": [400, 370, 264, 4282, 307, 300, 562, 341, 307, 257, 6191, 11, 1310, 257, 2099, 6691, 300, 321, 1062, 17041, 294, 264, 2027, 13, 583, 558, 586, 11, 5699, 291, 483, 2351, 281, 2464, 257, 17011, 11, 291, 767, 483, 1081, 12916, 337, 439, 2309, 3467, 13], "temperature": 0.0, "avg_logprob": -0.1807066973517923, "compression_ratio": 1.445945945945946, "no_speech_prob": 8.88692811713554e-05}, {"id": 195, "seek": 148000, "start": 1480.0, "end": 1496.0, "text": " But by default, it'll tag the migrations for the types that haven't changed as unchanged. So you could if you wanted to still apply a migration whenever there is a migration, if that makes sense. If there's a migration to any type, there's a migration to all six types.", "tokens": [583, 538, 7576, 11, 309, 603, 6162, 264, 6186, 12154, 337, 264, 3467, 300, 2378, 380, 3105, 382, 44553, 13, 407, 291, 727, 498, 291, 1415, 281, 920, 3079, 257, 17011, 5699, 456, 307, 257, 17011, 11, 498, 300, 1669, 2020, 13, 759, 456, 311, 257, 17011, 281, 604, 2010, 11, 456, 311, 257, 17011, 281, 439, 2309, 3467, 13], "temperature": 0.0, "avg_logprob": -0.16890940070152283, "compression_ratio": 1.68125, "no_speech_prob": 2.7532925741979852e-05}, {"id": 196, "seek": 149600, "start": 1496.0, "end": 1513.0, "text": " That's just the way that it's implemented in lambda right now. So you could do data transformations like across the board. But yeah, yeah, it's you have to get lambda lambda is evergreen implementation to think that something's changed in order for it to be like, fine, okay, let's get let's get some migrations involved.", "tokens": [663, 311, 445, 264, 636, 300, 309, 311, 12270, 294, 13607, 558, 586, 13, 407, 291, 727, 360, 1412, 34852, 411, 2108, 264, 3150, 13, 583, 1338, 11, 1338, 11, 309, 311, 291, 362, 281, 483, 13607, 13607, 307, 1562, 27399, 11420, 281, 519, 300, 746, 311, 3105, 294, 1668, 337, 309, 281, 312, 411, 11, 2489, 11, 1392, 11, 718, 311, 483, 718, 311, 483, 512, 6186, 12154, 3288, 13], "temperature": 0.0, "avg_logprob": -0.22335026565107327, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.6495129683753476e-05}, {"id": 197, "seek": 149600, "start": 1513.0, "end": 1522.0, "text": " If the current version is v6, and I manually create a v7 folder, would that trick lambda into generating a migration?", "tokens": [759, 264, 2190, 3037, 307, 371, 21, 11, 293, 286, 16945, 1884, 257, 371, 22, 10820, 11, 576, 300, 4282, 13607, 666, 17746, 257, 17011, 30], "temperature": 0.0, "avg_logprob": -0.22335026565107327, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.6495129683753476e-05}, {"id": 198, "seek": 152200, "start": 1522.0, "end": 1540.0, "text": " No. So if you manually create a lambda right now, what would happen if you manually created a version seven, and then you try to deploy it in production, it would say, Hey, I wasn't expecting there to be a migration, but I see a v7 migration file, I don't know what's going on. Like something something's bad. So it'll it'll try bail out.", "tokens": [883, 13, 407, 498, 291, 16945, 1884, 257, 13607, 558, 586, 11, 437, 576, 1051, 498, 291, 16945, 2942, 257, 3037, 3407, 11, 293, 550, 291, 853, 281, 7274, 309, 294, 4265, 11, 309, 576, 584, 11, 1911, 11, 286, 2067, 380, 9650, 456, 281, 312, 257, 17011, 11, 457, 286, 536, 257, 371, 22, 17011, 3991, 11, 286, 500, 380, 458, 437, 311, 516, 322, 13, 1743, 746, 746, 311, 1578, 13, 407, 309, 603, 309, 603, 853, 19313, 484, 13], "temperature": 0.0, "avg_logprob": -0.24091968979946402, "compression_ratio": 1.6095238095238096, "no_speech_prob": 0.0001740008738124743}, {"id": 199, "seek": 154000, "start": 1540.0, "end": 1568.0, "text": " So let's say that you wanted to do the the strawberry to mango migration in a more polite opt in way. So the migrations files, they give you the opportunity to I mean, to arbitrarily change the model, the back end model, the front end model. So you could, you could make whatever changes, you know, as you're in is describing, you know, changing a model, when the data hasn't really changed.", "tokens": [407, 718, 311, 584, 300, 291, 1415, 281, 360, 264, 264, 20440, 281, 23481, 17011, 294, 257, 544, 25171, 2427, 294, 636, 13, 407, 264, 6186, 12154, 7098, 11, 436, 976, 291, 264, 2650, 281, 286, 914, 11, 281, 19071, 3289, 1319, 264, 2316, 11, 264, 646, 917, 2316, 11, 264, 1868, 917, 2316, 13, 407, 291, 727, 11, 291, 727, 652, 2035, 2962, 11, 291, 458, 11, 382, 291, 434, 294, 307, 16141, 11, 291, 458, 11, 4473, 257, 2316, 11, 562, 264, 1412, 6132, 380, 534, 3105, 13], "temperature": 0.0, "avg_logprob": -0.2323418678121364, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0012446973705664277}, {"id": 200, "seek": 156800, "start": 1568.0, "end": 1590.0, "text": " And you need to sort of get lambda to create a migration file for you. But once you do that, you can make any data change, it doesn't necessarily have to be to get the types to fit together for the new format. That's correct. Yeah. And you also get a command for your for your back end and front end, you can trigger a command as part of that migration.", "tokens": [400, 291, 643, 281, 1333, 295, 483, 24688, 65, 2675, 281, 1884, 257, 17011, 3991, 337, 291, 13, 583, 1564, 291, 360, 300, 11, 291, 393, 652, 604, 1412, 1319, 11, 309, 1177, 380, 4725, 362, 281, 312, 281, 483, 264, 3467, 281, 3318, 1214, 337, 264, 777, 7877, 13, 663, 311, 3006, 13, 865, 13, 400, 291, 611, 483, 257, 5622, 337, 428, 337, 428, 646, 917, 293, 1868, 917, 11, 291, 393, 7875, 257, 5622, 382, 644, 295, 300, 17011, 13], "temperature": 0.0, "avg_logprob": -0.1871171710134923, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0006461801240220666}, {"id": 201, "seek": 159000, "start": 1590.0, "end": 1616.0, "text": " So so let's talk through that a little bit. If we wanted to do the polite version of the strawberry to mango, where we're saying, hey, unfortunately, strawberry is no longer an option. But so we are inviting you to opt into this. And now when you log in to the ice cream shop, you're going to see a, an announcement banner that says, we need you to choose a flavor. So how would you model that with a with an evergreen migration?", "tokens": [407, 370, 718, 311, 751, 807, 300, 257, 707, 857, 13, 759, 321, 1415, 281, 360, 264, 25171, 3037, 295, 264, 20440, 281, 23481, 11, 689, 321, 434, 1566, 11, 4177, 11, 7015, 11, 20440, 307, 572, 2854, 364, 3614, 13, 583, 370, 321, 366, 18202, 291, 281, 2427, 666, 341, 13, 400, 586, 562, 291, 3565, 294, 281, 264, 4435, 4689, 3945, 11, 291, 434, 516, 281, 536, 257, 11, 364, 12847, 24348, 300, 1619, 11, 321, 643, 291, 281, 2826, 257, 6813, 13, 407, 577, 576, 291, 2316, 300, 365, 257, 365, 364, 1562, 27399, 17011, 30], "temperature": 0.0, "avg_logprob": -0.19366955062718066, "compression_ratio": 1.625, "no_speech_prob": 0.0002034056233242154}, {"id": 202, "seek": 161600, "start": 1616.0, "end": 1645.0, "text": " Yeah, absolutely. So I think 90% of the answer to that question has got nothing to do with evergreen. The question first is how would we model this in an L map? So kind of what if I reflect back at you, what I'm hearing is in terms of state. So there's something now on the user profile, right, which I would say a Boolean, which is something like requested, what would we call it? Like, please, revalidate ice cream preferences question, right?", "tokens": [865, 11, 3122, 13, 407, 286, 519, 4289, 4, 295, 264, 1867, 281, 300, 1168, 575, 658, 1825, 281, 360, 365, 1562, 27399, 13, 440, 1168, 700, 307, 577, 576, 321, 2316, 341, 294, 364, 441, 4471, 30, 407, 733, 295, 437, 498, 286, 5031, 646, 412, 291, 11, 437, 286, 478, 4763, 307, 294, 2115, 295, 1785, 13, 407, 456, 311, 746, 586, 322, 264, 4195, 7964, 11, 558, 11, 597, 286, 576, 584, 257, 23351, 28499, 11, 597, 307, 746, 411, 16436, 11, 437, 576, 321, 818, 309, 30, 1743, 11, 1767, 11, 319, 3337, 327, 473, 4435, 4689, 21910, 1168, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21588328968394888, "compression_ratio": 1.6420664206642066, "no_speech_prob": 0.002511110156774521}, {"id": 203, "seek": 164500, "start": 1645.0, "end": 1674.0, "text": " Or show revalidate ice cream preference question, right? And that's a Boolean true or false. And so we add this to our front end model. Actually, we add it to our back end model. Let's see, because we've got accounts here. We've got user accounts, right? We have only saved preferences for users that have logged in, right? Otherwise, how will we know who they are when they come back? So we say, okay, every user profile has this new flag, right? And we add that to the back end model. And then we say, okay, in the front end model. Now, suddenly, we get type errors, right in the front end model, because in this case, it's lambda error, our types are shared.", "tokens": [1610, 855, 319, 3337, 327, 473, 4435, 4689, 17502, 1168, 11, 558, 30, 400, 300, 311, 257, 23351, 28499, 2074, 420, 7908, 13, 400, 370, 321, 909, 341, 281, 527, 1868, 917, 2316, 13, 5135, 11, 321, 909, 309, 281, 527, 646, 917, 2316, 13, 961, 311, 536, 11, 570, 321, 600, 658, 9402, 510, 13, 492, 600, 658, 4195, 9402, 11, 558, 30, 492, 362, 787, 6624, 21910, 337, 5022, 300, 362, 27231, 294, 11, 558, 30, 10328, 11, 577, 486, 321, 458, 567, 436, 366, 562, 436, 808, 646, 30, 407, 321, 584, 11, 1392, 11, 633, 4195, 7964, 575, 341, 777, 7166, 11, 558, 30, 400, 321, 909, 300, 281, 264, 646, 917, 2316, 13, 400, 550, 321, 584, 11, 1392, 11, 294, 264, 1868, 917, 2316, 13, 823, 11, 5800, 11, 321, 483, 2010, 13603, 11, 558, 294, 264, 1868, 917, 2316, 11, 570, 294, 341, 1389, 11, 309, 311, 13607, 6713, 11, 527, 3467, 366, 5507, 13], "temperature": 0.0, "avg_logprob": -0.2545349856457078, "compression_ratio": 1.9909638554216869, "no_speech_prob": 0.0006461344310082495}, {"id": 204, "seek": 167400, "start": 1674.0, "end": 1698.0, "text": " You know, so when a user logs in on the front end, the session hydrates their account. And so that value will come into the front end. And we go into the view and we say, okay, cool. If user.showRevalidateIceCreamPreference is equal to true, then show them this little bit of UI that asks them for this question. Else, let's just show nothing. We'll just leave it, right?", "tokens": [509, 458, 11, 370, 562, 257, 4195, 20820, 294, 322, 264, 1868, 917, 11, 264, 5481, 5796, 12507, 641, 2696, 13, 400, 370, 300, 2158, 486, 808, 666, 264, 1868, 917, 13, 400, 321, 352, 666, 264, 1910, 293, 321, 584, 11, 1392, 11, 1627, 13, 759, 4195, 13, 34436, 8524, 3337, 327, 473, 40, 384, 34, 1572, 42844, 5158, 307, 2681, 281, 2074, 11, 550, 855, 552, 341, 707, 857, 295, 15682, 300, 8962, 552, 337, 341, 1168, 13, 45472, 11, 718, 311, 445, 855, 1825, 13, 492, 603, 445, 1856, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21483123779296875, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.002590775955468416}, {"id": 205, "seek": 169800, "start": 1698.0, "end": 1721.0, "text": " And then maybe as part of that UI, we say, hey, like, it looks like you've chosen, you know, strawberry in the past. And, you know, you told us that you wanted to choose a different flavor when we had more available. That's now available. Like, click here to go to your profile page and edit your preferences. And I think, is that it from an app perspective? I think that's probably it.", "tokens": [400, 550, 1310, 382, 644, 295, 300, 15682, 11, 321, 584, 11, 4177, 11, 411, 11, 309, 1542, 411, 291, 600, 8614, 11, 291, 458, 11, 20440, 294, 264, 1791, 13, 400, 11, 291, 458, 11, 291, 1907, 505, 300, 291, 1415, 281, 2826, 257, 819, 6813, 562, 321, 632, 544, 2435, 13, 663, 311, 586, 2435, 13, 1743, 11, 2052, 510, 281, 352, 281, 428, 7964, 3028, 293, 8129, 428, 21910, 13, 400, 286, 519, 11, 307, 300, 309, 490, 364, 724, 4585, 30, 286, 519, 300, 311, 1391, 309, 13], "temperature": 0.0, "avg_logprob": -0.19913576046625772, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.00011234424164285883}, {"id": 206, "seek": 172100, "start": 1721.0, "end": 1749.0, "text": " Yeah. And I guess you could decide whether you want your custom type to include strawberry and have it be deprecated or remove strawberry and then set it as mango. But then you're going to need to have some application logic in the appropriate places so you don't accidentally ship them their monthly flavor of mango before they've chosen.", "tokens": [865, 13, 400, 286, 2041, 291, 727, 4536, 1968, 291, 528, 428, 2375, 2010, 281, 4090, 20440, 293, 362, 309, 312, 1367, 13867, 770, 420, 4159, 20440, 293, 550, 992, 309, 382, 23481, 13, 583, 550, 291, 434, 516, 281, 643, 281, 362, 512, 3861, 9952, 294, 264, 6854, 3190, 370, 291, 500, 380, 15715, 5374, 552, 641, 12878, 6813, 295, 23481, 949, 436, 600, 8614, 13], "temperature": 0.0, "avg_logprob": -0.2045307696705133, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.013635486364364624}, {"id": 207, "seek": 174900, "start": 1749.0, "end": 1775.0, "text": " And maybe, you know, the monthly shipment process, instead of automatically sending it, is going to send an email and say, hey, you need to log in to change this because we no longer have this flavor. If you want your shipments to resume, then please log in and select something. So, you know, as you say, a lot of it is just modeling the problem in a sort of high-level way.", "tokens": [400, 1310, 11, 291, 458, 11, 264, 12878, 49991, 1399, 11, 2602, 295, 6772, 7750, 309, 11, 307, 516, 281, 2845, 364, 3796, 293, 584, 11, 4177, 11, 291, 643, 281, 3565, 294, 281, 1319, 341, 570, 321, 572, 2854, 362, 341, 6813, 13, 759, 291, 528, 428, 5374, 1117, 281, 15358, 11, 550, 1767, 3565, 294, 293, 3048, 746, 13, 407, 11, 291, 458, 11, 382, 291, 584, 11, 257, 688, 295, 309, 307, 445, 15983, 264, 1154, 294, 257, 1333, 295, 1090, 12, 12418, 636, 13], "temperature": 0.0, "avg_logprob": -0.17457296537316364, "compression_ratio": 1.644736842105263, "no_speech_prob": 0.00010719823330873623}, {"id": 208, "seek": 177500, "start": 1775.0, "end": 1801.0, "text": " Yeah. So let's say you wanted to, so we're still, we're nowhere near evergreen yet, right? We're still in our application realm. So if you wanted to apply, like, make impossible states impossible, you know, if you wanted to be like, actually, you know what, like, what's the business rationale? Like, so let's invent some. Say the business rationale is enough people told us that they hate strawberry, that we're discontinuing strawberry. Right? So maybe we go, okay, well, we need to remove the strawberry variant, but we need somewhere to migrate these people to. Right?", "tokens": [865, 13, 407, 718, 311, 584, 291, 1415, 281, 11, 370, 321, 434, 920, 11, 321, 434, 11159, 2651, 1562, 27399, 1939, 11, 558, 30, 492, 434, 920, 294, 527, 3861, 15355, 13, 407, 498, 291, 1415, 281, 3079, 11, 411, 11, 652, 6243, 4368, 6243, 11, 291, 458, 11, 498, 291, 1415, 281, 312, 411, 11, 767, 11, 291, 458, 437, 11, 411, 11, 437, 311, 264, 1606, 41989, 30, 1743, 11, 370, 718, 311, 7962, 512, 13, 6463, 264, 1606, 41989, 307, 1547, 561, 1907, 505, 300, 436, 4700, 20440, 11, 300, 321, 434, 31420, 9635, 20440, 13, 1779, 30, 407, 1310, 321, 352, 11, 1392, 11, 731, 11, 321, 643, 281, 4159, 264, 20440, 17501, 11, 457, 321, 643, 4079, 281, 31821, 613, 561, 281, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2570029647262008, "compression_ratio": 1.9324324324324325, "no_speech_prob": 0.003428068943321705}, {"id": 209, "seek": 180100, "start": 1801.0, "end": 1829.0, "text": " So maybe we turn, you know, ice cream preference into a custom type itself. Well, maybe we turn it into a maybe, right? And nothing means they haven't selected. So we can't send them any shipments. Right? Because we don't know what flavor. Well, maybe we change it into a custom type where we say, you know, not selected or ice cream selected in this new type that only has the variants that we offer. And then a third state, which is like needs revalidation or, you know, ex, ex strawberry lover or something like that. Right?", "tokens": [407, 1310, 321, 1261, 11, 291, 458, 11, 4435, 4689, 17502, 666, 257, 2375, 2010, 2564, 13, 1042, 11, 1310, 321, 1261, 309, 666, 257, 1310, 11, 558, 30, 400, 1825, 1355, 436, 2378, 380, 8209, 13, 407, 321, 393, 380, 2845, 552, 604, 5374, 1117, 13, 1779, 30, 1436, 321, 500, 380, 458, 437, 6813, 13, 1042, 11, 1310, 321, 1319, 309, 666, 257, 2375, 2010, 689, 321, 584, 11, 291, 458, 11, 406, 8209, 420, 4435, 4689, 8209, 294, 341, 777, 2010, 300, 787, 575, 264, 21669, 300, 321, 2626, 13, 400, 550, 257, 2636, 1785, 11, 597, 307, 411, 2203, 319, 3337, 327, 399, 420, 11, 291, 458, 11, 454, 11, 454, 20440, 18009, 420, 746, 411, 300, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.23448055982589722, "compression_ratio": 1.8556338028169015, "no_speech_prob": 0.0004802851181011647}, {"id": 210, "seek": 182900, "start": 1829.0, "end": 1833.0, "text": " Yeah. I thought there was no one who likes strawberries.", "tokens": [865, 13, 286, 1194, 456, 390, 572, 472, 567, 5902, 26873, 13], "temperature": 0.0, "avg_logprob": -0.31447618147906137, "compression_ratio": 1.164835164835165, "no_speech_prob": 0.00035693563404493034}, {"id": 211, "seek": 182900, "start": 1833.0, "end": 1835.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.31447618147906137, "compression_ratio": 1.164835164835165, "no_speech_prob": 0.00035693563404493034}, {"id": 212, "seek": 182900, "start": 1835.0, "end": 1837.0, "text": " It's an impossible state.", "tokens": [467, 311, 364, 6243, 1785, 13], "temperature": 0.0, "avg_logprob": -0.31447618147906137, "compression_ratio": 1.164835164835165, "no_speech_prob": 0.00035693563404493034}, {"id": 213, "seek": 182900, "start": 1837.0, "end": 1839.0, "text": " Yeah, absolutely.", "tokens": [865, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.31447618147906137, "compression_ratio": 1.164835164835165, "no_speech_prob": 0.00035693563404493034}, {"id": 214, "seek": 183900, "start": 1839.0, "end": 1867.0, "text": " So yeah, we, we, we kind of do whatever modeling and I think this is the nice bit. And this is a nice bit that we go, we at this point, I think, unlike the way that you do it in, in kind of traditional full stack applications is like you're, you're simultaneously thinking about your data structure and the migration at the same time. Whereas in, in Elm, like via Lambda or specifically in that context, the idea is like, well, just forget about it. Just, just model what new state of affairs you would love to have in your app.", "tokens": [407, 1338, 11, 321, 11, 321, 11, 321, 733, 295, 360, 2035, 15983, 293, 286, 519, 341, 307, 264, 1481, 857, 13, 400, 341, 307, 257, 1481, 857, 300, 321, 352, 11, 321, 412, 341, 935, 11, 286, 519, 11, 8343, 264, 636, 300, 291, 360, 309, 294, 11, 294, 733, 295, 5164, 1577, 8630, 5821, 307, 411, 291, 434, 11, 291, 434, 16561, 1953, 466, 428, 1412, 3877, 293, 264, 17011, 412, 264, 912, 565, 13, 13813, 294, 11, 294, 2699, 76, 11, 411, 5766, 45691, 420, 4682, 294, 300, 4319, 11, 264, 1558, 307, 411, 11, 731, 11, 445, 2870, 466, 309, 13, 1449, 11, 445, 2316, 437, 777, 1785, 295, 17478, 291, 576, 959, 281, 362, 294, 428, 724, 13], "temperature": 0.0, "avg_logprob": -0.2593314103254183, "compression_ratio": 1.7718120805369129, "no_speech_prob": 0.000241521600401029}, {"id": 215, "seek": 186700, "start": 1867.0, "end": 1881.0, "text": " Like what is the actual value set up? That makes sense. And then we go great. Now that we've done, we've chosen whatever one of those it was. Now we go, okay, Lambda or deploy or Lambda or check like Lambda or deploy invokes Lambda or check first.", "tokens": [1743, 437, 307, 264, 3539, 2158, 992, 493, 30, 663, 1669, 2020, 13, 400, 550, 321, 352, 869, 13, 823, 300, 321, 600, 1096, 11, 321, 600, 8614, 2035, 472, 295, 729, 309, 390, 13, 823, 321, 352, 11, 1392, 11, 45691, 420, 7274, 420, 45691, 420, 1520, 411, 45691, 420, 7274, 1048, 8606, 45691, 420, 1520, 700, 13], "temperature": 0.0, "avg_logprob": -0.30994730147104416, "compression_ratio": 1.5732484076433122, "no_speech_prob": 0.002550724660977721}, {"id": 216, "seek": 188100, "start": 1881.0, "end": 1899.0, "text": " And Lambera goes, Ooh, I can see your types have changed. Okay. I'm going to go and attempt to make some migrations for you. I'm going to do my best. So that in the latest version, it goes, not only am I going to generate the scaffold, I'm going to do a best effort generation for you. I'm going to look at the tree of your types in the old version, the tree of your types in the new version.", "tokens": [400, 441, 335, 607, 64, 1709, 11, 7951, 11, 286, 393, 536, 428, 3467, 362, 3105, 13, 1033, 13, 286, 478, 516, 281, 352, 293, 5217, 281, 652, 512, 6186, 12154, 337, 291, 13, 286, 478, 516, 281, 360, 452, 1151, 13, 407, 300, 294, 264, 6792, 3037, 11, 309, 1709, 11, 406, 787, 669, 286, 516, 281, 8460, 264, 44094, 11, 286, 478, 516, 281, 360, 257, 1151, 4630, 5125, 337, 291, 13, 286, 478, 516, 281, 574, 412, 264, 4230, 295, 428, 3467, 294, 264, 1331, 3037, 11, 264, 4230, 295, 428, 3467, 294, 264, 777, 3037, 13], "temperature": 0.0, "avg_logprob": -0.20989601428692156, "compression_ratio": 1.893719806763285, "no_speech_prob": 7.842254126444459e-05}, {"id": 217, "seek": 189900, "start": 1899.0, "end": 1911.0, "text": " I'm going to try diff them, like zip them together and you know where they don't look like they've changed. I'm going to try and just intelligently make all those, those choices for you to keep everything the same.", "tokens": [286, 478, 516, 281, 853, 7593, 552, 11, 411, 20730, 552, 1214, 293, 291, 458, 689, 436, 500, 380, 574, 411, 436, 600, 3105, 13, 286, 478, 516, 281, 853, 293, 445, 5613, 2276, 652, 439, 729, 11, 729, 7994, 337, 291, 281, 1066, 1203, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2591806639225111, "compression_ratio": 1.8582375478927202, "no_speech_prob": 0.0002736538590397686}, {"id": 218, "seek": 189900, "start": 1911.0, "end": 1925.0, "text": " Mainly the concern is migrating custom types. Cause as we know in Elm, two equally semantically the same custom types in two different modules aren't actually equal. Right. So unlike two, two semantically identical record types in two different modules are equal. Right.", "tokens": [47468, 264, 3136, 307, 6186, 8754, 2375, 3467, 13, 10865, 382, 321, 458, 294, 2699, 76, 11, 732, 12309, 4361, 49505, 264, 912, 2375, 3467, 294, 732, 819, 16679, 3212, 380, 767, 2681, 13, 1779, 13, 407, 8343, 732, 11, 732, 4361, 49505, 14800, 2136, 3467, 294, 732, 819, 16679, 366, 2681, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.2591806639225111, "compression_ratio": 1.8582375478927202, "no_speech_prob": 0.0002736538590397686}, {"id": 219, "seek": 192500, "start": 1925.0, "end": 1937.0, "text": " So evergreen goes, okay, I'm going to automate all of those kind of crud transformations for you. But in the bits where things have definitely changed and I can't, you know, I can't do anything reasonable.", "tokens": [407, 1562, 27399, 1709, 11, 1392, 11, 286, 478, 516, 281, 31605, 439, 295, 729, 733, 295, 941, 532, 34852, 337, 291, 13, 583, 294, 264, 9239, 689, 721, 362, 2138, 3105, 293, 286, 393, 380, 11, 291, 458, 11, 286, 393, 380, 360, 1340, 10585, 13], "temperature": 0.0, "avg_logprob": -0.1829340232993072, "compression_ratio": 1.7035573122529644, "no_speech_prob": 6.204473902471364e-05}, {"id": 220, "seek": 192500, "start": 1937.0, "end": 1948.0, "text": " I'm going to put placeholders for you and go, okay, here, I've gotten to ice cream preference. This has changed. You know, it used to be this custom type from version one. Now it's this different custom type from version two.", "tokens": [286, 478, 516, 281, 829, 1081, 12916, 337, 291, 293, 352, 11, 1392, 11, 510, 11, 286, 600, 5768, 281, 4435, 4689, 17502, 13, 639, 575, 3105, 13, 509, 458, 11, 309, 1143, 281, 312, 341, 2375, 2010, 490, 3037, 472, 13, 823, 309, 311, 341, 819, 2375, 2010, 490, 3037, 732, 13], "temperature": 0.0, "avg_logprob": -0.1829340232993072, "compression_ratio": 1.7035573122529644, "no_speech_prob": 6.204473902471364e-05}, {"id": 221, "seek": 194800, "start": 1948.0, "end": 1964.0, "text": " How do you want to do with this? You know, how do you want to get this value across? And so you write the code that says, great, I've done all my business logic. Everything makes sense. And you know, I'll, I guess we've, we've invented some sort of ice cream subscription store here in this, in this, in this analogy.", "tokens": [1012, 360, 291, 528, 281, 360, 365, 341, 30, 509, 458, 11, 577, 360, 291, 528, 281, 483, 341, 2158, 2108, 30, 400, 370, 291, 2464, 264, 3089, 300, 1619, 11, 869, 11, 286, 600, 1096, 439, 452, 1606, 9952, 13, 5471, 1669, 2020, 13, 400, 291, 458, 11, 286, 603, 11, 286, 2041, 321, 600, 11, 321, 600, 14479, 512, 1333, 295, 4435, 4689, 17231, 3531, 510, 294, 341, 11, 294, 341, 11, 294, 341, 21663, 13], "temperature": 0.0, "avg_logprob": -0.2981081742506761, "compression_ratio": 1.713310580204778, "no_speech_prob": 9.222643711837009e-06}, {"id": 222, "seek": 194800, "start": 1964.0, "end": 1975.0, "text": " Sounds like a great business. Yeah. Except for the free frozen shipments. That could be. Yeah, that's tough. Tough. But yeah, I want, I definitely want ice cream now. But anyway, yeah.", "tokens": [14576, 411, 257, 869, 1606, 13, 865, 13, 16192, 337, 264, 1737, 12496, 5374, 1117, 13, 663, 727, 312, 13, 865, 11, 300, 311, 4930, 13, 48568, 13, 583, 1338, 11, 286, 528, 11, 286, 2138, 528, 4435, 4689, 586, 13, 583, 4033, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2981081742506761, "compression_ratio": 1.713310580204778, "no_speech_prob": 9.222643711837009e-06}, {"id": 223, "seek": 197500, "start": 1975.0, "end": 1990.0, "text": " So we've done it. We've done all the business logic. And then now it's kind of like, cool, how do we want to get from one value to the other? And the nice thing is once we've done all of this, you know, it type checks. So it goes, yeah, cool. Okay. That makes sense. You've, you've successfully done the transition.", "tokens": [407, 321, 600, 1096, 309, 13, 492, 600, 1096, 439, 264, 1606, 9952, 13, 400, 550, 586, 309, 311, 733, 295, 411, 11, 1627, 11, 577, 360, 321, 528, 281, 483, 490, 472, 2158, 281, 264, 661, 30, 400, 264, 1481, 551, 307, 1564, 321, 600, 1096, 439, 295, 341, 11, 291, 458, 11, 309, 2010, 13834, 13, 407, 309, 1709, 11, 1338, 11, 1627, 13, 1033, 13, 663, 1669, 2020, 13, 509, 600, 11, 291, 600, 10727, 1096, 264, 6034, 13], "temperature": 0.0, "avg_logprob": -0.2185550290484761, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.883014142047614e-05}, {"id": 224, "seek": 199000, "start": 1990.0, "end": 2006.0, "text": " And I suppose going back to the, to the unit test question, it's like, yeah, if, if that was a very complicated transition and you wanted to have certain invariants that hold, yeah, there's no reason why that function that you wrote in there for that particular part of the migration, you can put that function to the side.", "tokens": [400, 286, 7297, 516, 646, 281, 264, 11, 281, 264, 4985, 1500, 1168, 11, 309, 311, 411, 11, 1338, 11, 498, 11, 498, 300, 390, 257, 588, 6179, 6034, 293, 291, 1415, 281, 362, 1629, 33270, 1719, 300, 1797, 11, 1338, 11, 456, 311, 572, 1778, 983, 300, 2445, 300, 291, 4114, 294, 456, 337, 300, 1729, 644, 295, 264, 17011, 11, 291, 393, 829, 300, 2445, 281, 264, 1252, 13], "temperature": 0.0, "avg_logprob": -0.19277905782063803, "compression_ratio": 1.6564102564102565, "no_speech_prob": 9.168330871034414e-05}, {"id": 225, "seek": 200600, "start": 2006.0, "end": 2023.0, "text": " You could put it elsewhere. You could include it from the tests and then you could do all your scenarios and tests. You know, I put in these old versions. I expect these new versions is my migration kind of making sense. And I think that's nice as well, because you get to stay in Elm, right? If you think of that in any other system, now you're mocking perhaps the database.", "tokens": [509, 727, 829, 309, 14517, 13, 509, 727, 4090, 309, 490, 264, 6921, 293, 550, 291, 727, 360, 439, 428, 15077, 293, 6921, 13, 509, 458, 11, 286, 829, 294, 613, 1331, 9606, 13, 286, 2066, 613, 777, 9606, 307, 452, 17011, 733, 295, 1455, 2020, 13, 400, 286, 519, 300, 311, 1481, 382, 731, 11, 570, 291, 483, 281, 1754, 294, 2699, 76, 11, 558, 30, 759, 291, 519, 295, 300, 294, 604, 661, 1185, 11, 586, 291, 434, 49792, 4317, 264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.23484780815210235, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.00016339846479240805}, {"id": 226, "seek": 202300, "start": 2023.0, "end": 2036.0, "text": " You have to, you have to, if you're not mocking the database, you actually have to boot a database in your test setup. You have to set the first versions. You have to do the migration of the schema. Then you have to load the value. You know, there'd be a lot to get that working.", "tokens": [509, 362, 281, 11, 291, 362, 281, 11, 498, 291, 434, 406, 49792, 264, 8149, 11, 291, 767, 362, 281, 11450, 257, 8149, 294, 428, 1500, 8657, 13, 509, 362, 281, 992, 264, 700, 9606, 13, 509, 362, 281, 360, 264, 17011, 295, 264, 34078, 13, 1396, 291, 362, 281, 3677, 264, 2158, 13, 509, 458, 11, 456, 1116, 312, 257, 688, 281, 483, 300, 1364, 13], "temperature": 0.0, "avg_logprob": -0.17853134083298017, "compression_ratio": 1.8146551724137931, "no_speech_prob": 6.399615085683763e-05}, {"id": 227, "seek": 202300, "start": 2036.0, "end": 2044.0, "text": " Whereas in Elm, we get like a lot of confidence from the type checking and then you can cover the rest of the ground in tests if you need to.", "tokens": [13813, 294, 2699, 76, 11, 321, 483, 411, 257, 688, 295, 6687, 490, 264, 2010, 8568, 293, 550, 291, 393, 2060, 264, 1472, 295, 264, 2727, 294, 6921, 498, 291, 643, 281, 13], "temperature": 0.0, "avg_logprob": -0.17853134083298017, "compression_ratio": 1.8146551724137931, "no_speech_prob": 6.399615085683763e-05}, {"id": 228, "seek": 204400, "start": 2044.0, "end": 2055.0, "text": " Whenever there's a type change, you copy everything into like a V2. You copy all the types or do you copy all the code?", "tokens": [14159, 456, 311, 257, 2010, 1319, 11, 291, 5055, 1203, 666, 411, 257, 691, 17, 13, 509, 5055, 439, 264, 3467, 420, 360, 291, 5055, 439, 264, 3089, 30], "temperature": 0.0, "avg_logprob": -0.24985091686248778, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00011945552250836045}, {"id": 229, "seek": 204400, "start": 2055.0, "end": 2067.0, "text": " No, just the types. So what Lambda, so the implementation currently is Lambda kind of recursively trolls through your types from their known. So in Lambda you have to put the core types in a specific location, right?", "tokens": [883, 11, 445, 264, 3467, 13, 407, 437, 45691, 11, 370, 264, 11420, 4362, 307, 45691, 733, 295, 20560, 3413, 47749, 807, 428, 3467, 490, 641, 2570, 13, 407, 294, 45691, 291, 362, 281, 829, 264, 4965, 3467, 294, 257, 2685, 4914, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24985091686248778, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00011945552250836045}, {"id": 230, "seek": 206700, "start": 2067.0, "end": 2077.0, "text": " So Lambda goes, okay, I know where they have to be. I start from there and I kind of recursively troll through the type definition and any type definitions it references.", "tokens": [407, 45691, 1709, 11, 1392, 11, 286, 458, 689, 436, 362, 281, 312, 13, 286, 722, 490, 456, 293, 286, 733, 295, 20560, 3413, 20680, 807, 264, 2010, 7123, 293, 604, 2010, 21988, 309, 15400, 13], "temperature": 0.0, "avg_logprob": -0.1904053950047755, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.7959213184658438e-05}, {"id": 231, "seek": 206700, "start": 2077.0, "end": 2088.0, "text": " And it kind of progressively sucks those out. And as it goes, it namespaces them and it writes them into the snapshots file. So you get, yeah, like an extracted copy of your types tree for every single core type.", "tokens": [400, 309, 733, 295, 46667, 15846, 729, 484, 13, 400, 382, 309, 1709, 11, 309, 5288, 79, 2116, 552, 293, 309, 13657, 552, 666, 264, 19206, 27495, 3991, 13, 407, 291, 483, 11, 1338, 11, 411, 364, 34086, 5055, 295, 428, 3467, 4230, 337, 633, 2167, 4965, 2010, 13], "temperature": 0.0, "avg_logprob": -0.1904053950047755, "compression_ratio": 1.6367521367521367, "no_speech_prob": 2.7959213184658438e-05}, {"id": 232, "seek": 208800, "start": 2088.0, "end": 2102.0, "text": " Okay. Now there's a really good feature in Elm, which I don't know if you've ever heard us talk about, which is opaque types. How does it work for opaque types? Can you migrate them? Can you not?", "tokens": [1033, 13, 823, 456, 311, 257, 534, 665, 4111, 294, 2699, 76, 11, 597, 286, 500, 380, 458, 498, 291, 600, 1562, 2198, 505, 751, 466, 11, 597, 307, 42687, 3467, 13, 1012, 775, 309, 589, 337, 42687, 3467, 30, 1664, 291, 31821, 552, 30, 1664, 291, 406, 30], "temperature": 0.0, "avg_logprob": -0.19421986972584443, "compression_ratio": 1.565891472868217, "no_speech_prob": 2.3549693651148118e-05}, {"id": 233, "seek": 208800, "start": 2102.0, "end": 2117.0, "text": " Yeah, this is a great question. So sadly right now, there is a way around this. We could add compiler support and do some magic. But right now you sadly need to open up the constructors within your code base.", "tokens": [865, 11, 341, 307, 257, 869, 1168, 13, 407, 22023, 558, 586, 11, 456, 307, 257, 636, 926, 341, 13, 492, 727, 909, 31958, 1406, 293, 360, 512, 5585, 13, 583, 558, 586, 291, 22023, 643, 281, 1269, 493, 264, 7690, 830, 1951, 428, 3089, 3096, 13], "temperature": 0.0, "avg_logprob": -0.19421986972584443, "compression_ratio": 1.565891472868217, "no_speech_prob": 2.3549693651148118e-05}, {"id": 234, "seek": 211700, "start": 2117.0, "end": 2119.0, "text": " So there is a risk.", "tokens": [407, 456, 307, 257, 3148, 13], "temperature": 0.0, "avg_logprob": -0.21779029265693997, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.0952874617942143e-05}, {"id": 235, "seek": 211700, "start": 2119.0, "end": 2121.0, "text": " Do you mean only in migrations or always?", "tokens": [1144, 291, 914, 787, 294, 6186, 12154, 420, 1009, 30], "temperature": 0.0, "avg_logprob": -0.21779029265693997, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.0952874617942143e-05}, {"id": 236, "seek": 211700, "start": 2121.0, "end": 2132.0, "text": " Always. So the reason for that is if you consider like, so let's go back to our ice cream flavors. Say you made that an opaque type, all right?", "tokens": [11270, 13, 407, 264, 1778, 337, 300, 307, 498, 291, 1949, 411, 11, 370, 718, 311, 352, 646, 281, 527, 4435, 4689, 16303, 13, 6463, 291, 1027, 300, 364, 42687, 2010, 11, 439, 558, 30], "temperature": 0.0, "avg_logprob": -0.21779029265693997, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.0952874617942143e-05}, {"id": 237, "seek": 211700, "start": 2132.0, "end": 2133.0, "text": " Obviously.", "tokens": [7580, 13], "temperature": 0.0, "avg_logprob": -0.21779029265693997, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.0952874617942143e-05}, {"id": 238, "seek": 211700, "start": 2133.0, "end": 2143.0, "text": " Yeah, obviously. Nobody is allowed to specifically say what flavor they like. They have to go through the flavor constructor function, right?", "tokens": [865, 11, 2745, 13, 9297, 307, 4350, 281, 4682, 584, 437, 6813, 436, 411, 13, 814, 362, 281, 352, 807, 264, 6813, 47479, 2445, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21779029265693997, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.0952874617942143e-05}, {"id": 239, "seek": 214300, "start": 2143.0, "end": 2154.0, "text": " Okay, so maybe that's what you love. You're like, yes, this is the best way to do this. So in this maybe convoluted example, think about now how you express a migration for this, right?", "tokens": [1033, 11, 370, 1310, 300, 311, 437, 291, 959, 13, 509, 434, 411, 11, 2086, 11, 341, 307, 264, 1151, 636, 281, 360, 341, 13, 407, 294, 341, 1310, 3754, 2308, 292, 1365, 11, 519, 466, 586, 577, 291, 5109, 257, 17011, 337, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17738073012408087, "compression_ratio": 1.6506550218340612, "no_speech_prob": 3.9440448745153844e-05}, {"id": 240, "seek": 214300, "start": 2154.0, "end": 2169.0, "text": " So I'm asking you, I've written the function signature. So the function signature is from v1.icecream to v2.icecream, right? And into the value, into the migration, you get a value called old.", "tokens": [407, 286, 478, 3365, 291, 11, 286, 600, 3720, 264, 2445, 13397, 13, 407, 264, 2445, 13397, 307, 490, 371, 16, 13, 573, 30277, 281, 371, 17, 13, 573, 30277, 11, 558, 30, 400, 666, 264, 2158, 11, 666, 264, 17011, 11, 291, 483, 257, 2158, 1219, 1331, 13], "temperature": 0.0, "avg_logprob": -0.17738073012408087, "compression_ratio": 1.6506550218340612, "no_speech_prob": 3.9440448745153844e-05}, {"id": 241, "seek": 216900, "start": 2169.0, "end": 2180.0, "text": " That's what I call it by default, right? Old is the name of the old ice cream flavor. So this is going to be a specific instance, like a specific value instance of that type, right?", "tokens": [663, 311, 437, 286, 818, 309, 538, 7576, 11, 558, 30, 8633, 307, 264, 1315, 295, 264, 1331, 4435, 4689, 6813, 13, 407, 341, 307, 516, 281, 312, 257, 2685, 5197, 11, 411, 257, 2685, 2158, 5197, 295, 300, 2010, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16923094217756154, "compression_ratio": 1.7403100775193798, "no_speech_prob": 8.267663361039013e-06}, {"id": 242, "seek": 216900, "start": 2180.0, "end": 2190.0, "text": " The first thing you would normally do in migrations of a custom type is you would be case old of, and then you would pattern match on all the variants, right? And then you would return new variants.", "tokens": [440, 700, 551, 291, 576, 5646, 360, 294, 6186, 12154, 295, 257, 2375, 2010, 307, 291, 576, 312, 1389, 1331, 295, 11, 293, 550, 291, 576, 5102, 2995, 322, 439, 264, 21669, 11, 558, 30, 400, 550, 291, 576, 2736, 777, 21669, 13], "temperature": 0.0, "avg_logprob": -0.16923094217756154, "compression_ratio": 1.7403100775193798, "no_speech_prob": 8.267663361039013e-06}, {"id": 243, "seek": 216900, "start": 2190.0, "end": 2194.0, "text": " But in your case, if you've made an opaque type, what do you do now?", "tokens": [583, 294, 428, 1389, 11, 498, 291, 600, 1027, 364, 42687, 2010, 11, 437, 360, 291, 360, 586, 30], "temperature": 0.0, "avg_logprob": -0.16923094217756154, "compression_ratio": 1.7403100775193798, "no_speech_prob": 8.267663361039013e-06}, {"id": 244, "seek": 219400, "start": 2194.0, "end": 2207.0, "text": " Yeah, exactly. Yeah, that's why I was wondering, like, I think there's some problem with the opaque types. And I seem to remember that, yeah, opaque types didn't work well with Dundara or with the migration system.", "tokens": [865, 11, 2293, 13, 865, 11, 300, 311, 983, 286, 390, 6359, 11, 411, 11, 286, 519, 456, 311, 512, 1154, 365, 264, 42687, 3467, 13, 400, 286, 1643, 281, 1604, 300, 11, 1338, 11, 42687, 3467, 994, 380, 589, 731, 365, 413, 997, 2419, 420, 365, 264, 17011, 1185, 13], "temperature": 0.0, "avg_logprob": -0.27909407364694694, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.3687521206738893e-06}, {"id": 245, "seek": 219400, "start": 2207.0, "end": 2219.0, "text": " If you really, really wanted to, you could write specific code to try. Yeah, so you would have to write like special functions that were like basically constructor functions for your new types.", "tokens": [759, 291, 534, 11, 534, 1415, 281, 11, 291, 727, 2464, 2685, 3089, 281, 853, 13, 865, 11, 370, 291, 576, 362, 281, 2464, 411, 2121, 6828, 300, 645, 411, 1936, 47479, 6828, 337, 428, 777, 3467, 13], "temperature": 0.0, "avg_logprob": -0.27909407364694694, "compression_ratio": 1.6859504132231404, "no_speech_prob": 2.3687521206738893e-06}, {"id": 246, "seek": 221900, "start": 2219.0, "end": 2231.0, "text": " And then maybe you would inside your actual normal elm code, like within that module that has access to the constructors, you might write like a deconstructing or put the migration file directly in there, right?", "tokens": [400, 550, 1310, 291, 576, 1854, 428, 3539, 2710, 806, 76, 3089, 11, 411, 1951, 300, 10088, 300, 575, 2105, 281, 264, 7690, 830, 11, 291, 1062, 2464, 411, 257, 49473, 1757, 278, 420, 829, 264, 17011, 3991, 3838, 294, 456, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2231498891657049, "compression_ratio": 1.7670250896057347, "no_speech_prob": 4.0059167076833546e-05}, {"id": 247, "seek": 221900, "start": 2231.0, "end": 2238.0, "text": " So you could try and keep all of your types opaque and hidden and put the actual migration function in the file somehow, right?", "tokens": [407, 291, 727, 853, 293, 1066, 439, 295, 428, 3467, 42687, 293, 7633, 293, 829, 264, 3539, 17011, 2445, 294, 264, 3991, 6063, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2231498891657049, "compression_ratio": 1.7670250896057347, "no_speech_prob": 4.0059167076833546e-05}, {"id": 248, "seek": 221900, "start": 2238.0, "end": 2246.0, "text": " The thing that gets really weird and what I discourage with Lambda, although it is possible, is that obviously migrations are like a point in time thing.", "tokens": [440, 551, 300, 2170, 534, 3657, 293, 437, 286, 21497, 609, 365, 45691, 11, 4878, 309, 307, 1944, 11, 307, 300, 2745, 6186, 12154, 366, 411, 257, 935, 294, 565, 551, 13], "temperature": 0.0, "avg_logprob": -0.2231498891657049, "compression_ratio": 1.7670250896057347, "no_speech_prob": 4.0059167076833546e-05}, {"id": 249, "seek": 224600, "start": 2246.0, "end": 2252.0, "text": " And so part of the reason we do snapshots is as your app continues to evolve, your code is going to change, right?", "tokens": [400, 370, 644, 295, 264, 1778, 321, 360, 19206, 27495, 307, 382, 428, 724, 6515, 281, 16693, 11, 428, 3089, 307, 516, 281, 1319, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17787068030413458, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.00012924666225444525}, {"id": 250, "seek": 224600, "start": 2252.0, "end": 2261.0, "text": " So the weird thing is, if you put the migration function into the file that defines those constructors, in the next version, you're going to have to change the type that's in that file.", "tokens": [407, 264, 3657, 551, 307, 11, 498, 291, 829, 264, 17011, 2445, 666, 264, 3991, 300, 23122, 729, 7690, 830, 11, 294, 264, 958, 3037, 11, 291, 434, 516, 281, 362, 281, 1319, 264, 2010, 300, 311, 294, 300, 3991, 13], "temperature": 0.0, "avg_logprob": -0.17787068030413458, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.00012924666225444525}, {"id": 251, "seek": 224600, "start": 2261.0, "end": 2266.0, "text": " And suddenly those migration functions that are referring to the types that are in the same file are wrong.", "tokens": [400, 5800, 729, 17011, 6828, 300, 366, 13761, 281, 264, 3467, 300, 366, 294, 264, 912, 3991, 366, 2085, 13], "temperature": 0.0, "avg_logprob": -0.17787068030413458, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.00012924666225444525}, {"id": 252, "seek": 224600, "start": 2266.0, "end": 2274.0, "text": " So the question is like, okay, well, now where do I put them? You want to put them into the history, but the history can't access the constructors, right?", "tokens": [407, 264, 1168, 307, 411, 11, 1392, 11, 731, 11, 586, 689, 360, 286, 829, 552, 30, 509, 528, 281, 829, 552, 666, 264, 2503, 11, 457, 264, 2503, 393, 380, 2105, 264, 7690, 830, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17787068030413458, "compression_ratio": 1.9084745762711866, "no_speech_prob": 0.00012924666225444525}, {"id": 253, "seek": 227400, "start": 2274.0, "end": 2280.0, "text": " So the problem today is, okay, with those types, Lambda forces you to open it up. That's kind of the easy way.", "tokens": [407, 264, 1154, 965, 307, 11, 1392, 11, 365, 729, 3467, 11, 45691, 5874, 291, 281, 1269, 309, 493, 13, 663, 311, 733, 295, 264, 1858, 636, 13], "temperature": 0.0, "avg_logprob": -0.20204779366466485, "compression_ratio": 1.66798418972332, "no_speech_prob": 4.069255373906344e-05}, {"id": 254, "seek": 227400, "start": 2280.0, "end": 2288.0, "text": " Long-term, is there a way around it? Yes, the way around it would be at the compiler level for us to go when we're compiling,", "tokens": [8282, 12, 7039, 11, 307, 456, 257, 636, 926, 309, 30, 1079, 11, 264, 636, 926, 309, 576, 312, 412, 264, 31958, 1496, 337, 505, 281, 352, 562, 321, 434, 715, 4883, 11], "temperature": 0.0, "avg_logprob": -0.20204779366466485, "compression_ratio": 1.66798418972332, "no_speech_prob": 4.069255373906344e-05}, {"id": 255, "seek": 227400, "start": 2288.0, "end": 2301.0, "text": " when we see that we're compiling a module that is within the Evergreen namespace, magically the compiler unhinges its export restrictions and pretends like as if everything is exported.", "tokens": [562, 321, 536, 300, 321, 434, 715, 4883, 257, 10088, 300, 307, 1951, 264, 12123, 27399, 5288, 17940, 11, 39763, 264, 31958, 517, 571, 279, 1080, 10725, 14191, 293, 1162, 2581, 411, 382, 498, 1203, 307, 42055, 13], "temperature": 0.0, "avg_logprob": -0.20204779366466485, "compression_ratio": 1.66798418972332, "no_speech_prob": 4.069255373906344e-05}, {"id": 256, "seek": 230100, "start": 2301.0, "end": 2311.0, "text": " So then only in the migrations context, you can reference a opaque type constructor and you won't get a type error saying, oh, this is, you know, unexposed or hidden.", "tokens": [407, 550, 787, 294, 264, 6186, 12154, 4319, 11, 291, 393, 6408, 257, 42687, 2010, 47479, 293, 291, 1582, 380, 483, 257, 2010, 6713, 1566, 11, 1954, 11, 341, 307, 11, 291, 458, 11, 11572, 79, 1744, 420, 7633, 13], "temperature": 0.0, "avg_logprob": -0.1932204564412435, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.5935045666992664e-05}, {"id": 257, "seek": 230100, "start": 2311.0, "end": 2317.0, "text": " But then in your main code base, any way you referenced one, you would get the type error as usual.", "tokens": [583, 550, 294, 428, 2135, 3089, 3096, 11, 604, 636, 291, 32734, 472, 11, 291, 576, 483, 264, 2010, 6713, 382, 7713, 13], "temperature": 0.0, "avg_logprob": -0.1932204564412435, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.5935045666992664e-05}, {"id": 258, "seek": 230100, "start": 2317.0, "end": 2321.0, "text": " So that's the idea. I don't know how difficult that would be. I haven't delved into it yet.", "tokens": [407, 300, 311, 264, 1558, 13, 286, 500, 380, 458, 577, 2252, 300, 576, 312, 13, 286, 2378, 380, 1103, 937, 666, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1932204564412435, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.5935045666992664e-05}, {"id": 259, "seek": 232100, "start": 2321.0, "end": 2331.0, "text": " But at least in theory, I think we could improve those ergonomics and, you know, get back the same kind of opaque type protections that we have today.", "tokens": [583, 412, 1935, 294, 5261, 11, 286, 519, 321, 727, 3470, 729, 42735, 29884, 293, 11, 291, 458, 11, 483, 646, 264, 912, 733, 295, 42687, 2010, 29031, 300, 321, 362, 965, 13], "temperature": 0.0, "avg_logprob": -0.17714455863025702, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00019679380056913942}, {"id": 260, "seek": 232100, "start": 2331.0, "end": 2341.0, "text": " So yeah, to lovers of opaque types, I'm sorry, Jeroen, there's a little bit of a compromise there from, you know, ergonomics and language restrictions.", "tokens": [407, 1338, 11, 281, 22697, 295, 42687, 3467, 11, 286, 478, 2597, 11, 508, 2032, 268, 11, 456, 311, 257, 707, 857, 295, 257, 18577, 456, 490, 11, 291, 458, 11, 42735, 29884, 293, 2856, 14191, 13], "temperature": 0.0, "avg_logprob": -0.17714455863025702, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00019679380056913942}, {"id": 261, "seek": 232100, "start": 2341.0, "end": 2349.0, "text": " But yeah, I think for now it's probably not the end of the world, but we have a way to improve that in the future.", "tokens": [583, 1338, 11, 286, 519, 337, 586, 309, 311, 1391, 406, 264, 917, 295, 264, 1002, 11, 457, 321, 362, 257, 636, 281, 3470, 300, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.17714455863025702, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.00019679380056913942}, {"id": 262, "seek": 234900, "start": 2349.0, "end": 2355.0, "text": " Not too many people have complained about it so far. And by not too many people, I mean zero people.", "tokens": [1726, 886, 867, 561, 362, 33951, 466, 309, 370, 1400, 13, 400, 538, 406, 886, 867, 561, 11, 286, 914, 4018, 561, 13], "temperature": 0.0, "avg_logprob": -0.2789236958821615, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.627062531246338e-05}, {"id": 263, "seek": 234900, "start": 2355.0, "end": 2360.0, "text": " I'm complaining. Here's my official complaint, Mario.", "tokens": [286, 478, 20740, 13, 1692, 311, 452, 4783, 20100, 11, 9343, 13], "temperature": 0.0, "avg_logprob": -0.2789236958821615, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.627062531246338e-05}, {"id": 264, "seek": 234900, "start": 2360.0, "end": 2366.0, "text": " Okay. So far we have officially had one person complaining. So there is one people demanding.", "tokens": [1033, 13, 407, 1400, 321, 362, 12053, 632, 472, 954, 20740, 13, 407, 456, 307, 472, 561, 19960, 13], "temperature": 0.0, "avg_logprob": -0.2789236958821615, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.627062531246338e-05}, {"id": 265, "seek": 234900, "start": 2366.0, "end": 2370.0, "text": " First the strawberry, now opaque types. What next?", "tokens": [2386, 264, 20440, 11, 586, 42687, 3467, 13, 708, 958, 30], "temperature": 0.0, "avg_logprob": -0.2789236958821615, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.627062531246338e-05}, {"id": 266, "seek": 237000, "start": 2370.0, "end": 2379.0, "text": " Yeah, but also like with opaque types, what we tend to represent with the opaque types is invariance, right?", "tokens": [865, 11, 457, 611, 411, 365, 42687, 3467, 11, 437, 321, 3928, 281, 2906, 365, 264, 42687, 3467, 307, 33270, 719, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2873675755092076, "compression_ratio": 1.6569767441860466, "no_speech_prob": 4.4663935113931075e-05}, {"id": 267, "seek": 237000, "start": 2379.0, "end": 2391.0, "text": " So there's the actual data and there's the invariance. So like, even if the types matched, if we have an opaque type that makes sure you have selected three types of ice cream,", "tokens": [407, 456, 311, 264, 3539, 1412, 293, 456, 311, 264, 33270, 719, 13, 407, 411, 11, 754, 498, 264, 3467, 21447, 11, 498, 321, 362, 364, 42687, 2010, 300, 1669, 988, 291, 362, 8209, 1045, 3467, 295, 4435, 4689, 11], "temperature": 0.0, "avg_logprob": -0.2873675755092076, "compression_ratio": 1.6569767441860466, "no_speech_prob": 4.4663935113931075e-05}, {"id": 268, "seek": 239100, "start": 2391.0, "end": 2407.0, "text": " and then in another version, well, it's only supposed to be two now. Like, yeah, there's, you have some kind of revalidation to do anyway, or which, yeah, not sure how you would do it.", "tokens": [293, 550, 294, 1071, 3037, 11, 731, 11, 309, 311, 787, 3442, 281, 312, 732, 586, 13, 1743, 11, 1338, 11, 456, 311, 11, 291, 362, 512, 733, 295, 319, 3337, 327, 399, 281, 360, 4033, 11, 420, 597, 11, 1338, 11, 406, 988, 577, 291, 576, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.28905506487245913, "compression_ratio": 1.362962962962963, "no_speech_prob": 1.0124671462108381e-05}, {"id": 269, "seek": 240700, "start": 2407.0, "end": 2426.0, "text": " Yeah. So there's one interesting part of this where this actually has come up. So there's some kind of, there's some a little bit undocumented auto-generation support for specific package types that are opaque and people kind of commonly use.", "tokens": [865, 13, 407, 456, 311, 472, 1880, 644, 295, 341, 689, 341, 767, 575, 808, 493, 13, 407, 456, 311, 512, 733, 295, 11, 456, 311, 512, 257, 707, 857, 40472, 8399, 12, 30372, 1406, 337, 2685, 7372, 3467, 300, 366, 42687, 293, 561, 733, 295, 12719, 764, 13], "temperature": 0.0, "avg_logprob": -0.2854420463993864, "compression_ratio": 1.5316455696202531, "no_speech_prob": 1.8334882042836398e-05}, {"id": 270, "seek": 242600, "start": 2426.0, "end": 2441.0, "text": " So for example, something like non-empty. Now, the thing I don't think we've spoken about yet is when I say that LambdaEra extracts the type hierarchy, we only do that to the extent of user-defined types.", "tokens": [407, 337, 1365, 11, 746, 411, 2107, 12, 4543, 88, 13, 823, 11, 264, 551, 286, 500, 380, 519, 321, 600, 10759, 466, 1939, 307, 562, 286, 584, 300, 45691, 36, 424, 8947, 82, 264, 2010, 22333, 11, 321, 787, 360, 300, 281, 264, 8396, 295, 4195, 12, 37716, 3467, 13], "temperature": 0.0, "avg_logprob": -0.25071001052856445, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.071334140258841e-05}, {"id": 271, "seek": 242600, "start": 2441.0, "end": 2449.0, "text": " Right? So if you're referencing like non-empty string, or like the string type from the non-empty package, we won't...", "tokens": [1779, 30, 407, 498, 291, 434, 40582, 411, 2107, 12, 4543, 88, 6798, 11, 420, 411, 264, 6798, 2010, 490, 264, 2107, 12, 4543, 88, 7372, 11, 321, 1582, 380, 485], "temperature": 0.0, "avg_logprob": -0.25071001052856445, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.071334140258841e-05}, {"id": 272, "seek": 242600, "start": 2449.0, "end": 2451.0, "text": " Which is an opaque type?", "tokens": [3013, 307, 364, 42687, 2010, 30], "temperature": 0.0, "avg_logprob": -0.25071001052856445, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.071334140258841e-05}, {"id": 273, "seek": 245100, "start": 2451.0, "end": 2462.0, "text": " Yes. Yeah. So you have a constructor where you have to give it a string that has something in it, and it'll only return a non-empty string value if you give it a non-empty string.", "tokens": [1079, 13, 865, 13, 407, 291, 362, 257, 47479, 689, 291, 362, 281, 976, 309, 257, 6798, 300, 575, 746, 294, 309, 11, 293, 309, 603, 787, 2736, 257, 2107, 12, 4543, 88, 6798, 2158, 498, 291, 976, 309, 257, 2107, 12, 4543, 88, 6798, 13], "temperature": 0.0, "avg_logprob": -0.19689138643034212, "compression_ratio": 1.7135416666666667, "no_speech_prob": 2.9770604669465683e-05}, {"id": 274, "seek": 245100, "start": 2462.0, "end": 2471.0, "text": " So once you have a value of that type, you know that it's definitely non-empty. So yeah, in that case, LambdaEra isn't snapshotting the package type.", "tokens": [407, 1564, 291, 362, 257, 2158, 295, 300, 2010, 11, 291, 458, 300, 309, 311, 2138, 2107, 12, 4543, 88, 13, 407, 1338, 11, 294, 300, 1389, 11, 45691, 36, 424, 1943, 380, 30163, 783, 264, 7372, 2010, 13], "temperature": 0.0, "avg_logprob": -0.19689138643034212, "compression_ratio": 1.7135416666666667, "no_speech_prob": 2.9770604669465683e-05}, {"id": 275, "seek": 247100, "start": 2471.0, "end": 2484.0, "text": " There's a few reasons for that, that I'm not sure entirely worth going into, but long story short, it focuses just on the user types now. Mainly, actually, the best reason for it is the opaque type discussion that we're having.", "tokens": [821, 311, 257, 1326, 4112, 337, 300, 11, 300, 286, 478, 406, 988, 7696, 3163, 516, 666, 11, 457, 938, 1657, 2099, 11, 309, 16109, 445, 322, 264, 4195, 3467, 586, 13, 47468, 11, 767, 11, 264, 1151, 1778, 337, 309, 307, 264, 42687, 2010, 5017, 300, 321, 434, 1419, 13], "temperature": 0.0, "avg_logprob": -0.19257788224653763, "compression_ratio": 1.6434426229508197, "no_speech_prob": 3.534944335115142e-05}, {"id": 276, "seek": 247100, "start": 2484.0, "end": 2493.0, "text": " The best reason for it is we can't really do migrations on package types, because if it's something like non-empty, they don't offer us the internals of that package. Right?", "tokens": [440, 1151, 1778, 337, 309, 307, 321, 393, 380, 534, 360, 6186, 12154, 322, 7372, 3467, 11, 570, 498, 309, 311, 746, 411, 2107, 12, 4543, 88, 11, 436, 500, 380, 2626, 505, 264, 2154, 1124, 295, 300, 7372, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.19257788224653763, "compression_ratio": 1.6434426229508197, "no_speech_prob": 3.534944335115142e-05}, {"id": 277, "seek": 249300, "start": 2493.0, "end": 2503.0, "text": " So what I do is I generate some code that basically does what the sensible default would be if you knew you had a...", "tokens": [407, 437, 286, 360, 307, 286, 8460, 512, 3089, 300, 1936, 775, 437, 264, 25380, 7576, 576, 312, 498, 291, 2586, 291, 632, 257, 485], "temperature": 0.0, "avg_logprob": -0.1609884770711263, "compression_ratio": 1.528497409326425, "no_speech_prob": 8.217612776206806e-05}, {"id": 278, "seek": 249300, "start": 2503.0, "end": 2512.0, "text": " Well, the other thing is that package types don't change, at least if the package version hasn't changed. So if we already have a value of non-empty, we don't need to migrate it.", "tokens": [1042, 11, 264, 661, 551, 307, 300, 7372, 3467, 500, 380, 1319, 11, 412, 1935, 498, 264, 7372, 3037, 6132, 380, 3105, 13, 407, 498, 321, 1217, 362, 257, 2158, 295, 2107, 12, 4543, 88, 11, 321, 500, 380, 643, 281, 31821, 309, 13], "temperature": 0.0, "avg_logprob": -0.1609884770711263, "compression_ratio": 1.528497409326425, "no_speech_prob": 8.217612776206806e-05}, {"id": 279, "seek": 251200, "start": 2512.0, "end": 2524.0, "text": " But there has been some cases where... Okay, so a better type than non-empty string, which is kind of monomorphic, or one that isn't, one that would be polymorphic, would be like, say, the anydict.", "tokens": [583, 456, 575, 668, 512, 3331, 689, 485, 1033, 11, 370, 257, 1101, 2010, 813, 2107, 12, 4543, 88, 6798, 11, 597, 307, 733, 295, 1108, 32702, 299, 11, 420, 472, 300, 1943, 380, 11, 472, 300, 576, 312, 6754, 76, 18191, 299, 11, 576, 312, 411, 11, 584, 11, 264, 604, 67, 985, 13], "temperature": 0.0, "avg_logprob": -0.25595719935530326, "compression_ratio": 1.7323420074349443, "no_speech_prob": 2.281729393871501e-05}, {"id": 280, "seek": 251200, "start": 2524.0, "end": 2540.0, "text": " One of the anydict packages, right? Like, so packages that let you define a custom type as your key, and then it lets you do lookups and inserts on this dictionary that you can't do with the vanilla dictionary implementation, because it requires keys to be comparable.", "tokens": [1485, 295, 264, 604, 67, 985, 17401, 11, 558, 30, 1743, 11, 370, 17401, 300, 718, 291, 6964, 257, 2375, 2010, 382, 428, 2141, 11, 293, 550, 309, 6653, 291, 360, 574, 7528, 293, 49163, 322, 341, 25890, 300, 291, 393, 380, 360, 365, 264, 17528, 25890, 11420, 11, 570, 309, 7029, 9317, 281, 312, 25323, 13], "temperature": 0.0, "avg_logprob": -0.25595719935530326, "compression_ratio": 1.7323420074349443, "no_speech_prob": 2.281729393871501e-05}, {"id": 281, "seek": 254000, "start": 2540.0, "end": 2550.0, "text": " So if you use one of these types, now you've got like a parametric type, right? You are putting your own user-defined custom type into this third-party package.", "tokens": [407, 498, 291, 764, 472, 295, 613, 3467, 11, 586, 291, 600, 658, 411, 257, 6220, 17475, 2010, 11, 558, 30, 509, 366, 3372, 428, 1065, 4195, 12, 37716, 2375, 2010, 666, 341, 2636, 12, 23409, 7372, 13], "temperature": 0.0, "avg_logprob": -0.18767236821791705, "compression_ratio": 1.585, "no_speech_prob": 1.8627106328494847e-05}, {"id": 282, "seek": 254000, "start": 2550.0, "end": 2561.0, "text": " So if you've changed that type, when it comes to a migration, you may have anydict icecream version 1, and now you need to go to anydict icecream version 2.", "tokens": [407, 498, 291, 600, 3105, 300, 2010, 11, 562, 309, 1487, 281, 257, 17011, 11, 291, 815, 362, 604, 67, 985, 4435, 30277, 3037, 502, 11, 293, 586, 291, 643, 281, 352, 281, 604, 67, 985, 4435, 30277, 3037, 568, 13], "temperature": 0.0, "avg_logprob": -0.18767236821791705, "compression_ratio": 1.585, "no_speech_prob": 1.8627106328494847e-05}, {"id": 283, "seek": 256100, "start": 2561.0, "end": 2573.0, "text": " So you need to extract all of the values from that first dictionary, and then do the migration, and then create a new dictionary. So for some of these types, it's kind of like, that's annoying, and it's mechanical.", "tokens": [407, 291, 643, 281, 8947, 439, 295, 264, 4190, 490, 300, 700, 25890, 11, 293, 550, 360, 264, 17011, 11, 293, 550, 1884, 257, 777, 25890, 13, 407, 337, 512, 295, 613, 3467, 11, 309, 311, 733, 295, 411, 11, 300, 311, 11304, 11, 293, 309, 311, 12070, 13], "temperature": 0.0, "avg_logprob": -0.1880373578322561, "compression_ratio": 1.57, "no_speech_prob": 2.318399856449105e-05}, {"id": 284, "seek": 256100, "start": 2573.0, "end": 2579.0, "text": " And we know pretty much what people are going to do, especially if that custom type hasn't changed.", "tokens": [400, 321, 458, 1238, 709, 437, 561, 366, 516, 281, 360, 11, 2318, 498, 300, 2375, 2010, 6132, 380, 3105, 13], "temperature": 0.0, "avg_logprob": -0.1880373578322561, "compression_ratio": 1.57, "no_speech_prob": 2.318399856449105e-05}, {"id": 285, "seek": 257900, "start": 2579.0, "end": 2592.0, "text": " So lamdara in some cases will detect certain common library types, and it'll try and do the sensible migration for you if your custom type that's being used there hasn't changed.", "tokens": [407, 24688, 67, 2419, 294, 512, 3331, 486, 5531, 1629, 2689, 6405, 3467, 11, 293, 309, 603, 853, 293, 360, 264, 25380, 17011, 337, 291, 498, 428, 2375, 2010, 300, 311, 885, 1143, 456, 6132, 380, 3105, 13], "temperature": 0.0, "avg_logprob": -0.2498062421690743, "compression_ratio": 1.3870967741935485, "no_speech_prob": 1.9523318769643083e-05}, {"id": 286, "seek": 257900, "start": 2592.0, "end": 2594.0, "text": " Is that making sense? Am I tracking?", "tokens": [1119, 300, 1455, 2020, 30, 2012, 286, 11603, 30], "temperature": 0.0, "avg_logprob": -0.2498062421690743, "compression_ratio": 1.3870967741935485, "no_speech_prob": 1.9523318769643083e-05}, {"id": 287, "seek": 259400, "start": 2594.0, "end": 2609.0, "text": " Yeah, does that mean that if I were to make a new package with a new data structure, like anydict, that it would be better if I contacted you to add support for that?", "tokens": [865, 11, 775, 300, 914, 300, 498, 286, 645, 281, 652, 257, 777, 7372, 365, 257, 777, 1412, 3877, 11, 411, 604, 67, 985, 11, 300, 309, 576, 312, 1101, 498, 286, 21546, 291, 281, 909, 1406, 337, 300, 30], "temperature": 0.0, "avg_logprob": -0.22046719868977865, "compression_ratio": 1.541871921182266, "no_speech_prob": 3.646347249741666e-05}, {"id": 288, "seek": 259400, "start": 2609.0, "end": 2617.0, "text": " I definitely wouldn't want to encourage a perspective in the ARM community that everyone should be thinking of lamdara concerns in their packages.", "tokens": [286, 2138, 2759, 380, 528, 281, 5373, 257, 4585, 294, 264, 220, 1899, 44, 1768, 300, 1518, 820, 312, 1953, 295, 24688, 67, 2419, 7389, 294, 641, 17401, 13], "temperature": 0.0, "avg_logprob": -0.22046719868977865, "compression_ratio": 1.541871921182266, "no_speech_prob": 3.646347249741666e-05}, {"id": 289, "seek": 261700, "start": 2617.0, "end": 2628.0, "text": " I'd rather tackle it when it came up. But yeah, what would make it easier for a lamdara user to use your type if it's a type that doesn't contain user types, then I don't think it matters.", "tokens": [286, 1116, 2831, 14896, 309, 562, 309, 1361, 493, 13, 583, 1338, 11, 437, 576, 652, 309, 3571, 337, 257, 24688, 67, 2419, 4195, 281, 764, 428, 2010, 498, 309, 311, 257, 2010, 300, 1177, 380, 5304, 4195, 3467, 11, 550, 286, 500, 380, 519, 309, 7001, 13], "temperature": 0.0, "avg_logprob": -0.2440542120682566, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.00017380580538883805}, {"id": 290, "seek": 261700, "start": 2628.0, "end": 2637.0, "text": " Because they can just put the old ones in the new one. But if you are publishing a package that does contain the ability for users to put in custom types, then putting in the ability to migrate.", "tokens": [1436, 436, 393, 445, 829, 264, 1331, 2306, 294, 264, 777, 472, 13, 583, 498, 291, 366, 17832, 257, 7372, 300, 775, 5304, 264, 3485, 337, 5022, 281, 829, 294, 2375, 3467, 11, 550, 3372, 294, 264, 3485, 281, 31821, 13], "temperature": 0.0, "avg_logprob": -0.2440542120682566, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.00017380580538883805}, {"id": 291, "seek": 263700, "start": 2637.0, "end": 2652.0, "text": " If there was a map function that sensibly made sense, where they could give you a function of type A to B, and that let them migrate your type A to your type B in the package, then that would definitely make it easier for them to construct migrations.", "tokens": [759, 456, 390, 257, 4471, 2445, 300, 2923, 3545, 1027, 2020, 11, 689, 436, 727, 976, 291, 257, 2445, 295, 2010, 316, 281, 363, 11, 293, 300, 718, 552, 31821, 428, 2010, 316, 281, 428, 2010, 363, 294, 264, 7372, 11, 550, 300, 576, 2138, 652, 309, 3571, 337, 552, 281, 7690, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.1780893843052751, "compression_ratio": 1.608974358974359, "no_speech_prob": 0.0005344737437553704}, {"id": 292, "seek": 265200, "start": 2652.0, "end": 2673.0, "text": " But otherwise, yeah, an ability to somehow exhaustively deconstruct values in a meaningful way and exhaustively construct values in a meaningful way of your package type, I think would be the key primitives that people would need to be able to express this idea of going from an old type to a new type for any conceivable type change, if that makes sense.", "tokens": [583, 5911, 11, 1338, 11, 364, 3485, 281, 6063, 14687, 3413, 49473, 1757, 4190, 294, 257, 10995, 636, 293, 14687, 3413, 7690, 4190, 294, 257, 10995, 636, 295, 428, 7372, 2010, 11, 286, 519, 576, 312, 264, 2141, 2886, 38970, 300, 561, 576, 643, 281, 312, 1075, 281, 5109, 341, 1558, 295, 516, 490, 364, 1331, 2010, 281, 257, 777, 2010, 337, 604, 10413, 34376, 2010, 1319, 11, 498, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.15244331607570896, "compression_ratio": 1.7317073170731707, "no_speech_prob": 4.0687045839149505e-05}, {"id": 293, "seek": 267300, "start": 2673.0, "end": 2692.0, "text": " Yeah, I could also imagine for some use cases, instead of directly having an opaque type, if you really wanted to model your application logic with an opaque type, especially in the front end, with the back end, there might be more performance concerns in some instances.", "tokens": [865, 11, 286, 727, 611, 3811, 337, 512, 764, 3331, 11, 2602, 295, 3838, 1419, 364, 42687, 2010, 11, 498, 291, 534, 1415, 281, 2316, 428, 3861, 9952, 365, 364, 42687, 2010, 11, 2318, 294, 264, 1868, 917, 11, 365, 264, 646, 917, 11, 456, 1062, 312, 544, 3389, 7389, 294, 512, 14519, 13], "temperature": 0.0, "avg_logprob": -0.23425320921273068, "compression_ratio": 1.5397727272727273, "no_speech_prob": 2.282418245158624e-05}, {"id": 294, "seek": 269200, "start": 2692.0, "end": 2712.0, "text": " But you could have your raw type stored, let's say, in your front end model. And then you could have a wrapper that takes it from the raw type to your opaque types.", "tokens": [583, 291, 727, 362, 428, 8936, 2010, 12187, 11, 718, 311, 584, 11, 294, 428, 1868, 917, 2316, 13, 400, 550, 291, 727, 362, 257, 46906, 300, 2516, 309, 490, 264, 8936, 2010, 281, 428, 42687, 3467, 13], "temperature": 0.0, "avg_logprob": -0.2152486074538458, "compression_ratio": 1.4260869565217391, "no_speech_prob": 4.198363239993341e-05}, {"id": 295, "seek": 271200, "start": 2712.0, "end": 2730.0, "text": " So you could have a function that transforms the raw types to opaque types. So immediately, as soon as you're actually working with those types, it's turning it into some opaque type that you can work with those guarantees.", "tokens": [407, 291, 727, 362, 257, 2445, 300, 35592, 264, 8936, 3467, 281, 42687, 3467, 13, 407, 4258, 11, 382, 2321, 382, 291, 434, 767, 1364, 365, 729, 3467, 11, 309, 311, 6246, 309, 666, 512, 42687, 2010, 300, 291, 393, 589, 365, 729, 32567, 13], "temperature": 0.0, "avg_logprob": -0.21680156552061744, "compression_ratio": 1.5379310344827586, "no_speech_prob": 2.507007957319729e-05}, {"id": 296, "seek": 273000, "start": 2730.0, "end": 2746.0, "text": " But then, as you're modifying it, you also need a way to transform that too. So it's a challenging problem. But that idea you have of being able to reach into the opaque types in the context of a migration is very intriguing.", "tokens": [583, 550, 11, 382, 291, 434, 42626, 309, 11, 291, 611, 643, 257, 636, 281, 4088, 300, 886, 13, 407, 309, 311, 257, 7595, 1154, 13, 583, 300, 1558, 291, 362, 295, 885, 1075, 281, 2524, 666, 264, 42687, 3467, 294, 264, 4319, 295, 257, 17011, 307, 588, 32503, 13], "temperature": 0.0, "avg_logprob": -0.1841746966044108, "compression_ratio": 1.4423076923076923, "no_speech_prob": 7.840985927032307e-05}, {"id": 297, "seek": 274600, "start": 2746.0, "end": 2772.0, "text": " And it does, I mean, it's a, it's a, an interesting philosophical place to be. But overall, the feeling that I'm getting what I'm realizing with evergreen migrations is that so much of what happens with, you know, the traditional way that many people may have worked with migrating, you know, between a some sort of JavaScript front end and a Rails back end, or whatever it might be,", "tokens": [400, 309, 775, 11, 286, 914, 11, 309, 311, 257, 11, 309, 311, 257, 11, 364, 1880, 25066, 1081, 281, 312, 13, 583, 4787, 11, 264, 2633, 300, 286, 478, 1242, 437, 286, 478, 16734, 365, 1562, 27399, 6186, 12154, 307, 300, 370, 709, 295, 437, 2314, 365, 11, 291, 458, 11, 264, 5164, 636, 300, 867, 561, 815, 362, 2732, 365, 6186, 8754, 11, 291, 458, 11, 1296, 257, 512, 1333, 295, 15778, 1868, 917, 293, 257, 48526, 646, 917, 11, 420, 2035, 309, 1062, 312, 11], "temperature": 0.0, "avg_logprob": -0.2563434891078783, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.0005191027885302901}, {"id": 298, "seek": 277200, "start": 2772.0, "end": 2798.0, "text": " the traditional experience that I've had doing conceptually migrations, maybe there are some, you know, there are probably some back end migrations involved. And then you need to handle that with different versions of the front end is you, you very carefully queue up the changes you're going to make, you very carefully, you know, test your actual migrations on some test data.", "tokens": [264, 5164, 1752, 300, 286, 600, 632, 884, 3410, 671, 6186, 12154, 11, 1310, 456, 366, 512, 11, 291, 458, 11, 456, 366, 1391, 512, 646, 917, 6186, 12154, 3288, 13, 400, 550, 291, 643, 281, 4813, 300, 365, 819, 9606, 295, 264, 1868, 917, 307, 291, 11, 291, 588, 7500, 18639, 493, 264, 2962, 291, 434, 516, 281, 652, 11, 291, 588, 7500, 11, 291, 458, 11, 1500, 428, 3539, 6186, 12154, 322, 512, 1500, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2566679512582174, "compression_ratio": 1.75, "no_speech_prob": 0.0007672369247302413}, {"id": 299, "seek": 279800, "start": 2798.0, "end": 2813.0, "text": " And maybe you sort of hope that that in between state works out okay, and don't think too carefully about it. Or maybe you think very carefully about it. But even so, even if you're thinking very carefully about it, you're thinking very carefully about an implicit contract.", "tokens": [400, 1310, 291, 1333, 295, 1454, 300, 300, 294, 1296, 1785, 1985, 484, 1392, 11, 293, 500, 380, 519, 886, 7500, 466, 309, 13, 1610, 1310, 291, 519, 588, 7500, 466, 309, 13, 583, 754, 370, 11, 754, 498, 291, 434, 1953, 588, 7500, 466, 309, 11, 291, 434, 1953, 588, 7500, 466, 364, 26947, 4364, 13], "temperature": 0.0, "avg_logprob": -0.18066059175084848, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0045379167422652245}, {"id": 300, "seek": 281300, "start": 2813.0, "end": 2838.0, "text": " Whereas what I'm realizing is that what lambda gives you is an explicit contract for all of these pieces, and they're all living in the same ecosystem. And, you know, if you, you could make something implicit in a lambda app by depending on something in the outside world, but for the things that are self contained within a lambda back end and lambda front end code base, it's an explicit contract.", "tokens": [13813, 437, 286, 478, 16734, 307, 300, 437, 24688, 65, 2675, 2709, 291, 307, 364, 13691, 4364, 337, 439, 295, 613, 3755, 11, 293, 436, 434, 439, 2647, 294, 264, 912, 11311, 13, 400, 11, 291, 458, 11, 498, 291, 11, 291, 727, 652, 746, 26947, 294, 257, 13607, 724, 538, 5413, 322, 746, 294, 264, 2380, 1002, 11, 457, 337, 264, 721, 300, 366, 2698, 16212, 1951, 257, 13607, 646, 917, 293, 13607, 1868, 917, 3089, 3096, 11, 309, 311, 364, 13691, 4364, 13], "temperature": 0.0, "avg_logprob": -0.24596953659914852, "compression_ratio": 1.7577092511013215, "no_speech_prob": 3.9439386455342174e-05}, {"id": 301, "seek": 283800, "start": 2838.0, "end": 2862.0, "text": " And it has the elm type system and all the guarantees that come with that of type safety and purity and no escape patches and immutability and all these things. So you put all those pieces together and what you get is an explicit contract for managing the entire migration, which is a really interesting feature to have. I mean, that's kind of a game changer.", "tokens": [400, 309, 575, 264, 806, 76, 2010, 1185, 293, 439, 264, 32567, 300, 808, 365, 300, 295, 2010, 4514, 293, 34382, 293, 572, 7615, 26531, 293, 3397, 325, 2310, 293, 439, 613, 721, 13, 407, 291, 829, 439, 729, 3755, 1214, 293, 437, 291, 483, 307, 364, 13691, 4364, 337, 11642, 264, 2302, 17011, 11, 597, 307, 257, 534, 1880, 4111, 281, 362, 13, 286, 914, 11, 300, 311, 733, 295, 257, 1216, 22822, 13], "temperature": 0.0, "avg_logprob": -0.18480962439428403, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0006262781680561602}, {"id": 302, "seek": 286200, "start": 2862.0, "end": 2891.0, "text": " Yeah, absolutely. So for people thinking about like their traditional migration setup, I think it's kind of the pitch of evergreen is similar to the elm pitch against JavaScript. It comes up in a lot of different areas, but let's take like JavaScript, you know, decoders, right? So the idea is that you go, okay, well, there's like a whole, you know, we know, that first name is always going to be there. Like we know this, in quotes, know this, right?", "tokens": [865, 11, 3122, 13, 407, 337, 561, 1953, 466, 411, 641, 5164, 17011, 8657, 11, 286, 519, 309, 311, 733, 295, 264, 7293, 295, 1562, 27399, 307, 2531, 281, 264, 806, 76, 7293, 1970, 15778, 13, 467, 1487, 493, 294, 257, 688, 295, 819, 3179, 11, 457, 718, 311, 747, 411, 15778, 11, 291, 458, 11, 979, 378, 433, 11, 558, 30, 407, 264, 1558, 307, 300, 291, 352, 11, 1392, 11, 731, 11, 456, 311, 411, 257, 1379, 11, 291, 458, 11, 321, 458, 11, 300, 700, 1315, 307, 1009, 516, 281, 312, 456, 13, 1743, 321, 458, 341, 11, 294, 19963, 11, 458, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23576439376425953, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.0009108359226956964}, {"id": 303, "seek": 289100, "start": 2891.0, "end": 2913.0, "text": " Then at some point in the future, someone changes it and now our code's broken. Right? So like an elm, it's like, yeah, yeah, I know, you know, this, but no, like, you've got to, you got to tell me how does this decode? I'm going to, I'm going to validate it. Okay. We validate it with the decoder. Now we know, now we know for sure. Right? Like, it's not just a, like you said, we've turned that implicit thing into the explicit.", "tokens": [1396, 412, 512, 935, 294, 264, 2027, 11, 1580, 2962, 309, 293, 586, 527, 3089, 311, 5463, 13, 1779, 30, 407, 411, 364, 806, 76, 11, 309, 311, 411, 11, 1338, 11, 1338, 11, 286, 458, 11, 291, 458, 11, 341, 11, 457, 572, 11, 411, 11, 291, 600, 658, 281, 11, 291, 658, 281, 980, 385, 577, 775, 341, 979, 1429, 30, 286, 478, 516, 281, 11, 286, 478, 516, 281, 29562, 309, 13, 1033, 13, 492, 29562, 309, 365, 264, 979, 19866, 13, 823, 321, 458, 11, 586, 321, 458, 337, 988, 13, 1779, 30, 1743, 11, 309, 311, 406, 445, 257, 11, 411, 291, 848, 11, 321, 600, 3574, 300, 26947, 551, 666, 264, 13691, 13], "temperature": 0.0, "avg_logprob": -0.22804965817831396, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.00011591750080697238}, {"id": 304, "seek": 291300, "start": 2913.0, "end": 2942.0, "text": " And so I think there's a lot of stuff with migrations. And part of why I was really excited about this is like, there's all this, there's all this cognitive overhead that you have to keep. Right? Because there's nothing snapshotting, nothing keeping your types for you. You have to remember when you're building this new thing, like exactly what you've changed, exactly what you've added, exactly what you've removed to the point where, like, I think it's, it's kind of nuts that the industry response seems to me, the industry standard response is like, oh, well, obviously the sensible solution is the one that's going to be the most effective.", "tokens": [400, 370, 286, 519, 456, 311, 257, 688, 295, 1507, 365, 6186, 12154, 13, 400, 644, 295, 983, 286, 390, 534, 2919, 466, 341, 307, 411, 11, 456, 311, 439, 341, 11, 456, 311, 439, 341, 15605, 19922, 300, 291, 362, 281, 1066, 13, 1779, 30, 1436, 456, 311, 1825, 30163, 783, 11, 1825, 5145, 428, 3467, 337, 291, 13, 509, 362, 281, 1604, 562, 291, 434, 2390, 341, 777, 551, 11, 411, 2293, 437, 291, 600, 3105, 11, 2293, 437, 291, 600, 3869, 11, 2293, 437, 291, 600, 7261, 281, 264, 935, 689, 11, 411, 11, 286, 519, 309, 311, 11, 309, 311, 733, 295, 10483, 300, 264, 3518, 4134, 2544, 281, 385, 11, 264, 3518, 3832, 4134, 307, 411, 11, 1954, 11, 731, 11, 2745, 264, 25380, 3827, 307, 264, 472, 300, 311, 516, 281, 312, 264, 881, 4942, 13], "temperature": 0.0, "avg_logprob": -0.31503071197091714, "compression_ratio": 1.877906976744186, "no_speech_prob": 0.00030060121207498014}, {"id": 305, "seek": 294200, "start": 2942.0, "end": 2971.0, "text": " The sensible solution is to never remove anything ever again. And that's like, I get it. Like, that's, that's probably the only way that we can cut that, at least that part of the problem out. Right. In like the traditional kind of stack, or if you're working at scale, right, you go, okay, we never remove everything, never deprecate it. That's why you have stuff like, you know, protobufs. So it's like, yeah, you've allocated a field. Well, that's there forever in your payloads, every future payload, even if you never use it, that byte range is now allocated. Right?", "tokens": [440, 25380, 3827, 307, 281, 1128, 4159, 1340, 1562, 797, 13, 400, 300, 311, 411, 11, 286, 483, 309, 13, 1743, 11, 300, 311, 11, 300, 311, 1391, 264, 787, 636, 300, 321, 393, 1723, 300, 11, 412, 1935, 300, 644, 295, 264, 1154, 484, 13, 1779, 13, 682, 411, 264, 5164, 733, 295, 8630, 11, 420, 498, 291, 434, 1364, 412, 4373, 11, 558, 11, 291, 352, 11, 1392, 11, 321, 1128, 4159, 1203, 11, 1128, 1367, 13867, 473, 309, 13, 663, 311, 983, 291, 362, 1507, 411, 11, 291, 458, 11, 1742, 996, 2947, 82, 13, 407, 309, 311, 411, 11, 1338, 11, 291, 600, 29772, 257, 2519, 13, 1042, 11, 300, 311, 456, 5680, 294, 428, 30918, 82, 11, 633, 2027, 30918, 11, 754, 498, 291, 1128, 764, 309, 11, 300, 40846, 3613, 307, 586, 29772, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.20446907984067317, "compression_ratio": 1.7955974842767295, "no_speech_prob": 0.002757241018116474}, {"id": 306, "seek": 297100, "start": 2971.0, "end": 2988.0, "text": " You'd have to do a major version deprecation or shift your, your schema to a new endpoint to change that or to optimize it. And so the other nice thing that we get there is that all of that stuff becomes explicit. But also, on the other hand, something I think we haven't talked about is that gap problem. Right?", "tokens": [509, 1116, 362, 281, 360, 257, 2563, 3037, 1367, 13867, 399, 420, 5513, 428, 11, 428, 34078, 281, 257, 777, 35795, 281, 1319, 300, 420, 281, 19719, 309, 13, 400, 370, 264, 661, 1481, 551, 300, 321, 483, 456, 307, 300, 439, 295, 300, 1507, 3643, 13691, 13, 583, 611, 11, 322, 264, 661, 1011, 11, 746, 286, 519, 321, 2378, 380, 2825, 466, 307, 300, 7417, 1154, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2198504638671875, "compression_ratio": 1.5219512195121951, "no_speech_prob": 0.00169996393378824}, {"id": 307, "seek": 298800, "start": 2988.0, "end": 3014.0, "text": " So I talk about in my talk, which is what if you have, if you don't have a system where everything's integrated together, and especially if you work at a company where potentially different teams can deploy different things at different times, right, you merge this change in for your backend, maybe if you're lucky, I mean, not if you're lucky, that's the wrong word. Let's say your company has chosen chosen to have a mono repo, maybe you've got one pull request, right? So the thing you merge is the front end and the backend changes at the same time.", "tokens": [407, 286, 751, 466, 294, 452, 751, 11, 597, 307, 437, 498, 291, 362, 11, 498, 291, 500, 380, 362, 257, 1185, 689, 1203, 311, 10919, 1214, 11, 293, 2318, 498, 291, 589, 412, 257, 2237, 689, 7263, 819, 5491, 393, 7274, 819, 721, 412, 819, 1413, 11, 558, 11, 291, 22183, 341, 1319, 294, 337, 428, 38087, 11, 1310, 498, 291, 434, 6356, 11, 286, 914, 11, 406, 498, 291, 434, 6356, 11, 300, 311, 264, 2085, 1349, 13, 961, 311, 584, 428, 2237, 575, 8614, 8614, 281, 362, 257, 35624, 49040, 11, 1310, 291, 600, 658, 472, 2235, 5308, 11, 558, 30, 407, 264, 551, 291, 22183, 307, 264, 1868, 917, 293, 264, 38087, 2962, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.2069485830882239, "compression_ratio": 1.8716216216216217, "no_speech_prob": 0.0015008654445409775}, {"id": 308, "seek": 301400, "start": 3014.0, "end": 3036.0, "text": " So at least that step is synchronized. But worst case, you have two different repositories, right? So those things could merge at different times. In both cases, regardless of whether you merge at the same time, or whether you merged at two different times, the deploy could still happen at two different times, right? And in most systems, they're not necessarily synchronized from a user perspective.", "tokens": [407, 412, 1935, 300, 1823, 307, 19331, 1602, 13, 583, 5855, 1389, 11, 291, 362, 732, 819, 22283, 2083, 11, 558, 30, 407, 729, 721, 727, 22183, 412, 819, 1413, 13, 682, 1293, 3331, 11, 10060, 295, 1968, 291, 22183, 412, 264, 912, 565, 11, 420, 1968, 291, 36427, 412, 732, 819, 1413, 11, 264, 7274, 727, 920, 1051, 412, 732, 819, 1413, 11, 558, 30, 400, 294, 881, 3652, 11, 436, 434, 406, 4725, 19331, 1602, 490, 257, 4195, 4585, 13], "temperature": 0.0, "avg_logprob": -0.19972404213838799, "compression_ratio": 1.7743362831858407, "no_speech_prob": 0.0014100187690928578}, {"id": 309, "seek": 303600, "start": 3036.0, "end": 3063.0, "text": " So it is very possible that you have a situation where you have version one front end and version one backend in production. And you get a version two front end, that's trying to talk to a version one backend for a moment in time, or you could have the version two backend launches faster. So you have version one front end that's trying to talk to a future version two backend. And that inverses as well, right?", "tokens": [407, 309, 307, 588, 1944, 300, 291, 362, 257, 2590, 689, 291, 362, 3037, 472, 1868, 917, 293, 3037, 472, 38087, 294, 4265, 13, 400, 291, 483, 257, 3037, 732, 1868, 917, 11, 300, 311, 1382, 281, 751, 281, 257, 3037, 472, 38087, 337, 257, 1623, 294, 565, 11, 420, 291, 727, 362, 264, 3037, 732, 38087, 31841, 4663, 13, 407, 291, 362, 3037, 472, 1868, 917, 300, 311, 1382, 281, 751, 281, 257, 2027, 3037, 732, 38087, 13, 400, 300, 21378, 279, 382, 731, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21626246493795645, "compression_ratio": 2.0497512437810945, "no_speech_prob": 0.00679410807788372}, {"id": 310, "seek": 306300, "start": 3063.0, "end": 3079.0, "text": " So you could have a version two backend that's trying to respond to a version one front end or a version two front end that's trying to respond back to a version one backend. So there's four variations there, right? There's the outbound payloads, and there's the inbound payloads, and you could have failures on both sides.", "tokens": [407, 291, 727, 362, 257, 3037, 732, 38087, 300, 311, 1382, 281, 4196, 281, 257, 3037, 472, 1868, 917, 420, 257, 3037, 732, 1868, 917, 300, 311, 1382, 281, 4196, 646, 281, 257, 3037, 472, 38087, 13, 407, 456, 311, 1451, 17840, 456, 11, 558, 30, 821, 311, 264, 484, 18767, 30918, 82, 11, 293, 456, 311, 264, 294, 18767, 30918, 82, 11, 293, 291, 727, 362, 20774, 322, 1293, 4881, 13], "temperature": 0.0, "avg_logprob": -0.17501923912449888, "compression_ratio": 1.98159509202454, "no_speech_prob": 0.002714660484343767}, {"id": 311, "seek": 307900, "start": 3079.0, "end": 3099.0, "text": " And I think generally, the thing that like, we kind of think about, okay, what tools or techniques can we apply to solve that? And I think generally, the answer is none. Right? Like there isn't really anything we can do to get explicit guarantees. One thing we can do is say we never remove fields, right? But we can still get the wrong semantics, right?", "tokens": [400, 286, 519, 5101, 11, 264, 551, 300, 411, 11, 321, 733, 295, 519, 466, 11, 1392, 11, 437, 3873, 420, 7512, 393, 321, 3079, 281, 5039, 300, 30, 400, 286, 519, 5101, 11, 264, 1867, 307, 6022, 13, 1779, 30, 1743, 456, 1943, 380, 534, 1340, 321, 393, 360, 281, 483, 13691, 32567, 13, 1485, 551, 321, 393, 360, 307, 584, 321, 1128, 4159, 7909, 11, 558, 30, 583, 321, 393, 920, 483, 264, 2085, 4361, 45298, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20749832602108226, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00043051395914517343}, {"id": 312, "seek": 309900, "start": 3099.0, "end": 3128.0, "text": " Like it may be compatible, that a version two front end message gets ingested by the version one backend, but it may process the wrong business logic, you know, stuff that we've said, or should now change, right? It should be version two. Version two should be handling, you know, strawberry responses coming in and becoming mango, but we've actually gotten, you know, a mango response in the future, or we've gotten a strawberry response in the future, and a version one backend has created yet another subscription to strawberry, which shouldn't be possible anymore.", "tokens": [1743, 309, 815, 312, 18218, 11, 300, 257, 3037, 732, 1868, 917, 3636, 2170, 3957, 21885, 538, 264, 3037, 472, 38087, 11, 457, 309, 815, 1399, 264, 2085, 1606, 9952, 11, 291, 458, 11, 1507, 300, 321, 600, 848, 11, 420, 820, 586, 1319, 11, 558, 30, 467, 820, 312, 3037, 732, 13, 35965, 732, 820, 312, 13175, 11, 291, 458, 11, 20440, 13019, 1348, 294, 293, 5617, 23481, 11, 457, 321, 600, 767, 5768, 11, 291, 458, 11, 257, 23481, 4134, 294, 264, 2027, 11, 420, 321, 600, 5768, 257, 20440, 4134, 294, 264, 2027, 11, 293, 257, 3037, 472, 38087, 575, 2942, 1939, 1071, 17231, 281, 20440, 11, 597, 4659, 380, 312, 1944, 3602, 13], "temperature": 0.0, "avg_logprob": -0.20249811282827834, "compression_ratio": 1.9653979238754324, "no_speech_prob": 0.0013664092402905226}, {"id": 313, "seek": 312800, "start": 3128.0, "end": 3156.0, "text": " So yeah, one thing. This is why I mentioned this before about, I think evergreen works really well in a system where you have full and total control of both the front end and the backend migration synchronicity, synchronicity, I'm not sure the correct pronunciation of that word. So that, you know, we apply this evergreen concepts to everything in lockstep. Right? So we know that you only there's never going to be a scenario where we're getting events from the future to older versions.", "tokens": [407, 1338, 11, 472, 551, 13, 639, 307, 983, 286, 2835, 341, 949, 466, 11, 286, 519, 1562, 27399, 1985, 534, 731, 294, 257, 1185, 689, 291, 362, 1577, 293, 3217, 1969, 295, 1293, 264, 1868, 917, 293, 264, 38087, 17011, 5451, 339, 10011, 507, 11, 5451, 339, 10011, 507, 11, 286, 478, 406, 988, 264, 3006, 23338, 295, 300, 1349, 13, 407, 300, 11, 291, 458, 11, 321, 3079, 341, 1562, 27399, 10392, 281, 1203, 294, 4017, 16792, 13, 1779, 30, 407, 321, 458, 300, 291, 787, 456, 311, 1128, 516, 281, 312, 257, 9005, 689, 321, 434, 1242, 3931, 490, 264, 2027, 281, 4906, 9606, 13], "temperature": 0.0, "avg_logprob": -0.23294891629900252, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.00045114499516785145}, {"id": 314, "seek": 315600, "start": 3156.0, "end": 3185.0, "text": " Everything's always being pushed forward. And so yeah, lambda has like, like a migration, kind of like a staging thing where it basically hot loads everything and everything's prepped and ready, all the new versions are ready, everything's live. And then there's like a sudden lockstep where everything in the same instant as much as possible, kind of all slides into the future. But if we've missed anything, you know, if there's any old front end that comes online later, or there's a backend that lags for some reason, or whatever, I mean, that's technically not possible.", "tokens": [5471, 311, 1009, 885, 9152, 2128, 13, 400, 370, 1338, 11, 13607, 575, 411, 11, 411, 257, 17011, 11, 733, 295, 411, 257, 41085, 551, 689, 309, 1936, 2368, 12668, 1203, 293, 1203, 311, 659, 3320, 293, 1919, 11, 439, 264, 777, 9606, 366, 1919, 11, 1203, 311, 1621, 13, 400, 550, 456, 311, 411, 257, 3990, 4017, 16792, 689, 1203, 294, 264, 912, 9836, 382, 709, 382, 1944, 11, 733, 295, 439, 9788, 666, 264, 2027, 13, 583, 498, 321, 600, 6721, 1340, 11, 291, 458, 11, 498, 456, 311, 604, 1331, 1868, 917, 300, 1487, 2950, 1780, 11, 420, 456, 311, 257, 38087, 300, 8953, 82, 337, 512, 1778, 11, 420, 2035, 11, 286, 914, 11, 300, 311, 12120, 406, 1944, 13], "temperature": 0.0, "avg_logprob": -0.20174145698547363, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.005216696299612522}, {"id": 315, "seek": 318500, "start": 3185.0, "end": 3208.0, "text": " Let's say in the future, we had, you know, distributed setup, and it was possible, it was a backend that was lagging, because that all those new versions have this migration thing set up, the first thing they do be like, Oh, I'm receiving a version one, but I'm actually on version two, let's run that through the migration first. And so now you always have consistent, like that's consistently being executed in the latest version, regardless of kind of what's at play in that synchronicity.", "tokens": [961, 311, 584, 294, 264, 2027, 11, 321, 632, 11, 291, 458, 11, 12631, 8657, 11, 293, 309, 390, 1944, 11, 309, 390, 257, 38087, 300, 390, 8953, 3249, 11, 570, 300, 439, 729, 777, 9606, 362, 341, 17011, 551, 992, 493, 11, 264, 700, 551, 436, 360, 312, 411, 11, 876, 11, 286, 478, 10040, 257, 3037, 472, 11, 457, 286, 478, 767, 322, 3037, 732, 11, 718, 311, 1190, 300, 807, 264, 17011, 700, 13, 400, 370, 586, 291, 1009, 362, 8398, 11, 411, 300, 311, 14961, 885, 17577, 294, 264, 6792, 3037, 11, 10060, 295, 733, 295, 437, 311, 412, 862, 294, 300, 5451, 339, 10011, 507, 13], "temperature": 0.0, "avg_logprob": -0.21384789840034815, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.00364856724627316}, {"id": 316, "seek": 320800, "start": 3208.0, "end": 3232.0, "text": " So I think that that is a really difficult problem to solve outside of the context of a type safe, pure, immutable and exhaustive language. And I think that's what makes this thing so nice. And I'm like evergreen in Elm, I think is super delightful. It just sheds all these delightful properties. And so yeah, we leverage that as much as possible as we can.", "tokens": [407, 286, 519, 300, 300, 307, 257, 534, 2252, 1154, 281, 5039, 2380, 295, 264, 4319, 295, 257, 2010, 3273, 11, 6075, 11, 3397, 32148, 293, 14687, 488, 2856, 13, 400, 286, 519, 300, 311, 437, 1669, 341, 551, 370, 1481, 13, 400, 286, 478, 411, 1562, 27399, 294, 2699, 76, 11, 286, 519, 307, 1687, 35194, 13, 467, 445, 402, 5147, 439, 613, 35194, 7221, 13, 400, 370, 1338, 11, 321, 13982, 300, 382, 709, 382, 1944, 382, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.2148919423421224, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007553567411378026}, {"id": 317, "seek": 320800, "start": 3232.0, "end": 3237.0, "text": " I don't think you would need the type safe parts, but it definitely helps.", "tokens": [286, 500, 380, 519, 291, 576, 643, 264, 2010, 3273, 3166, 11, 457, 309, 2138, 3665, 13], "temperature": 0.0, "avg_logprob": -0.2148919423421224, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007553567411378026}, {"id": 318, "seek": 323700, "start": 3237.0, "end": 3244.0, "text": " Yeah, it definitely you could with discipline get the same effect. But yeah, you could.", "tokens": [865, 11, 309, 2138, 291, 727, 365, 13635, 483, 264, 912, 1802, 13, 583, 1338, 11, 291, 727, 13], "temperature": 0.0, "avg_logprob": -0.36479854583740234, "compression_ratio": 1.1012658227848102, "no_speech_prob": 0.0012643313966691494}, {"id": 319, "seek": 324400, "start": 3244.0, "end": 3273.0, "text": " And it makes the contract more explicit. I find that like thinking about migrations in this paradigm, like it, it's making me think of the data modeling in a different way, as you said, like more, more focused on the ideal data modeling, which often just a vanilla LMAP makes us do this too, I think, right? Just think about the data modeling in a, I don't know, less hacky way, like just what would be the ideal way to model this?", "tokens": [400, 309, 1669, 264, 4364, 544, 13691, 13, 286, 915, 300, 411, 1953, 466, 6186, 12154, 294, 341, 24709, 11, 411, 309, 11, 309, 311, 1455, 385, 519, 295, 264, 1412, 15983, 294, 257, 819, 636, 11, 382, 291, 848, 11, 411, 544, 11, 544, 5178, 322, 264, 7157, 1412, 15983, 11, 597, 2049, 445, 257, 17528, 441, 44, 4715, 1669, 505, 360, 341, 886, 11, 286, 519, 11, 558, 30, 1449, 519, 466, 264, 1412, 15983, 294, 257, 11, 286, 500, 380, 458, 11, 1570, 10339, 88, 636, 11, 411, 445, 437, 576, 312, 264, 7157, 636, 281, 2316, 341, 30], "temperature": 0.0, "avg_logprob": -0.2226444460311026, "compression_ratio": 1.724, "no_speech_prob": 0.00041079422226175666}, {"id": 320, "seek": 327300, "start": 3273.0, "end": 3299.0, "text": " And then you just kind of do that and, and let everything flow from there. And like, for the, you know, strawberry ice cream deprecation, like I was talking about that strategy, you could use of, you know, leaving the users flavor, selected flavors as mango, and then having some separate data that tracks that it's actually, it's actually not mango, right?", "tokens": [400, 550, 291, 445, 733, 295, 360, 300, 293, 11, 293, 718, 1203, 3095, 490, 456, 13, 400, 411, 11, 337, 264, 11, 291, 458, 11, 20440, 4435, 4689, 1367, 13867, 399, 11, 411, 286, 390, 1417, 466, 300, 5206, 11, 291, 727, 764, 295, 11, 291, 458, 11, 5012, 264, 5022, 6813, 11, 8209, 16303, 382, 23481, 11, 293, 550, 1419, 512, 4994, 1412, 300, 10218, 300, 309, 311, 767, 11, 309, 311, 767, 406, 23481, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2271358399164109, "compression_ratio": 1.6527777777777777, "no_speech_prob": 0.0009999850299209356}, {"id": 321, "seek": 329900, "start": 3299.0, "end": 3315.0, "text": " And if you're doing, if you're doing a sort of migration in this more old school way, where you have these implicit contracts, then that that's not really any worse than modeling, modeling it explicitly.", "tokens": [400, 498, 291, 434, 884, 11, 498, 291, 434, 884, 257, 1333, 295, 17011, 294, 341, 544, 1331, 1395, 636, 11, 689, 291, 362, 613, 26947, 13952, 11, 550, 300, 300, 311, 406, 534, 604, 5324, 813, 15983, 11, 15983, 309, 20803, 13], "temperature": 0.0, "avg_logprob": -0.20362709938211643, "compression_ratio": 1.4710144927536233, "no_speech_prob": 0.0035933987237513065}, {"id": 322, "seek": 331500, "start": 3315.0, "end": 3336.0, "text": " But if you, if you have the ability to have all these pieces, connects together very explicitly, I would tend to think of it differently, I would tend to think of it as I want my data to reflect exactly what it is. So I would tend to want to say, you know, maybe it's like a variant of flavors, or maybe it's like a wrapper around that.", "tokens": [583, 498, 291, 11, 498, 291, 362, 264, 3485, 281, 362, 439, 613, 3755, 11, 16967, 1214, 588, 20803, 11, 286, 576, 3928, 281, 519, 295, 309, 7614, 11, 286, 576, 3928, 281, 519, 295, 309, 382, 286, 528, 452, 1412, 281, 5031, 2293, 437, 309, 307, 13, 407, 286, 576, 3928, 281, 528, 281, 584, 11, 291, 458, 11, 1310, 309, 311, 411, 257, 17501, 295, 16303, 11, 420, 1310, 309, 311, 411, 257, 46906, 926, 300, 13], "temperature": 0.0, "avg_logprob": -0.1741293022431523, "compression_ratio": 1.7409326424870466, "no_speech_prob": 0.00018234970048069954}, {"id": 323, "seek": 333600, "start": 3336.0, "end": 3356.0, "text": " So you have, you either have like a selected flavor, or you have a legacy selection, in which case, shipments can't proceed or whatever. But I would want to model that state very explicitly, because otherwise, you might end up with a code path where you aren't considering that case.", "tokens": [407, 291, 362, 11, 291, 2139, 362, 411, 257, 8209, 6813, 11, 420, 291, 362, 257, 11711, 9450, 11, 294, 597, 1389, 11, 5374, 1117, 393, 380, 8991, 420, 2035, 13, 583, 286, 576, 528, 281, 2316, 300, 1785, 588, 20803, 11, 570, 5911, 11, 291, 1062, 917, 493, 365, 257, 3089, 3100, 689, 291, 3212, 380, 8079, 300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.18635950822096606, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.0005112510989420116}, {"id": 324, "seek": 335600, "start": 3356.0, "end": 3373.0, "text": " And what ends up happening all the time with sort of supporting these, these migrations, and these conceptual changes to the domain, in a code base is you don't consider how a change affects the entire system.", "tokens": [400, 437, 5314, 493, 2737, 439, 264, 565, 365, 1333, 295, 7231, 613, 11, 613, 6186, 12154, 11, 293, 613, 24106, 2962, 281, 264, 9274, 11, 294, 257, 3089, 3096, 307, 291, 500, 380, 1949, 577, 257, 1319, 11807, 264, 2302, 1185, 13], "temperature": 0.0, "avg_logprob": -0.2327037161969124, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.001115954713895917}, {"id": 325, "seek": 337300, "start": 3373.0, "end": 3396.0, "text": " You end up with all these sort of conditionals scattered around where you could easily forget something. But as with any Elm custom type, you have, you know, when some when an Elm custom type changes, it forces you to consider the impact of that change everywhere, even if it's trivial, it forces you to explicitly, you know, recognize that it's changed.", "tokens": [509, 917, 493, 365, 439, 613, 1333, 295, 4188, 1124, 21986, 926, 689, 291, 727, 3612, 2870, 746, 13, 583, 382, 365, 604, 2699, 76, 2375, 2010, 11, 291, 362, 11, 291, 458, 11, 562, 512, 562, 364, 2699, 76, 2375, 2010, 2962, 11, 309, 5874, 291, 281, 1949, 264, 2712, 295, 300, 1319, 5315, 11, 754, 498, 309, 311, 26703, 11, 309, 5874, 291, 281, 20803, 11, 291, 458, 11, 5521, 300, 309, 311, 3105, 13], "temperature": 0.0, "avg_logprob": -0.23381405112184125, "compression_ratio": 1.7184466019417475, "no_speech_prob": 8.219545998144895e-05}, {"id": 326, "seek": 339600, "start": 3396.0, "end": 3425.0, "text": " And so instead of just scattering some conditionals around and saying, Oh, if it's this weird, extra Boolean that I need to check for that it's this weird case, like that ends up being really unpleasant to maintain and a huge source of bugs and also a huge source of like, you know, the more veteran programmers on that team who know this code base are like, Oh, like, talk to talk to this one programmer, you know, they know all the ins and outs of this code base,", "tokens": [400, 370, 2602, 295, 445, 42314, 512, 4188, 1124, 926, 293, 1566, 11, 876, 11, 498, 309, 311, 341, 3657, 11, 2857, 23351, 28499, 300, 286, 643, 281, 1520, 337, 300, 309, 311, 341, 3657, 1389, 11, 411, 300, 5314, 493, 885, 534, 29128, 281, 6909, 293, 257, 2603, 4009, 295, 15120, 293, 611, 257, 2603, 4009, 295, 411, 11, 291, 458, 11, 264, 544, 18324, 41504, 322, 300, 1469, 567, 458, 341, 3089, 3096, 366, 411, 11, 876, 11, 411, 11, 751, 281, 751, 281, 341, 472, 32116, 11, 291, 458, 11, 436, 458, 439, 264, 1028, 293, 14758, 295, 341, 3089, 3096, 11], "temperature": 0.0, "avg_logprob": -0.23704381820258744, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.0034291627816855907}, {"id": 327, "seek": 342500, "start": 3425.0, "end": 3453.0, "text": " they know all the conditionals you need to be sure to check for, they know all the strange Booleans in the system that you need to check for anytime you do something. But if you model it as a custom type where you're saying exactly what it is, like a user doesn't necessarily have a flavor, they may have some legacy thing. And why does that happen? And maybe, maybe you get to a point where you can drop that at a certain point, maybe you, you have, you are able to confirm that everybody has migrated off of that.", "tokens": [436, 458, 439, 264, 4188, 1124, 291, 643, 281, 312, 988, 281, 1520, 337, 11, 436, 458, 439, 264, 5861, 23351, 24008, 294, 264, 1185, 300, 291, 643, 281, 1520, 337, 13038, 291, 360, 746, 13, 583, 498, 291, 2316, 309, 382, 257, 2375, 2010, 689, 291, 434, 1566, 2293, 437, 309, 307, 11, 411, 257, 4195, 1177, 380, 4725, 362, 257, 6813, 11, 436, 815, 362, 512, 11711, 551, 13, 400, 983, 775, 300, 1051, 30, 400, 1310, 11, 1310, 291, 483, 281, 257, 935, 689, 291, 393, 3270, 300, 412, 257, 1629, 935, 11, 1310, 291, 11, 291, 362, 11, 291, 366, 1075, 281, 9064, 300, 2201, 575, 48329, 766, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.20101257131881073, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0012065224582329392}, {"id": 328, "seek": 345300, "start": 3453.0, "end": 3466.0, "text": " And then you can migrate off of that and, and reflect that. But it really allows you to think in terms of your ideal domain modeling instead of hacking something together and throwing some conditionals in there.", "tokens": [400, 550, 291, 393, 31821, 766, 295, 300, 293, 11, 293, 5031, 300, 13, 583, 309, 534, 4045, 291, 281, 519, 294, 2115, 295, 428, 7157, 9274, 15983, 2602, 295, 31422, 746, 1214, 293, 10238, 512, 4188, 1124, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.20347249772813586, "compression_ratio": 1.4859154929577465, "no_speech_prob": 0.002396616619080305}, {"id": 329, "seek": 346600, "start": 3466.0, "end": 3495.0, "text": " Yeah, absolutely. So this is this is slightly a tangent away from evergreen. And I kind of see this as more a benefit of Lambdaera itself, or at least a benefit of the idea of like, what if we didn't have this disconnection between the way that we choose to store our data and the way that we choose to model our data? Right. And so like, if you're already familiar with Elm, and you have experienced like the delight of Elm's type system in, in the large, right, I reckon in most,", "tokens": [865, 11, 3122, 13, 407, 341, 307, 341, 307, 4748, 257, 27747, 1314, 490, 1562, 27399, 13, 400, 286, 733, 295, 536, 341, 382, 544, 257, 5121, 295, 45691, 1663, 2564, 11, 420, 412, 1935, 257, 5121, 295, 264, 1558, 295, 411, 11, 437, 498, 321, 994, 380, 362, 341, 14299, 313, 1296, 264, 636, 300, 321, 2826, 281, 3531, 527, 1412, 293, 264, 636, 300, 321, 2826, 281, 2316, 527, 1412, 30, 1779, 13, 400, 370, 411, 11, 498, 291, 434, 1217, 4963, 365, 2699, 76, 11, 293, 291, 362, 6751, 411, 264, 11627, 295, 2699, 76, 311, 2010, 1185, 294, 11, 294, 264, 2416, 11, 558, 11, 286, 29548, 294, 881, 11], "temperature": 0.0, "avg_logprob": -0.2252624964309951, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.00025308504700660706}, {"id": 330, "seek": 349500, "start": 3495.0, "end": 3516.0, "text": " cases, you can model, like your view of the world really nicely, right, especially like with custom types. And so there's this, I think, when you're in a more traditional code base, I think, at least the way that I'm thinking of more traditional code base is like, I might model those invariants with a new custom type on the front end.", "tokens": [3331, 11, 291, 393, 2316, 11, 411, 428, 1910, 295, 264, 1002, 534, 9594, 11, 558, 11, 2318, 411, 365, 2375, 3467, 13, 400, 370, 456, 311, 341, 11, 286, 519, 11, 562, 291, 434, 294, 257, 544, 5164, 3089, 3096, 11, 286, 519, 11, 412, 1935, 264, 636, 300, 286, 478, 1953, 295, 544, 5164, 3089, 3096, 307, 411, 11, 286, 1062, 2316, 729, 33270, 1719, 365, 257, 777, 2375, 2010, 322, 264, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.19949872994128567, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.001432151417247951}, {"id": 331, "seek": 351600, "start": 3516.0, "end": 3540.0, "text": " But usually I start to, like, especially for something like this, right, where we go, okay, we're discontinuing this product, right? The marketing team has said, like, could we, you know, could we do this, like, it would make our lives easier. Could you do this stuff in the UI, but you know, like, if it's going to take more than a day, and it's going to impact our backlog, like, we just don't, like, just don't bother, we'll just manually send some emails, or we'll do something, right?", "tokens": [583, 2673, 286, 722, 281, 11, 411, 11, 2318, 337, 746, 411, 341, 11, 558, 11, 689, 321, 352, 11, 1392, 11, 321, 434, 31420, 9635, 341, 1674, 11, 558, 30, 440, 6370, 1469, 575, 848, 11, 411, 11, 727, 321, 11, 291, 458, 11, 727, 321, 360, 341, 11, 411, 11, 309, 576, 652, 527, 2909, 3571, 13, 7497, 291, 360, 341, 1507, 294, 264, 15682, 11, 457, 291, 458, 11, 411, 11, 498, 309, 311, 516, 281, 747, 544, 813, 257, 786, 11, 293, 309, 311, 516, 281, 2712, 527, 47364, 11, 411, 11, 321, 445, 500, 380, 11, 411, 11, 445, 500, 380, 8677, 11, 321, 603, 445, 16945, 2845, 512, 12524, 11, 420, 321, 603, 360, 746, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20752350986003876, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.005467136390507221}, {"id": 332, "seek": 354000, "start": 3540.0, "end": 3559.0, "text": " Like, because engineering time, I think, becomes scarce and precious in organizations, usually. So, you know, if you sit down, you're like, okay, well, yeah, you know, we could try and manage this for you. Okay, front end, easy, that's fine. We'll just put this new state and we'll do that. But, okay, so we're going to change the profile system in the backend, we're gonna have to add this new field, we're gonna have to do that migration.", "tokens": [1743, 11, 570, 7043, 565, 11, 286, 519, 11, 3643, 41340, 293, 12406, 294, 6150, 11, 2673, 13, 407, 11, 291, 458, 11, 498, 291, 1394, 760, 11, 291, 434, 411, 11, 1392, 11, 731, 11, 1338, 11, 291, 458, 11, 321, 727, 853, 293, 3067, 341, 337, 291, 13, 1033, 11, 1868, 917, 11, 1858, 11, 300, 311, 2489, 13, 492, 603, 445, 829, 341, 777, 1785, 293, 321, 603, 360, 300, 13, 583, 11, 1392, 11, 370, 321, 434, 516, 281, 1319, 264, 7964, 1185, 294, 264, 38087, 11, 321, 434, 799, 362, 281, 909, 341, 777, 2519, 11, 321, 434, 799, 362, 281, 360, 300, 17011, 13], "temperature": 0.0, "avg_logprob": -0.22930476540013364, "compression_ratio": 1.7120622568093384, "no_speech_prob": 0.007230780553072691}, {"id": 333, "seek": 355900, "start": 3559.0, "end": 3587.0, "text": " But this other system uses that way to make sure, you know, like, and you're thinking about all these layers and the different systems that you're gonna have to coordinate. Whereas in Lambdaera, you can kind of go, okay, let's add this field to the thing. And bam, we forced all these failures across the, you know, we can see immediately what the impact is, like, we're driving, what is our to do list in terms of implementing this feature. And we can see straight away, like, oh, yeah, you know what, we forgot, this is used in this module, and it causes heaps of, you know what, guys, this isn't going to be worth it.", "tokens": [583, 341, 661, 1185, 4960, 300, 636, 281, 652, 988, 11, 291, 458, 11, 411, 11, 293, 291, 434, 1953, 466, 439, 613, 7914, 293, 264, 819, 3652, 300, 291, 434, 799, 362, 281, 15670, 13, 13813, 294, 45691, 1663, 11, 291, 393, 733, 295, 352, 11, 1392, 11, 718, 311, 909, 341, 2519, 281, 264, 551, 13, 400, 18132, 11, 321, 7579, 439, 613, 20774, 2108, 264, 11, 291, 458, 11, 321, 393, 536, 4258, 437, 264, 2712, 307, 11, 411, 11, 321, 434, 4840, 11, 437, 307, 527, 281, 360, 1329, 294, 2115, 295, 18114, 341, 4111, 13, 400, 321, 393, 536, 2997, 1314, 11, 411, 11, 1954, 11, 1338, 11, 291, 458, 437, 11, 321, 5298, 11, 341, 307, 1143, 294, 341, 10088, 11, 293, 309, 7700, 415, 2382, 295, 11, 291, 458, 437, 11, 1074, 11, 341, 1943, 380, 516, 281, 312, 3163, 309, 13], "temperature": 0.0, "avg_logprob": -0.2505948933121426, "compression_ratio": 1.8452380952380953, "no_speech_prob": 0.0046089948154985905}, {"id": 334, "seek": 358700, "start": 3587.0, "end": 3617.0, "text": " Or we can say, oh, I got two type errors, like, no, I reckon we can do this, you know, and you're not thinking about all these extra steps about how that change or set of changes is going to be translated into these kind of primitives, like into this primitive obsession that we talk about, or like, you know, Boolean blindness, or, you know, just this generally this idea where we have to dumb down to types, which we kind of, I mean, until we are graced with this much anticipated, you know,", "tokens": [50364, 1610, 321, 393, 584, 11, 1954, 11, 286, 658, 732, 2010, 13603, 11, 411, 11, 572, 11, 286, 29548, 321, 393, 360, 341, 11, 291, 458, 11, 293, 291, 434, 406, 1953, 466, 439, 613, 2857, 4439, 466, 577, 300, 1319, 420, 992, 295, 2962, 307, 516, 281, 312, 16805, 666, 613, 733, 295, 2886, 38970, 11, 411, 666, 341, 28540, 30521, 300, 321, 751, 466, 11, 420, 411, 11, 291, 458, 11, 23351, 28499, 46101, 11, 420, 11, 291, 458, 11, 445, 341, 5101, 341, 1558, 689, 321, 362, 281, 10316, 760, 281, 3467, 11, 597, 321, 733, 295, 11, 286, 914, 11, 1826, 321, 366, 677, 3839, 365, 341, 709, 23267, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.25784224822741597, "compression_ratio": 1.8191881918819188, "no_speech_prob": 0.000743623124435544}, {"id": 335, "seek": 361700, "start": 3617.0, "end": 3646.0, "text": " anticipated future release of Evan's work and whatever's happening in the database side, right, but at the moment, there isn't a really great way to put custom types natively into Postgres, for example, right, so on the project, so I work on those elements, the front end, your custom types become something entirely different, there's a lot of glue around modeling that, and modeling those changes. And so I think a lot of the time, doing silly little things like that, it's kind of like, ah, too hard basket, let's not bother trying to, let's not mess with the stack.", "tokens": [23267, 2027, 4374, 295, 22613, 311, 589, 293, 2035, 311, 2737, 294, 264, 8149, 1252, 11, 558, 11, 457, 412, 264, 1623, 11, 456, 1943, 380, 257, 534, 869, 636, 281, 829, 2375, 3467, 8470, 356, 666, 10223, 45189, 11, 337, 1365, 11, 558, 11, 370, 322, 264, 1716, 11, 370, 286, 589, 322, 729, 4959, 11, 264, 1868, 917, 11, 428, 2375, 3467, 1813, 746, 7696, 819, 11, 456, 311, 257, 688, 295, 8998, 926, 15983, 300, 11, 293, 15983, 729, 2962, 13, 400, 370, 286, 519, 257, 688, 295, 264, 565, 11, 884, 11774, 707, 721, 411, 300, 11, 309, 311, 733, 295, 411, 11, 3716, 11, 886, 1152, 8390, 11, 718, 311, 406, 8677, 1382, 281, 11, 718, 311, 406, 2082, 365, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.27722497419877484, "compression_ratio": 1.7561728395061729, "no_speech_prob": 0.4996773898601532}, {"id": 336, "seek": 364600, "start": 3646.0, "end": 3674.0, "text": " Whereas I think in the Lambda situation, it's like, yeah, let's mess with the stack. Let's actively mess with it, because, you know, later on, the compiler is going to be like, all right, cool. I saw you messed with XYZ, can you please fix that? Tell me what you want to do with the migration. So I think that, yeah, that's a cool part. I'm excited for that on my own projects, because that's what I want. You know, when I'm dealing with my hobby projects, or I'm picking up a project I haven't touched for months. And I'm like, I want to add all this and change this and do that.", "tokens": [13813, 286, 519, 294, 264, 45691, 2590, 11, 309, 311, 411, 11, 1338, 11, 718, 311, 2082, 365, 264, 8630, 13, 961, 311, 13022, 2082, 365, 309, 11, 570, 11, 291, 458, 11, 1780, 322, 11, 264, 31958, 307, 516, 281, 312, 411, 11, 439, 558, 11, 1627, 13, 286, 1866, 291, 16507, 365, 48826, 57, 11, 393, 291, 1767, 3191, 300, 30, 5115, 385, 437, 291, 528, 281, 360, 365, 264, 17011, 13, 407, 286, 519, 300, 11, 1338, 11, 300, 311, 257, 1627, 644, 13, 286, 478, 2919, 337, 300, 322, 452, 1065, 4455, 11, 570, 300, 311, 437, 286, 528, 13, 509, 458, 11, 562, 286, 478, 6260, 365, 452, 18240, 4455, 11, 420, 286, 478, 8867, 493, 257, 1716, 286, 2378, 380, 9828, 337, 2493, 13, 400, 286, 478, 411, 11, 286, 528, 281, 909, 439, 341, 293, 1319, 341, 293, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.22375653919420743, "compression_ratio": 1.8125, "no_speech_prob": 6.013589518261142e-05}, {"id": 337, "seek": 367400, "start": 3674.0, "end": 3698.0, "text": " I want to have that joy of not now worrying like, oh, is my modeling correct? Do I need to fix how I've set my stuff up in Postgres? Am I going to have SQL queries somewhere that are now wrong? Yeah, I really like what that gives you from whatever green gives you, whatever green ideology kind of gives you in that kind of full stack Elm context. So yeah, and Lambdaera pitch rant at this point.", "tokens": [286, 528, 281, 362, 300, 6258, 295, 406, 586, 18788, 411, 11, 1954, 11, 307, 452, 15983, 3006, 30, 1144, 286, 643, 281, 3191, 577, 286, 600, 992, 452, 1507, 493, 294, 10223, 45189, 30, 2012, 286, 516, 281, 362, 19200, 24109, 4079, 300, 366, 586, 2085, 30, 865, 11, 286, 534, 411, 437, 300, 2709, 291, 490, 2035, 3092, 2709, 291, 11, 2035, 3092, 23101, 733, 295, 2709, 291, 294, 300, 733, 295, 1577, 8630, 2699, 76, 4319, 13, 407, 1338, 11, 293, 45691, 1663, 7293, 45332, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.26315043369929, "compression_ratio": 1.5863453815261044, "no_speech_prob": 0.010012147016823292}, {"id": 338, "seek": 369800, "start": 3698.0, "end": 3726.0, "text": " No, it's amazing. And Jeroen mentioned the idea of testing before, like that also strikes me as something that, you know, I mean, for a team that really, really wants to robustly manage their migrations and data integrity and all these sorts of things, just having evergreen migrations is huge. And, like, you know, makes the whole process so much more explicit.", "tokens": [883, 11, 309, 311, 2243, 13, 400, 508, 2032, 268, 2835, 264, 1558, 295, 4997, 949, 11, 411, 300, 611, 16750, 385, 382, 746, 300, 11, 291, 458, 11, 286, 914, 11, 337, 257, 1469, 300, 534, 11, 534, 2738, 281, 13956, 356, 3067, 641, 6186, 12154, 293, 1412, 16000, 293, 439, 613, 7527, 295, 721, 11, 445, 1419, 1562, 27399, 6186, 12154, 307, 2603, 13, 400, 11, 411, 11, 291, 458, 11, 1669, 264, 1379, 1399, 370, 709, 544, 13691, 13], "temperature": 0.0, "avg_logprob": -0.23351582815480787, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.01566195674240589}, {"id": 339, "seek": 372600, "start": 3726.0, "end": 3754.0, "text": " And you could probably write some sort of tests manually for that. But I could imagine some sort of automated things around even like testing the UI after a migration with, you know, Lambdaera program tests or, you know, testing the hot swapping. There are so many things you could imagine conceptually when you just have these pieces fitting together in this way. So it's really intriguing, really exciting stuff.", "tokens": [400, 291, 727, 1391, 2464, 512, 1333, 295, 6921, 16945, 337, 300, 13, 583, 286, 727, 3811, 512, 1333, 295, 18473, 721, 926, 754, 411, 4997, 264, 15682, 934, 257, 17011, 365, 11, 291, 458, 11, 45691, 1663, 1461, 6921, 420, 11, 291, 458, 11, 4997, 264, 2368, 1693, 10534, 13, 821, 366, 370, 867, 721, 291, 727, 3811, 3410, 671, 562, 291, 445, 362, 613, 3755, 15669, 1214, 294, 341, 636, 13, 407, 309, 311, 534, 32503, 11, 534, 4670, 1507, 13], "temperature": 0.0, "avg_logprob": -0.22956462290095186, "compression_ratio": 1.6967213114754098, "no_speech_prob": 0.00150075473356992}, {"id": 340, "seek": 375400, "start": 3754.0, "end": 3777.0, "text": " Yeah, definitely. So that kind of thing, like being able to test migrations is not something that's easily done today in Lambdaera. Like if you're in Lambdaera live, like the live development environment locally, and you've changed all your types, like you'll be working on an app with the latest version. Right. But there's not a super easy way. You can with some effort do it manually. And I've helped people try and figure that out. Like the pieces are there, but it's not ergonomic.", "tokens": [865, 11, 2138, 13, 407, 300, 733, 295, 551, 11, 411, 885, 1075, 281, 1500, 6186, 12154, 307, 406, 746, 300, 311, 3612, 1096, 965, 294, 45691, 1663, 13, 1743, 498, 291, 434, 294, 45691, 1663, 1621, 11, 411, 264, 1621, 3250, 2823, 16143, 11, 293, 291, 600, 3105, 439, 428, 3467, 11, 411, 291, 603, 312, 1364, 322, 364, 724, 365, 264, 6792, 3037, 13, 1779, 13, 583, 456, 311, 406, 257, 1687, 1858, 636, 13, 509, 393, 365, 512, 4630, 360, 309, 16945, 13, 400, 286, 600, 4254, 561, 853, 293, 2573, 300, 484, 13, 1743, 264, 3755, 366, 456, 11, 457, 309, 311, 406, 42735, 21401, 13], "temperature": 0.0, "avg_logprob": -0.20307355178029915, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.0033703919034451246}, {"id": 341, "seek": 377700, "start": 3777.0, "end": 3789.0, "text": " But yeah, I think that would be really, really cool in future for you to be like, hey, I want to pull down my production model and run it through the migration. And then I want to play with that result locally and see what that looks like. So that would be one cool improvement.", "tokens": [583, 1338, 11, 286, 519, 300, 576, 312, 534, 11, 534, 1627, 294, 2027, 337, 291, 281, 312, 411, 11, 4177, 11, 286, 528, 281, 2235, 760, 452, 4265, 2316, 293, 1190, 309, 807, 264, 17011, 13, 400, 550, 286, 528, 281, 862, 365, 300, 1874, 16143, 293, 536, 437, 300, 1542, 411, 13, 407, 300, 576, 312, 472, 1627, 10444, 13], "temperature": 0.0, "avg_logprob": -0.17082517797296698, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.005136922467499971}, {"id": 342, "seek": 378900, "start": 3789.0, "end": 3815.0, "text": " Another one that, and so this is kind of like a full disclosure on a downside of this approach. So something that's kind of come up recently, there's been more, increasingly more teams using Lambdaera. And so when you have a team of people using or trying to develop an app together, we end up with this problem of, you know, it's kind of fairly common if you're working on a backlog and you have a few features and these teams are like kind of in an agency setting.", "tokens": [3996, 472, 300, 11, 293, 370, 341, 307, 733, 295, 411, 257, 1577, 30392, 322, 257, 25060, 295, 341, 3109, 13, 407, 746, 300, 311, 733, 295, 808, 493, 3938, 11, 456, 311, 668, 544, 11, 12980, 544, 5491, 1228, 45691, 1663, 13, 400, 370, 562, 291, 362, 257, 1469, 295, 561, 1228, 420, 1382, 281, 1499, 364, 724, 1214, 11, 321, 917, 493, 365, 341, 1154, 295, 11, 291, 458, 11, 309, 311, 733, 295, 6457, 2689, 498, 291, 434, 1364, 322, 257, 47364, 293, 291, 362, 257, 1326, 4122, 293, 613, 5491, 366, 411, 733, 295, 294, 364, 7934, 3287, 13], "temperature": 0.0, "avg_logprob": -0.2087378635584751, "compression_ratio": 1.713235294117647, "no_speech_prob": 5.3046594985062256e-05}, {"id": 343, "seek": 381500, "start": 3815.0, "end": 3829.0, "text": " Right. So they're working on behalf of a customer. So they go, okay, well, you know, I've got a pull request for this particular feature. Can I deploy like a preview version of this app so we can kind of show the customer we can do some QA, right. Or some kind of review so they can take a look at this.", "tokens": [1779, 13, 407, 436, 434, 1364, 322, 9490, 295, 257, 5474, 13, 407, 436, 352, 11, 1392, 11, 731, 11, 291, 458, 11, 286, 600, 658, 257, 2235, 5308, 337, 341, 1729, 4111, 13, 1664, 286, 7274, 411, 257, 14281, 3037, 295, 341, 724, 370, 321, 393, 733, 295, 855, 264, 5474, 321, 393, 360, 512, 1249, 32, 11, 558, 13, 1610, 512, 733, 295, 3131, 370, 436, 393, 747, 257, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.22819463512565516, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.0014548886101692915}, {"id": 344, "seek": 382900, "start": 3829.0, "end": 3848.0, "text": " Now this has caused an interesting problem with Evergreen or some confusion, because if we think about the Evergreen assumption, right, the assumption is that you've got a straight linear change version one to two to three to four. And the idea is that when you're doing a check, what you're checking against is the production app.", "tokens": [823, 341, 575, 7008, 364, 1880, 1154, 365, 12123, 27399, 420, 512, 15075, 11, 570, 498, 321, 519, 466, 264, 12123, 27399, 15302, 11, 558, 11, 264, 15302, 307, 300, 291, 600, 658, 257, 2997, 8213, 1319, 3037, 472, 281, 732, 281, 1045, 281, 1451, 13, 400, 264, 1558, 307, 300, 562, 291, 434, 884, 257, 1520, 11, 437, 291, 434, 8568, 1970, 307, 264, 4265, 724, 13], "temperature": 0.0, "avg_logprob": -0.20374490155114067, "compression_ratio": 1.6225490196078431, "no_speech_prob": 4.985751729691401e-05}, {"id": 345, "seek": 384800, "start": 3848.0, "end": 3863.0, "text": " The being the operative word there as in the singular the, because if you have multiple production apps, suddenly everything starts to not make sense. You know, if you've changed your types, you've changed it relative to which production app.", "tokens": [440, 885, 264, 2208, 1166, 1349, 456, 382, 294, 264, 20010, 264, 11, 570, 498, 291, 362, 3866, 4265, 7733, 11, 5800, 1203, 3719, 281, 406, 652, 2020, 13, 509, 458, 11, 498, 291, 600, 3105, 428, 3467, 11, 291, 600, 3105, 309, 4972, 281, 597, 4265, 724, 13], "temperature": 0.0, "avg_logprob": -0.20381142958155218, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.00043047714279964566}, {"id": 346, "seek": 386300, "start": 3863.0, "end": 3879.0, "text": " So we kind of had this issue where teams would go to create, they would just manually create another app. They would manually call it, you know, my app dash preview one, some feature. And then first things first, they would try to deploy their Lambda app that was only previously going to production.", "tokens": [407, 321, 733, 295, 632, 341, 2734, 689, 5491, 576, 352, 281, 1884, 11, 436, 576, 445, 16945, 1884, 1071, 724, 13, 814, 576, 16945, 818, 309, 11, 291, 458, 11, 452, 724, 8240, 14281, 472, 11, 512, 4111, 13, 400, 550, 700, 721, 700, 11, 436, 576, 853, 281, 7274, 641, 45691, 724, 300, 390, 787, 8046, 516, 281, 4265, 13], "temperature": 0.0, "avg_logprob": -0.24047816883433948, "compression_ratio": 1.5873015873015872, "no_speech_prob": 7.527226898673689e-06}, {"id": 347, "seek": 387900, "start": 3879.0, "end": 3893.0, "text": " Let's say it's at version seven. So they tried to deploy to this new app and the app goes, well, hold on, you're deploying version one, but I'm seeing like version seven snapshots, like what's going on? Right? So the compulsion there, I think the natural thing is to be like, oh, that's weird.", "tokens": [961, 311, 584, 309, 311, 412, 3037, 3407, 13, 407, 436, 3031, 281, 7274, 281, 341, 777, 724, 293, 264, 724, 1709, 11, 731, 11, 1797, 322, 11, 291, 434, 34198, 3037, 472, 11, 457, 286, 478, 2577, 411, 3037, 3407, 19206, 27495, 11, 411, 437, 311, 516, 322, 30, 1779, 30, 407, 264, 715, 22973, 456, 11, 286, 519, 264, 3303, 551, 307, 281, 312, 411, 11, 1954, 11, 300, 311, 3657, 13], "temperature": 0.0, "avg_logprob": -0.24401148160298666, "compression_ratio": 1.8262295081967213, "no_speech_prob": 1.2217483345011715e-05}, {"id": 348, "seek": 387900, "start": 3893.0, "end": 3906.0, "text": " I don't know what to do about this. I'm just going to delete Evergreen folder. And now it goes, okay, great, cool. I can deploy version one for you. Right? So now you've got this code base that's deployed to version seven and to version one in two different apps.", "tokens": [286, 500, 380, 458, 437, 281, 360, 466, 341, 13, 286, 478, 445, 516, 281, 12097, 12123, 27399, 10820, 13, 400, 586, 309, 1709, 11, 1392, 11, 869, 11, 1627, 13, 286, 393, 7274, 3037, 472, 337, 291, 13, 1779, 30, 407, 586, 291, 600, 658, 341, 3089, 3096, 300, 311, 17826, 281, 3037, 3407, 293, 281, 3037, 472, 294, 732, 819, 7733, 13], "temperature": 0.0, "avg_logprob": -0.24401148160298666, "compression_ratio": 1.8262295081967213, "no_speech_prob": 1.2217483345011715e-05}, {"id": 349, "seek": 390600, "start": 3906.0, "end": 3918.0, "text": " Now, a Lambdaera doesn't have a preview app concept right now. So as far as Evergreen is concerned, there's two production apps. Right? So let's say throughout the course of your pull request, you change types.", "tokens": [823, 11, 257, 45691, 1663, 1177, 380, 362, 257, 14281, 724, 3410, 558, 586, 13, 407, 382, 1400, 382, 12123, 27399, 307, 5922, 11, 456, 311, 732, 4265, 7733, 13, 1779, 30, 407, 718, 311, 584, 3710, 264, 1164, 295, 428, 2235, 5308, 11, 291, 1319, 3467, 13], "temperature": 0.0, "avg_logprob": -0.21931877136230468, "compression_ratio": 1.6145038167938932, "no_speech_prob": 1.4970130905567203e-05}, {"id": 350, "seek": 390600, "start": 3918.0, "end": 3926.0, "text": " And whenever you do, and you're checking against your preview deployment, Evergreen is going to be like, okay, well, this is your production app. Looks like your types have changed. You need to write a migration.", "tokens": [400, 5699, 291, 360, 11, 293, 291, 434, 8568, 1970, 428, 14281, 19317, 11, 12123, 27399, 307, 516, 281, 312, 411, 11, 1392, 11, 731, 11, 341, 307, 428, 4265, 724, 13, 10027, 411, 428, 3467, 362, 3105, 13, 509, 643, 281, 2464, 257, 17011, 13], "temperature": 0.0, "avg_logprob": -0.21931877136230468, "compression_ratio": 1.6145038167938932, "no_speech_prob": 1.4970130905567203e-05}, {"id": 351, "seek": 392600, "start": 3926.0, "end": 3938.0, "text": " Right? There is a way in Lambdaera currently to be like, do you ignore the migration? Or like, I'm happy for you to just drop it on the floor and reset my backend model to init.", "tokens": [1779, 30, 821, 307, 257, 636, 294, 45691, 1663, 4362, 281, 312, 411, 11, 360, 291, 11200, 264, 17011, 30, 1610, 411, 11, 286, 478, 2055, 337, 291, 281, 445, 3270, 309, 322, 264, 4123, 293, 14322, 452, 38087, 2316, 281, 3157, 13], "temperature": 0.0, "avg_logprob": -0.22678125988353381, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00010885418305406347}, {"id": 352, "seek": 392600, "start": 3938.0, "end": 3945.0, "text": " So they might do that because they're like, oh, this isn't important. You know, my state isn't important on this preview app. And so they do that a few times.", "tokens": [407, 436, 1062, 360, 300, 570, 436, 434, 411, 11, 1954, 11, 341, 1943, 380, 1021, 13, 509, 458, 11, 452, 1785, 1943, 380, 1021, 322, 341, 14281, 724, 13, 400, 370, 436, 360, 300, 257, 1326, 1413, 13], "temperature": 0.0, "avg_logprob": -0.22678125988353381, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00010885418305406347}, {"id": 353, "seek": 394500, "start": 3945.0, "end": 3958.0, "text": " And then if they're unlucky, what's happened a couple of times was you get everything's good and they're like, okay, great. Let's merge. And now you merge that in and you've clobbered kind of your Evergreen history.", "tokens": [400, 550, 498, 436, 434, 38838, 11, 437, 311, 2011, 257, 1916, 295, 1413, 390, 291, 483, 1203, 311, 665, 293, 436, 434, 411, 11, 1392, 11, 869, 13, 961, 311, 22183, 13, 400, 586, 291, 22183, 300, 294, 293, 291, 600, 596, 996, 607, 292, 733, 295, 428, 12123, 27399, 2503, 13], "temperature": 0.0, "avg_logprob": -0.2054274944548911, "compression_ratio": 1.641255605381166, "no_speech_prob": 4.539013025350869e-05}, {"id": 354, "seek": 394500, "start": 3958.0, "end": 3965.0, "text": " And now you're on your main app, the one that's already at version seven, you're trying to deploy and it's looking at maybe like an app version three.", "tokens": [400, 586, 291, 434, 322, 428, 2135, 724, 11, 264, 472, 300, 311, 1217, 412, 3037, 3407, 11, 291, 434, 1382, 281, 7274, 293, 309, 311, 1237, 412, 1310, 411, 364, 724, 3037, 1045, 13], "temperature": 0.0, "avg_logprob": -0.2054274944548911, "compression_ratio": 1.641255605381166, "no_speech_prob": 4.539013025350869e-05}, {"id": 355, "seek": 396500, "start": 3965.0, "end": 3975.0, "text": " And so in that context, it goes, okay, cool. You've got app version three, app version seven's improved. And it tries to figure out what's going on and it doesn't really make sense.", "tokens": [400, 370, 294, 300, 4319, 11, 309, 1709, 11, 1392, 11, 1627, 13, 509, 600, 658, 724, 3037, 1045, 11, 724, 3037, 3407, 311, 9689, 13, 400, 309, 9898, 281, 2573, 484, 437, 311, 516, 322, 293, 309, 1177, 380, 534, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.20123267400832404, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.568761217640713e-05}, {"id": 356, "seek": 396500, "start": 3975.0, "end": 3987.0, "text": " And then we end up in a really confusing position. So the way that I'm thinking to fix this, and maybe there's multiple ways to address it, but at least the Lambdaera way that I think we're going to do the first version is going to be,", "tokens": [400, 550, 321, 917, 493, 294, 257, 534, 13181, 2535, 13, 407, 264, 636, 300, 286, 478, 1953, 281, 3191, 341, 11, 293, 1310, 456, 311, 3866, 2098, 281, 2985, 309, 11, 457, 412, 1935, 264, 45691, 1663, 636, 300, 286, 519, 321, 434, 516, 281, 360, 264, 700, 3037, 307, 516, 281, 312, 11], "temperature": 0.0, "avg_logprob": -0.20123267400832404, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.568761217640713e-05}, {"id": 357, "seek": 398700, "start": 3987.0, "end": 3998.0, "text": " okay, let's have the concept of preview apps. And in a preview app, like it's a first class concept, your main production application has this thing called preview apps.", "tokens": [1392, 11, 718, 311, 362, 264, 3410, 295, 14281, 7733, 13, 400, 294, 257, 14281, 724, 11, 411, 309, 311, 257, 700, 1508, 3410, 11, 428, 2135, 4265, 3861, 575, 341, 551, 1219, 14281, 7733, 13], "temperature": 0.0, "avg_logprob": -0.216861054703996, "compression_ratio": 1.5670103092783505, "no_speech_prob": 5.063809658167884e-05}, {"id": 358, "seek": 398700, "start": 3998.0, "end": 4006.0, "text": " So maybe somewhat similar to kind of like what people might be familiar with, like Netlify, or I think Vercel might do the same thing.", "tokens": [407, 1310, 8344, 2531, 281, 733, 295, 411, 437, 561, 1062, 312, 4963, 365, 11, 411, 6188, 75, 2505, 11, 420, 286, 519, 691, 2869, 338, 1062, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.216861054703996, "compression_ratio": 1.5670103092783505, "no_speech_prob": 5.063809658167884e-05}, {"id": 359, "seek": 400600, "start": 4006.0, "end": 4017.0, "text": " And so the idea is that when you deploy to a preview app, Lambdaera will be like, ah, okay, this is a preview app. We're going to completely ignore anything to do with migrations.", "tokens": [400, 370, 264, 1558, 307, 300, 562, 291, 7274, 281, 257, 14281, 724, 11, 45691, 1663, 486, 312, 411, 11, 3716, 11, 1392, 11, 341, 307, 257, 14281, 724, 13, 492, 434, 516, 281, 2584, 11200, 1340, 281, 360, 365, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.17895355224609374, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.0129378097190056e-05}, {"id": 360, "seek": 400600, "start": 4017.0, "end": 4023.0, "text": " The assumption is that we're always deploying a version one, and we're always going to re-initialize.", "tokens": [440, 15302, 307, 300, 321, 434, 1009, 34198, 257, 3037, 472, 11, 293, 321, 434, 1009, 516, 281, 319, 12, 259, 270, 831, 1125, 13], "temperature": 0.0, "avg_logprob": -0.17895355224609374, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.0129378097190056e-05}, {"id": 361, "seek": 400600, "start": 4023.0, "end": 4031.0, "text": " And actually, we might be cheeky and be like, we'll try restore the existing state into the new version.", "tokens": [400, 767, 11, 321, 1062, 312, 12839, 88, 293, 312, 411, 11, 321, 603, 853, 15227, 264, 6741, 1785, 666, 264, 777, 3037, 13], "temperature": 0.0, "avg_logprob": -0.17895355224609374, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.0129378097190056e-05}, {"id": 362, "seek": 403100, "start": 4031.0, "end": 4039.0, "text": " But if it happens that you've changed your types, we're just going to reset it. If the decoder fails, and it's a strict decoder, it's slightly variant from Elm's decoder.", "tokens": [583, 498, 309, 2314, 300, 291, 600, 3105, 428, 3467, 11, 321, 434, 445, 516, 281, 14322, 309, 13, 759, 264, 979, 19866, 18199, 11, 293, 309, 311, 257, 10910, 979, 19866, 11, 309, 311, 4748, 17501, 490, 2699, 76, 311, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1956262127045662, "compression_ratio": 1.6793103448275861, "no_speech_prob": 1.129984411818441e-05}, {"id": 363, "seek": 403100, "start": 4039.0, "end": 4049.0, "text": " So if all the bytes aren't consumed perfectly, then it will fail. So it has to be a perfect decoding from bytes into the value.", "tokens": [407, 498, 439, 264, 36088, 3212, 380, 21226, 6239, 11, 550, 309, 486, 3061, 13, 407, 309, 575, 281, 312, 257, 2176, 979, 8616, 490, 36088, 666, 264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1956262127045662, "compression_ratio": 1.6793103448275861, "no_speech_prob": 1.129984411818441e-05}, {"id": 364, "seek": 403100, "start": 4049.0, "end": 4059.0, "text": " Then we've got this separate chain. So that would restore back to Lambdaera's assumptions, which is if you have an app, there is only one production, and there is only one evergreen story.", "tokens": [1396, 321, 600, 658, 341, 4994, 5021, 13, 407, 300, 576, 15227, 646, 281, 45691, 1663, 311, 17695, 11, 597, 307, 498, 291, 362, 364, 724, 11, 456, 307, 787, 472, 4265, 11, 293, 456, 307, 787, 472, 1562, 27399, 1657, 13], "temperature": 0.0, "avg_logprob": -0.1956262127045662, "compression_ratio": 1.6793103448275861, "no_speech_prob": 1.129984411818441e-05}, {"id": 365, "seek": 405900, "start": 4059.0, "end": 4067.0, "text": " There is only one chain history of changes into production. And then we've got this separate mechanism, which is preview apps, no migrations, no nothing there.", "tokens": [821, 307, 787, 472, 5021, 2503, 295, 2962, 666, 4265, 13, 400, 550, 321, 600, 658, 341, 4994, 7513, 11, 597, 307, 14281, 7733, 11, 572, 6186, 12154, 11, 572, 1825, 456, 13], "temperature": 0.0, "avg_logprob": -0.19836651728703425, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.822632788796909e-05}, {"id": 366, "seek": 405900, "start": 4067.0, "end": 4072.0, "text": " So the idea is that you do your pull request, you go through all the changes, evergreen doesn't bother you at all.", "tokens": [407, 264, 1558, 307, 300, 291, 360, 428, 2235, 5308, 11, 291, 352, 807, 439, 264, 2962, 11, 1562, 27399, 1177, 380, 8677, 291, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.19836651728703425, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.822632788796909e-05}, {"id": 367, "seek": 405900, "start": 4072.0, "end": 4080.0, "text": " And then once it's approved, you merge that into main. And now when you're trying to deploy main, now you've got that.", "tokens": [400, 550, 1564, 309, 311, 10826, 11, 291, 22183, 300, 666, 2135, 13, 400, 586, 562, 291, 434, 1382, 281, 7274, 2135, 11, 586, 291, 600, 658, 300, 13], "temperature": 0.0, "avg_logprob": -0.19836651728703425, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.822632788796909e-05}, {"id": 368, "seek": 405900, "start": 4080.0, "end": 4086.0, "text": " That's when that check comes in and goes, oh, okay, you're trying to deploy. Let's consistently look at what's in production now.", "tokens": [663, 311, 562, 300, 1520, 1487, 294, 293, 1709, 11, 1954, 11, 1392, 11, 291, 434, 1382, 281, 7274, 13, 961, 311, 14961, 574, 412, 437, 311, 294, 4265, 586, 13], "temperature": 0.0, "avg_logprob": -0.19836651728703425, "compression_ratio": 1.7849829351535835, "no_speech_prob": 3.822632788796909e-05}, {"id": 369, "seek": 408600, "start": 4086.0, "end": 4090.0, "text": " What have you been up to locally? And what's changed? Let's get you to do that migration.", "tokens": [708, 362, 291, 668, 493, 281, 16143, 30, 400, 437, 311, 3105, 30, 961, 311, 483, 291, 281, 360, 300, 17011, 13], "temperature": 0.0, "avg_logprob": -0.2828808386348984, "compression_ratio": 1.5362903225806452, "no_speech_prob": 4.3312837078701705e-05}, {"id": 370, "seek": 408600, "start": 4090.0, "end": 4096.0, "text": " So yeah, that's coming soon. TM, trademark.", "tokens": [407, 1338, 11, 300, 311, 1348, 2321, 13, 33550, 11, 31361, 13], "temperature": 0.0, "avg_logprob": -0.2828808386348984, "compression_ratio": 1.5362903225806452, "no_speech_prob": 4.3312837078701705e-05}, {"id": 371, "seek": 408600, "start": 4096.0, "end": 4104.0, "text": " Yeah, what I would have imagined to be maybe slightly easier, or well, the previous thing sounds awesome.", "tokens": [865, 11, 437, 286, 576, 362, 16590, 281, 312, 1310, 4748, 3571, 11, 420, 731, 11, 264, 3894, 551, 3263, 3476, 13], "temperature": 0.0, "avg_logprob": -0.2828808386348984, "compression_ratio": 1.5362903225806452, "no_speech_prob": 4.3312837078701705e-05}, {"id": 372, "seek": 408600, "start": 4104.0, "end": 4114.0, "text": " But like, if you had a v7 in production, and you're trying to deploy and it's a v10, because you tried to do some migrations in the meantime.", "tokens": [583, 411, 11, 498, 291, 632, 257, 371, 22, 294, 4265, 11, 293, 291, 434, 1382, 281, 7274, 293, 309, 311, 257, 371, 3279, 11, 570, 291, 3031, 281, 360, 512, 6186, 12154, 294, 264, 14991, 13], "temperature": 0.0, "avg_logprob": -0.2828808386348984, "compression_ratio": 1.5362903225806452, "no_speech_prob": 4.3312837078701705e-05}, {"id": 373, "seek": 411400, "start": 4114.0, "end": 4121.0, "text": " I feel like maybe accepting unseen versions would have been easier.", "tokens": [286, 841, 411, 1310, 17391, 40608, 9606, 576, 362, 668, 3571, 13], "temperature": 0.0, "avg_logprob": -0.2078398680075621, "compression_ratio": 1.5804878048780489, "no_speech_prob": 2.7107706046081148e-05}, {"id": 374, "seek": 411400, "start": 4121.0, "end": 4128.0, "text": " So there's another scenario that I can see, which could be problematic when your team grows, when you're working with teams,", "tokens": [407, 456, 311, 1071, 9005, 300, 286, 393, 536, 11, 597, 727, 312, 19011, 562, 428, 1469, 13156, 11, 562, 291, 434, 1364, 365, 5491, 11], "temperature": 0.0, "avg_logprob": -0.2078398680075621, "compression_ratio": 1.5804878048780489, "no_speech_prob": 2.7107706046081148e-05}, {"id": 375, "seek": 411400, "start": 4128.0, "end": 4139.0, "text": " is that, like, I'm adding a new feature, or I'm changing a feature and the requires to write a migration, maybe complex, maybe not.", "tokens": [307, 300, 11, 411, 11, 286, 478, 5127, 257, 777, 4111, 11, 420, 286, 478, 4473, 257, 4111, 293, 264, 7029, 281, 2464, 257, 17011, 11, 1310, 3997, 11, 1310, 406, 13], "temperature": 0.0, "avg_logprob": -0.2078398680075621, "compression_ratio": 1.5804878048780489, "no_speech_prob": 2.7107706046081148e-05}, {"id": 376, "seek": 413900, "start": 4139.0, "end": 4148.0, "text": " And two other people on my team do the same thing. And then we all leave on vacation, and someone else tries to deploy and has to write those migrations,", "tokens": [400, 732, 661, 561, 322, 452, 1469, 360, 264, 912, 551, 13, 400, 550, 321, 439, 1856, 322, 12830, 11, 293, 1580, 1646, 9898, 281, 7274, 293, 575, 281, 2464, 729, 6186, 12154, 11], "temperature": 0.0, "avg_logprob": -0.2126126475148387, "compression_ratio": 1.5257731958762886, "no_speech_prob": 6.204836972756311e-05}, {"id": 377, "seek": 413900, "start": 4148.0, "end": 4159.0, "text": " because we didn't in the meantime, right? But I could imagine like, well, I'm done with my work, let me write a migration that will make a v8,", "tokens": [570, 321, 994, 380, 294, 264, 14991, 11, 558, 30, 583, 286, 727, 3811, 411, 11, 731, 11, 286, 478, 1096, 365, 452, 589, 11, 718, 385, 2464, 257, 17011, 300, 486, 652, 257, 371, 23, 11], "temperature": 0.0, "avg_logprob": -0.2126126475148387, "compression_ratio": 1.5257731958762886, "no_speech_prob": 6.204836972756311e-05}, {"id": 378, "seek": 415900, "start": 4159.0, "end": 4169.0, "text": " which we're never gonna ship. But we'll merge it anyway. And then someone else does the same thing v9 and v10. And we ship that.", "tokens": [597, 321, 434, 1128, 799, 5374, 13, 583, 321, 603, 22183, 309, 4033, 13, 400, 550, 1580, 1646, 775, 264, 912, 551, 371, 24, 293, 371, 3279, 13, 400, 321, 5374, 300, 13], "temperature": 0.0, "avg_logprob": -0.252648219883999, "compression_ratio": 1.58, "no_speech_prob": 1.7230613593710586e-05}, {"id": 379, "seek": 415900, "start": 4169.0, "end": 4178.0, "text": " I just say this, like, not just, I'm just throwing this out, but I'm sure that there's some problems that you have in mind.", "tokens": [286, 445, 584, 341, 11, 411, 11, 406, 445, 11, 286, 478, 445, 10238, 341, 484, 11, 457, 286, 478, 988, 300, 456, 311, 512, 2740, 300, 291, 362, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.252648219883999, "compression_ratio": 1.58, "no_speech_prob": 1.7230613593710586e-05}, {"id": 380, "seek": 415900, "start": 4178.0, "end": 4180.0, "text": " Like, nope, that's not gonna work.", "tokens": [1743, 11, 23444, 11, 300, 311, 406, 799, 589, 13], "temperature": 0.0, "avg_logprob": -0.252648219883999, "compression_ratio": 1.58, "no_speech_prob": 1.7230613593710586e-05}, {"id": 381, "seek": 415900, "start": 4180.0, "end": 4185.0, "text": " Yeah. So the first thing that would happen, remember that the snapshot only happens when you deploy. Right?", "tokens": [865, 13, 407, 264, 700, 551, 300, 576, 1051, 11, 1604, 300, 264, 30163, 787, 2314, 562, 291, 7274, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.252648219883999, "compression_ratio": 1.58, "no_speech_prob": 1.7230613593710586e-05}, {"id": 382, "seek": 418500, "start": 4185.0, "end": 4191.0, "text": " So let's say we have three pull requests, and every single pull, let's, so the worst case scenario is this.", "tokens": [407, 718, 311, 584, 321, 362, 1045, 2235, 12475, 11, 293, 633, 2167, 2235, 11, 718, 311, 11, 370, 264, 5855, 1389, 9005, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.27619927504967, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.00010070342250401154}, {"id": 383, "seek": 418500, "start": 4191.0, "end": 4197.0, "text": " Worst case scenario is you have three separate pull requests, all three team members have all changed the backend model.", "tokens": [26363, 372, 1389, 9005, 307, 291, 362, 1045, 4994, 2235, 12475, 11, 439, 1045, 1469, 2679, 362, 439, 3105, 264, 38087, 2316, 13], "temperature": 0.0, "avg_logprob": -0.27619927504967, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.00010070342250401154}, {"id": 384, "seek": 418500, "start": 4197.0, "end": 4204.0, "text": " All three team members independently on their branch have done a Lambda check against production, they've generated migrations, and they've implemented them.", "tokens": [1057, 1045, 1469, 2679, 21761, 322, 641, 9819, 362, 1096, 257, 45691, 1520, 1970, 4265, 11, 436, 600, 10833, 6186, 12154, 11, 293, 436, 600, 12270, 552, 13], "temperature": 0.0, "avg_logprob": -0.27619927504967, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.00010070342250401154}, {"id": 385, "seek": 418500, "start": 4204.0, "end": 4211.0, "text": " Right? And then now they start merging. So first person merges, they get in first. Hunky-dory, no problems.", "tokens": [1779, 30, 400, 550, 586, 436, 722, 44559, 13, 407, 700, 954, 3551, 2880, 11, 436, 483, 294, 700, 13, 389, 25837, 12, 67, 827, 11, 572, 2740, 13], "temperature": 0.0, "avg_logprob": -0.27619927504967, "compression_ratio": 1.802919708029197, "no_speech_prob": 0.00010070342250401154}, {"id": 386, "seek": 421100, "start": 4211.0, "end": 4218.0, "text": " Second person now is probably gonna have GitHub conflicts on their pull request. Right?", "tokens": [5736, 954, 586, 307, 1391, 799, 362, 23331, 19807, 322, 641, 2235, 5308, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2210212556442412, "compression_ratio": 1.672, "no_speech_prob": 2.144446443708148e-05}, {"id": 387, "seek": 421100, "start": 4218.0, "end": 4223.0, "text": " Let's say even worst case version, let's say it's a clean merge for some reason. Right?", "tokens": [961, 311, 584, 754, 5855, 1389, 3037, 11, 718, 311, 584, 309, 311, 257, 2541, 22183, 337, 512, 1778, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2210212556442412, "compression_ratio": 1.672, "no_speech_prob": 2.144446443708148e-05}, {"id": 388, "seek": 421100, "start": 4223.0, "end": 4226.0, "text": " So they don't have any conflicts. Right?", "tokens": [407, 436, 500, 380, 362, 604, 19807, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2210212556442412, "compression_ratio": 1.672, "no_speech_prob": 2.144446443708148e-05}, {"id": 389, "seek": 421100, "start": 4226.0, "end": 4231.0, "text": " You mean that both would have generated a v8 migration and that would...", "tokens": [509, 914, 300, 1293, 576, 362, 10833, 257, 371, 23, 17011, 293, 300, 576, 485], "temperature": 0.0, "avg_logprob": -0.2210212556442412, "compression_ratio": 1.672, "no_speech_prob": 2.144446443708148e-05}, {"id": 390, "seek": 421100, "start": 4231.0, "end": 4240.0, "text": " But it would have been identical except for the differences. So let's say they changed very different parts of the model. Right?", "tokens": [583, 309, 576, 362, 668, 14800, 3993, 337, 264, 7300, 13, 407, 718, 311, 584, 436, 3105, 588, 819, 3166, 295, 264, 2316, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2210212556442412, "compression_ratio": 1.672, "no_speech_prob": 2.144446443708148e-05}, {"id": 391, "seek": 424000, "start": 4240.0, "end": 4244.0, "text": " All of the snapshots were almost identical except for these deep changes. Right?", "tokens": [1057, 295, 264, 19206, 27495, 645, 1920, 14800, 3993, 337, 613, 2452, 2962, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2048852333655724, "compression_ratio": 1.7560137457044673, "no_speech_prob": 4.198083479423076e-05}, {"id": 392, "seek": 424000, "start": 4244.0, "end": 4247.0, "text": " So maybe the changes ended up in different snapshot files. Right?", "tokens": [407, 1310, 264, 2962, 4590, 493, 294, 819, 30163, 7098, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2048852333655724, "compression_ratio": 1.7560137457044673, "no_speech_prob": 4.198083479423076e-05}, {"id": 393, "seek": 424000, "start": 4247.0, "end": 4251.0, "text": " So maybe Git's like clever and it's like, ah, yeah, you're merging the same thing except for this.", "tokens": [407, 1310, 16939, 311, 411, 13494, 293, 309, 311, 411, 11, 3716, 11, 1338, 11, 291, 434, 44559, 264, 912, 551, 3993, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.2048852333655724, "compression_ratio": 1.7560137457044673, "no_speech_prob": 4.198083479423076e-05}, {"id": 394, "seek": 424000, "start": 4251.0, "end": 4255.0, "text": " And I can figure out the merge. I'll merge it for you.", "tokens": [400, 286, 393, 2573, 484, 264, 22183, 13, 286, 603, 22183, 309, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.2048852333655724, "compression_ratio": 1.7560137457044673, "no_speech_prob": 4.198083479423076e-05}, {"id": 395, "seek": 424000, "start": 4255.0, "end": 4257.0, "text": " And let's say the third person does the same. Right?", "tokens": [400, 718, 311, 584, 264, 2636, 954, 775, 264, 912, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.2048852333655724, "compression_ratio": 1.7560137457044673, "no_speech_prob": 4.198083479423076e-05}, {"id": 396, "seek": 424000, "start": 4257.0, "end": 4262.0, "text": " What's gonna happen now is because there's only one deploy that's possible,", "tokens": [708, 311, 799, 1051, 586, 307, 570, 456, 311, 787, 472, 7274, 300, 311, 1944, 11], "temperature": 0.0, "avg_logprob": -0.2048852333655724, "compression_ratio": 1.7560137457044673, "no_speech_prob": 4.198083479423076e-05}, {"id": 397, "seek": 424000, "start": 4262.0, "end": 4267.0, "text": " whoever gets to the deploy is gonna have to go through that Lambda check process.", "tokens": [11387, 2170, 281, 264, 7274, 307, 799, 362, 281, 352, 807, 300, 45691, 1520, 1399, 13], "temperature": 0.0, "avg_logprob": -0.2048852333655724, "compression_ratio": 1.7560137457044673, "no_speech_prob": 4.198083479423076e-05}, {"id": 398, "seek": 426700, "start": 4267.0, "end": 4272.0, "text": " And potentially, if Git has tried to be too clever, you're gonna get type errors. Right?", "tokens": [400, 7263, 11, 498, 16939, 575, 3031, 281, 312, 886, 13494, 11, 291, 434, 799, 483, 2010, 13603, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.20949041275751024, "compression_ratio": 1.7572463768115942, "no_speech_prob": 4.356636509328382e-06}, {"id": 399, "seek": 426700, "start": 4272.0, "end": 4278.0, "text": " Because something as part of those mergers might not have fully carefully covered things.", "tokens": [1436, 746, 382, 644, 295, 729, 3551, 9458, 1062, 406, 362, 4498, 7500, 5343, 721, 13], "temperature": 0.0, "avg_logprob": -0.20949041275751024, "compression_ratio": 1.7572463768115942, "no_speech_prob": 4.356636509328382e-06}, {"id": 400, "seek": 426700, "start": 4278.0, "end": 4283.0, "text": " So there, if you think of like, you know, like what kind of burden are they stuck with?", "tokens": [407, 456, 11, 498, 291, 519, 295, 411, 11, 291, 458, 11, 411, 437, 733, 295, 12578, 366, 436, 5541, 365, 30], "temperature": 0.0, "avg_logprob": -0.20949041275751024, "compression_ratio": 1.7572463768115942, "no_speech_prob": 4.356636509328382e-06}, {"id": 401, "seek": 426700, "start": 4283.0, "end": 4286.0, "text": " You know, all three of you have conveniently gone on holiday.", "tokens": [509, 458, 11, 439, 1045, 295, 291, 362, 44375, 2780, 322, 9960, 13], "temperature": 0.0, "avg_logprob": -0.20949041275751024, "compression_ratio": 1.7572463768115942, "no_speech_prob": 4.356636509328382e-06}, {"id": 402, "seek": 426700, "start": 4286.0, "end": 4291.0, "text": " So it's the worst, worst, worst case. Like what's the absolute worst that happens?", "tokens": [407, 309, 311, 264, 5855, 11, 5855, 11, 5855, 1389, 13, 1743, 437, 311, 264, 8236, 5855, 300, 2314, 30], "temperature": 0.0, "avg_logprob": -0.20949041275751024, "compression_ratio": 1.7572463768115942, "no_speech_prob": 4.356636509328382e-06}, {"id": 403, "seek": 426700, "start": 4291.0, "end": 4294.0, "text": " The absolute worst that happens is, let's say it's me. I'm stuck with it.", "tokens": [440, 8236, 5855, 300, 2314, 307, 11, 718, 311, 584, 309, 311, 385, 13, 286, 478, 5541, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.20949041275751024, "compression_ratio": 1.7572463768115942, "no_speech_prob": 4.356636509328382e-06}, {"id": 404, "seek": 429400, "start": 4294.0, "end": 4300.0, "text": " I go, ah, you know, Dillon and Jeroen have left me with this.", "tokens": [286, 352, 11, 3716, 11, 291, 458, 11, 28160, 293, 508, 2032, 268, 362, 1411, 385, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.21396391480057328, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.5445639292011037e-05}, {"id": 405, "seek": 429400, "start": 4300.0, "end": 4303.0, "text": " So I go, you know what? I'm gonna delete the migration.", "tokens": [407, 286, 352, 11, 291, 458, 437, 30, 286, 478, 799, 12097, 264, 17011, 13], "temperature": 0.0, "avg_logprob": -0.21396391480057328, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.5445639292011037e-05}, {"id": 406, "seek": 429400, "start": 4303.0, "end": 4309.0, "text": " I'm gonna do a Lambda check again. Lambda always does the type snapshots. Right?", "tokens": [286, 478, 799, 360, 257, 45691, 1520, 797, 13, 45691, 1009, 775, 264, 2010, 19206, 27495, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.21396391480057328, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.5445639292011037e-05}, {"id": 407, "seek": 429400, "start": 4309.0, "end": 4313.0, "text": " So say you did type snapshots, you didn't deploy, you committed them,", "tokens": [407, 584, 291, 630, 2010, 19206, 27495, 11, 291, 994, 380, 7274, 11, 291, 7784, 552, 11], "temperature": 0.0, "avg_logprob": -0.21396391480057328, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.5445639292011037e-05}, {"id": 408, "seek": 429400, "start": 4313.0, "end": 4317.0, "text": " and then you changed more types and you did a check again.", "tokens": [293, 550, 291, 3105, 544, 3467, 293, 291, 630, 257, 1520, 797, 13], "temperature": 0.0, "avg_logprob": -0.21396391480057328, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.5445639292011037e-05}, {"id": 409, "seek": 429400, "start": 4317.0, "end": 4320.0, "text": " Lambda is gonna replace those version snapshots. Right?", "tokens": [45691, 307, 799, 7406, 729, 3037, 19206, 27495, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.21396391480057328, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.5445639292011037e-05}, {"id": 410, "seek": 432000, "start": 4320.0, "end": 4324.0, "text": " Because you haven't deployed yet. So if you haven't deployed version five, if that's the next version,", "tokens": [1436, 291, 2378, 380, 17826, 1939, 13, 407, 498, 291, 2378, 380, 17826, 3037, 1732, 11, 498, 300, 311, 264, 958, 3037, 11], "temperature": 0.0, "avg_logprob": -0.18534065052202553, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.3630812645715196e-05}, {"id": 411, "seek": 432000, "start": 4324.0, "end": 4328.0, "text": " but you've been changing stuff, it's just gonna keep replacing the snapshots until you deployed.", "tokens": [457, 291, 600, 668, 4473, 1507, 11, 309, 311, 445, 799, 1066, 19139, 264, 19206, 27495, 1826, 291, 17826, 13], "temperature": 0.0, "avg_logprob": -0.18534065052202553, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.3630812645715196e-05}, {"id": 412, "seek": 432000, "start": 4328.0, "end": 4332.0, "text": " Once you've deployed, it's gonna be like, okay, well fine. Next one's version six. Right?", "tokens": [3443, 291, 600, 17826, 11, 309, 311, 799, 312, 411, 11, 1392, 11, 731, 2489, 13, 3087, 472, 311, 3037, 2309, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.18534065052202553, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.3630812645715196e-05}, {"id": 413, "seek": 432000, "start": 4332.0, "end": 4335.0, "text": " So if I delete the migration file, I do a Lambda check.", "tokens": [407, 498, 286, 12097, 264, 17011, 3991, 11, 286, 360, 257, 45691, 1520, 13], "temperature": 0.0, "avg_logprob": -0.18534065052202553, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.3630812645715196e-05}, {"id": 414, "seek": 432000, "start": 4335.0, "end": 4338.0, "text": " I'm gonna get now consistent snapshots with everybody's changes together.", "tokens": [286, 478, 799, 483, 586, 8398, 19206, 27495, 365, 2201, 311, 2962, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18534065052202553, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.3630812645715196e-05}, {"id": 415, "seek": 432000, "start": 4338.0, "end": 4343.0, "text": " If their changes together, the mergers didn't type check, like if there was an Elm compiler error,", "tokens": [759, 641, 2962, 1214, 11, 264, 3551, 9458, 994, 380, 2010, 1520, 11, 411, 498, 456, 390, 364, 2699, 76, 31958, 6713, 11], "temperature": 0.0, "avg_logprob": -0.18534065052202553, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.3630812645715196e-05}, {"id": 416, "seek": 432000, "start": 4343.0, "end": 4347.0, "text": " I wouldn't even be able to generate the snapshots. I would just get an Elm compiler error first.", "tokens": [286, 2759, 380, 754, 312, 1075, 281, 8460, 264, 19206, 27495, 13, 286, 576, 445, 483, 364, 2699, 76, 31958, 6713, 700, 13], "temperature": 0.0, "avg_logprob": -0.18534065052202553, "compression_ratio": 1.8636363636363635, "no_speech_prob": 1.3630812645715196e-05}, {"id": 417, "seek": 434700, "start": 4347.0, "end": 4350.0, "text": " So let's say it's like worst, worst, worst, worst case.", "tokens": [407, 718, 311, 584, 309, 311, 411, 5855, 11, 5855, 11, 5855, 11, 5855, 1389, 13], "temperature": 0.0, "avg_logprob": -0.17959673264447382, "compression_ratio": 1.6688311688311688, "no_speech_prob": 1.4970700249250513e-05}, {"id": 418, "seek": 434700, "start": 4350.0, "end": 4354.0, "text": " The code is broken. It's been merge broken. So A, I fix all the Elm types. Great.", "tokens": [440, 3089, 307, 5463, 13, 467, 311, 668, 22183, 5463, 13, 407, 316, 11, 286, 3191, 439, 264, 2699, 76, 3467, 13, 3769, 13], "temperature": 0.0, "avg_logprob": -0.17959673264447382, "compression_ratio": 1.6688311688311688, "no_speech_prob": 1.4970700249250513e-05}, {"id": 419, "seek": 434700, "start": 4354.0, "end": 4359.0, "text": " Now Elm's compiling. B, I run Lambda check. It redoes the snapshots.", "tokens": [823, 2699, 76, 311, 715, 4883, 13, 363, 11, 286, 1190, 45691, 1520, 13, 467, 2182, 78, 279, 264, 19206, 27495, 13], "temperature": 0.0, "avg_logprob": -0.17959673264447382, "compression_ratio": 1.6688311688311688, "no_speech_prob": 1.4970700249250513e-05}, {"id": 420, "seek": 434700, "start": 4359.0, "end": 4363.0, "text": " Cool. C, Lambda has now done the snapshots and it sees the migration's not there.", "tokens": [8561, 13, 383, 11, 45691, 575, 586, 1096, 264, 19206, 27495, 293, 309, 8194, 264, 17011, 311, 406, 456, 13], "temperature": 0.0, "avg_logprob": -0.17959673264447382, "compression_ratio": 1.6688311688311688, "no_speech_prob": 1.4970700249250513e-05}, {"id": 421, "seek": 434700, "start": 4363.0, "end": 4366.0, "text": " So it goes, cool, I'm gonna generate the whole migration file for you. Right?", "tokens": [407, 309, 1709, 11, 1627, 11, 286, 478, 799, 8460, 264, 1379, 17011, 3991, 337, 291, 13, 1779, 30], "temperature": 0.0, "avg_logprob": -0.17959673264447382, "compression_ratio": 1.6688311688311688, "no_speech_prob": 1.4970700249250513e-05}, {"id": 422, "seek": 434700, "start": 4366.0, "end": 4369.0, "text": " With the placeholders for the bits that I can't migrate.", "tokens": [2022, 264, 1081, 12916, 337, 264, 9239, 300, 286, 393, 380, 31821, 13], "temperature": 0.0, "avg_logprob": -0.17959673264447382, "compression_ratio": 1.6688311688311688, "no_speech_prob": 1.4970700249250513e-05}, {"id": 423, "seek": 434700, "start": 4369.0, "end": 4374.0, "text": " And so now my job is to go, okay, let me see if I can go to those individual pull requests", "tokens": [400, 370, 586, 452, 1691, 307, 281, 352, 11, 1392, 11, 718, 385, 536, 498, 286, 393, 352, 281, 729, 2609, 2235, 12475], "temperature": 0.0, "avg_logprob": -0.17959673264447382, "compression_ratio": 1.6688311688311688, "no_speech_prob": 1.4970700249250513e-05}, {"id": 424, "seek": 437400, "start": 4374.0, "end": 4379.0, "text": " and slice out the individual specific migration implementations that everyone's already done.", "tokens": [293, 13153, 484, 264, 2609, 2685, 17011, 4445, 763, 300, 1518, 311, 1217, 1096, 13], "temperature": 0.0, "avg_logprob": -0.21372276306152344, "compression_ratio": 1.6342412451361867, "no_speech_prob": 3.089389338128967e-06}, {"id": 425, "seek": 437400, "start": 4379.0, "end": 4384.0, "text": " If I can, and they fit in, and then it type checks, then I go, great.", "tokens": [759, 286, 393, 11, 293, 436, 3318, 294, 11, 293, 550, 309, 2010, 13834, 11, 550, 286, 352, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.21372276306152344, "compression_ratio": 1.6342412451361867, "no_speech_prob": 3.089389338128967e-06}, {"id": 426, "seek": 437400, "start": 4384.0, "end": 4388.0, "text": " I've gone through pretty much a mechanical process just following the types", "tokens": [286, 600, 2780, 807, 1238, 709, 257, 12070, 1399, 445, 3480, 264, 3467], "temperature": 0.0, "avg_logprob": -0.21372276306152344, "compression_ratio": 1.6342412451361867, "no_speech_prob": 3.089389338128967e-06}, {"id": 427, "seek": 437400, "start": 4388.0, "end": 4391.0, "text": " and getting stuff done in.", "tokens": [293, 1242, 1507, 1096, 294, 13], "temperature": 0.0, "avg_logprob": -0.21372276306152344, "compression_ratio": 1.6342412451361867, "no_speech_prob": 3.089389338128967e-06}, {"id": 428, "seek": 437400, "start": 4391.0, "end": 4397.0, "text": " If instead it was, hey, two people have actually changed stuff in the same thing,", "tokens": [759, 2602, 309, 390, 11, 4177, 11, 732, 561, 362, 767, 3105, 1507, 294, 264, 912, 551, 11], "temperature": 0.0, "avg_logprob": -0.21372276306152344, "compression_ratio": 1.6342412451361867, "no_speech_prob": 3.089389338128967e-06}, {"id": 429, "seek": 437400, "start": 4397.0, "end": 4401.0, "text": " like somebody's both added and removed variants on the same custom type", "tokens": [411, 2618, 311, 1293, 3869, 293, 7261, 21669, 322, 264, 912, 2375, 2010], "temperature": 0.0, "avg_logprob": -0.21372276306152344, "compression_ratio": 1.6342412451361867, "no_speech_prob": 3.089389338128967e-06}, {"id": 430, "seek": 440100, "start": 4401.0, "end": 4406.0, "text": " on two different PRs all at the same time, that's a really great stopping point", "tokens": [322, 732, 819, 11568, 82, 439, 412, 264, 912, 565, 11, 300, 311, 257, 534, 869, 12767, 935], "temperature": 0.0, "avg_logprob": -0.17934071071564206, "compression_ratio": 1.6866666666666668, "no_speech_prob": 4.2889569158433005e-06}, {"id": 431, "seek": 440100, "start": 4406.0, "end": 4410.0, "text": " for me to look at this and be like, this is nuts.", "tokens": [337, 385, 281, 574, 412, 341, 293, 312, 411, 11, 341, 307, 10483, 13], "temperature": 0.0, "avg_logprob": -0.17934071071564206, "compression_ratio": 1.6866666666666668, "no_speech_prob": 4.2889569158433005e-06}, {"id": 432, "seek": 440100, "start": 4410.0, "end": 4413.0, "text": " We need them to come back from holiday and explain what should happen,", "tokens": [492, 643, 552, 281, 808, 646, 490, 9960, 293, 2903, 437, 820, 1051, 11], "temperature": 0.0, "avg_logprob": -0.17934071071564206, "compression_ratio": 1.6866666666666668, "no_speech_prob": 4.2889569158433005e-06}, {"id": 433, "seek": 440100, "start": 4413.0, "end": 4417.0, "text": " or I need to go to a product person, or I need to figure out what's actually going on here.", "tokens": [420, 286, 643, 281, 352, 281, 257, 1674, 954, 11, 420, 286, 643, 281, 2573, 484, 437, 311, 767, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.17934071071564206, "compression_ratio": 1.6866666666666668, "no_speech_prob": 4.2889569158433005e-06}, {"id": 434, "seek": 440100, "start": 4417.0, "end": 4421.0, "text": " And I think that's cool because if that happened without this setup,", "tokens": [400, 286, 519, 300, 311, 1627, 570, 498, 300, 2011, 1553, 341, 8657, 11], "temperature": 0.0, "avg_logprob": -0.17934071071564206, "compression_ratio": 1.6866666666666668, "no_speech_prob": 4.2889569158433005e-06}, {"id": 435, "seek": 440100, "start": 4421.0, "end": 4425.0, "text": " there wouldn't necessarily be an indicator there that something's gone wrong.", "tokens": [456, 2759, 380, 4725, 312, 364, 16961, 456, 300, 746, 311, 2780, 2085, 13], "temperature": 0.0, "avg_logprob": -0.17934071071564206, "compression_ratio": 1.6866666666666668, "no_speech_prob": 4.2889569158433005e-06}, {"id": 436, "seek": 440100, "start": 4425.0, "end": 4430.0, "text": " You might merge these migrations together, or actually even worse,", "tokens": [509, 1062, 22183, 613, 6186, 12154, 1214, 11, 420, 767, 754, 5324, 11], "temperature": 0.0, "avg_logprob": -0.17934071071564206, "compression_ratio": 1.6866666666666668, "no_speech_prob": 4.2889569158433005e-06}, {"id": 437, "seek": 443000, "start": 4430.0, "end": 4435.0, "text": " if I think of Rails, I have a lot of experience with Rails as a counterpoint.", "tokens": [498, 286, 519, 295, 48526, 11, 286, 362, 257, 688, 295, 1752, 365, 48526, 382, 257, 5682, 6053, 13], "temperature": 0.0, "avg_logprob": -0.20434909057617187, "compression_ratio": 1.722972972972973, "no_speech_prob": 2.9768918466288596e-05}, {"id": 438, "seek": 443000, "start": 4435.0, "end": 4439.0, "text": " In Rails, each developer does their own migrations as a separate file.", "tokens": [682, 48526, 11, 1184, 10754, 775, 641, 1065, 6186, 12154, 382, 257, 4994, 3991, 13], "temperature": 0.0, "avg_logprob": -0.20434909057617187, "compression_ratio": 1.722972972972973, "no_speech_prob": 2.9768918466288596e-05}, {"id": 439, "seek": 443000, "start": 4439.0, "end": 4443.0, "text": " So you wouldn't necessarily even be aware that someone else was adding and removing stuff", "tokens": [407, 291, 2759, 380, 4725, 754, 312, 3650, 300, 1580, 1646, 390, 5127, 293, 12720, 1507], "temperature": 0.0, "avg_logprob": -0.20434909057617187, "compression_ratio": 1.722972972972973, "no_speech_prob": 2.9768918466288596e-05}, {"id": 440, "seek": 443000, "start": 4443.0, "end": 4446.0, "text": " to the same model because that would all be in different migrations.", "tokens": [281, 264, 912, 2316, 570, 300, 576, 439, 312, 294, 819, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.20434909057617187, "compression_ratio": 1.722972972972973, "no_speech_prob": 2.9768918466288596e-05}, {"id": 441, "seek": 443000, "start": 4446.0, "end": 4449.0, "text": " There'd be no natural mechanism to see a conflict.", "tokens": [821, 1116, 312, 572, 3303, 7513, 281, 536, 257, 6596, 13], "temperature": 0.0, "avg_logprob": -0.20434909057617187, "compression_ratio": 1.722972972972973, "no_speech_prob": 2.9768918466288596e-05}, {"id": 442, "seek": 443000, "start": 4449.0, "end": 4454.0, "text": " So you could end up in a situation where you deploy these non-commutative migrations", "tokens": [407, 291, 727, 917, 493, 294, 257, 2590, 689, 291, 7274, 613, 2107, 12, 13278, 325, 1166, 6186, 12154], "temperature": 0.0, "avg_logprob": -0.20434909057617187, "compression_ratio": 1.722972972972973, "no_speech_prob": 2.9768918466288596e-05}, {"id": 443, "seek": 443000, "start": 4454.0, "end": 4459.0, "text": " that end you up in a weird schema state, but that nothing catches.", "tokens": [300, 917, 291, 493, 294, 257, 3657, 34078, 1785, 11, 457, 300, 1825, 25496, 13], "temperature": 0.0, "avg_logprob": -0.20434909057617187, "compression_ratio": 1.722972972972973, "no_speech_prob": 2.9768918466288596e-05}, {"id": 444, "seek": 445900, "start": 4459.0, "end": 4464.0, "text": " Whereas at least in the Elm Lambdaera world, there'd be warning signs there.", "tokens": [13813, 412, 1935, 294, 264, 2699, 76, 45691, 1663, 1002, 11, 456, 1116, 312, 9164, 7880, 456, 13], "temperature": 0.0, "avg_logprob": -0.23270350963145764, "compression_ratio": 1.6718146718146718, "no_speech_prob": 6.747675342921866e-06}, {"id": 445, "seek": 445900, "start": 4464.0, "end": 4470.0, "text": " There's things that in the worst case would make you be like, huh, what's going on?", "tokens": [821, 311, 721, 300, 294, 264, 5855, 1389, 576, 652, 291, 312, 411, 11, 7020, 11, 437, 311, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.23270350963145764, "compression_ratio": 1.6718146718146718, "no_speech_prob": 6.747675342921866e-06}, {"id": 446, "seek": 445900, "start": 4470.0, "end": 4473.0, "text": " So I think that's pretty cool. That's a decent outcome, I think,", "tokens": [407, 286, 519, 300, 311, 1238, 1627, 13, 663, 311, 257, 8681, 9700, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.23270350963145764, "compression_ratio": 1.6718146718146718, "no_speech_prob": 6.747675342921866e-06}, {"id": 447, "seek": 445900, "start": 4473.0, "end": 4475.0, "text": " even though there might be some pain.", "tokens": [754, 1673, 456, 1062, 312, 512, 1822, 13], "temperature": 0.0, "avg_logprob": -0.23270350963145764, "compression_ratio": 1.6718146718146718, "no_speech_prob": 6.747675342921866e-06}, {"id": 448, "seek": 445900, "start": 4475.0, "end": 4479.0, "text": " So to summarize, your recommendation is for everyone to run Lambdaera check", "tokens": [407, 281, 20858, 11, 428, 11879, 307, 337, 1518, 281, 1190, 45691, 1663, 1520], "temperature": 0.0, "avg_logprob": -0.23270350963145764, "compression_ratio": 1.6718146718146718, "no_speech_prob": 6.747675342921866e-06}, {"id": 449, "seek": 445900, "start": 4479.0, "end": 4486.0, "text": " and write a migration, and then those migrations get merged somehow by someone at some point?", "tokens": [293, 2464, 257, 17011, 11, 293, 550, 729, 6186, 12154, 483, 36427, 6063, 538, 1580, 412, 512, 935, 30], "temperature": 0.0, "avg_logprob": -0.23270350963145764, "compression_ratio": 1.6718146718146718, "no_speech_prob": 6.747675342921866e-06}, {"id": 450, "seek": 448600, "start": 4486.0, "end": 4491.0, "text": " No. So my recommendation would be that you do the migration part separately", "tokens": [883, 13, 407, 452, 11879, 576, 312, 300, 291, 360, 264, 17011, 644, 14759], "temperature": 0.0, "avg_logprob": -0.2576019131407446, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.8044902390101925e-05}, {"id": 451, "seek": 448600, "start": 4491.0, "end": 4495.0, "text": " on the main branch if you've got lots of people editing the same stuff.", "tokens": [322, 264, 2135, 9819, 498, 291, 600, 658, 3195, 295, 561, 10000, 264, 912, 1507, 13], "temperature": 0.0, "avg_logprob": -0.2576019131407446, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.8044902390101925e-05}, {"id": 452, "seek": 448600, "start": 4495.0, "end": 4498.0, "text": " Or that would be a point that you communicate together as a team.", "tokens": [1610, 300, 576, 312, 257, 935, 300, 291, 7890, 1214, 382, 257, 1469, 13], "temperature": 0.0, "avg_logprob": -0.2576019131407446, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.8044902390101925e-05}, {"id": 453, "seek": 448600, "start": 4498.0, "end": 4506.0, "text": " It brings that kind of idea of continuous deployment back a step.", "tokens": [467, 5607, 300, 733, 295, 1558, 295, 10957, 19317, 646, 257, 1823, 13], "temperature": 0.0, "avg_logprob": -0.2576019131407446, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.8044902390101925e-05}, {"id": 454, "seek": 448600, "start": 4506.0, "end": 4510.0, "text": " So it's not as freeform as some companies may practice,", "tokens": [407, 309, 311, 406, 382, 1737, 837, 382, 512, 3431, 815, 3124, 11], "temperature": 0.0, "avg_logprob": -0.2576019131407446, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.8044902390101925e-05}, {"id": 455, "seek": 448600, "start": 4510.0, "end": 4513.0, "text": " like, oh, everybody can deploy and we deploy all the time,", "tokens": [411, 11, 1954, 11, 2201, 393, 7274, 293, 321, 7274, 439, 264, 565, 11], "temperature": 0.0, "avg_logprob": -0.2576019131407446, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.8044902390101925e-05}, {"id": 456, "seek": 451300, "start": 4513.0, "end": 4516.0, "text": " we just don't think about it kind of thing.", "tokens": [321, 445, 500, 380, 519, 466, 309, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 457, "seek": 451300, "start": 4516.0, "end": 4519.0, "text": " So it makes that a little bit more centralized.", "tokens": [407, 309, 1669, 300, 257, 707, 857, 544, 32395, 13], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 458, "seek": 451300, "start": 4519.0, "end": 4523.0, "text": " But I think what you get as a result is you get a much greater ability", "tokens": [583, 286, 519, 437, 291, 483, 382, 257, 1874, 307, 291, 483, 257, 709, 5044, 3485], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 459, "seek": 451300, "start": 4523.0, "end": 4526.0, "text": " to model your business logic directly.", "tokens": [281, 2316, 428, 1606, 9952, 3838, 13], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 460, "seek": 451300, "start": 4526.0, "end": 4530.0, "text": " And it means that you don't have to think as carefully about the consistency", "tokens": [400, 309, 1355, 300, 291, 500, 380, 362, 281, 519, 382, 7500, 466, 264, 14416], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 461, "seek": 451300, "start": 4530.0, "end": 4533.0, "text": " or the invariance that you're holding in that migration.", "tokens": [420, 264, 33270, 719, 300, 291, 434, 5061, 294, 300, 17011, 13], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 462, "seek": 451300, "start": 4533.0, "end": 4537.0, "text": " So it trades off, I suppose, some of that kind of maybe,", "tokens": [407, 309, 21287, 766, 11, 286, 7297, 11, 512, 295, 300, 733, 295, 1310, 11], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 463, "seek": 451300, "start": 4537.0, "end": 4540.0, "text": " I don't know if you'd call it decentralized deployment model", "tokens": [286, 500, 380, 458, 498, 291, 1116, 818, 309, 32870, 19317, 2316], "temperature": 0.0, "avg_logprob": -0.20381350352846342, "compression_ratio": 1.7159090909090908, "no_speech_prob": 5.917856833548285e-05}, {"id": 464, "seek": 454000, "start": 4540.0, "end": 4543.0, "text": " to something that's a bit more centralized, but that gives you back", "tokens": [281, 746, 300, 311, 257, 857, 544, 32395, 11, 457, 300, 2709, 291, 646], "temperature": 0.0, "avg_logprob": -0.22582692366379958, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00014423894754145294}, {"id": 465, "seek": 454000, "start": 4543.0, "end": 4546.0, "text": " a bunch of guarantees in return, if that makes sense.", "tokens": [257, 3840, 295, 32567, 294, 2736, 11, 498, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.22582692366379958, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00014423894754145294}, {"id": 466, "seek": 454000, "start": 4546.0, "end": 4551.0, "text": " So I remember that at one point, someone told me that Arm Review", "tokens": [407, 286, 1604, 300, 412, 472, 935, 11, 1580, 1907, 385, 300, 11893, 19954], "temperature": 0.0, "avg_logprob": -0.22582692366379958, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00014423894754145294}, {"id": 467, "seek": 454000, "start": 4551.0, "end": 4555.0, "text": " was kind of slow with their projects.", "tokens": [390, 733, 295, 2964, 365, 641, 4455, 13], "temperature": 0.0, "avg_logprob": -0.22582692366379958, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00014423894754145294}, {"id": 468, "seek": 454000, "start": 4555.0, "end": 4558.0, "text": " If I recall correctly, that was James Carlson,", "tokens": [759, 286, 9901, 8944, 11, 300, 390, 5678, 14256, 3015, 11], "temperature": 0.0, "avg_logprob": -0.22582692366379958, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00014423894754145294}, {"id": 469, "seek": 454000, "start": 4558.0, "end": 4561.0, "text": " who's a fervent user of Landera.", "tokens": [567, 311, 257, 283, 1978, 317, 4195, 295, 441, 4483, 64, 13], "temperature": 0.0, "avg_logprob": -0.22582692366379958, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00014423894754145294}, {"id": 470, "seek": 454000, "start": 4561.0, "end": 4566.0, "text": " And I checked it out and was like, yeah, this is a bit slow.", "tokens": [400, 286, 10033, 309, 484, 293, 390, 411, 11, 1338, 11, 341, 307, 257, 857, 2964, 13], "temperature": 0.0, "avg_logprob": -0.22582692366379958, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00014423894754145294}, {"id": 471, "seek": 456600, "start": 4566.0, "end": 4570.0, "text": " And I figured out why.", "tokens": [400, 286, 8932, 484, 983, 13], "temperature": 0.0, "avg_logprob": -0.24610748291015624, "compression_ratio": 1.4540229885057472, "no_speech_prob": 1.2606597920239437e-05}, {"id": 472, "seek": 456600, "start": 4570.0, "end": 4573.0, "text": " I think I know why.", "tokens": [286, 519, 286, 458, 983, 13], "temperature": 0.0, "avg_logprob": -0.24610748291015624, "compression_ratio": 1.4540229885057472, "no_speech_prob": 1.2606597920239437e-05}, {"id": 473, "seek": 456600, "start": 4573.0, "end": 4579.0, "text": " There was an evergreen folder with over 600 versions,", "tokens": [821, 390, 364, 1562, 27399, 10820, 365, 670, 11849, 9606, 11], "temperature": 0.0, "avg_logprob": -0.24610748291015624, "compression_ratio": 1.4540229885057472, "no_speech_prob": 1.2606597920239437e-05}, {"id": 474, "seek": 456600, "start": 4579.0, "end": 4584.0, "text": " meaning over six, you don't have a migration for every version,", "tokens": [3620, 670, 2309, 11, 291, 500, 380, 362, 257, 17011, 337, 633, 3037, 11], "temperature": 0.0, "avg_logprob": -0.24610748291015624, "compression_ratio": 1.4540229885057472, "no_speech_prob": 1.2606597920239437e-05}, {"id": 475, "seek": 456600, "start": 4584.0, "end": 4590.0, "text": " but a few hundreds migrations and snapshots.", "tokens": [457, 257, 1326, 6779, 6186, 12154, 293, 19206, 27495, 13], "temperature": 0.0, "avg_logprob": -0.24610748291015624, "compression_ratio": 1.4540229885057472, "no_speech_prob": 1.2606597920239437e-05}, {"id": 476, "seek": 456600, "start": 4590.0, "end": 4594.0, "text": " And there was code that remained in the project", "tokens": [400, 456, 390, 3089, 300, 12780, 294, 264, 1716], "temperature": 0.0, "avg_logprob": -0.24610748291015624, "compression_ratio": 1.4540229885057472, "no_speech_prob": 1.2606597920239437e-05}, {"id": 477, "seek": 459400, "start": 4594.0, "end": 4598.0, "text": " and that Arm Review had to run to go through,", "tokens": [293, 300, 11893, 19954, 632, 281, 1190, 281, 352, 807, 11], "temperature": 0.0, "avg_logprob": -0.1934427160965769, "compression_ratio": 1.5294117647058822, "no_speech_prob": 3.7852760215173475e-06}, {"id": 478, "seek": 459400, "start": 4598.0, "end": 4603.0, "text": " which is now a bit faster, so I'm not getting those issues anymore.", "tokens": [597, 307, 586, 257, 857, 4663, 11, 370, 286, 478, 406, 1242, 729, 2663, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1934427160965769, "compression_ratio": 1.5294117647058822, "no_speech_prob": 3.7852760215173475e-06}, {"id": 479, "seek": 459400, "start": 4603.0, "end": 4609.0, "text": " But yeah, I was wondering, when should you remove those migrations?", "tokens": [583, 1338, 11, 286, 390, 6359, 11, 562, 820, 291, 4159, 729, 6186, 12154, 30], "temperature": 0.0, "avg_logprob": -0.1934427160965769, "compression_ratio": 1.5294117647058822, "no_speech_prob": 3.7852760215173475e-06}, {"id": 480, "seek": 459400, "start": 4609.0, "end": 4613.0, "text": " Should you? And can you remove v1?", "tokens": [6454, 291, 30, 400, 393, 291, 4159, 371, 16, 30], "temperature": 0.0, "avg_logprob": -0.1934427160965769, "compression_ratio": 1.5294117647058822, "no_speech_prob": 3.7852760215173475e-06}, {"id": 481, "seek": 459400, "start": 4613.0, "end": 4617.0, "text": " Is there a way to tell, oh, well, no one is using v1 anymore", "tokens": [1119, 456, 257, 636, 281, 980, 11, 1954, 11, 731, 11, 572, 472, 307, 1228, 371, 16, 3602], "temperature": 0.0, "avg_logprob": -0.1934427160965769, "compression_ratio": 1.5294117647058822, "no_speech_prob": 3.7852760215173475e-06}, {"id": 482, "seek": 459400, "start": 4617.0, "end": 4623.0, "text": " because Landera knows which client applications are running?", "tokens": [570, 441, 4483, 64, 3255, 597, 6423, 5821, 366, 2614, 30], "temperature": 0.0, "avg_logprob": -0.1934427160965769, "compression_ratio": 1.5294117647058822, "no_speech_prob": 3.7852760215173475e-06}, {"id": 483, "seek": 462300, "start": 4623.0, "end": 4626.0, "text": " Do you have the knowledge or not at all?", "tokens": [1144, 291, 362, 264, 3601, 420, 406, 412, 439, 30], "temperature": 0.0, "avg_logprob": -0.18301795391326256, "compression_ratio": 1.6490384615384615, "no_speech_prob": 3.943526462535374e-05}, {"id": 484, "seek": 462300, "start": 4626.0, "end": 4631.0, "text": " Yeah, so the reason, maybe I'll answer your question backwards.", "tokens": [865, 11, 370, 264, 1778, 11, 1310, 286, 603, 1867, 428, 1168, 12204, 13], "temperature": 0.0, "avg_logprob": -0.18301795391326256, "compression_ratio": 1.6490384615384615, "no_speech_prob": 3.943526462535374e-05}, {"id": 485, "seek": 462300, "start": 4631.0, "end": 4635.0, "text": " So here's why you wouldn't want to remove all the migrations.", "tokens": [407, 510, 311, 983, 291, 2759, 380, 528, 281, 4159, 439, 264, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.18301795391326256, "compression_ratio": 1.6490384615384615, "no_speech_prob": 3.943526462535374e-05}, {"id": 486, "seek": 462300, "start": 4635.0, "end": 4638.0, "text": " You wouldn't want to remove all the migrations", "tokens": [509, 2759, 380, 528, 281, 4159, 439, 264, 6186, 12154], "temperature": 0.0, "avg_logprob": -0.18301795391326256, "compression_ratio": 1.6490384615384615, "no_speech_prob": 3.943526462535374e-05}, {"id": 487, "seek": 462300, "start": 4638.0, "end": 4642.0, "text": " if you wanted to preserve a full stack,", "tokens": [498, 291, 1415, 281, 15665, 257, 1577, 8630, 11], "temperature": 0.0, "avg_logprob": -0.18301795391326256, "compression_ratio": 1.6490384615384615, "no_speech_prob": 3.943526462535374e-05}, {"id": 488, "seek": 462300, "start": 4642.0, "end": 4646.0, "text": " full time history, time traveling debugger.", "tokens": [1577, 565, 2503, 11, 565, 9712, 24083, 1321, 13], "temperature": 0.0, "avg_logprob": -0.18301795391326256, "compression_ratio": 1.6490384615384615, "no_speech_prob": 3.943526462535374e-05}, {"id": 489, "seek": 462300, "start": 4646.0, "end": 4650.0, "text": " And this is a feature that doesn't exist yet.", "tokens": [400, 341, 307, 257, 4111, 300, 1177, 380, 2514, 1939, 13], "temperature": 0.0, "avg_logprob": -0.18301795391326256, "compression_ratio": 1.6490384615384615, "no_speech_prob": 3.943526462535374e-05}, {"id": 490, "seek": 465000, "start": 4650.0, "end": 4654.0, "text": " But if you wanted to use this feature when it does exist,", "tokens": [583, 498, 291, 1415, 281, 764, 341, 4111, 562, 309, 775, 2514, 11], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 491, "seek": 465000, "start": 4654.0, "end": 4658.0, "text": " you can imagine a slider and if you can slide back like 17 versions", "tokens": [291, 393, 3811, 257, 26046, 293, 498, 291, 393, 4137, 646, 411, 3282, 9606], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 492, "seek": 465000, "start": 4658.0, "end": 4662.0, "text": " to a point in time backup", "tokens": [281, 257, 935, 294, 565, 14807], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 493, "seek": 465000, "start": 4662.0, "end": 4666.0, "text": " or like a log stream restoration in Landera, you could do that.", "tokens": [420, 411, 257, 3565, 4309, 23722, 294, 441, 4483, 64, 11, 291, 727, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 494, "seek": 465000, "start": 4666.0, "end": 4669.0, "text": " Because it could go through the migration chain", "tokens": [1436, 309, 727, 352, 807, 264, 17011, 5021], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 495, "seek": 465000, "start": 4669.0, "end": 4672.0, "text": " to get you to the right data source.", "tokens": [281, 483, 291, 281, 264, 558, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 496, "seek": 465000, "start": 4672.0, "end": 4675.0, "text": " So that's why you wouldn't maybe want to remove those.", "tokens": [407, 300, 311, 983, 291, 2759, 380, 1310, 528, 281, 4159, 729, 13], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 497, "seek": 465000, "start": 4675.0, "end": 4677.0, "text": " But saying that...", "tokens": [583, 1566, 300, 485], "temperature": 0.0, "avg_logprob": -0.19460976477896813, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.6821827456587926e-06}, {"id": 498, "seek": 467700, "start": 4677.0, "end": 4680.0, "text": " Can you migrate backwards?", "tokens": [1664, 291, 31821, 12204, 30], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 499, "seek": 467700, "start": 4680.0, "end": 4684.0, "text": " No, but it would be being able to get to the exact state", "tokens": [883, 11, 457, 309, 576, 312, 885, 1075, 281, 483, 281, 264, 1900, 1785], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 500, "seek": 467700, "start": 4684.0, "end": 4687.0, "text": " that you were in for any given version", "tokens": [300, 291, 645, 294, 337, 604, 2212, 3037], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 501, "seek": 467700, "start": 4687.0, "end": 4690.0, "text": " at any point in time in history.", "tokens": [412, 604, 935, 294, 565, 294, 2503, 13], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 502, "seek": 467700, "start": 4690.0, "end": 4693.0, "text": " So if you had the old migration chains,", "tokens": [407, 498, 291, 632, 264, 1331, 17011, 12626, 11], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 503, "seek": 467700, "start": 4693.0, "end": 4696.0, "text": " you can make sure that if the last snapshot was in version 10", "tokens": [291, 393, 652, 988, 300, 498, 264, 1036, 30163, 390, 294, 3037, 1266], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 504, "seek": 467700, "start": 4696.0, "end": 4699.0, "text": " and you were trying to get to it, well, this wouldn't happen", "tokens": [293, 291, 645, 1382, 281, 483, 281, 309, 11, 731, 11, 341, 2759, 380, 1051], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 505, "seek": 467700, "start": 4699.0, "end": 4702.0, "text": " because we take snapshots anyway. So it's a bit of a moot point.", "tokens": [570, 321, 747, 19206, 27495, 4033, 13, 407, 309, 311, 257, 857, 295, 257, 705, 310, 935, 13], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 506, "seek": 467700, "start": 4702.0, "end": 4705.0, "text": " But this idea that you could slide a piece of value through,", "tokens": [583, 341, 1558, 300, 291, 727, 4137, 257, 2522, 295, 2158, 807, 11], "temperature": 0.0, "avg_logprob": -0.20778311752691503, "compression_ratio": 1.6754716981132076, "no_speech_prob": 2.143733036064077e-05}, {"id": 507, "seek": 470500, "start": 4705.0, "end": 4708.0, "text": " like the scenario that somebody told me", "tokens": [411, 264, 9005, 300, 2618, 1907, 385], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 508, "seek": 470500, "start": 4708.0, "end": 4711.0, "text": " where they had a customer that came back like a year later", "tokens": [689, 436, 632, 257, 5474, 300, 1361, 646, 411, 257, 1064, 1780], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 509, "seek": 470500, "start": 4711.0, "end": 4714.0, "text": " and they started seeing events from a year before.", "tokens": [293, 436, 1409, 2577, 3931, 490, 257, 1064, 949, 13], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 510, "seek": 470500, "start": 4714.0, "end": 4717.0, "text": " That person could get a hot reload a year later.", "tokens": [663, 954, 727, 483, 257, 2368, 25628, 257, 1064, 1780, 13], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 511, "seek": 470500, "start": 4717.0, "end": 4720.0, "text": " Like their app could go from version 10,", "tokens": [1743, 641, 724, 727, 352, 490, 3037, 1266, 11], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 512, "seek": 470500, "start": 4720.0, "end": 4723.0, "text": " in Jim's case, to version like 1500", "tokens": [294, 6637, 311, 1389, 11, 281, 3037, 411, 22671], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 513, "seek": 470500, "start": 4723.0, "end": 4727.0, "text": " and it could slide that value like all the way up all the versions", "tokens": [293, 309, 727, 4137, 300, 2158, 411, 439, 264, 636, 493, 439, 264, 9606], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 514, "seek": 470500, "start": 4727.0, "end": 4731.0, "text": " and if they were halfway through filling in a form, in theory,", "tokens": [293, 498, 436, 645, 15461, 807, 10623, 294, 257, 1254, 11, 294, 5261, 11], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 515, "seek": 470500, "start": 4731.0, "end": 4734.0, "text": " the form would upgrade and all their stuff would still be there.", "tokens": [264, 1254, 576, 11484, 293, 439, 641, 1507, 576, 920, 312, 456, 13], "temperature": 0.0, "avg_logprob": -0.22715425892036503, "compression_ratio": 1.7669172932330828, "no_speech_prob": 8.745153900235891e-05}, {"id": 516, "seek": 473400, "start": 4734.0, "end": 4737.0, "text": " That's far-fetched.", "tokens": [663, 311, 1400, 12, 69, 7858, 292, 13], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 517, "seek": 473400, "start": 4737.0, "end": 4740.0, "text": " In theory, that would be possible.", "tokens": [682, 5261, 11, 300, 576, 312, 1944, 13], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 518, "seek": 473400, "start": 4740.0, "end": 4743.0, "text": " From a practical point of view, if you weren't concerned about that,", "tokens": [3358, 257, 8496, 935, 295, 1910, 11, 498, 291, 4999, 380, 5922, 466, 300, 11], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 519, "seek": 473400, "start": 4743.0, "end": 4746.0, "text": " there's no really good reason to keep more than a few versions.", "tokens": [456, 311, 572, 534, 665, 1778, 281, 1066, 544, 813, 257, 1326, 9606, 13], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 520, "seek": 473400, "start": 4746.0, "end": 4749.0, "text": " The only reason you want to keep a few versions is if you wanted to roll back your data.", "tokens": [440, 787, 1778, 291, 528, 281, 1066, 257, 1326, 9606, 307, 498, 291, 1415, 281, 3373, 646, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 521, "seek": 473400, "start": 4749.0, "end": 4752.0, "text": " To say you've done a migration", "tokens": [1407, 584, 291, 600, 1096, 257, 17011], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 522, "seek": 473400, "start": 4752.0, "end": 4755.0, "text": " and you've done the wrong thing in the migration.", "tokens": [293, 291, 600, 1096, 264, 2085, 551, 294, 264, 17011, 13], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 523, "seek": 473400, "start": 4755.0, "end": 4758.0, "text": " You set a dict.empty or a set.empty somewhere where you were being lazy", "tokens": [509, 992, 257, 12569, 13, 4543, 88, 420, 257, 992, 13, 4543, 88, 4079, 689, 291, 645, 885, 14847], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 524, "seek": 473400, "start": 4758.0, "end": 4761.0, "text": " for the moment because you were like, I'll do the migration later", "tokens": [337, 264, 1623, 570, 291, 645, 411, 11, 286, 603, 360, 264, 17011, 1780], "temperature": 0.0, "avg_logprob": -0.2072283660664278, "compression_ratio": 1.7615658362989324, "no_speech_prob": 5.919221439398825e-05}, {"id": 525, "seek": 476100, "start": 4761.0, "end": 4764.0, "text": " and you did it and suddenly you destroyed all your users.", "tokens": [293, 291, 630, 309, 293, 5800, 291, 8937, 439, 428, 5022, 13], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 526, "seek": 476100, "start": 4764.0, "end": 4767.0, "text": " If you wanted to go back in that case or if you had a customer", "tokens": [759, 291, 1415, 281, 352, 646, 294, 300, 1389, 420, 498, 291, 632, 257, 5474], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 527, "seek": 476100, "start": 4767.0, "end": 4770.0, "text": " that had been like, oh my god, it's Wednesday but we just found out on Tuesday", "tokens": [300, 632, 668, 411, 11, 1954, 452, 3044, 11, 309, 311, 10579, 457, 321, 445, 1352, 484, 322, 10017], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 528, "seek": 476100, "start": 4770.0, "end": 4774.0, "text": " that Mario went in and deleted 600 blog posts,", "tokens": [300, 9343, 1437, 294, 293, 22981, 11849, 6968, 12300, 11], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 529, "seek": 476100, "start": 4774.0, "end": 4777.0, "text": " can you please roll back to our state on Sunday?", "tokens": [393, 291, 1767, 3373, 646, 281, 527, 1785, 322, 7776, 30], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 530, "seek": 476100, "start": 4777.0, "end": 4780.0, "text": " That happened to be four versions ago.", "tokens": [663, 2011, 281, 312, 1451, 9606, 2057, 13], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 531, "seek": 476100, "start": 4780.0, "end": 4784.0, "text": " Keeping those migrations around lets us go, yeah, sure, we can load up that old version", "tokens": [30187, 729, 6186, 12154, 926, 6653, 505, 352, 11, 1338, 11, 988, 11, 321, 393, 3677, 493, 300, 1331, 3037], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 532, "seek": 476100, "start": 4784.0, "end": 4788.0, "text": " and bam, it'll upgrade through the last four migration functions", "tokens": [293, 18132, 11, 309, 603, 11484, 807, 264, 1036, 1451, 17011, 6828], "temperature": 0.0, "avg_logprob": -0.2587534790039063, "compression_ratio": 1.6735395189003437, "no_speech_prob": 9.454580867895856e-05}, {"id": 533, "seek": 478800, "start": 4788.0, "end": 4791.0, "text": " into your current app.", "tokens": [666, 428, 2190, 724, 13], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 534, "seek": 478800, "start": 4791.0, "end": 4794.0, "text": " We wouldn't have to roll your whole app back, you can have all the new features", "tokens": [492, 2759, 380, 362, 281, 3373, 428, 1379, 724, 646, 11, 291, 393, 362, 439, 264, 777, 4122], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 535, "seek": 478800, "start": 4794.0, "end": 4797.0, "text": " we can just roll the data back in a safe way.", "tokens": [321, 393, 445, 3373, 264, 1412, 646, 294, 257, 3273, 636, 13], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 536, "seek": 478800, "start": 4797.0, "end": 4800.0, "text": " I guess you could use Git to restore those migrations again.", "tokens": [286, 2041, 291, 727, 764, 16939, 281, 15227, 729, 6186, 12154, 797, 13], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 537, "seek": 478800, "start": 4800.0, "end": 4803.0, "text": " Yeah, so you've got it.", "tokens": [865, 11, 370, 291, 600, 658, 309, 13], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 538, "seek": 478800, "start": 4803.0, "end": 4806.0, "text": " Which is a good feature of Git, right?", "tokens": [3013, 307, 257, 665, 4111, 295, 16939, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 539, "seek": 478800, "start": 4806.0, "end": 4809.0, "text": " Yes, you've got it exactly. From a technical perspective, there's no reason", "tokens": [1079, 11, 291, 600, 658, 309, 2293, 13, 3358, 257, 6191, 4585, 11, 456, 311, 572, 1778], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 540, "seek": 478800, "start": 4809.0, "end": 4812.0, "text": " given that the snapshots would have always had to exist when you deploy,", "tokens": [2212, 300, 264, 19206, 27495, 576, 362, 1009, 632, 281, 2514, 562, 291, 7274, 11], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 541, "seek": 478800, "start": 4812.0, "end": 4815.0, "text": " there's no reason you couldn't clean that up or that Lambda Era", "tokens": [456, 311, 572, 1778, 291, 2809, 380, 2541, 300, 493, 420, 300, 45691, 23071], "temperature": 0.0, "avg_logprob": -0.22132814291751746, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.7964981200057082e-05}, {"id": 542, "seek": 481500, "start": 4815.0, "end": 4818.0, "text": " couldn't clean that up for you.", "tokens": [2809, 380, 2541, 300, 493, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 543, "seek": 481500, "start": 4818.0, "end": 4821.0, "text": " The actual reason that everything is there is I decided early on that I wanted", "tokens": [440, 3539, 1778, 300, 1203, 307, 456, 307, 286, 3047, 2440, 322, 300, 286, 1415], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 544, "seek": 481500, "start": 4821.0, "end": 4824.0, "text": " to keep everything really explicit and visible", "tokens": [281, 1066, 1203, 534, 13691, 293, 8974], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 545, "seek": 481500, "start": 4824.0, "end": 4827.0, "text": " because in my head that would demystify it.", "tokens": [570, 294, 452, 1378, 300, 576, 1371, 38593, 2505, 309, 13], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 546, "seek": 481500, "start": 4827.0, "end": 4830.0, "text": " People could go and actually look at the Elm code and be like,", "tokens": [3432, 727, 352, 293, 767, 574, 412, 264, 2699, 76, 3089, 293, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 547, "seek": 481500, "start": 4830.0, "end": 4833.0, "text": " there's no actual magic here, it's literally just the Elm code", "tokens": [456, 311, 572, 3539, 5585, 510, 11, 309, 311, 3736, 445, 264, 2699, 76, 3089], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 548, "seek": 481500, "start": 4833.0, "end": 4836.0, "text": " and it's literally just there.", "tokens": [293, 309, 311, 3736, 445, 456, 13], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 549, "seek": 481500, "start": 4836.0, "end": 4839.0, "text": " The fact that it's kind of there and you can see it,", "tokens": [440, 1186, 300, 309, 311, 733, 295, 456, 293, 291, 393, 536, 309, 11], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 550, "seek": 481500, "start": 4839.0, "end": 4842.0, "text": " yes, we're doing code generation but we're not hiding it away somewhere", "tokens": [2086, 11, 321, 434, 884, 3089, 5125, 457, 321, 434, 406, 10596, 309, 1314, 4079], "temperature": 0.0, "avg_logprob": -0.18381318520373247, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00012470094952732325}, {"id": 551, "seek": 484200, "start": 4842.0, "end": 4845.0, "text": " and it's not going to break in really weird ways.", "tokens": [293, 309, 311, 406, 516, 281, 1821, 294, 534, 3657, 2098, 13], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 552, "seek": 484200, "start": 4845.0, "end": 4848.0, "text": " If it breaks, it should break with Elm type errors pointing to files", "tokens": [759, 309, 9857, 11, 309, 820, 1821, 365, 2699, 76, 2010, 13603, 12166, 281, 7098], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 553, "seek": 484200, "start": 4848.0, "end": 4851.0, "text": " that you can go and look at and be like, oh yeah, that looks wrong or right", "tokens": [300, 291, 393, 352, 293, 574, 412, 293, 312, 411, 11, 1954, 1338, 11, 300, 1542, 2085, 420, 558], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 554, "seek": 484200, "start": 4851.0, "end": 4854.0, "text": " or whatever it is.", "tokens": [420, 2035, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 555, "seek": 484200, "start": 4854.0, "end": 4857.0, "text": " It doesn't seem to happen often to my absolute bewilderment", "tokens": [467, 1177, 380, 1643, 281, 1051, 2049, 281, 452, 8236, 17897, 793, 260, 518], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 556, "seek": 484200, "start": 4857.0, "end": 4860.0, "text": " but I've always got this terror that I've implemented something wrong", "tokens": [457, 286, 600, 1009, 658, 341, 8127, 300, 286, 600, 12270, 746, 2085], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 557, "seek": 484200, "start": 4860.0, "end": 4863.0, "text": " and someone's going to do a migration", "tokens": [293, 1580, 311, 516, 281, 360, 257, 17011], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 558, "seek": 484200, "start": 4863.0, "end": 4866.0, "text": " and it's just going to horrendously generate the wrong stuff.", "tokens": [293, 309, 311, 445, 516, 281, 49520, 5098, 8460, 264, 2085, 1507, 13], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 559, "seek": 484200, "start": 4866.0, "end": 4869.0, "text": " So I kind of want that also to be visible so that the user can see", "tokens": [407, 286, 733, 295, 528, 300, 611, 281, 312, 8974, 370, 300, 264, 4195, 393, 536], "temperature": 0.0, "avg_logprob": -0.16452519452130354, "compression_ratio": 1.705685618729097, "no_speech_prob": 5.6819626479409635e-06}, {"id": 560, "seek": 486900, "start": 4869.0, "end": 4872.0, "text": " and if I've done the wrong thing, they could fix it.", "tokens": [293, 498, 286, 600, 1096, 264, 2085, 551, 11, 436, 727, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 561, "seek": 486900, "start": 4872.0, "end": 4875.0, "text": " You could fix the type snapshot yourself or you could modify stuff.", "tokens": [509, 727, 3191, 264, 2010, 30163, 1803, 420, 291, 727, 16927, 1507, 13], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 562, "seek": 486900, "start": 4875.0, "end": 4878.0, "text": " Thankfully, there's some gen issues in the latest migration generation", "tokens": [28344, 11, 456, 311, 512, 1049, 2663, 294, 264, 6792, 17011, 5125], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 563, "seek": 486900, "start": 4878.0, "end": 4881.0, "text": " but so far there haven't been many or maybe only one or two a long time ago", "tokens": [457, 370, 1400, 456, 2378, 380, 668, 867, 420, 1310, 787, 472, 420, 732, 257, 938, 565, 2057], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 564, "seek": 486900, "start": 4881.0, "end": 4884.0, "text": " issues with the type snapshotting.", "tokens": [2663, 365, 264, 2010, 30163, 783, 13], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 565, "seek": 486900, "start": 4884.0, "end": 4887.0, "text": " So far the type snapshots extract correctly it seems.", "tokens": [407, 1400, 264, 2010, 19206, 27495, 8947, 8944, 309, 2544, 13], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 566, "seek": 486900, "start": 4887.0, "end": 4890.0, "text": " Knock on wood, obviously.", "tokens": [34017, 322, 4576, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 567, "seek": 486900, "start": 4890.0, "end": 4893.0, "text": " So yeah, it's more a social reality thing.", "tokens": [407, 1338, 11, 309, 311, 544, 257, 2093, 4103, 551, 13], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 568, "seek": 486900, "start": 4893.0, "end": 4896.0, "text": " I think it's a bit of a shame that Elm is not a big part of the Elm community", "tokens": [286, 519, 309, 311, 257, 857, 295, 257, 10069, 300, 2699, 76, 307, 406, 257, 955, 644, 295, 264, 2699, 76, 1768], "temperature": 0.0, "avg_logprob": -0.4771132967365322, "compression_ratio": 1.7226027397260273, "no_speech_prob": 1.2217541552672628e-05}, {"id": 569, "seek": 489600, "start": 4896.0, "end": 4899.0, "text": " so yeah, it's more a social reason", "tokens": [370, 1338, 11, 309, 311, 544, 257, 2093, 1778], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 570, "seek": 489600, "start": 4899.0, "end": 4904.0, "text": " for them being there than a technical necessity, so to speak.", "tokens": [337, 552, 885, 456, 813, 257, 6191, 24217, 11, 370, 281, 1710, 13], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 571, "seek": 489600, "start": 4904.0, "end": 4907.0, "text": " So yeah, we might make that more magical in future", "tokens": [407, 1338, 11, 321, 1062, 652, 300, 544, 12066, 294, 2027], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 572, "seek": 489600, "start": 4907.0, "end": 4912.0, "text": " and then Elm review won't have problems, side effects as it were.", "tokens": [293, 550, 2699, 76, 3131, 1582, 380, 362, 2740, 11, 1252, 5065, 382, 309, 645, 13], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 573, "seek": 489600, "start": 4912.0, "end": 4916.0, "text": " So the Evergreen migration auto generation", "tokens": [407, 264, 12123, 27399, 17011, 8399, 5125], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 574, "seek": 489600, "start": 4916.0, "end": 4919.0, "text": " which we haven't really explicitly talked about", "tokens": [597, 321, 2378, 380, 534, 20803, 2825, 466], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 575, "seek": 489600, "start": 4919.0, "end": 4922.0, "text": " but I understand that was a big pain point", "tokens": [457, 286, 1223, 300, 390, 257, 955, 1822, 935], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 576, "seek": 489600, "start": 4922.0, "end": 4925.0, "text": " that was addressed by this latest release", "tokens": [300, 390, 13847, 538, 341, 6792, 4374], "temperature": 0.0, "avg_logprob": -0.2203107366756517, "compression_ratio": 1.58130081300813, "no_speech_prob": 4.4688604248221964e-05}, {"id": 577, "seek": 492500, "start": 4925.0, "end": 4928.0, "text": " which is v1.1.", "tokens": [597, 307, 371, 16, 13, 16, 13], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 578, "seek": 492500, "start": 4928.0, "end": 4931.0, "text": " Is there much to say about that", "tokens": [1119, 456, 709, 281, 584, 466, 300], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 579, "seek": 492500, "start": 4931.0, "end": 4936.0, "text": " besides that it does most of the tedious work for you?", "tokens": [11868, 300, 309, 775, 881, 295, 264, 38284, 589, 337, 291, 30], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 580, "seek": 492500, "start": 4936.0, "end": 4938.0, "text": " That's kind of the headline of it.", "tokens": [663, 311, 733, 295, 264, 28380, 295, 309, 13], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 581, "seek": 492500, "start": 4938.0, "end": 4939.0, "text": " Yeah, that's the headline.", "tokens": [865, 11, 300, 311, 264, 28380, 13], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 582, "seek": 492500, "start": 4939.0, "end": 4942.0, "text": " So the thing that people would run into that I think people would find confusing", "tokens": [407, 264, 551, 300, 561, 576, 1190, 666, 300, 286, 519, 561, 576, 915, 13181], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 583, "seek": 492500, "start": 4942.0, "end": 4944.0, "text": " is like, say you had a custom type.", "tokens": [741, 82, 411, 11, 584, 291, 632, 257, 2375, 2010, 13], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 584, "seek": 492500, "start": 4944.0, "end": 4946.0, "text": " Let's say we had the Ice Cream custom type", "tokens": [961, 311, 584, 321, 632, 264, 15332, 25358, 2375, 2010], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 585, "seek": 492500, "start": 4946.0, "end": 4948.0, "text": " but with lots and lots and lots of flavors.", "tokens": [457, 365, 3195, 293, 3195, 293, 3195, 295, 16303, 13], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 586, "seek": 492500, "start": 4948.0, "end": 4950.0, "text": " Let's say we had 200 flavors.", "tokens": [961, 311, 584, 321, 632, 2331, 16303, 13], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 587, "seek": 492500, "start": 4950.0, "end": 4953.0, "text": " And say you've changed a field somewhere else", "tokens": [400, 584, 291, 600, 3105, 257, 2519, 4079, 1646], "temperature": 0.6, "avg_logprob": -0.2964140426280887, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.00017117825336754322}, {"id": 588, "seek": 495300, "start": 4953.0, "end": 4954.88, "text": " and you have to write migrations.", "tokens": [293, 291, 362, 281, 2464, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 589, "seek": 495300, "start": 4954.88, "end": 4958.64, "text": " So like, not sadly, but as a trade-off", "tokens": [407, 411, 11, 406, 22023, 11, 457, 382, 257, 4923, 12, 4506], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 590, "seek": 495300, "start": 4958.64, "end": 4961.2, "text": " of Elm's current equality model, right?", "tokens": [295, 2699, 76, 311, 2190, 14949, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 591, "seek": 495300, "start": 4961.2, "end": 4963.64, "text": " You couldn't just take that old custom type", "tokens": [509, 2809, 380, 445, 747, 300, 1331, 2375, 2010], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 592, "seek": 495300, "start": 4963.64, "end": 4965.0, "text": " and cram it into the new one, right?", "tokens": [293, 941, 335, 309, 666, 264, 777, 472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 593, "seek": 495300, "start": 4965.0, "end": 4966.88, "text": " Like Elm would be like, well, these are different.", "tokens": [1743, 2699, 76, 576, 312, 411, 11, 731, 11, 613, 366, 819, 13], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 594, "seek": 495300, "start": 4966.88, "end": 4968.88, "text": " They're in different files, right?", "tokens": [814, 434, 294, 819, 7098, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 595, "seek": 495300, "start": 4968.88, "end": 4969.88, "text": " They're different namespaces.", "tokens": [814, 434, 819, 5288, 79, 2116, 13], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 596, "seek": 495300, "start": 4969.88, "end": 4971.24, "text": " These are different values,", "tokens": [1981, 366, 819, 4190, 11], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 597, "seek": 495300, "start": 4971.24, "end": 4973.44, "text": " even though they're structurally the same.", "tokens": [754, 1673, 436, 434, 6594, 6512, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 598, "seek": 495300, "start": 4973.44, "end": 4977.84, "text": " So in prior to version 1.1,", "tokens": [407, 294, 4059, 281, 3037, 502, 13, 16, 11], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 599, "seek": 495300, "start": 4977.84, "end": 4980.24, "text": " the migration would only generate the placeholders.", "tokens": [264, 17011, 576, 787, 8460, 264, 1081, 12916, 13], "temperature": 0.0, "avg_logprob": -0.2586515651029699, "compression_ratio": 1.6788321167883211, "no_speech_prob": 0.020959850400686264}, {"id": 600, "seek": 498024, "start": 4980.24, "end": 4983.28, "text": " Like, so the top level function types and names,", "tokens": [1743, 11, 370, 264, 1192, 1496, 2445, 3467, 293, 5288, 11], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 601, "seek": 498024, "start": 4983.28, "end": 4984.12, "text": " and it'd be like, okay,", "tokens": [293, 309, 1116, 312, 411, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 602, "seek": 498024, "start": 4984.12, "end": 4986.66, "text": " here's the six migration functions I need,", "tokens": [510, 311, 264, 2309, 17011, 6828, 286, 643, 11], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 603, "seek": 498024, "start": 4986.66, "end": 4988.0, "text": " but you have to go to all the work", "tokens": [457, 291, 362, 281, 352, 281, 439, 264, 589], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 604, "seek": 498024, "start": 4988.0, "end": 4990.0199999999995, "text": " of implementing what's inside them,", "tokens": [295, 18114, 437, 311, 1854, 552, 11], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 605, "seek": 498024, "start": 4990.0199999999995, "end": 4992.28, "text": " including writing a massive function,", "tokens": [3009, 3579, 257, 5994, 2445, 11], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 606, "seek": 498024, "start": 4992.28, "end": 4994.32, "text": " migrate ice cream flavor,", "tokens": [31821, 4435, 4689, 6813, 11], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 607, "seek": 498024, "start": 4994.32, "end": 4998.96, "text": " case old of every single old variant", "tokens": [1389, 1331, 295, 633, 2167, 1331, 17501], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 608, "seek": 498024, "start": 4998.96, "end": 5001.7, "text": " matches to every single new variant.", "tokens": [10676, 281, 633, 2167, 777, 17501, 13], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 609, "seek": 498024, "start": 5001.7, "end": 5003.76, "text": " The only difference being they're in different namespaces,", "tokens": [440, 787, 2649, 885, 436, 434, 294, 819, 5288, 79, 2116, 11], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 610, "seek": 498024, "start": 5003.76, "end": 5005.099999999999, "text": " but otherwise it's like the same text", "tokens": [457, 5911, 309, 311, 411, 264, 912, 2487], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 611, "seek": 498024, "start": 5005.099999999999, "end": 5006.76, "text": " over and over and over and over, right?", "tokens": [670, 293, 670, 293, 670, 293, 670, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24759356689453124, "compression_ratio": 1.7330827067669172, "no_speech_prob": 2.2125221221358515e-05}, {"id": 612, "seek": 500676, "start": 5006.76, "end": 5010.280000000001, "text": " So I think this was frustrating as a user experience", "tokens": [407, 286, 519, 341, 390, 16522, 382, 257, 4195, 1752], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 613, "seek": 500676, "start": 5010.280000000001, "end": 5013.2, "text": " because it was like, if I'd only changed one field,", "tokens": [570, 309, 390, 411, 11, 498, 286, 1116, 787, 3105, 472, 2519, 11], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 614, "seek": 500676, "start": 5013.2, "end": 5015.42, "text": " now I'm writing migrations for all fields", "tokens": [586, 286, 478, 3579, 6186, 12154, 337, 439, 7909], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 615, "seek": 500676, "start": 5015.42, "end": 5017.2, "text": " and all custom types everywhere,", "tokens": [293, 439, 2375, 3467, 5315, 11], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 616, "seek": 500676, "start": 5017.2, "end": 5019.04, "text": " and I have to do this every time.", "tokens": [293, 286, 362, 281, 360, 341, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 617, "seek": 500676, "start": 5019.04, "end": 5020.54, "text": " So it wasn't the end of the world.", "tokens": [407, 309, 2067, 380, 264, 917, 295, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 618, "seek": 500676, "start": 5020.54, "end": 5023.88, "text": " Some people found like, okay, once I've done it once,", "tokens": [2188, 561, 1352, 411, 11, 1392, 11, 1564, 286, 600, 1096, 309, 1564, 11], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 619, "seek": 500676, "start": 5023.88, "end": 5025.68, "text": " I can pretty much kind of copy paste", "tokens": [286, 393, 1238, 709, 733, 295, 5055, 9163], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 620, "seek": 500676, "start": 5025.68, "end": 5027.76, "text": " a lot of my migration implementation,", "tokens": [257, 688, 295, 452, 17011, 11420, 11], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 621, "seek": 500676, "start": 5027.76, "end": 5028.6, "text": " but I wasn't happy with it.", "tokens": [457, 286, 2067, 380, 2055, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 622, "seek": 500676, "start": 5028.6, "end": 5030.24, "text": " If you've done it 600 times.", "tokens": [759, 291, 600, 1096, 309, 11849, 1413, 13], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 623, "seek": 500676, "start": 5030.24, "end": 5033.360000000001, "text": " Yeah, Jim got really, really, really good", "tokens": [865, 11, 6637, 658, 534, 11, 534, 11, 534, 665], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 624, "seek": 500676, "start": 5033.360000000001, "end": 5035.860000000001, "text": " at doing these migrations, clearly.", "tokens": [412, 884, 613, 6186, 12154, 11, 4448, 13], "temperature": 0.0, "avg_logprob": -0.21587511011072108, "compression_ratio": 1.7181208053691275, "no_speech_prob": 2.3090647971457656e-07}, {"id": 625, "seek": 503586, "start": 5035.86, "end": 5038.5199999999995, "text": " But for newcomers as well, it was really confusing, right?", "tokens": [583, 337, 40014, 433, 382, 731, 11, 309, 390, 534, 13181, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 626, "seek": 503586, "start": 5038.5199999999995, "end": 5043.42, "text": " Like it made them confused, extra confused.", "tokens": [1743, 309, 1027, 552, 9019, 11, 2857, 9019, 13], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 627, "seek": 503586, "start": 5043.42, "end": 5044.5599999999995, "text": " Cause it's like, I'm like, oh yeah,", "tokens": [10865, 309, 311, 411, 11, 286, 478, 411, 11, 1954, 1338, 11], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 628, "seek": 503586, "start": 5044.5599999999995, "end": 5046.96, "text": " you generate migrations for your change types", "tokens": [291, 8460, 6186, 12154, 337, 428, 1319, 3467], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 629, "seek": 503586, "start": 5046.96, "end": 5048.92, "text": " and also for these types that haven't changed at all.", "tokens": [293, 611, 337, 613, 3467, 300, 2378, 380, 3105, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 630, "seek": 503586, "start": 5048.92, "end": 5050.32, "text": " And it's like, once you get through it", "tokens": [400, 309, 311, 411, 11, 1564, 291, 483, 807, 309], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 631, "seek": 503586, "start": 5050.32, "end": 5052.04, "text": " and once you think about it, you're like, oh yeah, okay.", "tokens": [293, 1564, 291, 519, 466, 309, 11, 291, 434, 411, 11, 1954, 1338, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 632, "seek": 503586, "start": 5052.04, "end": 5054.32, "text": " I can understand now if I understand Elm", "tokens": [286, 393, 1223, 586, 498, 286, 1223, 2699, 76], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 633, "seek": 503586, "start": 5054.32, "end": 5056.5199999999995, "text": " why this is necessary, but yeah,", "tokens": [983, 341, 307, 4818, 11, 457, 1338, 11], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 634, "seek": 503586, "start": 5056.5199999999995, "end": 5059.0, "text": " it was getting you to have to think about something else.", "tokens": [309, 390, 1242, 291, 281, 362, 281, 519, 466, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 635, "seek": 503586, "start": 5059.0, "end": 5063.96, "text": " So yeah, long story short now, Evergreen,", "tokens": [407, 1338, 11, 938, 1657, 2099, 586, 11, 12123, 27399, 11], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 636, "seek": 503586, "start": 5063.96, "end": 5065.679999999999, "text": " where those types haven't changed,", "tokens": [689, 729, 3467, 2378, 380, 3105, 11], "temperature": 0.0, "avg_logprob": -0.22798018022017044, "compression_ratio": 1.8221476510067114, "no_speech_prob": 1.1189395081601106e-06}, {"id": 637, "seek": 506568, "start": 5065.68, "end": 5068.52, "text": " it does a pretty good job at basically generating", "tokens": [309, 775, 257, 1238, 665, 1691, 412, 1936, 17746], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 638, "seek": 506568, "start": 5068.52, "end": 5070.04, "text": " a bunch of that for you.", "tokens": [257, 3840, 295, 300, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 639, "seek": 506568, "start": 5070.04, "end": 5072.76, "text": " And so it tries to, as deeply as possible,", "tokens": [400, 370, 309, 9898, 281, 11, 382, 8760, 382, 1944, 11], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 640, "seek": 506568, "start": 5072.76, "end": 5076.280000000001, "text": " I mentioned it zips effectively these two types.", "tokens": [286, 2835, 309, 710, 2600, 8659, 613, 732, 3467, 13], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 641, "seek": 506568, "start": 5076.280000000001, "end": 5078.280000000001, "text": " It starts at the top and keeps going through them.", "tokens": [467, 3719, 412, 264, 1192, 293, 5965, 516, 807, 552, 13], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 642, "seek": 506568, "start": 5078.280000000001, "end": 5080.92, "text": " So if it's a record, it tries to pair the record fields", "tokens": [407, 498, 309, 311, 257, 2136, 11, 309, 9898, 281, 6119, 264, 2136, 7909], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 643, "seek": 506568, "start": 5080.92, "end": 5082.8, "text": " by name and so on and so forth.", "tokens": [538, 1315, 293, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 644, "seek": 506568, "start": 5082.8, "end": 5085.780000000001, "text": " And then yeah, anything that's been added,", "tokens": [400, 550, 1338, 11, 1340, 300, 311, 668, 3869, 11], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 645, "seek": 506568, "start": 5085.780000000001, "end": 5087.56, "text": " it gives you like a little notice to be like,", "tokens": [309, 2709, 291, 411, 257, 707, 3449, 281, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 646, "seek": 506568, "start": 5087.56, "end": 5089.08, "text": " hey, this variant has been added.", "tokens": [4177, 11, 341, 17501, 575, 668, 3869, 13], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 647, "seek": 506568, "start": 5089.08, "end": 5091.4400000000005, "text": " It's just a reminder in case you wanted some old variants", "tokens": [467, 311, 445, 257, 13548, 294, 1389, 291, 1415, 512, 1331, 21669], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 648, "seek": 506568, "start": 5091.4400000000005, "end": 5093.9800000000005, "text": " to map to this new variant that doesn't exist yet.", "tokens": [281, 4471, 281, 341, 777, 17501, 300, 1177, 380, 2514, 1939, 13], "temperature": 0.0, "avg_logprob": -0.19709273020426432, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.6432107814762276e-06}, {"id": 649, "seek": 509398, "start": 5093.98, "end": 5097.5599999999995, "text": " And also, hey, this variant has been removed, right?", "tokens": [400, 611, 11, 4177, 11, 341, 17501, 575, 668, 7261, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 650, "seek": 509398, "start": 5097.5599999999995, "end": 5100.04, "text": " Like what do you wanna do with this old value?", "tokens": [1743, 437, 360, 291, 1948, 360, 365, 341, 1331, 2158, 30], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 651, "seek": 509398, "start": 5100.04, "end": 5101.4, "text": " Cause it has nowhere to go.", "tokens": [10865, 309, 575, 11159, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 652, "seek": 509398, "start": 5101.4, "end": 5106.0, "text": " So yeah, that now tries to be a lot more kind of automatic.", "tokens": [407, 1338, 11, 300, 586, 9898, 281, 312, 257, 688, 544, 733, 295, 12509, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 653, "seek": 509398, "start": 5106.0, "end": 5108.28, "text": " So yeah, the feedback so far is pretty good.", "tokens": [407, 1338, 11, 264, 5824, 370, 1400, 307, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 654, "seek": 509398, "start": 5108.28, "end": 5109.5599999999995, "text": " And Jim's happy at least.", "tokens": [400, 6637, 311, 2055, 412, 1935, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 655, "seek": 509398, "start": 5109.5599999999995, "end": 5113.0, "text": " He's my, I think he suffered the pain point the most", "tokens": [634, 311, 452, 11, 286, 519, 415, 12770, 264, 1822, 935, 264, 881], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 656, "seek": 509398, "start": 5113.0, "end": 5115.24, "text": " of anybody categorically.", "tokens": [295, 4472, 19250, 984, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 657, "seek": 509398, "start": 5115.24, "end": 5116.639999999999, "text": " So he's told me he's enjoying it.", "tokens": [407, 415, 311, 1907, 385, 415, 311, 9929, 309, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 658, "seek": 509398, "start": 5116.639999999999, "end": 5119.48, "text": " And he says migrations only take him like,", "tokens": [400, 415, 1619, 6186, 12154, 787, 747, 796, 411, 11], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 659, "seek": 509398, "start": 5119.48, "end": 5121.16, "text": " you know, a minute or two now to sort out.", "tokens": [291, 458, 11, 257, 3456, 420, 732, 586, 281, 1333, 484, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 660, "seek": 509398, "start": 5121.16, "end": 5122.5599999999995, "text": " So yeah, that was the call.", "tokens": [407, 1338, 11, 300, 390, 264, 818, 13], "temperature": 0.0, "avg_logprob": -0.2582875329094964, "compression_ratio": 1.6385135135135136, "no_speech_prob": 1.9637752757262206e-06}, {"id": 661, "seek": 512256, "start": 5122.56, "end": 5123.84, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 662, "seek": 512256, "start": 5123.84, "end": 5124.96, "text": " That's great.", "tokens": [663, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 663, "seek": 512256, "start": 5124.96, "end": 5126.88, "text": " So one thing I've been curious about,", "tokens": [407, 472, 551, 286, 600, 668, 6369, 466, 11], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 664, "seek": 512256, "start": 5126.88, "end": 5131.88, "text": " so this new release also ships with the Elm PKG's JS spec.", "tokens": [370, 341, 777, 4374, 611, 11434, 365, 264, 2699, 76, 49475, 38, 311, 33063, 1608, 13], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 665, "seek": 512256, "start": 5132.92, "end": 5135.6, "text": " And I've been curious like how,", "tokens": [400, 286, 600, 668, 6369, 411, 577, 11], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 666, "seek": 512256, "start": 5135.6, "end": 5139.4800000000005, "text": " so from what I understand before this,", "tokens": [370, 490, 437, 286, 1223, 949, 341, 11], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 667, "seek": 512256, "start": 5139.4800000000005, "end": 5143.400000000001, "text": " with a Lambda app, you couldn't just add a.js file", "tokens": [365, 257, 45691, 724, 11, 291, 2809, 380, 445, 909, 257, 2411, 25530, 3991], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 668, "seek": 512256, "start": 5143.400000000001, "end": 5147.96, "text": " and ship that and arbitrarily add ports", "tokens": [293, 5374, 300, 293, 19071, 3289, 909, 18160], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 669, "seek": 512256, "start": 5147.96, "end": 5150.92, "text": " and JavaScript behavior on the page, right?", "tokens": [293, 15778, 5223, 322, 264, 3028, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.28090441956812023, "compression_ratio": 1.4504504504504505, "no_speech_prob": 5.5072709983505774e-06}, {"id": 670, "seek": 515092, "start": 5150.92, "end": 5154.4800000000005, "text": " So I was really curious to understand like,", "tokens": [407, 286, 390, 534, 6369, 281, 1223, 411, 11], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 671, "seek": 515092, "start": 5154.4800000000005, "end": 5159.4800000000005, "text": " how does that design decision and how does Elm PKG JS", "tokens": [577, 775, 300, 1715, 3537, 293, 577, 775, 2699, 76, 49475, 38, 33063], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 672, "seek": 515092, "start": 5161.56, "end": 5164.4400000000005, "text": " fit into the concept of evergreen migrations", "tokens": [3318, 666, 264, 3410, 295, 1562, 27399, 6186, 12154], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 673, "seek": 515092, "start": 5164.4400000000005, "end": 5167.56, "text": " with the front end and the guarantees you're trying to give", "tokens": [365, 264, 1868, 917, 293, 264, 32567, 291, 434, 1382, 281, 976], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 674, "seek": 515092, "start": 5167.56, "end": 5169.4, "text": " in a front end Lambda application", "tokens": [294, 257, 1868, 917, 45691, 3861], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 675, "seek": 515092, "start": 5169.4, "end": 5172.24, "text": " or the front end part of a Lambda application?", "tokens": [420, 264, 1868, 917, 644, 295, 257, 45691, 3861, 30], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 676, "seek": 515092, "start": 5172.24, "end": 5173.08, "text": " Yeah, absolutely.", "tokens": [865, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 677, "seek": 515092, "start": 5173.08, "end": 5173.9, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 678, "seek": 515092, "start": 5173.9, "end": 5176.4800000000005, "text": " So in short, there's a few features in Lambda", "tokens": [407, 294, 2099, 11, 456, 311, 257, 1326, 4122, 294, 45691], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 679, "seek": 515092, "start": 5176.4800000000005, "end": 5178.42, "text": " that have actually been there for a few versions.", "tokens": [300, 362, 767, 668, 456, 337, 257, 1326, 9606, 13], "temperature": 0.0, "avg_logprob": -0.23535258429391043, "compression_ratio": 1.6356589147286822, "no_speech_prob": 9.721159131004242e-07}, {"id": 680, "seek": 517842, "start": 5178.42, "end": 5181.04, "text": " And I've been kind of trialing it out with some customers", "tokens": [400, 286, 600, 668, 733, 295, 1376, 4270, 309, 484, 365, 512, 4581], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 681, "seek": 517842, "start": 5181.04, "end": 5183.68, "text": " who've ran into certain kind of limitations", "tokens": [567, 600, 5872, 666, 1629, 733, 295, 15705], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 682, "seek": 517842, "start": 5183.68, "end": 5185.2, "text": " and they needed solutions for.", "tokens": [293, 436, 2978, 6547, 337, 13], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 683, "seek": 517842, "start": 5185.2, "end": 5187.16, "text": " And so what I announced in the last version,", "tokens": [400, 370, 437, 286, 7548, 294, 264, 1036, 3037, 11], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 684, "seek": 517842, "start": 5187.16, "end": 5189.36, "text": " I think was this idea of labs.", "tokens": [286, 519, 390, 341, 1558, 295, 20339, 13], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 685, "seek": 517842, "start": 5189.36, "end": 5192.0, "text": " So Lambda Labs is like a set of features", "tokens": [407, 45691, 40047, 307, 411, 257, 992, 295, 4122], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 686, "seek": 517842, "start": 5192.0, "end": 5194.4800000000005, "text": " that are in Lambda that you can use in production,", "tokens": [300, 366, 294, 45691, 300, 291, 393, 764, 294, 4265, 11], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 687, "seek": 517842, "start": 5194.4800000000005, "end": 5195.64, "text": " but they're marked labs", "tokens": [457, 436, 434, 12658, 20339], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 688, "seek": 517842, "start": 5195.64, "end": 5197.68, "text": " because it's kind of like buyer beware, right?", "tokens": [570, 309, 311, 733, 295, 411, 24645, 312, 3039, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 689, "seek": 517842, "start": 5197.68, "end": 5201.04, "text": " It's like, there's a reason this isn't recommended yet", "tokens": [467, 311, 411, 11, 456, 311, 257, 1778, 341, 1943, 380, 9628, 1939], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 690, "seek": 517842, "start": 5201.04, "end": 5203.04, "text": " or isn't part of the mainline platform.", "tokens": [420, 1943, 380, 644, 295, 264, 2135, 1889, 3663, 13], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 691, "seek": 517842, "start": 5203.04, "end": 5205.68, "text": " So Elm PKG JS, which kind of, yeah,", "tokens": [407, 2699, 76, 49475, 38, 33063, 11, 597, 733, 295, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 692, "seek": 517842, "start": 5205.68, "end": 5207.2, "text": " kind of started from the spec,", "tokens": [733, 295, 1409, 490, 264, 1608, 11], "temperature": 0.0, "avg_logprob": -0.20504062627655228, "compression_ratio": 1.6761006289308176, "no_speech_prob": 1.061486364051234e-05}, {"id": 693, "seek": 520720, "start": 5207.2, "end": 5209.48, "text": " which I was hoping maybe it would take off,", "tokens": [597, 286, 390, 7159, 1310, 309, 576, 747, 766, 11], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 694, "seek": 520720, "start": 5209.48, "end": 5212.28, "text": " but it hasn't yet, but maybe there's still time.", "tokens": [457, 309, 6132, 380, 1939, 11, 457, 1310, 456, 311, 920, 565, 13], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 695, "seek": 520720, "start": 5212.28, "end": 5213.96, "text": " But the idea was to be like,", "tokens": [583, 264, 1558, 390, 281, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 696, "seek": 520720, "start": 5213.96, "end": 5216.08, "text": " I'd noticed this problem where a lot of the JavaScript", "tokens": [286, 1116, 5694, 341, 1154, 689, 257, 688, 295, 264, 15778], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 697, "seek": 520720, "start": 5216.08, "end": 5218.679999999999, "text": " that people wanted to use is this.", "tokens": [300, 561, 1415, 281, 764, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 698, "seek": 520720, "start": 5218.679999999999, "end": 5220.58, "text": " And I think as an Elm community,", "tokens": [400, 286, 519, 382, 364, 2699, 76, 1768, 11], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 699, "seek": 520720, "start": 5220.58, "end": 5222.0, "text": " we've talked about this problem a few times", "tokens": [321, 600, 2825, 466, 341, 1154, 257, 1326, 1413], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 700, "seek": 520720, "start": 5222.0, "end": 5222.84, "text": " where we've got like,", "tokens": [689, 321, 600, 658, 411, 11], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 701, "seek": 520720, "start": 5222.84, "end": 5224.44, "text": " there's certain Elm packages that are like,", "tokens": [456, 311, 1629, 2699, 76, 17401, 300, 366, 411, 11], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 702, "seek": 520720, "start": 5224.44, "end": 5227.5199999999995, "text": " hey, this Elm package requires some ports", "tokens": [4177, 11, 341, 2699, 76, 7372, 7029, 512, 18160], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 703, "seek": 520720, "start": 5227.5199999999995, "end": 5229.36, "text": " and some JavaScript set up.", "tokens": [293, 512, 15778, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 704, "seek": 520720, "start": 5229.36, "end": 5231.88, "text": " Here's a bunch of long-winded instructions", "tokens": [1692, 311, 257, 3840, 295, 938, 12, 12199, 292, 9415], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 705, "seek": 520720, "start": 5231.88, "end": 5234.88, "text": " of varying consistency between packages", "tokens": [295, 22984, 14416, 1296, 17401], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 706, "seek": 520720, "start": 5234.88, "end": 5236.32, "text": " of how to do that.", "tokens": [295, 577, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.22965975443522135, "compression_ratio": 1.7189542483660132, "no_speech_prob": 6.143970949779032e-06}, {"id": 707, "seek": 523632, "start": 5236.32, "end": 5237.799999999999, "text": " And it feels like,", "tokens": [400, 309, 3417, 411, 11], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 708, "seek": 523632, "start": 5239.12, "end": 5240.5599999999995, "text": " I don't think it's a massive issue,", "tokens": [286, 500, 380, 519, 309, 311, 257, 5994, 2734, 11], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 709, "seek": 523632, "start": 5240.5599999999995, "end": 5243.08, "text": " but it's kind of like, it's just a bit,", "tokens": [457, 309, 311, 733, 295, 411, 11, 309, 311, 445, 257, 857, 11], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 710, "seek": 523632, "start": 5243.08, "end": 5244.12, "text": " it's a bit painful.", "tokens": [309, 311, 257, 857, 11697, 13], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 711, "seek": 523632, "start": 5244.12, "end": 5245.5599999999995, "text": " I was always like, oh, how do I do this?", "tokens": [286, 390, 1009, 411, 11, 1954, 11, 577, 360, 286, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 712, "seek": 523632, "start": 5245.5599999999995, "end": 5247.16, "text": " And you paste this and where do I paste it?", "tokens": [400, 291, 9163, 341, 293, 689, 360, 286, 9163, 309, 30], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 713, "seek": 523632, "start": 5247.16, "end": 5248.5599999999995, "text": " And should I put that on this file?", "tokens": [400, 820, 286, 829, 300, 322, 341, 3991, 30], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 714, "seek": 523632, "start": 5248.5599999999995, "end": 5249.5599999999995, "text": " And what bundler do I use?", "tokens": [400, 437, 13882, 1918, 360, 286, 764, 30], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 715, "seek": 523632, "start": 5249.5599999999995, "end": 5250.88, "text": " I was kind of thinking about that experience", "tokens": [286, 390, 733, 295, 1953, 466, 300, 1752], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 716, "seek": 523632, "start": 5250.88, "end": 5251.719999999999, "text": " with Lambda being like,", "tokens": [365, 45691, 885, 411, 11], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 717, "seek": 523632, "start": 5251.719999999999, "end": 5253.12, "text": " what would be a nicer way to do this?", "tokens": [437, 576, 312, 257, 22842, 636, 281, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 718, "seek": 523632, "start": 5253.12, "end": 5254.639999999999, "text": " And with Evergreen in mind, right?", "tokens": [400, 365, 12123, 27399, 294, 1575, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 719, "seek": 523632, "start": 5254.639999999999, "end": 5256.66, "text": " And also this restriction where,", "tokens": [400, 611, 341, 29529, 689, 11], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 720, "seek": 523632, "start": 5256.66, "end": 5258.719999999999, "text": " we don't want this on the backend at the moment.", "tokens": [321, 500, 380, 528, 341, 322, 264, 38087, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 721, "seek": 523632, "start": 5258.719999999999, "end": 5261.36, "text": " So in the front end, it was like, okay,", "tokens": [407, 294, 264, 1868, 917, 11, 309, 390, 411, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 722, "seek": 523632, "start": 5261.36, "end": 5264.92, "text": " a great example and a package that kind of got native,", "tokens": [257, 869, 1365, 293, 257, 7372, 300, 733, 295, 658, 8470, 11], "temperature": 0.0, "avg_logprob": -0.23518723868281463, "compression_ratio": 1.8444444444444446, "no_speech_prob": 4.6837423724355176e-05}, {"id": 723, "seek": 526492, "start": 5264.92, "end": 5266.96, "text": " quote unquote, a support for Lambda era", "tokens": [6513, 37557, 11, 257, 1406, 337, 45691, 4249], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 724, "seek": 526492, "start": 5266.96, "end": 5269.28, "text": " is Martin's Elm audio package, right?", "tokens": [307, 9184, 311, 2699, 76, 6278, 7372, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 725, "seek": 526492, "start": 5269.28, "end": 5271.64, "text": " So he added a specific,", "tokens": [407, 415, 3869, 257, 2685, 11], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 726, "seek": 526492, "start": 5271.64, "end": 5273.64, "text": " like Lambda era front end with audio.", "tokens": [411, 45691, 4249, 1868, 917, 365, 6278, 13], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 727, "seek": 526492, "start": 5273.64, "end": 5275.6, "text": " So it's a function where you put your Lambda era app", "tokens": [407, 309, 311, 257, 2445, 689, 291, 829, 428, 45691, 4249, 724], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 728, "seek": 526492, "start": 5275.6, "end": 5277.36, "text": " in that particular wrapper.", "tokens": [294, 300, 1729, 46906, 13], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 729, "seek": 526492, "start": 5277.36, "end": 5278.64, "text": " And then he has an app wrapper", "tokens": [400, 550, 415, 575, 364, 724, 46906], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 730, "seek": 526492, "start": 5278.64, "end": 5280.18, "text": " that depends on certain ports", "tokens": [300, 5946, 322, 1629, 18160], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 731, "seek": 526492, "start": 5280.18, "end": 5282.72, "text": " and add some extra functionality to support", "tokens": [293, 909, 512, 2857, 14980, 281, 1406], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 732, "seek": 526492, "start": 5282.72, "end": 5285.24, "text": " like loading audio and playing audio", "tokens": [411, 15114, 6278, 293, 2433, 6278], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 733, "seek": 526492, "start": 5285.24, "end": 5287.0, "text": " and managing like the various,", "tokens": [293, 11642, 411, 264, 3683, 11], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 734, "seek": 526492, "start": 5287.0, "end": 5288.88, "text": " the state bits of that, right?", "tokens": [264, 1785, 9239, 295, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 735, "seek": 526492, "start": 5288.88, "end": 5290.24, "text": " So as a user, you can kind of be like,", "tokens": [407, 382, 257, 4195, 11, 291, 393, 733, 295, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 736, "seek": 526492, "start": 5290.24, "end": 5292.6, "text": " yeah, I have a normal app", "tokens": [1338, 11, 286, 362, 257, 2710, 724], "temperature": 0.0, "avg_logprob": -0.2423326255631273, "compression_ratio": 1.7402135231316727, "no_speech_prob": 6.144039616629016e-06}, {"id": 737, "seek": 529260, "start": 5292.6, "end": 5295.4800000000005, "text": " and then I wrap it in this Lambda era front end with audio", "tokens": [293, 550, 286, 7019, 309, 294, 341, 45691, 4249, 1868, 917, 365, 6278], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 738, "seek": 529260, "start": 5295.4800000000005, "end": 5297.38, "text": " and then I get like some extra bits, right?", "tokens": [293, 550, 286, 483, 411, 512, 2857, 9239, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 739, "seek": 529260, "start": 5297.38, "end": 5301.200000000001, "text": " So I can manage audio and that requires some JavaScript.", "tokens": [407, 286, 393, 3067, 6278, 293, 300, 7029, 512, 15778, 13], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 740, "seek": 529260, "start": 5301.200000000001, "end": 5302.84, "text": " So the idea was to say, okay,", "tokens": [407, 264, 1558, 390, 281, 584, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 741, "seek": 529260, "start": 5302.84, "end": 5304.120000000001, "text": " well, there should be some way,", "tokens": [731, 11, 456, 820, 312, 512, 636, 11], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 742, "seek": 529260, "start": 5304.120000000001, "end": 5309.120000000001, "text": " like what effectively does this slimline JavaScript need?", "tokens": [411, 437, 8659, 775, 341, 25357, 1889, 15778, 643, 30], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 743, "seek": 529260, "start": 5309.120000000001, "end": 5311.52, "text": " Effectively, it needs a way to hook into init, right?", "tokens": [17764, 3413, 11, 309, 2203, 257, 636, 281, 6328, 666, 3157, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 744, "seek": 529260, "start": 5311.52, "end": 5314.3, "text": " So when the Elm app is being initialized,", "tokens": [407, 562, 264, 2699, 76, 724, 307, 885, 5883, 1602, 11], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 745, "seek": 529260, "start": 5314.3, "end": 5316.64, "text": " we want to get an instance of that Elm app", "tokens": [321, 528, 281, 483, 364, 5197, 295, 300, 2699, 76, 724], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 746, "seek": 529260, "start": 5316.64, "end": 5319.200000000001, "text": " so that we can bind our subscriptions,", "tokens": [370, 300, 321, 393, 14786, 527, 44951, 11], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 747, "seek": 529260, "start": 5319.200000000001, "end": 5322.04, "text": " like our port, our inbound and our outbound ports.", "tokens": [411, 527, 2436, 11, 527, 294, 18767, 293, 527, 484, 18767, 18160, 13], "temperature": 0.0, "avg_logprob": -0.2155904569825926, "compression_ratio": 1.7577854671280277, "no_speech_prob": 3.7266083836584585e-06}, {"id": 748, "seek": 532204, "start": 5322.04, "end": 5324.64, "text": " So Elm package.js was being like, okay,", "tokens": [407, 2699, 76, 7372, 13, 25530, 390, 885, 411, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 749, "seek": 532204, "start": 5324.64, "end": 5326.56, "text": " how could we, what would it look like", "tokens": [577, 727, 321, 11, 437, 576, 309, 574, 411], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 750, "seek": 532204, "start": 5326.56, "end": 5329.08, "text": " to have like a really delightful standard", "tokens": [281, 362, 411, 257, 534, 35194, 3832], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 751, "seek": 532204, "start": 5329.08, "end": 5331.56, "text": " for shipping a bit of extra JavaScript", "tokens": [337, 14122, 257, 857, 295, 2857, 15778], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 752, "seek": 532204, "start": 5331.56, "end": 5334.92, "text": " and some ports with an Elm app", "tokens": [293, 512, 18160, 365, 364, 2699, 76, 724], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 753, "seek": 532204, "start": 5334.92, "end": 5339.08, "text": " in a way where ideally it was type safe, right?", "tokens": [294, 257, 636, 689, 22915, 309, 390, 2010, 3273, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 754, "seek": 532204, "start": 5339.08, "end": 5341.14, "text": " Like it was very clear what ports are there,", "tokens": [1743, 309, 390, 588, 1850, 437, 18160, 366, 456, 11], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 755, "seek": 532204, "start": 5341.14, "end": 5342.88, "text": " what things go in, what things go out,", "tokens": [437, 721, 352, 294, 11, 437, 721, 352, 484, 11], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 756, "seek": 532204, "start": 5342.88, "end": 5344.32, "text": " how should they be used", "tokens": [577, 820, 436, 312, 1143], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 757, "seek": 532204, "start": 5344.32, "end": 5346.0, "text": " and for it to be able to check this, right?", "tokens": [293, 337, 309, 281, 312, 1075, 281, 1520, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 758, "seek": 532204, "start": 5346.0, "end": 5349.04, "text": " And there was some bigger grand ideas about like,", "tokens": [400, 456, 390, 512, 3801, 2697, 3487, 466, 411, 11], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 759, "seek": 532204, "start": 5349.04, "end": 5351.96, "text": " you know, should we do like community verification", "tokens": [291, 458, 11, 820, 321, 360, 411, 1768, 30206], "temperature": 0.0, "avg_logprob": -0.19928812458567377, "compression_ratio": 1.719298245614035, "no_speech_prob": 5.955093001830392e-06}, {"id": 760, "seek": 535196, "start": 5351.96, "end": 5355.32, "text": " that the JavaScript isn't gonna launch a blockchain client,", "tokens": [300, 264, 15778, 1943, 380, 799, 4025, 257, 17176, 6423, 11], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 761, "seek": 535196, "start": 5355.32, "end": 5356.32, "text": " you know, or something like that, you know,", "tokens": [291, 458, 11, 420, 746, 411, 300, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 762, "seek": 535196, "start": 5356.32, "end": 5359.16, "text": " like maybe, you know, introduce like some safety stuff", "tokens": [411, 1310, 11, 291, 458, 11, 5366, 411, 512, 4514, 1507], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 763, "seek": 535196, "start": 5359.16, "end": 5362.24, "text": " to be like, you know, if you do like Elm package.js", "tokens": [281, 312, 411, 11, 291, 458, 11, 498, 291, 360, 411, 2699, 76, 7372, 13, 25530], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 764, "seek": 535196, "start": 5362.24, "end": 5363.8, "text": " install some package,", "tokens": [3625, 512, 7372, 11], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 765, "seek": 535196, "start": 5363.8, "end": 5365.76, "text": " it's gonna do an Elm install of the package", "tokens": [309, 311, 799, 360, 364, 2699, 76, 3625, 295, 264, 7372], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 766, "seek": 535196, "start": 5365.76, "end": 5367.52, "text": " and it's gonna pull the JavaScript down", "tokens": [293, 309, 311, 799, 2235, 264, 15778, 760], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 767, "seek": 535196, "start": 5367.52, "end": 5370.12, "text": " and it's gonna set everything up in a consistent way.", "tokens": [293, 309, 311, 799, 992, 1203, 493, 294, 257, 8398, 636, 13], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 768, "seek": 535196, "start": 5370.12, "end": 5372.68, "text": " And maybe that would make it really nice, you know,", "tokens": [400, 1310, 300, 576, 652, 309, 534, 1481, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 769, "seek": 535196, "start": 5372.68, "end": 5375.22, "text": " for the use cases.", "tokens": [337, 264, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 770, "seek": 535196, "start": 5375.22, "end": 5376.06, "text": " So there's, you know,", "tokens": [407, 456, 311, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 771, "seek": 535196, "start": 5376.06, "end": 5378.08, "text": " there's like a copy to clipboard example", "tokens": [456, 311, 411, 257, 5055, 281, 7353, 3787, 1365], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 772, "seek": 535196, "start": 5378.08, "end": 5380.84, "text": " in the Elm package.js spec", "tokens": [294, 264, 2699, 76, 7372, 13, 25530, 1608], "temperature": 0.0, "avg_logprob": -0.21496392543019813, "compression_ratio": 1.959409594095941, "no_speech_prob": 4.784985321748536e-06}, {"id": 773, "seek": 538084, "start": 5380.84, "end": 5382.4800000000005, "text": " and a few other examples of like, you know,", "tokens": [293, 257, 1326, 661, 5110, 295, 411, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 774, "seek": 538084, "start": 5382.4800000000005, "end": 5384.56, "text": " what would it look like to have these little bits,", "tokens": [437, 576, 309, 574, 411, 281, 362, 613, 707, 9239, 11], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 775, "seek": 538084, "start": 5384.56, "end": 5388.04, "text": " you know, little port bindings to web APIs", "tokens": [291, 458, 11, 707, 2436, 14786, 1109, 281, 3670, 21445], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 776, "seek": 538084, "start": 5388.04, "end": 5391.2, "text": " usually that aren't available natively in Elm.", "tokens": [2673, 300, 3212, 380, 2435, 8470, 356, 294, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 777, "seek": 538084, "start": 5391.2, "end": 5395.6, "text": " So yeah, the second concern there was when I was,", "tokens": [407, 1338, 11, 264, 1150, 3136, 456, 390, 562, 286, 390, 11], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 778, "seek": 538084, "start": 5395.6, "end": 5397.92, "text": " so there's a proof of concept implementation of this spec.", "tokens": [370, 456, 311, 257, 8177, 295, 3410, 11420, 295, 341, 1608, 13], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 779, "seek": 538084, "start": 5397.92, "end": 5399.88, "text": " The spec has got nothing to do with Lamdera,", "tokens": [440, 1608, 575, 658, 1825, 281, 360, 365, 18825, 67, 1663, 11], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 780, "seek": 538084, "start": 5399.88, "end": 5402.04, "text": " but Lamdera implements that on package.js spec,", "tokens": [457, 18825, 1068, 64, 704, 17988, 300, 322, 7372, 13, 25530, 1608, 11], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 781, "seek": 538084, "start": 5402.04, "end": 5403.8, "text": " at least in its first version, as far as I'm aware,", "tokens": [412, 1935, 294, 1080, 700, 3037, 11, 382, 1400, 382, 286, 478, 3650, 11], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 782, "seek": 538084, "start": 5403.8, "end": 5407.4800000000005, "text": " it's the only consumer or implementer of the spec,", "tokens": [309, 311, 264, 787, 9711, 420, 4445, 260, 295, 264, 1608, 11], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 783, "seek": 538084, "start": 5407.4800000000005, "end": 5410.24, "text": " which was also written by me, so maybe that's why.", "tokens": [597, 390, 611, 3720, 538, 385, 11, 370, 1310, 300, 311, 983, 13], "temperature": 0.0, "avg_logprob": -0.24028148035849295, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.1659293704724405e-05}, {"id": 784, "seek": 541024, "start": 5410.24, "end": 5412.96, "text": " But I haven't pushed it too hard, I guess.", "tokens": [583, 286, 2378, 380, 9152, 309, 886, 1152, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 785, "seek": 541024, "start": 5412.96, "end": 5414.5599999999995, "text": " And so yeah, in the Lamdera implementation,", "tokens": [400, 370, 1338, 11, 294, 264, 18825, 67, 1663, 11420, 11], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 786, "seek": 541024, "start": 5414.5599999999995, "end": 5416.08, "text": " we only have this init,", "tokens": [321, 787, 362, 341, 3157, 11], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 787, "seek": 541024, "start": 5416.08, "end": 5418.84, "text": " but in the spec I was also considering like an upgrade,", "tokens": [457, 294, 264, 1608, 286, 390, 611, 8079, 411, 364, 11484, 11], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 788, "seek": 541024, "start": 5418.84, "end": 5419.679999999999, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 789, "seek": 541024, "start": 5419.679999999999, "end": 5422.48, "text": " So what happens when a front end is upgrading,", "tokens": [407, 437, 2314, 562, 257, 1868, 917, 307, 36249, 11], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 790, "seek": 541024, "start": 5422.48, "end": 5425.08, "text": " maybe rather than init being re-invoked", "tokens": [1310, 2831, 813, 3157, 885, 319, 12, 259, 85, 9511], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 791, "seek": 541024, "start": 5425.08, "end": 5427.36, "text": " and you having to carefully think about,", "tokens": [293, 291, 1419, 281, 7500, 519, 466, 11], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 792, "seek": 541024, "start": 5427.36, "end": 5430.48, "text": " well, what happens if init gets invoked multiple times?", "tokens": [731, 11, 437, 2314, 498, 3157, 2170, 1048, 9511, 3866, 1413, 30], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 793, "seek": 541024, "start": 5430.48, "end": 5434.48, "text": " You know, yes, you want to rebind your ports to the new app,", "tokens": [509, 458, 11, 2086, 11, 291, 528, 281, 12970, 471, 428, 18160, 281, 264, 777, 724, 11], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 794, "seek": 541024, "start": 5434.48, "end": 5436.16, "text": " but maybe you don't wanna re-initialize", "tokens": [457, 1310, 291, 500, 380, 1948, 319, 12, 259, 270, 831, 1125], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 795, "seek": 541024, "start": 5436.16, "end": 5438.12, "text": " like the audio context, right?", "tokens": [411, 264, 6278, 4319, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21014192515406116, "compression_ratio": 1.663265306122449, "no_speech_prob": 9.516134923615027e-06}, {"id": 796, "seek": 543812, "start": 5438.12, "end": 5440.28, "text": " Because the user's browser hasn't reloaded.", "tokens": [1436, 264, 4195, 311, 11185, 6132, 380, 25628, 292, 13], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 797, "seek": 543812, "start": 5440.28, "end": 5441.96, "text": " So there was an idea of like,", "tokens": [407, 456, 390, 364, 1558, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 798, "seek": 543812, "start": 5441.96, "end": 5443.5199999999995, "text": " well, could we just do all that in init", "tokens": [731, 11, 727, 321, 445, 360, 439, 300, 294, 3157], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 799, "seek": 543812, "start": 5443.5199999999995, "end": 5444.44, "text": " and say to people,", "tokens": [293, 584, 281, 561, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 800, "seek": 543812, "start": 5444.44, "end": 5447.08, "text": " you have to think about init as being kind of like", "tokens": [291, 362, 281, 519, 466, 3157, 382, 885, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 801, "seek": 543812, "start": 5447.08, "end": 5448.72, "text": " item potent, I guess, you know,", "tokens": [3174, 27073, 11, 286, 2041, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 802, "seek": 543812, "start": 5448.72, "end": 5450.28, "text": " like it can be run multiple times", "tokens": [411, 309, 393, 312, 1190, 3866, 1413], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 803, "seek": 543812, "start": 5450.28, "end": 5454.24, "text": " and you have to do what's sensible when that happens,", "tokens": [293, 291, 362, 281, 360, 437, 311, 25380, 562, 300, 2314, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 804, "seek": 543812, "start": 5454.24, "end": 5455.599999999999, "text": " or should we explicitly have,", "tokens": [420, 820, 321, 20803, 362, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 805, "seek": 543812, "start": 5455.599999999999, "end": 5458.599999999999, "text": " okay, this is an init thing, and then here's an upgrade.", "tokens": [1392, 11, 341, 307, 364, 3157, 551, 11, 293, 550, 510, 311, 364, 11484, 13], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 806, "seek": 543812, "start": 5458.599999999999, "end": 5460.16, "text": " So that in upgrade, you could just be like,", "tokens": [407, 300, 294, 11484, 11, 291, 727, 445, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 807, "seek": 543812, "start": 5460.16, "end": 5462.4, "text": " okay, you know, I know I don't have to do any of the init", "tokens": [1392, 11, 291, 458, 11, 286, 458, 286, 500, 380, 362, 281, 360, 604, 295, 264, 3157], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 808, "seek": 543812, "start": 5462.4, "end": 5463.8, "text": " stuff, it's already there,", "tokens": [1507, 11, 309, 311, 1217, 456, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 809, "seek": 543812, "start": 5463.8, "end": 5466.08, "text": " I can only do the code that needs to happen for upgrade,", "tokens": [286, 393, 787, 360, 264, 3089, 300, 2203, 281, 1051, 337, 11484, 11], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 810, "seek": 543812, "start": 5466.08, "end": 5467.84, "text": " which is probably rebinding ports.", "tokens": [597, 307, 1391, 12970, 9245, 18160, 13], "temperature": 0.0, "avg_logprob": -0.2107723659939236, "compression_ratio": 1.834834834834835, "no_speech_prob": 2.4824294087011367e-06}, {"id": 811, "seek": 546784, "start": 5467.84, "end": 5469.2, "text": " So that's kind of an open question.", "tokens": [407, 300, 311, 733, 295, 364, 1269, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 812, "seek": 546784, "start": 5469.2, "end": 5471.24, "text": " And that's why on package JS implementation,", "tokens": [400, 300, 311, 983, 322, 7372, 33063, 11420, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 813, "seek": 546784, "start": 5471.24, "end": 5472.92, "text": " Lambda is still in labs,", "tokens": [45691, 307, 920, 294, 20339, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 814, "seek": 546784, "start": 5472.92, "end": 5474.88, "text": " because that's not fully handled.", "tokens": [570, 300, 311, 406, 4498, 18033, 13], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 815, "seek": 546784, "start": 5474.88, "end": 5476.84, "text": " So there is a way,", "tokens": [407, 456, 307, 257, 636, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 816, "seek": 546784, "start": 5476.84, "end": 5478.68, "text": " currently the process is you contact me,", "tokens": [4362, 264, 1399, 307, 291, 3385, 385, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 817, "seek": 546784, "start": 5478.68, "end": 5480.56, "text": " but there is a way to opt out of", "tokens": [457, 456, 307, 257, 636, 281, 2427, 484, 295], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 818, "seek": 546784, "start": 5480.56, "end": 5482.76, "text": " the hot reload stuff in production.", "tokens": [264, 2368, 25628, 1507, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 819, "seek": 546784, "start": 5482.76, "end": 5485.4800000000005, "text": " And some people have chosen to do that on their apps", "tokens": [400, 512, 561, 362, 8614, 281, 360, 300, 322, 641, 7733], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 820, "seek": 546784, "start": 5485.4800000000005, "end": 5486.32, "text": " where they're like, you know,", "tokens": [689, 436, 434, 411, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 821, "seek": 546784, "start": 5486.32, "end": 5487.6, "text": " I don't have that many users,", "tokens": [286, 500, 380, 362, 300, 867, 5022, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 822, "seek": 546784, "start": 5487.6, "end": 5489.4400000000005, "text": " or I don't worry about that,", "tokens": [420, 286, 500, 380, 3292, 466, 300, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 823, "seek": 546784, "start": 5489.4400000000005, "end": 5490.52, "text": " but I've got like some, you know,", "tokens": [457, 286, 600, 658, 411, 512, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 824, "seek": 546784, "start": 5490.52, "end": 5492.32, "text": " complicated JavaScript setup", "tokens": [6179, 15778, 8657], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 825, "seek": 546784, "start": 5492.32, "end": 5493.76, "text": " that I haven't figured out how to handle", "tokens": [300, 286, 2378, 380, 8932, 484, 577, 281, 4813], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 826, "seek": 546784, "start": 5493.76, "end": 5494.8, "text": " this evergreen concept.", "tokens": [341, 1562, 27399, 3410, 13], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 827, "seek": 546784, "start": 5494.8, "end": 5497.24, "text": " And so they might opt out of the live reload.", "tokens": [400, 370, 436, 1062, 2427, 484, 295, 264, 1621, 25628, 13], "temperature": 0.0, "avg_logprob": -0.2318219128776999, "compression_ratio": 1.7696969696969698, "no_speech_prob": 6.240424681891454e-06}, {"id": 828, "seek": 549724, "start": 5497.24, "end": 5498.84, "text": " And then instead what happens is that", "tokens": [400, 550, 2602, 437, 2314, 307, 300], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 829, "seek": 549724, "start": 5498.84, "end": 5501.2, "text": " when the app deploys and then your front end version happens", "tokens": [562, 264, 724, 368, 49522, 293, 550, 428, 1868, 917, 3037, 2314], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 830, "seek": 549724, "start": 5501.2, "end": 5503.04, "text": " it forces a hard refresh.", "tokens": [309, 5874, 257, 1152, 15134, 13], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 831, "seek": 549724, "start": 5503.04, "end": 5507.5199999999995, "text": " So it forces a full page reload and reinitialization,", "tokens": [407, 309, 5874, 257, 1577, 3028, 25628, 293, 6561, 270, 831, 2144, 11], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 832, "seek": 549724, "start": 5507.5199999999995, "end": 5509.32, "text": " you know, which will lose some front end state for people,", "tokens": [291, 458, 11, 597, 486, 3624, 512, 1868, 917, 1785, 337, 561, 11], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 833, "seek": 549724, "start": 5509.32, "end": 5512.16, "text": " but gets back the, you know, things aren't,", "tokens": [457, 2170, 646, 264, 11, 291, 458, 11, 721, 3212, 380, 11], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 834, "seek": 549724, "start": 5512.16, "end": 5514.0, "text": " you don't end up in that state where, you know,", "tokens": [291, 500, 380, 917, 493, 294, 300, 1785, 689, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 835, "seek": 549724, "start": 5514.0, "end": 5515.8, "text": " things are broken or you're sending old versions", "tokens": [721, 366, 5463, 420, 291, 434, 7750, 1331, 9606], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 836, "seek": 549724, "start": 5515.8, "end": 5517.44, "text": " and you don't have the new app version.", "tokens": [293, 291, 500, 380, 362, 264, 777, 724, 3037, 13], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 837, "seek": 549724, "start": 5517.44, "end": 5519.96, "text": " So we still get the consistency at the loss", "tokens": [407, 321, 920, 483, 264, 14416, 412, 264, 4470], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 838, "seek": 549724, "start": 5519.96, "end": 5521.5599999999995, "text": " of some front end state.", "tokens": [295, 512, 1868, 917, 1785, 13], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 839, "seek": 549724, "start": 5521.5599999999995, "end": 5522.599999999999, "text": " Does that make sense?", "tokens": [4402, 300, 652, 2020, 30], "temperature": 0.0, "avg_logprob": -0.21501366635586353, "compression_ratio": 1.885185185185185, "no_speech_prob": 8.66422396939015e-06}, {"id": 840, "seek": 552260, "start": 5522.6, "end": 5527.6, "text": " Right, yeah, so how does that fit in with like the guarantee?", "tokens": [1779, 11, 1338, 11, 370, 577, 775, 300, 3318, 294, 365, 411, 264, 10815, 30], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 841, "seek": 552260, "start": 5527.92, "end": 5532.04, "text": " So like what would happen if it was just a free for all,", "tokens": [407, 411, 437, 576, 1051, 498, 309, 390, 445, 257, 1737, 337, 439, 11], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 842, "seek": 552260, "start": 5532.04, "end": 5535.68, "text": " you can run JavaScript on the front end of a Lambda app,", "tokens": [291, 393, 1190, 15778, 322, 264, 1868, 917, 295, 257, 45691, 724, 11], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 843, "seek": 552260, "start": 5535.68, "end": 5539.400000000001, "text": " like, or what do you gain by the design decision", "tokens": [411, 11, 420, 437, 360, 291, 6052, 538, 264, 1715, 3537], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 844, "seek": 552260, "start": 5539.400000000001, "end": 5541.6, "text": " to not allow that?", "tokens": [281, 406, 2089, 300, 30], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 845, "seek": 552260, "start": 5541.6, "end": 5543.76, "text": " What's the motivation behind that?", "tokens": [708, 311, 264, 12335, 2261, 300, 30], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 846, "seek": 552260, "start": 5543.76, "end": 5546.4400000000005, "text": " Yeah, so there is no constraint actually,", "tokens": [865, 11, 370, 456, 307, 572, 25534, 767, 11], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 847, "seek": 552260, "start": 5546.4400000000005, "end": 5548.200000000001, "text": " like you can do anything in that JavaScript", "tokens": [411, 291, 393, 360, 1340, 294, 300, 15778], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 848, "seek": 552260, "start": 5548.200000000001, "end": 5549.04, "text": " on the front end.", "tokens": [322, 264, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.2169173231748777, "compression_ratio": 1.6536796536796536, "no_speech_prob": 1.8162063497584313e-06}, {"id": 849, "seek": 554904, "start": 5549.04, "end": 5552.6, "text": " What I suppose, like the guarantee that gets made", "tokens": [708, 286, 7297, 11, 411, 264, 10815, 300, 2170, 1027], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 850, "seek": 554904, "start": 5552.6, "end": 5555.08, "text": " is that that JavaScript will only run", "tokens": [307, 300, 300, 15778, 486, 787, 1190], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 851, "seek": 554904, "start": 5555.08, "end": 5557.8, "text": " in the context of an app being initialized,", "tokens": [294, 264, 4319, 295, 364, 724, 885, 5883, 1602, 11], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 852, "seek": 554904, "start": 5557.8, "end": 5559.16, "text": " which isn't a huge guarantee,", "tokens": [597, 1943, 380, 257, 2603, 10815, 11], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 853, "seek": 554904, "start": 5559.16, "end": 5563.24, "text": " but it means that you, your code doesn't have to worry", "tokens": [457, 309, 1355, 300, 291, 11, 428, 3089, 1177, 380, 362, 281, 3292], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 854, "seek": 554904, "start": 5563.24, "end": 5564.92, "text": " about the initialization setup", "tokens": [466, 264, 5883, 2144, 8657], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 855, "seek": 554904, "start": 5564.92, "end": 5567.84, "text": " and Lambdaera can continue to control,", "tokens": [293, 45691, 1663, 393, 2354, 281, 1969, 11], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 856, "seek": 554904, "start": 5567.84, "end": 5569.12, "text": " like there's a harness, right?", "tokens": [411, 456, 311, 257, 19700, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 857, "seek": 554904, "start": 5569.12, "end": 5572.12, "text": " Like that does the evergreen magic and some other stuff", "tokens": [1743, 300, 775, 264, 1562, 27399, 5585, 293, 512, 661, 1507], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 858, "seek": 554904, "start": 5572.12, "end": 5573.2, "text": " and does special bindings", "tokens": [293, 775, 2121, 14786, 1109], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 859, "seek": 554904, "start": 5573.2, "end": 5574.72, "text": " to make the front end back end stuff work, right?", "tokens": [281, 652, 264, 1868, 917, 646, 917, 1507, 589, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 860, "seek": 554904, "start": 5574.72, "end": 5577.44, "text": " So Lambdaera platform has a bunch of harness things", "tokens": [407, 45691, 1663, 3663, 575, 257, 3840, 295, 19700, 721], "temperature": 0.0, "avg_logprob": -0.20604642232259116, "compression_ratio": 1.7216494845360826, "no_speech_prob": 3.905441644747043e-06}, {"id": 861, "seek": 557744, "start": 5577.44, "end": 5579.679999999999, "text": " that it does for you to make everything seamless", "tokens": [300, 309, 775, 337, 291, 281, 652, 1203, 28677], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 862, "seek": 557744, "start": 5579.679999999999, "end": 5582.4, "text": " and everything work.", "tokens": [293, 1203, 589, 13], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 863, "seek": 557744, "start": 5582.4, "end": 5585.0, "text": " So yeah, the thing that we get from a Lambdaera perspective", "tokens": [407, 1338, 11, 264, 551, 300, 321, 483, 490, 257, 45691, 1663, 4585], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 864, "seek": 557744, "start": 5585.0, "end": 5587.08, "text": " is that we let the user kind of plug into that", "tokens": [307, 300, 321, 718, 264, 4195, 733, 295, 5452, 666, 300], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 865, "seek": 557744, "start": 5587.08, "end": 5589.4, "text": " into a sensible way, but yeah, 100%,", "tokens": [666, 257, 25380, 636, 11, 457, 1338, 11, 2319, 8923], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 866, "seek": 557744, "start": 5589.4, "end": 5592.24, "text": " like a user can throw exceptions in that JavaScript code,", "tokens": [411, 257, 4195, 393, 3507, 22847, 294, 300, 15778, 3089, 11], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 867, "seek": 557744, "start": 5592.24, "end": 5593.4, "text": " the user could crash the app,", "tokens": [264, 4195, 727, 8252, 264, 724, 11], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 868, "seek": 557744, "start": 5593.4, "end": 5595.36, "text": " like that all the normal JavaScript stuff", "tokens": [411, 300, 439, 264, 2710, 15778, 1507], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 869, "seek": 557744, "start": 5595.36, "end": 5597.5199999999995, "text": " comes back into play.", "tokens": [1487, 646, 666, 862, 13], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 870, "seek": 557744, "start": 5597.5199999999995, "end": 5601.28, "text": " So the user has to be careful, but yeah,", "tokens": [407, 264, 4195, 575, 281, 312, 5026, 11, 457, 1338, 11], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 871, "seek": 557744, "start": 5601.28, "end": 5602.719999999999, "text": " I suppose the only other guarantee they get", "tokens": [286, 7297, 264, 787, 661, 10815, 436, 483], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 872, "seek": 557744, "start": 5602.719999999999, "end": 5605.879999999999, "text": " is that that in it will be called again", "tokens": [307, 300, 300, 294, 309, 486, 312, 1219, 797], "temperature": 0.0, "avg_logprob": -0.19722527948044638, "compression_ratio": 1.7625899280575539, "no_speech_prob": 6.5399776758567896e-06}, {"id": 873, "seek": 560588, "start": 5605.88, "end": 5607.400000000001, "text": " when the upgrade happens.", "tokens": [562, 264, 11484, 2314, 13], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 874, "seek": 560588, "start": 5607.400000000001, "end": 5610.04, "text": " So it gives them a mechanism if they wanted to try", "tokens": [407, 309, 2709, 552, 257, 7513, 498, 436, 1415, 281, 853], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 875, "seek": 560588, "start": 5610.04, "end": 5613.84, "text": " to make the evergreen philosophy work", "tokens": [281, 652, 264, 1562, 27399, 10675, 589], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 876, "seek": 560588, "start": 5613.84, "end": 5615.84, "text": " with their JavaScript stuff.", "tokens": [365, 641, 15778, 1507, 13], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 877, "seek": 560588, "start": 5615.84, "end": 5617.32, "text": " If the JavaScript simple enough,", "tokens": [759, 264, 15778, 2199, 1547, 11], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 878, "seek": 560588, "start": 5617.32, "end": 5618.96, "text": " you don't really have to do anything,", "tokens": [291, 500, 380, 534, 362, 281, 360, 1340, 11], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 879, "seek": 560588, "start": 5618.96, "end": 5620.4800000000005, "text": " but yeah, if you had something a bit more complex,", "tokens": [457, 1338, 11, 498, 291, 632, 746, 257, 857, 544, 3997, 11], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 880, "seek": 560588, "start": 5620.4800000000005, "end": 5622.36, "text": " like an audio context, then yeah,", "tokens": [411, 364, 6278, 4319, 11, 550, 1338, 11], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 881, "seek": 560588, "start": 5622.36, "end": 5623.4400000000005, "text": " you could be like, okay, cool,", "tokens": [291, 727, 312, 411, 11, 1392, 11, 1627, 11], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 882, "seek": 560588, "start": 5623.4400000000005, "end": 5625.2, "text": " I know and it's gonna get called again.", "tokens": [286, 458, 293, 309, 311, 799, 483, 1219, 797, 13], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 883, "seek": 560588, "start": 5625.2, "end": 5628.04, "text": " I'll put in some guards to check", "tokens": [286, 603, 829, 294, 512, 17652, 281, 1520], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 884, "seek": 560588, "start": 5628.04, "end": 5631.04, "text": " whether I'm doing the first initialization", "tokens": [1968, 286, 478, 884, 264, 700, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 885, "seek": 560588, "start": 5631.04, "end": 5633.0, "text": " or maybe I'm already on my second one.", "tokens": [420, 1310, 286, 478, 1217, 322, 452, 1150, 472, 13], "temperature": 0.0, "avg_logprob": -0.20952259793001063, "compression_ratio": 1.611295681063123, "no_speech_prob": 5.093624622531934e-06}, {"id": 886, "seek": 563300, "start": 5633.0, "end": 5635.32, "text": " Mm-hmm, got it, so conceptually,", "tokens": [8266, 12, 10250, 11, 658, 309, 11, 370, 3410, 671, 11], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 887, "seek": 563300, "start": 5635.32, "end": 5637.56, "text": " like if you really wanted to model", "tokens": [411, 498, 291, 534, 1415, 281, 2316], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 888, "seek": 563300, "start": 5637.56, "end": 5641.84, "text": " what happens when you invoke a port,", "tokens": [437, 2314, 562, 291, 41117, 257, 2436, 11], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 889, "seek": 563300, "start": 5641.84, "end": 5646.12, "text": " I guess you could just say, if an exception happens,", "tokens": [286, 2041, 291, 727, 445, 584, 11, 498, 364, 11183, 2314, 11], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 890, "seek": 563300, "start": 5646.12, "end": 5648.8, "text": " we wanna model that explicitly and you could,", "tokens": [321, 1948, 2316, 300, 20803, 293, 291, 727, 11], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 891, "seek": 563300, "start": 5648.8, "end": 5653.8, "text": " but I mean, since an outgoing port doesn't get anything back,", "tokens": [457, 286, 914, 11, 1670, 364, 41565, 2436, 1177, 380, 483, 1340, 646, 11], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 892, "seek": 563300, "start": 5653.92, "end": 5658.16, "text": " couldn't you just like let a port call happen", "tokens": [2809, 380, 291, 445, 411, 718, 257, 2436, 818, 1051], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 893, "seek": 563300, "start": 5658.16, "end": 5660.12, "text": " and then say, if it crashes,", "tokens": [293, 550, 584, 11, 498, 309, 28642, 11], "temperature": 0.0, "avg_logprob": -0.21847081921764255, "compression_ratio": 1.6585365853658536, "no_speech_prob": 4.222741608828073e-06}, {"id": 894, "seek": 566012, "start": 5660.12, "end": 5663.32, "text": " we just catch the exception and log something,", "tokens": [321, 445, 3745, 264, 11183, 293, 3565, 746, 11], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 895, "seek": 566012, "start": 5663.32, "end": 5664.24, "text": " something like that.", "tokens": [746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 896, "seek": 566012, "start": 5664.24, "end": 5668.64, "text": " But I guess having the Elm package JS spec", "tokens": [583, 286, 2041, 1419, 264, 2699, 76, 7372, 33063, 1608], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 897, "seek": 566012, "start": 5668.64, "end": 5672.28, "text": " gives you a box to make those,", "tokens": [2709, 291, 257, 2424, 281, 652, 729, 11], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 898, "seek": 566012, "start": 5672.28, "end": 5675.28, "text": " to organize things and to make those safety guarantees", "tokens": [281, 13859, 721, 293, 281, 652, 729, 4514, 32567], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 899, "seek": 566012, "start": 5675.28, "end": 5678.64, "text": " within a port, is that sort of the motivation?", "tokens": [1951, 257, 2436, 11, 307, 300, 1333, 295, 264, 12335, 30], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 900, "seek": 566012, "start": 5678.64, "end": 5683.5599999999995, "text": " Yeah, so the Elm package JS spec does kind of philosophize", "tokens": [865, 11, 370, 264, 2699, 76, 7372, 33063, 1608, 775, 733, 295, 14529, 1125], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 901, "seek": 566012, "start": 5683.5599999999995, "end": 5686.36, "text": " about what would be a good overall mechanism there,", "tokens": [466, 437, 576, 312, 257, 665, 4787, 7513, 456, 11], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 902, "seek": 566012, "start": 5686.36, "end": 5688.24, "text": " which could include those safety guards,", "tokens": [597, 727, 4090, 729, 4514, 17652, 11], "temperature": 0.0, "avg_logprob": -0.22300168579699947, "compression_ratio": 1.673728813559322, "no_speech_prob": 2.368773493799381e-06}, {"id": 903, "seek": 568824, "start": 5688.24, "end": 5690.16, "text": " but the Lambda implementation currently,", "tokens": [457, 264, 45691, 11420, 4362, 11], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 904, "seek": 568824, "start": 5690.16, "end": 5692.4, "text": " it just goes to the, at least,", "tokens": [309, 445, 1709, 281, 264, 11, 412, 1935, 11], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 905, "seek": 568824, "start": 5692.4, "end": 5694.2, "text": " it just kind of goes to the concept of being like,", "tokens": [309, 445, 733, 295, 1709, 281, 264, 3410, 295, 885, 411, 11], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 906, "seek": 568824, "start": 5694.2, "end": 5697.92, "text": " okay, you put your JavaScript for your individual port", "tokens": [1392, 11, 291, 829, 428, 15778, 337, 428, 2609, 2436], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 907, "seek": 568824, "start": 5697.92, "end": 5700.48, "text": " use cases in this folder in this way,", "tokens": [764, 3331, 294, 341, 10820, 294, 341, 636, 11], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 908, "seek": 568824, "start": 5700.48, "end": 5701.76, "text": " and then anything that's in that folder,", "tokens": [293, 550, 1340, 300, 311, 294, 300, 10820, 11], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 909, "seek": 568824, "start": 5701.76, "end": 5703.32, "text": " I'm looking for an init in each file", "tokens": [286, 478, 1237, 337, 364, 3157, 294, 1184, 3991], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 910, "seek": 568824, "start": 5703.32, "end": 5705.84, "text": " and I'm gonna run it for you, and that's as far as it goes.", "tokens": [293, 286, 478, 799, 1190, 309, 337, 291, 11, 293, 300, 311, 382, 1400, 382, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 911, "seek": 568824, "start": 5705.84, "end": 5708.12, "text": " So none of the spec stuff about like the types", "tokens": [407, 6022, 295, 264, 1608, 1507, 466, 411, 264, 3467], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 912, "seek": 568824, "start": 5708.12, "end": 5708.96, "text": " and the safety of that,", "tokens": [293, 264, 4514, 295, 300, 11], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 913, "seek": 568824, "start": 5708.96, "end": 5711.12, "text": " like you still have to implement that yourself.", "tokens": [411, 291, 920, 362, 281, 4445, 300, 1803, 13], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 914, "seek": 568824, "start": 5711.12, "end": 5713.32, "text": " So it's not like a full automatic implementation", "tokens": [407, 309, 311, 406, 411, 257, 1577, 12509, 11420], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 915, "seek": 568824, "start": 5713.32, "end": 5716.679999999999, "text": " of the spec, but I would like to have that eventually,", "tokens": [295, 264, 1608, 11, 457, 286, 576, 411, 281, 362, 300, 4728, 11], "temperature": 0.0, "avg_logprob": -0.22812317283290207, "compression_ratio": 1.8520900321543408, "no_speech_prob": 6.643090273428243e-06}, {"id": 916, "seek": 571668, "start": 5716.68, "end": 5719.12, "text": " like it would be nice if there,", "tokens": [411, 309, 576, 312, 1481, 498, 456, 11], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 917, "seek": 571668, "start": 5719.12, "end": 5720.56, "text": " like if that was a feature where you,", "tokens": [411, 498, 300, 390, 257, 4111, 689, 291, 11], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 918, "seek": 571668, "start": 5720.56, "end": 5723.52, "text": " if you did like a Lambda install of a package", "tokens": [498, 291, 630, 411, 257, 45691, 3625, 295, 257, 7372], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 919, "seek": 571668, "start": 5723.52, "end": 5724.72, "text": " that did have JavaScript,", "tokens": [300, 630, 362, 15778, 11], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 920, "seek": 571668, "start": 5724.72, "end": 5725.56, "text": " that Lambda would be like,", "tokens": [300, 45691, 576, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 921, "seek": 571668, "start": 5725.56, "end": 5727.4400000000005, "text": " hey, this package is JavaScript,", "tokens": [4177, 11, 341, 7372, 307, 15778, 11], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 922, "seek": 571668, "start": 5727.4400000000005, "end": 5729.360000000001, "text": " would you like me to set this up", "tokens": [576, 291, 411, 385, 281, 992, 341, 493], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 923, "seek": 571668, "start": 5729.360000000001, "end": 5732.6, "text": " and create the packageports.elm file", "tokens": [293, 1884, 264, 7372, 17845, 13, 338, 76, 3991], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 924, "seek": 571668, "start": 5732.6, "end": 5735.200000000001, "text": " and put all the types in there for you?", "tokens": [293, 829, 439, 264, 3467, 294, 456, 337, 291, 30], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 925, "seek": 571668, "start": 5735.200000000001, "end": 5736.84, "text": " And I'm like, then it's just ready to go.", "tokens": [400, 286, 478, 411, 11, 550, 309, 311, 445, 1919, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 926, "seek": 571668, "start": 5736.84, "end": 5738.6, "text": " I think that would be really nice.", "tokens": [286, 519, 300, 576, 312, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 927, "seek": 571668, "start": 5738.6, "end": 5740.240000000001, "text": " And then it's just a question of,", "tokens": [400, 550, 309, 311, 445, 257, 1168, 295, 11], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 928, "seek": 571668, "start": 5740.240000000001, "end": 5743.240000000001, "text": " would people find that interesting as a standalone tool?", "tokens": [576, 561, 915, 300, 1880, 382, 257, 37454, 2290, 30], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 929, "seek": 571668, "start": 5743.240000000001, "end": 5744.96, "text": " And would the community find that interesting", "tokens": [400, 576, 264, 1768, 915, 300, 1880], "temperature": 0.0, "avg_logprob": -0.23392350881691748, "compression_ratio": 1.8683274021352314, "no_speech_prob": 6.048674549674615e-06}, {"id": 930, "seek": 574496, "start": 5744.96, "end": 5747.64, "text": " as a way to be like, hey, this is how we bundle", "tokens": [382, 257, 636, 281, 312, 411, 11, 4177, 11, 341, 307, 577, 321, 24438], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 931, "seek": 574496, "start": 5747.64, "end": 5749.76, "text": " small bits of JavaScript,", "tokens": [1359, 9239, 295, 15778, 11], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 932, "seek": 574496, "start": 5749.76, "end": 5752.88, "text": " utility JavaScript with our packages.", "tokens": [14877, 15778, 365, 527, 17401, 13], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 933, "seek": 574496, "start": 5752.88, "end": 5756.4, "text": " It's definitely an anti-goal for it to be like,", "tokens": [467, 311, 2138, 364, 6061, 12, 1571, 304, 337, 309, 281, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 934, "seek": 574496, "start": 5756.4, "end": 5761.28, "text": " this is how you drag in 17 NPM dependencies into your project", "tokens": [341, 307, 577, 291, 5286, 294, 3282, 426, 18819, 36606, 666, 428, 1716], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 935, "seek": 574496, "start": 5761.28, "end": 5764.24, "text": " like it's absolutely not for that use case.", "tokens": [411, 309, 311, 3122, 406, 337, 300, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 936, "seek": 574496, "start": 5764.24, "end": 5766.2, "text": " There's an, I mean, there's an example", "tokens": [821, 311, 364, 11, 286, 914, 11, 456, 311, 364, 1365], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 937, "seek": 574496, "start": 5766.2, "end": 5768.84, "text": " of how you would use Elm package.js,", "tokens": [295, 577, 291, 576, 764, 2699, 76, 7372, 13, 25530, 11], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 938, "seek": 574496, "start": 5768.84, "end": 5770.68, "text": " but at this stage, you'd have to bundle", "tokens": [457, 412, 341, 3233, 11, 291, 1116, 362, 281, 24438], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 939, "seek": 574496, "start": 5770.68, "end": 5772.0, "text": " some of that stuff yourself, right?", "tokens": [512, 295, 300, 1507, 1803, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 940, "seek": 574496, "start": 5772.0, "end": 5773.56, "text": " Like you'd have to pre-package things.", "tokens": [1743, 291, 1116, 362, 281, 659, 12, 9539, 609, 721, 13], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 941, "seek": 574496, "start": 5773.56, "end": 5774.72, "text": " Like it's not really for that.", "tokens": [1743, 309, 311, 406, 534, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.2124780797633995, "compression_ratio": 1.7147887323943662, "no_speech_prob": 7.52761206967989e-06}, {"id": 942, "seek": 577472, "start": 5774.72, "end": 5777.360000000001, "text": " The idea was how do we like in a nice way", "tokens": [440, 1558, 390, 577, 360, 321, 411, 294, 257, 1481, 636], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 943, "seek": 577472, "start": 5777.360000000001, "end": 5780.08, "text": " get like this ancillary JavaScript for Elm packages.", "tokens": [483, 411, 341, 364, 7383, 822, 15778, 337, 2699, 76, 17401, 13], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 944, "seek": 577472, "start": 5781.2, "end": 5782.400000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 945, "seek": 577472, "start": 5782.400000000001, "end": 5784.280000000001, "text": " Well, amazing stuff, Mario.", "tokens": [1042, 11, 2243, 1507, 11, 9343, 13], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 946, "seek": 577472, "start": 5784.280000000001, "end": 5788.08, "text": " If people wanna learn more about the latest release,", "tokens": [759, 561, 1948, 1466, 544, 466, 264, 6792, 4374, 11], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 947, "seek": 577472, "start": 5788.08, "end": 5791.400000000001, "text": " more about Evergreen, more about Lemdira,", "tokens": [544, 466, 12123, 27399, 11, 544, 466, 16905, 67, 4271, 11], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 948, "seek": 577472, "start": 5791.400000000001, "end": 5792.6, "text": " what should they look at?", "tokens": [437, 820, 436, 574, 412, 30], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 949, "seek": 577472, "start": 5792.6, "end": 5793.6, "text": " Yeah, absolutely.", "tokens": [865, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 950, "seek": 577472, "start": 5793.6, "end": 5796.4800000000005, "text": " So lemdira.com, probably the easiest starting point", "tokens": [407, 7495, 67, 4271, 13, 1112, 11, 1391, 264, 12889, 2891, 935], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 951, "seek": 577472, "start": 5796.4800000000005, "end": 5798.12, "text": " and easiest to remember.", "tokens": [293, 12889, 281, 1604, 13], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 952, "seek": 577472, "start": 5798.12, "end": 5799.320000000001, "text": " With a B, right?", "tokens": [2022, 257, 363, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.27988967275231835, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.058020072581712e-06}, {"id": 953, "seek": 579932, "start": 5799.32, "end": 5805.639999999999, "text": " Oh, you're just ruining all of my naming.", "tokens": [876, 11, 291, 434, 445, 38938, 439, 295, 452, 25290, 13], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 954, "seek": 579932, "start": 5805.639999999999, "end": 5810.5599999999995, "text": " It's definitely not with a B, lemdira.com.", "tokens": [467, 311, 2138, 406, 365, 257, 363, 11, 7495, 67, 4271, 13, 1112, 13], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 955, "seek": 579932, "start": 5810.5599999999995, "end": 5813.16, "text": " But yeah, that'll get you through some of the pitch", "tokens": [583, 1338, 11, 300, 603, 483, 291, 807, 512, 295, 264, 7293], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 956, "seek": 579932, "start": 5813.16, "end": 5815.48, "text": " and pretty much straight through to the documentation.", "tokens": [293, 1238, 709, 2997, 807, 281, 264, 14333, 13], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 957, "seek": 579932, "start": 5815.48, "end": 5817.599999999999, "text": " So you can take a look at that.", "tokens": [407, 291, 393, 747, 257, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 958, "seek": 579932, "start": 5817.599999999999, "end": 5819.759999999999, "text": " Yeah, there's some example apps there", "tokens": [865, 11, 456, 311, 512, 1365, 7733, 456], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 959, "seek": 579932, "start": 5819.759999999999, "end": 5821.5199999999995, "text": " that are really quite small and contained.", "tokens": [300, 366, 534, 1596, 1359, 293, 16212, 13], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 960, "seek": 579932, "start": 5821.5199999999995, "end": 5823.96, "text": " So I'd recommend people take a look at those.", "tokens": [407, 286, 1116, 2748, 561, 747, 257, 574, 412, 729, 13], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 961, "seek": 579932, "start": 5823.96, "end": 5826.48, "text": " Evergreen, I wouldn't, I mean, I would say", "tokens": [12123, 27399, 11, 286, 2759, 380, 11, 286, 914, 11, 286, 576, 584], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 962, "seek": 579932, "start": 5826.48, "end": 5828.48, "text": " it probably makes more sense to look at Evergreen", "tokens": [309, 1391, 1669, 544, 2020, 281, 574, 412, 12123, 27399], "temperature": 0.0, "avg_logprob": -0.22398547675665909, "compression_ratio": 1.6654135338345866, "no_speech_prob": 9.720002935864613e-07}, {"id": 963, "seek": 582848, "start": 5828.48, "end": 5831.0, "text": " when you need to look at Evergreen.", "tokens": [562, 291, 643, 281, 574, 412, 12123, 27399, 13], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 964, "seek": 582848, "start": 5831.0, "end": 5833.12, "text": " If people are interested, you can read the Evergreen docs,", "tokens": [759, 561, 366, 3102, 11, 291, 393, 1401, 264, 12123, 27399, 45623, 11], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 965, "seek": 582848, "start": 5833.12, "end": 5836.44, "text": " but I think it probably makes more sense in practice", "tokens": [457, 286, 519, 309, 1391, 1669, 544, 2020, 294, 3124], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 966, "seek": 582848, "start": 5836.44, "end": 5838.679999999999, "text": " when you're actually trying to get from one model to another", "tokens": [562, 291, 434, 767, 1382, 281, 483, 490, 472, 2316, 281, 1071], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 967, "seek": 582848, "start": 5838.679999999999, "end": 5841.5599999999995, "text": " in a specific use case for your specific app,", "tokens": [294, 257, 2685, 764, 1389, 337, 428, 2685, 724, 11], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 968, "seek": 582848, "start": 5841.5599999999995, "end": 5842.759999999999, "text": " but then going through that process,", "tokens": [457, 550, 516, 807, 300, 1399, 11], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 969, "seek": 582848, "start": 5842.759999999999, "end": 5844.879999999999, "text": " I think you can be like, oh yeah, that makes sense.", "tokens": [286, 519, 291, 393, 312, 411, 11, 1954, 1338, 11, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 970, "seek": 582848, "start": 5844.879999999999, "end": 5847.36, "text": " I've done these changes and this migration", "tokens": [286, 600, 1096, 613, 2962, 293, 341, 17011], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 971, "seek": 582848, "start": 5847.36, "end": 5848.759999999999, "text": " that I want out of that.", "tokens": [300, 286, 528, 484, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 972, "seek": 582848, "start": 5848.759999999999, "end": 5851.2, "text": " Reading it as a high level, I'm not sure how well", "tokens": [29766, 309, 382, 257, 1090, 1496, 11, 286, 478, 406, 988, 577, 731], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 973, "seek": 582848, "start": 5851.2, "end": 5855.0, "text": " that lands.", "tokens": [300, 5949, 13], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 974, "seek": 582848, "start": 5855.0, "end": 5856.36, "text": " But yeah, anyway, all the docs are there.", "tokens": [583, 1338, 11, 4033, 11, 439, 264, 45623, 366, 456, 13], "temperature": 0.0, "avg_logprob": -0.23213769339181325, "compression_ratio": 1.7109634551495017, "no_speech_prob": 6.2392787185672205e-06}, {"id": 975, "seek": 585636, "start": 5856.36, "end": 5859.16, "text": " And yeah, as always, we've got a Discord", "tokens": [400, 1338, 11, 382, 1009, 11, 321, 600, 658, 257, 32623], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 976, "seek": 585636, "start": 5859.16, "end": 5861.719999999999, "text": " full of lots of lovely and helpful people.", "tokens": [1577, 295, 3195, 295, 7496, 293, 4961, 561, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 977, "seek": 585636, "start": 5861.719999999999, "end": 5865.08, "text": " So if you want to ask any questions or ponder anything,", "tokens": [407, 498, 291, 528, 281, 1029, 604, 1651, 420, 280, 8548, 1340, 11], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 978, "seek": 585636, "start": 5865.08, "end": 5867.12, "text": " you're always very welcome in there.", "tokens": [291, 434, 1009, 588, 2928, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 979, "seek": 585636, "start": 5867.12, "end": 5868.16, "text": " Yeah, that's pretty much it.", "tokens": [865, 11, 300, 311, 1238, 709, 309, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 980, "seek": 585636, "start": 5868.16, "end": 5871.16, "text": " Wonderful, thanks so much for coming on, Mario.", "tokens": [22768, 11, 3231, 370, 709, 337, 1348, 322, 11, 9343, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 981, "seek": 585636, "start": 5871.16, "end": 5872.0, "text": " Yeah, thank you.", "tokens": [865, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 982, "seek": 585636, "start": 5872.0, "end": 5873.24, "text": " No, it's been my absolute pleasure.", "tokens": [883, 11, 309, 311, 668, 452, 8236, 6834, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 983, "seek": 585636, "start": 5873.24, "end": 5875.12, "text": " Thanks for having me as always.", "tokens": [2561, 337, 1419, 385, 382, 1009, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 984, "seek": 585636, "start": 5875.12, "end": 5877.32, "text": " And you're in, until next time.", "tokens": [400, 291, 434, 294, 11, 1826, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 985, "seek": 585636, "start": 5877.32, "end": 5878.16, "text": " Until next time.", "tokens": [9088, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.21043683714785819, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.5916630117280874e-06}, {"id": 986, "seek": 587816, "start": 5878.16, "end": 5889.599999999999, "text": " Music", "tokens": [50364, 220, 22088, 299, 50936], "temperature": 1.0, "avg_logprob": -2.014149030049642, "compression_ratio": 0.38461538461538464, "no_speech_prob": 7.788619404891506e-05}], "language": "en"}