{"text": " Hello Jeroen. Hello Dillon. So I think our listeners may be shocked to hear but our episodes are usually unscripted. But today we're going to do something a little bit different. I'm never ready for you making a pun. I should be at this point. But I'm not. You knew it was coming. No I wasn't. What are we talking about today? Today we're going to talk about Elm Pages scripts. Scripts! That was the pun. Yeah. So we haven't talked about Elm Pages in a while. Yes. We did talk about it already in the first episode if I remember correctly. Yep that's right. So it's a project you've been working a long time on. And we haven't heard about it much this year except on Elm Radio because you keep mentioning it for good reason. You're still working on it. And yeah you've been working you've been making a lot of things in Elm Pages. A lot of very cool things that we're going to talk about in future episodes. But today we're just going to focus on one part of it. And that is going to be scripts. Yes. Because from the amount of content that I can see in Elm Pages we're not going to be able to fit that into even hour long episodes. Exactly. So yeah Elm Pages scripts. Like what is a script? What is Elm Pages? What is this cool new thing that people want to hear about? Yeah. So I'm really excited to see what people do with Elm Pages scripts. So yeah and just to reiterate we are talking about the Elm Pages V3 release. At the time of this recording it is pre-release ramping up to get the release getting some final API changes and feedback from the community and writing some docs updating some docs. But yes so that's what we're talking about here. Elm Pages V3. The scope of what you can do with Elm Pages V3 is kind of huge. We will talk about it in the future but Elm Pages V2 people may think of as a static site generator which is what it was. So Elm Pages V3 allows you to do everything you could with V2. You can still build a static site using it. Elmradio.com is built using Elm Pages V3 actually. It's on a pre-release and it does that by generating static assets but you can also do server rendered pages. So the scope of Elm Pages V3 has changed with what you can do quite a bit compared to V2. But the heart of Elm Pages is still the same throughout all of its permutations. I've always thought of the heart of Elm Pages as being this sort of engine that's able to like execute things on a back end and give you back data. In Elm Pages V2 that was called data sources. So you know you could do a data source to make an HTTP request. So like for Elmradio.com it uses a lot of local files with markdown and pulls them in, runs a markdown parser. It's reading files. You know obviously with Elm there's not a way to go to your local file system and read files directly. But Elm Pages gives you ways to do that. So in V2 that tool was called a data source and you know you can pull in data to your initial page render using a data source. So before your static page renders you can read from a file. That was called data sources in V2. In V3 because the scope of what Elm Pages does has changed the term data source has been renamed and the concept has changed a tiny bit. And the reason for that is because in V2 it was the model was much more you try to make an HTTP request. You try to read from a file and if anything goes wrong you just stop the build and fail. And then the developer can read the issue. They can read a nicely formatted error message and say oh this API turned to 404. Let me fix that and then rerun the build and it succeeds. With V3 so for example if your server rendering pages maybe you get a 404 in an HTTP request. Maybe you're doing a post and you need to update something and you need to handle that error in a graceful way. So that's one of the reasons why this concept has changed and become a little more powerful and part of the reason why the name has changed. So in V3 the term is no longer data source is now called a back end task. And in addition to that in V so back end task it actually looks and feels a lot like the Elm core tasks. Okay well that's not going to be very surprising then to use. Yeah I hope so. In fact a lot of the API looks quite similar. You can do back end task dot and then you can do back end task dot map or back end task dot map error. Now if you're familiar with data sources in Elm pages V2 the map error part might be surprising because in V2 there was no error. So an Elm task is task with an error type variable and a data type variable. So if you do an Elm HTTP task it's going to give you a task HTTP dot error and then your decoded data as the data type. And then you do task dot attempt. You have to do dot attempt if there is an error that could happen. And then you get a message where you can deal with that result. So you can do you know turn it into remote data or do whatever you want to with that result. So Elm pages V3 has these back end tasks. They can have an error. So the reason for that is because in V2 it was much more if anything goes wrong stop the stop the line stop the assembly line stop everything. And so there is a trade off there because that is very convenient in a lot of ways. It's more convenient but less powerful. Yeah because for instance you can't handle the fact that an error has happened. You can't retry an HTTP request when you know that your server is a bit flaky or not always available. Stuff like that. So you can handle errors. Exactly. Now you it's completely at your disposal what you do with error handling. So we'll get into that. And one of the things that I love about back end tasks as compared to the design in V2 with data sources is with a back end task. If there is an error in that type variable then it has a possibility of failing. If there is no error there. So if you have you know back end task never my data then you know it will never fail. And that's that's something that you couldn't just look at the types in a data source in V2 and know whether or not it's going to fail because that possible failure gets sort of tucked under the hood. It's not represented by the types. Right. So it's much more like a task in that sense where you you can have a task where you know that it doesn't have the possibility of failure. So if you if you have like an HTTP task which is task HTTP dot error my data and then you can do task dot on error and that allows you to do you know you could just say task dot succeed in that on error and turn it into some other fallback data for example. Yeah. So you can recover from the from the failure. Exactly. And in some cases that might make sense. In some cases you might want to do a follow up task and try something else whatever it might be but it's completely at your disposal. But if you if you were to do task dot succeed in the on error then you would get you would be able to recover from that failure and then the types would would reflect that it's not possible for it to fail anymore which is kind of cool. So you can do the same thing with Elm pages V3 back end tasks. It's it's the same mental model. And if the error type variable is unbound if if you don't have any errors there then you know it's not possible for it to have an error. So that's back in tasks back in tasks are the heart of Elm pages even more so in the in the V3 release that is coming up soon. And and they are also the heart of Elm pages scripts. So let's talk about what Elm pages scripts are. I almost forgot about that part. Yeah. If only we had a script that we should follow you know exactly we got we got a little bit off script there. So Elm pages scripts are really just a way of defining a back end task and executing it from the command line. So if you wanted to make an HTTP request and log something to the console. So what you would do is so the minimal setup for Elm pages scripts it actually doesn't require any of the Elm pages application folder or anything. You don't need any route modules defined. You don't need any of the config for Elm pages. All you need is a folder called script. So much like for an Elm review project you have a folder called review and it's a regular Elm project. It has an Elm.JSON Elm pages script is the same thing. So you have a script folder that script folder has to have an Elm.JSON. So it's a little Elm project. It has to have Elm pages as a dependency in that Elm.JSON. And and then what you do is you do Elm pages run hello. And now if you have something in your source directories like source slash hello dot Elm it's going to go and execute that module. So right. Yeah. What does it mean to execute an Elm module becomes the question. Yeah. So you mentioned it's basically running a back end task. Exactly. So back end task is used to fetch data from HTTP or from files so you can grab the data. But then that's it. Right. You can only grab data. You can only fetch things. What else can you do with it. Yeah. Exactly. And that would be much more the mental model in Elm pages V2. But now there's a little bit more of a notion of being able to do effectful things using back end tasks in V3 because if you're doing a server rendered route and responding to a form submission which you can do in V3 which we'll talk about in a future episode you might want to do something that has an effect on the world. It's not just about shoveling in data to generate static pages. So similarly a back end task is not only about grabbing data. It does let you pull in data and map that data back and test that map back and test that. And then. But you can also perform effects. So for example there is a back end task in the Elm pages API called script log script log takes a string and it gives you a back end task with no data. All right. Yeah. So that looks like back end task lowercase error because it's not possible for it to error and unit because it doesn't have any data. So the hello world for Elm pages script is you have your script folder you have an Elm.JSON you have source slash hello dot Elm and in your hello dot Elm you expose a function called run. Okay. That is the main function in a way. Exactly. Run is like the main function for an Elm pages script. You run has the type script and and then you do script dot without CLI options scripts dot log hello. That's hello world. So that's it. Then you run Elm pages run hello and it prints hello world. I hope that was clear to listeners. It's always hard to explain code all of this was not that much code. Yeah. And that's sort of the thing I hope people take away from this is like to write a hello world script and run it is a very small number of files that you need to create code you need to write and commands you need to run. Notice for example that we didn't say Elm pages compile script this and then make an index dot JS file and node and run a node script and all of that. And if we had a compiler error or anything like that in hello dot Elm then running Elm pages run hello would tell us about that. So it's it's a very minimal amount of code we will link to an example of a hello dot Elm will link to a starter repo that gives you like a minimal boiler plate so you can clone it and play around with it. But it's designed to be a very small amount of code. And also the the abstraction of a back end task is designed to be minimal in a way too because there's no in it. There's no update. There's no subscriptions like you just log. You don't return a model and then a command with a logging thing. And then you like if you wanted to make an HTTP request you just do you know back end task dot HTTP dot get JSON give it a URL give it a JSON decoder and then back end task dot and then and then you can log some data that you decoded. So right. It's designed to be more like the abstraction of a back end task lets you do things in a more lightweight way especially for this sort of mental model where it's just like just execute this thing. So it back end task maps very nicely to the idea of a script where it's just execute this thing or fail. Do this do this do that. And we're done. Right. Exactly. It's not updating the model. It's not responding to user input. So it's just a sequence of tasks that it's performing. Whereas if you had to listen to user input if you had to listen to the user typing into an input field or an on click that would sort of break that mental model of just saying there's one back end task it just runs and it finishes or it fails. Right. So is there no way to listen to user prompts or could that be added. That is a that is a very good question. I have thought about that. Wolfgang Schuster has been doing some experiments with some with a sort of Elm Inc project where you can do things like create interactive terminal applications in Elm. And I definitely think it could be interesting potentially to have a way to have an init update things like that if you wanted to do something like that. But it would be possible but it would add complexity. Yeah. But I mean even without having to have an init update and all those things you could probably have a define a back end tasks which succeeds with the user prompts without having this the Elm architecture lifecycle. Exactly. That's the that's the thing is you can and you actually can. I actually haven't tried that specific use case but you can run arbitrary Node.js code using back and task dot custom. We'll link to the back end task dot custom module docs which explains how to set that up. But essentially you just give it a JSON encoder and a JSON decoder and you can define a custom back end task and that custom back end task could be a wait user input and you could give it a prompt and you know you could block until you receive the user input and then return that data. And that would totally work. So. So yeah you can do that and it supports that simpler mental model of just running a script until you're done. The thing that's that I find really fun about back end tasks is that like it is this it's a type it's data. It's a description of an effect or of something to achieve and exactly get out of it. Exactly. And so because it's this sort of declarative description of an effect and how to respond to subsequent effects you can use it in a lot of different places. So like Elm pages scripts is one place you could use it. You know if you wanted to define some sort of runtime where you say oh yeah well you can turn that into you know not an Elm command but something like an Elm command and you return or like you can return a back end task in your in a tuple with a model change and a back end task or like it's possible to do that. And I think that sometimes people underestimate what you can model for frameworks to be able to do like effectful things using this pattern of describing effects as data. I think it's like it's actually a very powerful tool that we can do a lot with and as framework designers we can put guardrails so it's very very clear what what it's possible to do using those data types and where they can be used and where they cannot be used. So you know it's essentially the idea of a managed effect where like calling a back end creating a back end task in Elm pages doesn't do anything. You can create a back end task just like you can create a command but when you give it to Elm pages in a place where it accepts that type then it lets the framework do something with it. So the sky's the limit with how you build things with that. What I really like about this pattern is that because there's an abstraction layer because you make a new API that whose internals are hidden. I mean I'm guessing they're opaque right. Well that decouples you from how it's implemented under the hood. So however you implement back end tasks under the hood or how you implement logging, reading a file, writing to a file, all those things. I'm guessing they're implemented in Node.js at the moment but they could be implemented using a bash script or whatever. And all those options are available as long as you don't tie it to a specific implementation which because there's an abstraction layer they are not. So that is something that I really like is that you're free to implement it however you want, you as the framework author. But I'm guessing if you do it through a port a user can do so as well. Although it is going to be executed through JavaScript. But it could be done through other means maybe Absolutely. Well yeah as you say I mean at the end of the day Elm pages is creating, it's scaffolding up an application around your application. That's sort of what a framework is. And so it's at the end of the day compiling an Elm application and executing it in this case in Node.js. But it could be executing it in other contexts. It could be executing it with Deno or Cloudflare workers or with Bun with different run times. But at the end of the day it is using Elm which its way of communicating is through ports. And so it's just it's just building that. It's just like a little application. You know just like when we write Elm review and Elm GraphQL command line tools that you npm install somewhere in there where we have compiled Elm code where we take that JS we import that code and run it set up you know in it the Elm application subscribe to some ports send back some ports. So we're communicating to the Elm app through ports and that's that's all that Elm pages is doing. But it creates a set of abstractions for that that makes it easier for the user to basically execute things in a back end and run a script in a back end context which turns out is a very useful thing to do if you're you know making a static site because you want to read some files and then you want to pull that data in your front end. But that's also scripting right. So it does bring up the question like is Elm a good tool for this type of task like this kind of back end task. Right. Yeah exactly. Is Elm a good tool for writing a script. Is that a good idea. And I mean of course we're biased. We want to do everything in Elm and we write lots and lots of Node.js code so that we can have the ability to do things only in Elm. But I think it's I think it's quite nice to be able to just operate within the confines of type safe Elm code where you can write a JSON decoder and have have things fail and have this explicitness. But you can write to a file. You can log you can read files which by the way like writing to a file is like a built in thing in the script module that Elm pages provides. But you can define your own custom back end tasks as well. So it's just a way of binding Elm code and these back end tasks to a back end. That's that's really what a back end task is. OK. So you mentioned doing this in Elm or doing this in other languages or tools. So yeah. Like does it make sense to write a script in Elm or in Elm pages or is it sometimes better to do it in JavaScript or Bash or Python or whatever. So you say that there's at the moment logging there's writing a file. That's not a lot of things that you can do built out of the box but you can do more through custom back end tasks using ports. So basically you can probably do anything that a JavaScript script could do. But is it what are the gains what are the benefits that you have when you do it through Elm pages compared to just running a Node.js script for instance. Exactly. Yeah. Great question. And that's that's exactly the right question I think. So first of all a little bit of background. The motivation for Elm pages scripts and people might be asking like Elm pages scripts like why what does Elm pages have to do with scripts. Yeah the name don't match. Right. At the moment. So the Elm pages script was born out of this use case of generating like the scaffolding for a new route. So Elm pages v2 has an Elm pages add command so you can say Elm pages add blog dot slug underscore and it generates something for a route where it's blog slash some dynamics slug. And I wanted to have a way to let users customize that. Ryan has created a nice feature in Elm SPA where you can do some templating and create custom commands for for scaffolding new pages. I was really keen on on using Matt's Elm code gen tool for that. And so as I was starting to build that I'm like well it would be really nice if if I could use Elm code gen to create scaffolding for new routes. But I also want to be able to read an environment variable read some configuration from a JSON file maybe get some like JSON data from an API to figure out how I'm going to generate my my new routes. And so well that's kind of what back end tasks let you do. So I wanted I knew for a long time that I wanted to have the ability to use back end tasks to scaffold new routes because I just really like this abstraction of back end tasks and I want to use it for a lot of things. And then it's like well if I can like once I've built that it's like well this is no longer a scaffolding tool. This is just a way to like run back end tasks by like writing a module that defines a back end task to run. And OK maybe the special case is it's like writing a file in a specific format but why not just give a back end task to write a file and then you can use that. And now it's just on pages scripts. So it's really like Ruby on Rails generators where it's just like that was the main motivation was Ruby on Rails generators are used for if you want to create a new page with a form and then you just it's a tool for very quickly building up boilerplate. So it's like you know you create a new controller in Rails and your template and your template is defining a form and your form has these fields and you also want to create you know some some stuff for for working with active record to define this new user model or whatever. And so people are very productive using Rails generators where they'll say like Rails generate whatever and and you can build custom workflows. You can build custom generators. You can even install custom generators. So and then they say that Elm has a lot of boilerplates like we don't write write scripts for that usually. Right. Right. But I wanted a way to customize template templates for for adding new routes. And also you know if you want to create a new page and be super productive where you can say hey I'm going to make a new form and it has these fields. Why not be able to write a custom generator a custom Elm pages script that lets you just template that. And if you want to read some configuration from something or whatever you want to do why not let users do that. So that was the motivation. Now back to the question of like why what what benefit do you gain by doing this compared to a bash script or a node script. If we look at the pros and cons between like writing a script in in Elm and writing a script in bash or Node.js we can see some pretty pretty obvious pros and cons on either side. So let's look at like writing a vanilla Elm script. If we were to do it on our own we would need to compile we would you know we'd need to like compile some Elm script we would need to take that compiled Elm script and import it into Node.js so we could run it and do some boilerplate around that. And obviously that's not great. And Elm pages scripts takes care of that for you. So we no longer have to worry about that. But what if we just wanted to grab some HTTP data. Right. If we have to create and update to do that that becomes pretty verbose and tedious. So back end tasks make that less tedious because you just do back end tasks dot HTTP dot get JSON URL JSON decoder and then you can do back end tasks dot and then you don't have that boilerplate of init update subscriptions. Exactly. Now the other thing that becomes tedious there is dealing with failure. Right. So in Elm everything is very explicit when things can go wrong and you have to painstakingly handle every possible error. What if the decoded value does not successfully decode to the format you expected. What if there's an HTTP error. What if there's a file reading error. All these things you have to painstakingly handle every possible failure. So compare that with writing a script in bash or node. The challenge is well what what things can fail. So it's very easy to just run something and let it fail. Right. It just throws an exception. The problem is knowing where it might fail and what implicit assumptions there are and what possible runtime errors are lurking there. So if you want to write a quick and dirty script and you just say I want to hit this API I want to grab this data I want to map the data a little bit and I want to write some file or something like that. Right. Then writing a Node.js script is is great for that because it doesn't get in your way with saying hey the errors might be wrong. You just pull off JSON data. It doesn't get in your way with saying hey this HTTP request might fail. So that if you're just writing a vanilla Elm file you do have to deal with those cases and that becomes tedious. Elm Elm pages back end tasks try to address that problem. So now with with data sources and Elm pages v2 it was more like we talked about earlier it was more convenient because you can just let things fail but it was less powerful because you couldn't handle possible failures or see where it was possible for something to fail. So it was less safe and less powerful in v3 it's more safe and powerful. It's a little a little more verbose. So Elm pages v3 provides a new abstraction called a fatal error. And this is very important for the ergonomics of being able to do a quick and dirty task but it tries to achieve a balance between convenience and safety. So the way it works is it allows you to just give a fatal error to two Elm pages and it will just stop and report the error. So in the case of a script it will just print out what went wrong. So if you say backend task dot HTTP dot get JSON and it gives you a 404 error what you can do it you can do backend task dot allow fatal and that is going to take your HTTP backend task and and give you a backend task fatal error your decoded data. So that fatal error contains the the information that the Elm pages framework needs to print an error message describing what went wrong. So if you do Elm pages run hello and then you hit your API with allow fatal it's just going to print an error message saying hey I was running this HTTP backend task something went wrong. There was a 404 error and because you you opted out of handling and recovering from that error. So what happens if you don't write allow fatal. If you don't write allow fatal in the case of backend task dot HTTP dot get JSON then the types just won't line up because the error type in that get JSON function returns a backend task with with the error having a fatal error and a recoverable error data. So if you wanted to recover from it then you can do backend task dot map error and then you can pull off that recoverable data from that record which is going to be a nice structured HTTP error which could be your JSON decoding error it could be bad body timeout. And so if you want to say if it's a timeout try it again you can do that. You can do backend task on error case error dot recoverable timeout try again or if it is you know whatever else and for all of those different cases that structured data you can choose explicitly how to handle it. So if you don't write allow fatal then you have to do something. And what is that something that you have to do. If you don't. So the at the end of the day the Elm pages expects when you say script dot without CLI options and you give it a backend task the type of that backend task needs to be the error type can be a fatal error and the data type needs to be unit. So so at the end of the day you you need to give it either no possibility of an error or a fatal error if anything. So doing allow fatal just throws away that recoverable error data that has the nicely structured error whereas allow fit. Yeah allow fatal just grabs that fatal error and passes it through. But if you do on error then you can continue with something else. OK. So you have to transform the error type in a way that will print an error or succeed I guess. And if you don't then you have to write to use allow fatal. Right. At the end of the day that's the type of error that you can give it. So you can't you can't just give obviously any error type to Elm pages and have it do something because Elm doesn't let you have like variable return types for something. It's like so it needs to be returning back in task fatal error and you can and unit. And so you could you could define a back end task that has whatever error type just like you know a regular Elm task have an error type. You can map that error type you can do task dot map error. You can also do back and test that map error. It's the same thing whatever the error type is along the way doesn't concern Elm pages. You can you can do whatever you want. You can have whatever structured error data. It's just that if you have the possibility of a failure you have to turn that error type into a fatal error at the end of the day. Right. OK. So basically the fatal error concept in Elm pages is a way of saying hey let's have safety. Let's have a balance between safety and convenience because for the sake of safety we could. So in the design of this I could have just had a single type variable for the data not had an error type variable and just let you say oh yeah it can fail or I want to recover from the failure. What I wanted to have was I wanted to make it very explicit where failures happen and if there's no error type variable there's no possibility of failure. So you can tell just by looking at the types if it's possible for something to fail or not and how it could fail. Now the fatal error type is a very generic failure that doesn't contain any useful information for you. So it's just saying that's the that's the balance between the safety and the convenience. You know it can fail but you can't do anything meaningful to recover from it at that point because you have to kind of choose when you get that data. So that's so the core APIs and Elm pages like HTTP reading from files writing to files things that can fail. They give you these two different bits of data where you can choose I want to either recover or let the fatal exception through the fatal error through. So the point of that design is that you can have the convenience of just saying yeah just give this message to the framework and let it fail or you can recover from it. So it's trying to give you an ergonomic way to easily just say I don't care about this error or a way to recover from it while knowing explicitly whether failure is possible just based on the types. Right. And at the end of the script if it fails then it's always going to have some kind of nice error message or a reasonably nice error message I'm hoping. Oh yeah. I mean that's a major goal of Elm pages for sure is to you know strive for quality we expect in Elm community for error messages. OK. So what I like about this is that you said as you say like you can identify what is going to succeed and what is what can fail. So once you so once this script is compiled by Elm pages if it compiles then it's either going to fail in the intended way or well in a intended way in a or it's going to succeed in the intended way. Yeah. But it's never going to fail because of how you wrote the code. So something that happens a lot to people at least to me but I'm guessing to a lot of people who write scripts is that the script is going to fail because you did something stupid in your script like for instance you write a Node.js script and you mistyped a function name. So that's not going to happen anymore. The only thing the only reason that is going to fail is because some operation that's touched the external system like the file system or made requests across HTTP failed for some reason. But it's never going to fail because of how you wrote the code. So that is quite nice. So that is one of the plus sides that I find in using Elm pages scripts. But do you see other ones compared to writing because you compared it previously with writing a script in Elm without Elm pages which yeah sounds painful. Some people have done it. It's actually not that bad in practice. I have done so myself obviously. But how does it compare to writing something in JavaScript or in Bash or Perl or Python or whatever. When would you do one of those or when would you use Elm pages scripts. Right. Yeah. So you know there's there's a tradeoff in that you know again in the context of Bash or Node.js you don't you don't know where possible failures lurk because you know even if you're writing a script in TypeScript you don't know where you might have gotten some any data back that is actually leaking possibly incorrect type data somewhere. You don't know. So you for me if I'm trying to solve a problem for example like recently I was trying to I was writing a script for for Elm radio where I can automatically apply the right ID3 tags to MP3s that we publish that will apply the right album cover image which you need to do before publishing and the right track information and it helps pull in data from the Notion API so it can get title information and the number of the episode and things like that. And writing it in Node.js I found really frustrating because I even though I was even using like an NPM package for hitting the Notion API but there were all these incorrect assumptions about the format of the JSON data I was getting back even so with this helper package. NPM helper package? Yeah. And you know if I was writing writing in Elm it would be JSON decoders so I would I would immediately turn it into nicely structured data or an error and and be able to get like a shorter feedback cycle as I was working on that script instead of just having to like run it a little bit further run it a little bit further it would just tell me that I have decoding errors until I've gotten the data format as expected which is my preferred workflow. And also I just I can once like if you write a script in Node.js and then it succeeds you're like okay well it's possible for this script to succeed but you're not necessarily convinced that it will succeed for all cases. Whereas like if I if I write the script in Elm I would be much more confident that like oh yeah it it's good now like it's it's handling the expected JSON data I mean maybe the API sends slightly different data formats in different cases but I'm much more confident that I'm done at that point. So that's that's one thing is that confidence which I still want when I'm writing a script like and it's I still want to pull API data down and have some sanity around that being confident that the data format I'm getting is right I still want to work with nice types while I'm doing that and and know that the types I'm working with are correct not like half correct mixed in with some anys that trickled into my system. I mean you don't have anys in bash. Right. Oh man working with the API data responses in bash does not sound fun. I don't even know how you would do that. Yeah. I would just curl it and yeah pray that it works. JQ or something I don't know there yeah there are tools but it's it's not fun you know so it's it's nice to use like a programming language for that not just a bash script. But yes so like the other thing is if if I want to make it more robust to run this script maybe it's like when you write a quick and dirty script you want to just allow failures to just happen right. That would be like in Node.js you just don't do a try catch. So with Elm pages back end tasks you do need to be explicit where failures are possible. But at the same time I mean you know you do your get JSON and then a failure is possible. So the types will not fit together unless you do allow fatal back and test out allow fatal. And yes you do have to write that explicitly but that's all you do. And if if you just say hey I don't want to deal with any possible errors I just want to work with the happy path I expect everything to work. And if if anything goes wrong just give me an error message right. Then you just any time the types tell you to you just do back and test dot allow fatal. And now what you end up with is yes you had to write allow fatal a handful of places. But for one thing you can look at it and see where can fatal things happen. Right. Yeah. That's nice. Yeah. And if you want to recover from it at some point later when you have more time or you want to print out a nicer error message then you know where to look. Exactly. And you know exactly the possible failures that can happen. Like it always feels like uncomfortable for me doing like a try catch in Node.js and then just like expecting the cut exception to be this thing that has this key but then like it might not. And like do I do a try catch within my catch in case my expectations about the properties on that on that error are incorrect. Like so it's if you want to do error handling error recovery like you have nice types that let you do that. If you don't it's explicit where you're not doing that. And that's a very intentional design trade off that it's a little less convenient but it feels more safe. So to me that is a trade off I'm willing to make to write allow fatal a few extra places and have that explicit this and know where things can fail. And then if I want to make my script more robust over time and say like OK this error case I should really have proper error handling for this. This this script that I'm running fails on Sundays because this thing happens and I should really clean that up and add proper error handling. You can or if you want to present nicer error message in one case instead of just saying I got this HTTP error you could give a more custom error message. You can do that. You don't like it. No ends. Right. So you could you could say like instead of saying you know and you know or instead of saying the default can't read file error message that the that it gives you when you get that fatal error in pages in the core APIs you could have you could turn that into your custom error with nice error feedback whereas doing that in Node.js it's just going to be a lot harder. So I just feel like it like the goal of this design is to give you a way to be productive build things up with minimal boilerplate. You write your script hello.elm you expose run its type of script you define a back end task and then you want a quick and dirty script just the happy path you allow fatal. But as you want to deal with more error cases in a graceful way it gives you the tools to do that and to really maintain it. So it's trying to give a balance between convenience and maintainability. So I don't know like would you use Node.js in some cases instead of using an Elm pages script. I'm sure there are cases for that but I think if I if I have some little scripts for like helping with the Elm radio publishing process like I want that in an Elm pages script because I want I want that in Elm and I know. Yeah. Yeah. That makes sense. I mean you could just start writing a script in Node.js like because you start small you do like one thing because it's a prototype and then well you need one additional thing and then you need another additional thing and two three seven additional things and. Right. And at some point you think you should rewrite this in another language. Yeah. Let's rewrite this in bash. This makes a lot more sense. Perfect. Yeah. I would say for me like I feel like when I hit dealing with JSON data in Node.js that's when I really want to just use Elm for that. And I know there are tools like Zod to help you do it in a more Elm way where you're writing things in the style of a decoder. But I don't know for me I'm I'm going to tend to reach for Elm to do that type of task. And if you as you say you can you can start something in Node.js you can create custom back end tasks so you can like write whole chunks of Node.js code in your custom back end task dot TS file and then you can just execute that as a back end task. Yeah. So you can easily migrate from one to the other is what you're saying. Yeah exactly. So the so the custom back end tasks the way you define them you write your custom back end task TS file or JS whatever you prefer. It transpiles it using ES build and and you export async functions and then you do back end task dot custom dot run. You give it the name of the function that you exported from that TypeScript file. You encode some JSON data to pass in. You give it a JSON decoder and then you've got a back end task. And does Elm pages make sure that that port exists both in JavaScript and in Elm before you run it. So it it doesn't need to make sure the port exists in Elm because it's not actually it's it it's using it's not defining a port for each of those but it's just calling your async function. But it does make sure that your your custom back end task TypeScript file compiles. If there is a an error in the file you can recover from that as one as the recoverable error type for that back end task or if you allow fatal it'll print it out in a nice formatted way. Yeah. If you do not have an exported function of the name that you're trying to call it gives you that as part of the structured error type. And if you export something but it's not a function it even tells you about that. So all of those possible error variants will be automatically printed for you in a nice format if you allow fatal and if you want to recover from it you can even do that like it even has a custom type with all those possible failure cases for you. Yeah. So you can pat a match on it and print out a nice error message or something. Exactly. And it will also if you throw an exception in your port data source it will give you if you throw JSON data it will give you that as the error type. OK. So now another topic I do find it a little bit weird that to run a script in Elm which I would love to do and I don't mind necessarily writing a scripts folder with an Elm JSON file and all those boilerplate things. But do I really have to pull in all of Elm pages. Right. Well I mean how do I explain it to my co-worker like oh yeah of course use Elm pages. The name makes a lot of sense. Why not Elm scripts or. Right. No I mean that's fair. So you can you know you can create a script folder in any project doesn't need any of the Elm pages boilerplate. And you know could it someday make sense to have maybe slimmed down version of the NPM package with a different name. Sure. But right now that's not a priority right now it's like I mean right now it is a tool that's basically Rails generate for Elm pages. So it's a tool for helping Elm pages users be more productive. And it happens to be usable outside of an Elm pages project. But yeah it's definitely like a little funky. The thing is like the concept of a back end task is so tied to Elm pages right now. To Elm pages implementation you mean or. Because as you say it like doesn't have anything to do with Elm pages necessarily. It just happens to be code that is in that project also for good reasons. The use case of Elm pages but it doesn't have to be. Right. Yeah. The problem is like I've had this also for Elm review is like where do you draw the line. Like does it make sense to have Elm have the scripts parts in a separate CLI in a separate Elm package called Elm scripts or whatever which you would then use in Elm pages. But that adds a lot of complexity about how do you make sure that those are in sync and how do you handle some of the underlying things that have to be written in JavaScript or have to be written to something. So yeah it's a it's a bit annoying but also yeah. Yeah it feels a little funky but like the fact that you're calling Elm pages run in a project that is not an Elm pages project is like the main problem. And in that case like make an alias to solve that problem. But like in the future it definitely could be reasonable to have like you know like something called Elm engine or Elm back end task or Elm back end or something and have a package for that have fatal error and back end task and the things related to that concept exist there in that separate thing. And then at that point actually it could be could be cool because potentially I could make it like a standalone thing for resolving a back end task where the code to take something of that back end task type and execute that and then give you the resolved data could be like split off into something and then Elm review could let you use a back end task in some place or whichever tool. So I mean I would I am interested in like being being able to access arbitrary files. Right. Exactly. Maybe not HTTP but I mean I could make that limitation. So yeah that could be interesting. It probably wouldn't work that way but maybe under the hood. Right. So that's yeah it's definitely something that could happen in the future. For now I'm really keen to see like what people build with it and and go from there. But yeah you could definitely imagine a possible future where it's sort of designed to fit into more places and I would love to see people using it for more types of tasks. Yeah. If you split it off then the only thing that you gain is ergonomics I'm guessing because it's not going to be necessarily faster. Definitely one use case I see for for these scripts is if you want to work with existing data that you have in your own pages projects. Right. For instance it's used on the Elm Radio website to fetch episodes. Right. Episode data. Well now if you want to generate something you want to generate a file containing the list of episodes while you just reuse those same back end tasks. So that is really nice I think. Exactly. Yeah. For like generating our transcripts where there are a set of things that don't have transcripts yet that could just be an Elm pages script because right now there's a back end task that goes and looks at the file system and decodes a bunch of front matter from files and all these things that you can do with Elm pages back end tasks and it figures out the list of episodes which are which exist but don't yet have transcript data. So we could tie that in in an Elm pages script with actually just run the script and it goes and executes the transcripts for generating transcripts that you need and moving the files to the appropriate file locations and all that. So you mentioned before that this is mostly used for running scripts on your own computer right. I know that Elm pages also has support for serverless or all those kinds of things that I have to admit I don't understand too much. But would this be usable for serverless things as well or would that be different parts of Elm pages in which case we will talk about it in a later episode. Yeah. So yes and no. So it wouldn't be script but back end tasks can be resolved in serverless functions or on a server and that that's what server rendered routes are in Elm pages v3. It uses back end tasks you can do the same types of things but for scripts there's no reason why you wouldn't use an Elm pages script in your CI. So like you should you should absolutely use it like outside of your local machine. And again like it's designed to be relatively easy to write a quick and dirty script that only handles the happy path and then mature into a script with nice error handling and and be a really robust script. So like I think it's a great tool for like writing team scripts and maintaining them and having them on your CI and making them really robust over time. So one thing that we haven't touched on yet that I want to make sure we mention is the CLI options. So Elm pages scripts have the ability to to include a CLI options parser. So for anyone who hasn't heard the term CLI options it's just the term for you know running Elm review dash dash fix dash all that would be a command line option that's specifically a keyword option but is it right. Sorry that one is called a flag. Yeah that was a what was I thinking. Yeah. But yeah all the things that you can provide are options I'm guessing and the things that start with a dash or dash dash are flags. Is that it. So the ones that do not that only have a key but not a value are called flags. The ones that have a key and value are. So I looked through a lot of different names for these terms and based on common conventions and the the ones that were widely used and seemed like the most intuitive I I came up with a little label. So we'll link to my Elm CLI options parser package. This is actually what Elm CLI or what Elm pages scripts uses to parse command line options. But there's a little graphic in there that has little annotations of what these parts of a command line call are. But yeah so a flag does not have a value. You know if you write log dash dash stat. Yeah it's a Boolean in a way. Exactly exactly. It's going to give you a Boolean and then you have keyword ones. You can mix up the order of those ones and it's order independent. You have positional arguments. You can have optional positional arguments. So Elm CLI options parser is an Elm package that I built. I use it for Elm GraphQL. I've used it for years in Elm GraphQL and it turns a command line command into structured data or an error message that tells you the help options of what went wrong and why the command was not valid. So in Elm pages scripts if you want to you can accept command line arguments. So our hello world we said script dot without CLI options. But if you and that just takes a back end task and that's it. So script dot log hello world. That's it. Script dot without CLI options script dot log hello. If you want you can accept CLI options. So that would be script dot with CLI options. Then you give it your CLI options parser and then you receive that parsed data and return a back end task. So you could you know based on based on a flag do one type of back end task or another. Yeah that makes a lot of sense. Yeah. So just another sort of like essentially if you think about it if you if you want to do a simple scripting workflow you know in in bash you can just pull off positional arguments in node JS you read a bunch of stack overflow questions until you figure out the right incantation and which like which array index the actual user arguments start at and then where to get those. Yeah you mean until you learn which command line tool you have to use like the use commander or minimists or no not that one because it's deprecated or that other one has this problem with duplicate flags or exactly that stage to that stage. But stage one is just like oh wait the index zero of the arguments of the process dot argv or whatever is like the command that was called and then yeah the second one is whatever. And so yeah after you like finally figure that out and you pull a single argument because that's all you need and then you realize oh I actually need to parse different types of options and then you then you go through and figure out which of the NPM packages is cool for that now. And of course it's like Elm is really good for turning unstructured data into structured data. It's like parse don't validate is what makes Elm awesome to me I think among other things. But it really shines there whereas if you're using minimist or commander or whatever it's just not as nice to work with massaging these things into nicely structured data. So yeah I find that that's like a really nice workflow because in Elm pages script like it comes built in with this tool. You just define your command line options parser and like you sort of know the data you're going to end up with and it has built it. It's all wired in for you. So it has a baked in opinion about that. So again that's the philosophy is like trying to remove friction as much as possible while still giving you like tools for doing things in a powerful but safe way. And I think this this fits in with that where like I don't know I just I feel like it's prohibitively expensive to actually figure out how to build command line options parsing for a quick and dirty script in a lot of cases. But when I'm working with this I don't I don't feel that I feel like I should ask because there are alternatives to running. Scripts in Elm. I think the most known one is Elm POSIX. There's also ElmScript which is a name we've used unknowingly so far. At least I did. So ElmScript from Ian McKenzie and ElmPOSIX from Albert Dahlin. Have you used those for inspiration? Have you seen limitations of those or is it just that well it made sense for Elm pages and this is just an entirely novel approach and API. Right. Yeah I'm I've been aware of those tools but but yeah as you say it's more the latter that it's sort of emerged from the wanting to be able to use back end tasks in different places. And so rather than looking at what's out there how would I do it differently or do I like the way it's done and then designing based on that it was more just I want to be able to use back end tasks. What would that look like. But that said like comparing them like Elm POSIX for example it's it has this IO Monad concept where it makes the tradeoff of having a single type variable for the for the resulting data that you get meaning that errors are not represented in the type which as we talked about is a it's a tradeoff. It's a tradeoff of convenience versus explicitness of possible failures. So it chooses the tradeoff of convenience which is totally reasonable tradeoff for a command line tool. And yeah the Elm POSIX standard API has a lot more functions in the toolkit designed at designed for helping you do sort of scripty tasks like reading the the flags for a file and you know making things writable and things like that. So that's not really the it's it's certainly like a little bit confusing but that's that's not the main purpose of of Elm pages scripts. The main purpose of Elm pages scripts again it's like trying to be like a Rails generator type thing and trying to be a toolkit for helping to manage your project again like helping the publishing process for Elm radio dot com. That's like that's the type of thing it's designed for. You can do whatever you want to with the custom back end tasks but it's not like it wouldn't fit well in Elm pages to have a large API for making directories and making files executable and things like that. So that's not what it focuses on. I don't know. I think it could make sense at least creating directories. Yeah. If you say like this is not what's Elm pages scripts is meant for then you know that people are afraid to use it then in a sense that oh well if this is not what it was meant for then I might use it in a way that was unexpected or not meant for and then Dillon is going to pull the plug and remove those features from me. I mean we've seen this in Elmland so. I don't I don't look at it quite like that. So to me it's more about what exists in the standard API because a back end task gives you a way to define a custom back end task which is just JSON in JSON out. You can you can build anything with that. So if you want to build like the difference is that the standard library in Elm pages does not have a lot of functions for that built in. So but so you know maybe that in the future could be an argument for something like you were describing pulling out a separate thing and maybe having like an extended standard library. It's a difficult challenge of like how you package together Elm code and this like back end code for doing these Node.js things. But you know but potentially you could kind of have the ability to do these things sort of built in somewhere but then not expose the back end task set of functions for for using them by default. There are a number of ways I could imagine that going but. So what I'm hearing is Pinky promise I won't remove things. And I mean it's it's just like a back end task is a general purpose tool. It is not very opinionated. It's like like Elm Elm removed like custom the ability to do user defined custom operators but ports are there. It's not like oh our ports going to stop letting me do whatever it's like no port a port is a port. It's like a general purpose language feature that's like core to the design and it's not going to it's not going to go away. Like it's the same with a back end task like that's just a core concept in in Elm pages and that's not going to go away and that's not going to change like you can you can define your own back end tasks. Right. Yeah. I mostly wanted people to to know what they can rely on without being afraid of like things getting removed. Right. No it's a it's a great point to set expectations there and again the expectation is like it's a totally general purpose building block and you can you can run whatever you need to in your back end task. But the core libraries might not you might not expect the core standard library to expose functions for doing a wide variety of scripting tasks because that's not the main goal. So that's where I went then yourself. Exactly. And that's not going to change. Yeah. All right. OK. Well we are at the end of the script. If I want to make one final pun where can people try this out because Elm pages v3 has not been released but you can already try this out. So how can they try it out. How can they help. And what are you looking for. Yeah. So I will link to a starter repo both in Elm pages starter repo for v3 as well as a minimal boilerplate branch on that repo that gives you the minimum setup for an Elm pages script and also has some information about Elm pages scripts and how to run them. I would I would love to hear about what people do with them. I think it's like a pretty general tool and I think I'll be surprised by some of the use cases people find for this. Other than that yeah the Elm pages docs on pages v3 docs and and the Elm pages channel on Slack is a great place to ask questions. Yeah. So people will not use Elm pages they will use Elm pages v3 alpha. Was that correct. Yeah. There's a package Elm pages v3 beta in hindsight I probably should have called it like Elm pages pre-release or something. But yeah. So and and hopefully it won't be too much longer before that's a stable release. So hopefully that that instruction will be irrelevant soon. But but definitely keep an eye on the docs if it says deprecated this is now a stable release then keep an eye out for that. I did want to mention one more quick thing which is so I think one of the really exciting things that that comes from designing these things you know as as data back end tasks are just a type of data that you pass to a specific place a CLI options parser is just a piece of data. So one interesting thing that comes from that is because it's data you could turn the validator for CLI options into a web interface that presents input fields for all the different flags. So this is one thing on my mind that I think could be a cool project is to have like on the Elm pages dev server or to have maybe some separate command whatever it may be some way to pop up a web page that you can type in your command and get the validation messages because Elm CLI options parser lets you define validations for all of the CLI flags and it enumerates all the possible ways you could define you could run the CLI all the sub commands all of the optional keyword arguments all of the required keyword arguments. So you could have a cool set of like drop downs that tell you exactly what's required and what's optional and give you validation messages in real time as you type them so you can see before you run it what error message you're going to get and you know before you run it if the CLI option parsing will succeed and then you can just hit execute when you're done and it can just run it from that running server it can actually execute the command. So that is one thing that I think just again it's like using these tools that are built around pure functions and data types that describe these things opens up some really cool stuff. Absolutely. Yeah. Another thing on my radar for the future is I think it would be really cool to have a command for bundling a script so you could write a script run this bundle command and get a little optimized script that includes all the command line parsing and everything just a single self-contained node.js file and then you could publish that as an npm package or include it in your in your path somewhere in your bin folder and run it on your system. So yeah lots of lots of fun things that this could take different directions in the future. Yeah I'm excited to hear what people use it for as well. Yeah. Could be fun to use it inside of Elm Review but that would be an additional dependency that I don't think people want. Right. Which is why maybe bundling it could could become interesting in some use cases but that's that's fair. Yeah. Well stay tuned and Jeroen until next time. Until next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.24, "text": " Hello Jeroen. Hello Dillon. So I think our listeners may be shocked to hear but our episodes", "tokens": [2425, 508, 2032, 268, 13, 2425, 28160, 13, 407, 286, 519, 527, 23274, 815, 312, 12763, 281, 1568, 457, 527, 9313], "temperature": 0.0, "avg_logprob": -0.2757279504205763, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05574812367558479}, {"id": 1, "seek": 0, "start": 7.24, "end": 13.92, "text": " are usually unscripted. But today we're going to do something a little bit different. I'm", "tokens": [366, 2673, 2693, 5944, 292, 13, 583, 965, 321, 434, 516, 281, 360, 746, 257, 707, 857, 819, 13, 286, 478], "temperature": 0.0, "avg_logprob": -0.2757279504205763, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05574812367558479}, {"id": 2, "seek": 0, "start": 13.92, "end": 21.240000000000002, "text": " never ready for you making a pun. I should be at this point. But I'm not. You knew it", "tokens": [1128, 1919, 337, 291, 1455, 257, 4468, 13, 286, 820, 312, 412, 341, 935, 13, 583, 286, 478, 406, 13, 509, 2586, 309], "temperature": 0.0, "avg_logprob": -0.2757279504205763, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05574812367558479}, {"id": 3, "seek": 0, "start": 21.240000000000002, "end": 28.92, "text": " was coming. No I wasn't. What are we talking about today? Today we're going to talk about", "tokens": [390, 1348, 13, 883, 286, 2067, 380, 13, 708, 366, 321, 1417, 466, 965, 30, 2692, 321, 434, 516, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.2757279504205763, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05574812367558479}, {"id": 4, "seek": 2892, "start": 28.92, "end": 35.84, "text": " Elm Pages scripts. Scripts! That was the pun. Yeah. So we haven't talked about Elm Pages", "tokens": [2699, 76, 430, 1660, 23294, 13, 15675, 82, 0, 663, 390, 264, 4468, 13, 865, 13, 407, 321, 2378, 380, 2825, 466, 2699, 76, 430, 1660], "temperature": 0.0, "avg_logprob": -0.28843025285370494, "compression_ratio": 1.5454545454545454, "no_speech_prob": 8.741160127101466e-05}, {"id": 5, "seek": 2892, "start": 35.84, "end": 42.96, "text": " in a while. Yes. We did talk about it already in the first episode if I remember correctly.", "tokens": [294, 257, 1339, 13, 1079, 13, 492, 630, 751, 466, 309, 1217, 294, 264, 700, 3500, 498, 286, 1604, 8944, 13], "temperature": 0.0, "avg_logprob": -0.28843025285370494, "compression_ratio": 1.5454545454545454, "no_speech_prob": 8.741160127101466e-05}, {"id": 6, "seek": 2892, "start": 42.96, "end": 49.64, "text": " Yep that's right. So it's a project you've been working a long time on. And we haven't", "tokens": [7010, 300, 311, 558, 13, 407, 309, 311, 257, 1716, 291, 600, 668, 1364, 257, 938, 565, 322, 13, 400, 321, 2378, 380], "temperature": 0.0, "avg_logprob": -0.28843025285370494, "compression_ratio": 1.5454545454545454, "no_speech_prob": 8.741160127101466e-05}, {"id": 7, "seek": 2892, "start": 49.64, "end": 57.28, "text": " heard about it much this year except on Elm Radio because you keep mentioning it for good", "tokens": [2198, 466, 309, 709, 341, 1064, 3993, 322, 2699, 76, 17296, 570, 291, 1066, 18315, 309, 337, 665], "temperature": 0.0, "avg_logprob": -0.28843025285370494, "compression_ratio": 1.5454545454545454, "no_speech_prob": 8.741160127101466e-05}, {"id": 8, "seek": 5728, "start": 57.28, "end": 63.72, "text": " reason. You're still working on it. And yeah you've been working you've been making a lot", "tokens": [1778, 13, 509, 434, 920, 1364, 322, 309, 13, 400, 1338, 291, 600, 668, 1364, 291, 600, 668, 1455, 257, 688], "temperature": 0.0, "avg_logprob": -0.23103407657507694, "compression_ratio": 1.748792270531401, "no_speech_prob": 4.936004643241176e-06}, {"id": 9, "seek": 5728, "start": 63.72, "end": 69.4, "text": " of things in Elm Pages. A lot of very cool things that we're going to talk about in future", "tokens": [295, 721, 294, 2699, 76, 430, 1660, 13, 316, 688, 295, 588, 1627, 721, 300, 321, 434, 516, 281, 751, 466, 294, 2027], "temperature": 0.0, "avg_logprob": -0.23103407657507694, "compression_ratio": 1.748792270531401, "no_speech_prob": 4.936004643241176e-06}, {"id": 10, "seek": 5728, "start": 69.4, "end": 73.92, "text": " episodes. But today we're just going to focus on one part of it. And that is going to be", "tokens": [9313, 13, 583, 965, 321, 434, 445, 516, 281, 1879, 322, 472, 644, 295, 309, 13, 400, 300, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.23103407657507694, "compression_ratio": 1.748792270531401, "no_speech_prob": 4.936004643241176e-06}, {"id": 11, "seek": 5728, "start": 73.92, "end": 80.0, "text": " scripts. Yes. Because from the amount of content that I can see in Elm Pages we're not going", "tokens": [23294, 13, 1079, 13, 1436, 490, 264, 2372, 295, 2701, 300, 286, 393, 536, 294, 2699, 76, 430, 1660, 321, 434, 406, 516], "temperature": 0.0, "avg_logprob": -0.23103407657507694, "compression_ratio": 1.748792270531401, "no_speech_prob": 4.936004643241176e-06}, {"id": 12, "seek": 8000, "start": 80.0, "end": 87.84, "text": " to be able to fit that into even hour long episodes. Exactly. So yeah Elm Pages scripts.", "tokens": [281, 312, 1075, 281, 3318, 300, 666, 754, 1773, 938, 9313, 13, 7587, 13, 407, 1338, 2699, 76, 430, 1660, 23294, 13], "temperature": 0.0, "avg_logprob": -0.22600268354319564, "compression_ratio": 1.6418604651162791, "no_speech_prob": 6.143897280708188e-06}, {"id": 13, "seek": 8000, "start": 87.84, "end": 95.4, "text": " Like what is a script? What is Elm Pages? What is this cool new thing that people want", "tokens": [1743, 437, 307, 257, 5755, 30, 708, 307, 2699, 76, 430, 1660, 30, 708, 307, 341, 1627, 777, 551, 300, 561, 528], "temperature": 0.0, "avg_logprob": -0.22600268354319564, "compression_ratio": 1.6418604651162791, "no_speech_prob": 6.143897280708188e-06}, {"id": 14, "seek": 8000, "start": 95.4, "end": 101.24000000000001, "text": " to hear about? Yeah. So I'm really excited to see what people do with Elm Pages scripts.", "tokens": [281, 1568, 466, 30, 865, 13, 407, 286, 478, 534, 2919, 281, 536, 437, 561, 360, 365, 2699, 76, 430, 1660, 23294, 13], "temperature": 0.0, "avg_logprob": -0.22600268354319564, "compression_ratio": 1.6418604651162791, "no_speech_prob": 6.143897280708188e-06}, {"id": 15, "seek": 8000, "start": 101.24000000000001, "end": 106.53999999999999, "text": " So yeah and just to reiterate we are talking about the Elm Pages V3 release. At the time", "tokens": [407, 1338, 293, 445, 281, 33528, 321, 366, 1417, 466, 264, 2699, 76, 430, 1660, 691, 18, 4374, 13, 1711, 264, 565], "temperature": 0.0, "avg_logprob": -0.22600268354319564, "compression_ratio": 1.6418604651162791, "no_speech_prob": 6.143897280708188e-06}, {"id": 16, "seek": 10654, "start": 106.54, "end": 113.28, "text": " of this recording it is pre-release ramping up to get the release getting some final API", "tokens": [295, 341, 6613, 309, 307, 659, 12, 265, 1122, 12428, 278, 493, 281, 483, 264, 4374, 1242, 512, 2572, 9362], "temperature": 0.0, "avg_logprob": -0.23609681380422493, "compression_ratio": 1.6064814814814814, "no_speech_prob": 4.157283001404721e-06}, {"id": 17, "seek": 10654, "start": 113.28, "end": 118.84, "text": " changes and feedback from the community and writing some docs updating some docs. But", "tokens": [2962, 293, 5824, 490, 264, 1768, 293, 3579, 512, 45623, 25113, 512, 45623, 13, 583], "temperature": 0.0, "avg_logprob": -0.23609681380422493, "compression_ratio": 1.6064814814814814, "no_speech_prob": 4.157283001404721e-06}, {"id": 18, "seek": 10654, "start": 118.84, "end": 124.2, "text": " yes so that's what we're talking about here. Elm Pages V3. The scope of what you can do", "tokens": [2086, 370, 300, 311, 437, 321, 434, 1417, 466, 510, 13, 2699, 76, 430, 1660, 691, 18, 13, 440, 11923, 295, 437, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.23609681380422493, "compression_ratio": 1.6064814814814814, "no_speech_prob": 4.157283001404721e-06}, {"id": 19, "seek": 10654, "start": 124.2, "end": 131.36, "text": " with Elm Pages V3 is kind of huge. We will talk about it in the future but Elm Pages", "tokens": [365, 2699, 76, 430, 1660, 691, 18, 307, 733, 295, 2603, 13, 492, 486, 751, 466, 309, 294, 264, 2027, 457, 2699, 76, 430, 1660], "temperature": 0.0, "avg_logprob": -0.23609681380422493, "compression_ratio": 1.6064814814814814, "no_speech_prob": 4.157283001404721e-06}, {"id": 20, "seek": 13136, "start": 131.36, "end": 139.48000000000002, "text": " V2 people may think of as a static site generator which is what it was. So Elm Pages V3 allows", "tokens": [691, 17, 561, 815, 519, 295, 382, 257, 13437, 3621, 19265, 597, 307, 437, 309, 390, 13, 407, 2699, 76, 430, 1660, 691, 18, 4045], "temperature": 0.0, "avg_logprob": -0.16044336721437788, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.0904124085063813e-06}, {"id": 21, "seek": 13136, "start": 139.48000000000002, "end": 145.58, "text": " you to do everything you could with V2. You can still build a static site using it. Elmradio.com", "tokens": [291, 281, 360, 1203, 291, 727, 365, 691, 17, 13, 509, 393, 920, 1322, 257, 13437, 3621, 1228, 309, 13, 2699, 76, 6206, 1004, 13, 1112], "temperature": 0.0, "avg_logprob": -0.16044336721437788, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.0904124085063813e-06}, {"id": 22, "seek": 13136, "start": 145.58, "end": 151.28000000000003, "text": " is built using Elm Pages V3 actually. It's on a pre-release and it does that by generating", "tokens": [307, 3094, 1228, 2699, 76, 430, 1660, 691, 18, 767, 13, 467, 311, 322, 257, 659, 12, 265, 1122, 293, 309, 775, 300, 538, 17746], "temperature": 0.0, "avg_logprob": -0.16044336721437788, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.0904124085063813e-06}, {"id": 23, "seek": 13136, "start": 151.28000000000003, "end": 159.56, "text": " static assets but you can also do server rendered pages. So the scope of Elm Pages V3 has changed", "tokens": [13437, 9769, 457, 291, 393, 611, 360, 7154, 28748, 7183, 13, 407, 264, 11923, 295, 2699, 76, 430, 1660, 691, 18, 575, 3105], "temperature": 0.0, "avg_logprob": -0.16044336721437788, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.0904124085063813e-06}, {"id": 24, "seek": 15956, "start": 159.56, "end": 165.96, "text": " with what you can do quite a bit compared to V2. But the heart of Elm Pages is still", "tokens": [365, 437, 291, 393, 360, 1596, 257, 857, 5347, 281, 691, 17, 13, 583, 264, 1917, 295, 2699, 76, 430, 1660, 307, 920], "temperature": 0.0, "avg_logprob": -0.19282441187386562, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.2027386219415348e-05}, {"id": 25, "seek": 15956, "start": 165.96, "end": 173.72, "text": " the same throughout all of its permutations. I've always thought of the heart of Elm Pages", "tokens": [264, 912, 3710, 439, 295, 1080, 4784, 325, 763, 13, 286, 600, 1009, 1194, 295, 264, 1917, 295, 2699, 76, 430, 1660], "temperature": 0.0, "avg_logprob": -0.19282441187386562, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.2027386219415348e-05}, {"id": 26, "seek": 15956, "start": 173.72, "end": 180.8, "text": " as being this sort of engine that's able to like execute things on a back end and give", "tokens": [382, 885, 341, 1333, 295, 2848, 300, 311, 1075, 281, 411, 14483, 721, 322, 257, 646, 917, 293, 976], "temperature": 0.0, "avg_logprob": -0.19282441187386562, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.2027386219415348e-05}, {"id": 27, "seek": 15956, "start": 180.8, "end": 187.32, "text": " you back data. In Elm Pages V2 that was called data sources. So you know you could do a data", "tokens": [291, 646, 1412, 13, 682, 2699, 76, 430, 1660, 691, 17, 300, 390, 1219, 1412, 7139, 13, 407, 291, 458, 291, 727, 360, 257, 1412], "temperature": 0.0, "avg_logprob": -0.19282441187386562, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.2027386219415348e-05}, {"id": 28, "seek": 18732, "start": 187.32, "end": 196.6, "text": " source to make an HTTP request. So like for Elmradio.com it uses a lot of local files", "tokens": [4009, 281, 652, 364, 33283, 5308, 13, 407, 411, 337, 2699, 76, 6206, 1004, 13, 1112, 309, 4960, 257, 688, 295, 2654, 7098], "temperature": 0.0, "avg_logprob": -0.2423263153472504, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.6425504984217696e-06}, {"id": 29, "seek": 18732, "start": 196.6, "end": 205.44, "text": " with markdown and pulls them in, runs a markdown parser. It's reading files. You know obviously", "tokens": [365, 1491, 5093, 293, 16982, 552, 294, 11, 6676, 257, 1491, 5093, 21156, 260, 13, 467, 311, 3760, 7098, 13, 509, 458, 2745], "temperature": 0.0, "avg_logprob": -0.2423263153472504, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.6425504984217696e-06}, {"id": 30, "seek": 18732, "start": 205.44, "end": 211.16, "text": " with Elm there's not a way to go to your local file system and read files directly. But Elm", "tokens": [365, 2699, 76, 456, 311, 406, 257, 636, 281, 352, 281, 428, 2654, 3991, 1185, 293, 1401, 7098, 3838, 13, 583, 2699, 76], "temperature": 0.0, "avg_logprob": -0.2423263153472504, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.6425504984217696e-06}, {"id": 31, "seek": 21116, "start": 211.16, "end": 217.64, "text": " Pages gives you ways to do that. So in V2 that tool was called a data source and you", "tokens": [430, 1660, 2709, 291, 2098, 281, 360, 300, 13, 407, 294, 691, 17, 300, 2290, 390, 1219, 257, 1412, 4009, 293, 291], "temperature": 0.0, "avg_logprob": -0.2015791463327932, "compression_ratio": 1.7258883248730965, "no_speech_prob": 5.422157300927211e-06}, {"id": 32, "seek": 21116, "start": 217.64, "end": 223.51999999999998, "text": " know you can pull in data to your initial page render using a data source. So before", "tokens": [458, 291, 393, 2235, 294, 1412, 281, 428, 5883, 3028, 15529, 1228, 257, 1412, 4009, 13, 407, 949], "temperature": 0.0, "avg_logprob": -0.2015791463327932, "compression_ratio": 1.7258883248730965, "no_speech_prob": 5.422157300927211e-06}, {"id": 33, "seek": 21116, "start": 223.51999999999998, "end": 229.56, "text": " your static page renders you can read from a file. That was called data sources in V2.", "tokens": [428, 13437, 3028, 6125, 433, 291, 393, 1401, 490, 257, 3991, 13, 663, 390, 1219, 1412, 7139, 294, 691, 17, 13], "temperature": 0.0, "avg_logprob": -0.2015791463327932, "compression_ratio": 1.7258883248730965, "no_speech_prob": 5.422157300927211e-06}, {"id": 34, "seek": 21116, "start": 229.56, "end": 238.88, "text": " In V3 because the scope of what Elm Pages does has changed the term data source has", "tokens": [682, 691, 18, 570, 264, 11923, 295, 437, 2699, 76, 430, 1660, 775, 575, 3105, 264, 1433, 1412, 4009, 575], "temperature": 0.0, "avg_logprob": -0.2015791463327932, "compression_ratio": 1.7258883248730965, "no_speech_prob": 5.422157300927211e-06}, {"id": 35, "seek": 23888, "start": 238.88, "end": 244.51999999999998, "text": " been renamed and the concept has changed a tiny bit. And the reason for that is because", "tokens": [668, 40949, 293, 264, 3410, 575, 3105, 257, 5870, 857, 13, 400, 264, 1778, 337, 300, 307, 570], "temperature": 0.0, "avg_logprob": -0.2220848383528463, "compression_ratio": 1.575221238938053, "no_speech_prob": 2.684118726392626e-06}, {"id": 36, "seek": 23888, "start": 244.51999999999998, "end": 251.4, "text": " in V2 it was the model was much more you try to make an HTTP request. You try to read from", "tokens": [294, 691, 17, 309, 390, 264, 2316, 390, 709, 544, 291, 853, 281, 652, 364, 33283, 5308, 13, 509, 853, 281, 1401, 490], "temperature": 0.0, "avg_logprob": -0.2220848383528463, "compression_ratio": 1.575221238938053, "no_speech_prob": 2.684118726392626e-06}, {"id": 37, "seek": 23888, "start": 251.4, "end": 257.08, "text": " a file and if anything goes wrong you just stop the build and fail. And then the developer", "tokens": [257, 3991, 293, 498, 1340, 1709, 2085, 291, 445, 1590, 264, 1322, 293, 3061, 13, 400, 550, 264, 10754], "temperature": 0.0, "avg_logprob": -0.2220848383528463, "compression_ratio": 1.575221238938053, "no_speech_prob": 2.684118726392626e-06}, {"id": 38, "seek": 23888, "start": 257.08, "end": 265.12, "text": " can read the issue. They can read a nicely formatted error message and say oh this API", "tokens": [393, 1401, 264, 2734, 13, 814, 393, 1401, 257, 9594, 1254, 32509, 6713, 3636, 293, 584, 1954, 341, 9362], "temperature": 0.0, "avg_logprob": -0.2220848383528463, "compression_ratio": 1.575221238938053, "no_speech_prob": 2.684118726392626e-06}, {"id": 39, "seek": 26512, "start": 265.12, "end": 272.8, "text": " turned to 404. Let me fix that and then rerun the build and it succeeds. With V3 so for", "tokens": [3574, 281, 3356, 19, 13, 961, 385, 3191, 300, 293, 550, 43819, 409, 264, 1322, 293, 309, 49263, 13, 2022, 691, 18, 370, 337], "temperature": 0.0, "avg_logprob": -0.22569683033932922, "compression_ratio": 1.5493562231759657, "no_speech_prob": 1.9333529053255916e-06}, {"id": 40, "seek": 26512, "start": 272.8, "end": 279.84000000000003, "text": " example if your server rendering pages maybe you get a 404 in an HTTP request. Maybe you're", "tokens": [1365, 498, 428, 7154, 22407, 7183, 1310, 291, 483, 257, 3356, 19, 294, 364, 33283, 5308, 13, 2704, 291, 434], "temperature": 0.0, "avg_logprob": -0.22569683033932922, "compression_ratio": 1.5493562231759657, "no_speech_prob": 1.9333529053255916e-06}, {"id": 41, "seek": 26512, "start": 279.84000000000003, "end": 286.04, "text": " doing a post and you need to update something and you need to handle that error in a graceful", "tokens": [884, 257, 2183, 293, 291, 643, 281, 5623, 746, 293, 291, 643, 281, 4813, 300, 6713, 294, 257, 10042, 906], "temperature": 0.0, "avg_logprob": -0.22569683033932922, "compression_ratio": 1.5493562231759657, "no_speech_prob": 1.9333529053255916e-06}, {"id": 42, "seek": 26512, "start": 286.04, "end": 292.16, "text": " way. So that's one of the reasons why this concept has changed and become a little more", "tokens": [636, 13, 407, 300, 311, 472, 295, 264, 4112, 983, 341, 3410, 575, 3105, 293, 1813, 257, 707, 544], "temperature": 0.0, "avg_logprob": -0.22569683033932922, "compression_ratio": 1.5493562231759657, "no_speech_prob": 1.9333529053255916e-06}, {"id": 43, "seek": 29216, "start": 292.16, "end": 298.08000000000004, "text": " powerful and part of the reason why the name has changed. So in V3 the term is no longer", "tokens": [4005, 293, 644, 295, 264, 1778, 983, 264, 1315, 575, 3105, 13, 407, 294, 691, 18, 264, 1433, 307, 572, 2854], "temperature": 0.0, "avg_logprob": -0.3244136991954985, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.568928564665839e-05}, {"id": 44, "seek": 29216, "start": 298.08000000000004, "end": 305.28000000000003, "text": " data source is now called a back end task. And in addition to that in V so back end task", "tokens": [1412, 4009, 307, 586, 1219, 257, 646, 917, 5633, 13, 400, 294, 4500, 281, 300, 294, 691, 370, 646, 917, 5633], "temperature": 0.0, "avg_logprob": -0.3244136991954985, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.568928564665839e-05}, {"id": 45, "seek": 29216, "start": 305.28000000000003, "end": 309.96000000000004, "text": " it actually looks and feels a lot like the Elm core tasks.", "tokens": [309, 767, 1542, 293, 3417, 257, 688, 411, 264, 2699, 76, 4965, 9608, 13], "temperature": 0.0, "avg_logprob": -0.3244136991954985, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.568928564665839e-05}, {"id": 46, "seek": 29216, "start": 309.96000000000004, "end": 314.44000000000005, "text": " Okay well that's not going to be very surprising then to use.", "tokens": [1033, 731, 300, 311, 406, 516, 281, 312, 588, 8830, 550, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.3244136991954985, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.568928564665839e-05}, {"id": 47, "seek": 29216, "start": 314.44000000000005, "end": 321.58000000000004, "text": " Yeah I hope so. In fact a lot of the API looks quite similar. You can do back end task dot", "tokens": [865, 286, 1454, 370, 13, 682, 1186, 257, 688, 295, 264, 9362, 1542, 1596, 2531, 13, 509, 393, 360, 646, 917, 5633, 5893], "temperature": 0.0, "avg_logprob": -0.3244136991954985, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.568928564665839e-05}, {"id": 48, "seek": 32158, "start": 321.58, "end": 328.52, "text": " and then you can do back end task dot map or back end task dot map error. Now if you're", "tokens": [293, 550, 291, 393, 360, 646, 917, 5633, 5893, 4471, 420, 646, 917, 5633, 5893, 4471, 6713, 13, 823, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.22393205132282956, "compression_ratio": 1.6158536585365855, "no_speech_prob": 8.714239356777398e-07}, {"id": 49, "seek": 32158, "start": 328.52, "end": 334.32, "text": " familiar with data sources in Elm pages V2 the map error part might be surprising because", "tokens": [4963, 365, 1412, 7139, 294, 2699, 76, 7183, 691, 17, 264, 4471, 6713, 644, 1062, 312, 8830, 570], "temperature": 0.0, "avg_logprob": -0.22393205132282956, "compression_ratio": 1.6158536585365855, "no_speech_prob": 8.714239356777398e-07}, {"id": 50, "seek": 32158, "start": 334.32, "end": 343.24, "text": " in V2 there was no error. So an Elm task is task with an error type variable and a data", "tokens": [294, 691, 17, 456, 390, 572, 6713, 13, 407, 364, 2699, 76, 5633, 307, 5633, 365, 364, 6713, 2010, 7006, 293, 257, 1412], "temperature": 0.0, "avg_logprob": -0.22393205132282956, "compression_ratio": 1.6158536585365855, "no_speech_prob": 8.714239356777398e-07}, {"id": 51, "seek": 34324, "start": 343.24, "end": 354.56, "text": " type variable. So if you do an Elm HTTP task it's going to give you a task HTTP dot error", "tokens": [2010, 7006, 13, 407, 498, 291, 360, 364, 2699, 76, 33283, 5633, 309, 311, 516, 281, 976, 291, 257, 5633, 33283, 5893, 6713], "temperature": 0.0, "avg_logprob": -0.2345455222659641, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.136490368480736e-06}, {"id": 52, "seek": 34324, "start": 354.56, "end": 361.08, "text": " and then your decoded data as the data type. And then you do task dot attempt. You have", "tokens": [293, 550, 428, 979, 12340, 1412, 382, 264, 1412, 2010, 13, 400, 550, 291, 360, 5633, 5893, 5217, 13, 509, 362], "temperature": 0.0, "avg_logprob": -0.2345455222659641, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.136490368480736e-06}, {"id": 53, "seek": 34324, "start": 361.08, "end": 369.1, "text": " to do dot attempt if there is an error that could happen. And then you get a message where", "tokens": [281, 360, 5893, 5217, 498, 456, 307, 364, 6713, 300, 727, 1051, 13, 400, 550, 291, 483, 257, 3636, 689], "temperature": 0.0, "avg_logprob": -0.2345455222659641, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.136490368480736e-06}, {"id": 54, "seek": 36910, "start": 369.1, "end": 375.1, "text": " you can deal with that result. So you can do you know turn it into remote data or do", "tokens": [291, 393, 2028, 365, 300, 1874, 13, 407, 291, 393, 360, 291, 458, 1261, 309, 666, 8607, 1412, 420, 360], "temperature": 0.0, "avg_logprob": -0.21125306975975466, "compression_ratio": 1.674757281553398, "no_speech_prob": 1.4823442597844405e-06}, {"id": 55, "seek": 36910, "start": 375.1, "end": 381.20000000000005, "text": " whatever you want to with that result. So Elm pages V3 has these back end tasks. They", "tokens": [2035, 291, 528, 281, 365, 300, 1874, 13, 407, 2699, 76, 7183, 691, 18, 575, 613, 646, 917, 9608, 13, 814], "temperature": 0.0, "avg_logprob": -0.21125306975975466, "compression_ratio": 1.674757281553398, "no_speech_prob": 1.4823442597844405e-06}, {"id": 56, "seek": 36910, "start": 381.20000000000005, "end": 387.46000000000004, "text": " can have an error. So the reason for that is because in V2 it was much more if anything", "tokens": [393, 362, 364, 6713, 13, 407, 264, 1778, 337, 300, 307, 570, 294, 691, 17, 309, 390, 709, 544, 498, 1340], "temperature": 0.0, "avg_logprob": -0.21125306975975466, "compression_ratio": 1.674757281553398, "no_speech_prob": 1.4823442597844405e-06}, {"id": 57, "seek": 36910, "start": 387.46000000000004, "end": 394.62, "text": " goes wrong stop the stop the line stop the assembly line stop everything. And so there", "tokens": [1709, 2085, 1590, 264, 1590, 264, 1622, 1590, 264, 12103, 1622, 1590, 1203, 13, 400, 370, 456], "temperature": 0.0, "avg_logprob": -0.21125306975975466, "compression_ratio": 1.674757281553398, "no_speech_prob": 1.4823442597844405e-06}, {"id": 58, "seek": 39462, "start": 394.62, "end": 400.46, "text": " is a trade off there because that is very convenient in a lot of ways. It's more convenient", "tokens": [307, 257, 4923, 766, 456, 570, 300, 307, 588, 10851, 294, 257, 688, 295, 2098, 13, 467, 311, 544, 10851], "temperature": 0.0, "avg_logprob": -0.25006783450091324, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1365525551809696e-06}, {"id": 59, "seek": 39462, "start": 400.46, "end": 405.82, "text": " but less powerful. Yeah because for instance you can't handle the fact that an error has", "tokens": [457, 1570, 4005, 13, 865, 570, 337, 5197, 291, 393, 380, 4813, 264, 1186, 300, 364, 6713, 575], "temperature": 0.0, "avg_logprob": -0.25006783450091324, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1365525551809696e-06}, {"id": 60, "seek": 39462, "start": 405.82, "end": 410.54, "text": " happened. You can't retry an HTTP request when you know that your server is a bit flaky", "tokens": [2011, 13, 509, 393, 380, 1533, 627, 364, 33283, 5308, 562, 291, 458, 300, 428, 7154, 307, 257, 857, 932, 15681], "temperature": 0.0, "avg_logprob": -0.25006783450091324, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1365525551809696e-06}, {"id": 61, "seek": 39462, "start": 410.54, "end": 416.7, "text": " or not always available. Stuff like that. So you can handle errors. Exactly. Now you", "tokens": [420, 406, 1009, 2435, 13, 31347, 411, 300, 13, 407, 291, 393, 4813, 13603, 13, 7587, 13, 823, 291], "temperature": 0.0, "avg_logprob": -0.25006783450091324, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1365525551809696e-06}, {"id": 62, "seek": 39462, "start": 416.7, "end": 421.86, "text": " it's completely at your disposal what you do with error handling. So we'll get into", "tokens": [309, 311, 2584, 412, 428, 26400, 437, 291, 360, 365, 6713, 13175, 13, 407, 321, 603, 483, 666], "temperature": 0.0, "avg_logprob": -0.25006783450091324, "compression_ratio": 1.6553030303030303, "no_speech_prob": 1.1365525551809696e-06}, {"id": 63, "seek": 42186, "start": 421.86, "end": 429.42, "text": " that. And one of the things that I love about back end tasks as compared to the design in", "tokens": [300, 13, 400, 472, 295, 264, 721, 300, 286, 959, 466, 646, 917, 9608, 382, 5347, 281, 264, 1715, 294], "temperature": 0.0, "avg_logprob": -0.23216809397158417, "compression_ratio": 1.7463414634146341, "no_speech_prob": 3.205809946393856e-07}, {"id": 64, "seek": 42186, "start": 429.42, "end": 436.18, "text": " V2 with data sources is with a back end task. If there is an error in that type variable", "tokens": [691, 17, 365, 1412, 7139, 307, 365, 257, 646, 917, 5633, 13, 759, 456, 307, 364, 6713, 294, 300, 2010, 7006], "temperature": 0.0, "avg_logprob": -0.23216809397158417, "compression_ratio": 1.7463414634146341, "no_speech_prob": 3.205809946393856e-07}, {"id": 65, "seek": 42186, "start": 436.18, "end": 441.7, "text": " then it has a possibility of failing. If there is no error there. So if you have you know", "tokens": [550, 309, 575, 257, 7959, 295, 18223, 13, 759, 456, 307, 572, 6713, 456, 13, 407, 498, 291, 362, 291, 458], "temperature": 0.0, "avg_logprob": -0.23216809397158417, "compression_ratio": 1.7463414634146341, "no_speech_prob": 3.205809946393856e-07}, {"id": 66, "seek": 42186, "start": 441.7, "end": 448.14, "text": " back end task never my data then you know it will never fail. And that's that's something", "tokens": [646, 917, 5633, 1128, 452, 1412, 550, 291, 458, 309, 486, 1128, 3061, 13, 400, 300, 311, 300, 311, 746], "temperature": 0.0, "avg_logprob": -0.23216809397158417, "compression_ratio": 1.7463414634146341, "no_speech_prob": 3.205809946393856e-07}, {"id": 67, "seek": 44814, "start": 448.14, "end": 453.74, "text": " that you couldn't just look at the types in a data source in V2 and know whether or not", "tokens": [300, 291, 2809, 380, 445, 574, 412, 264, 3467, 294, 257, 1412, 4009, 294, 691, 17, 293, 458, 1968, 420, 406], "temperature": 0.0, "avg_logprob": -0.2278811205988345, "compression_ratio": 1.6529680365296804, "no_speech_prob": 3.7265324408508604e-06}, {"id": 68, "seek": 44814, "start": 453.74, "end": 459.06, "text": " it's going to fail because that possible failure gets sort of tucked under the hood. It's not", "tokens": [309, 311, 516, 281, 3061, 570, 300, 1944, 7763, 2170, 1333, 295, 36089, 833, 264, 13376, 13, 467, 311, 406], "temperature": 0.0, "avg_logprob": -0.2278811205988345, "compression_ratio": 1.6529680365296804, "no_speech_prob": 3.7265324408508604e-06}, {"id": 69, "seek": 44814, "start": 459.06, "end": 464.9, "text": " represented by the types. Right. So it's much more like a task in that sense where you you", "tokens": [10379, 538, 264, 3467, 13, 1779, 13, 407, 309, 311, 709, 544, 411, 257, 5633, 294, 300, 2020, 689, 291, 291], "temperature": 0.0, "avg_logprob": -0.2278811205988345, "compression_ratio": 1.6529680365296804, "no_speech_prob": 3.7265324408508604e-06}, {"id": 70, "seek": 44814, "start": 464.9, "end": 469.62, "text": " can have a task where you know that it doesn't have the possibility of failure. So if you", "tokens": [393, 362, 257, 5633, 689, 291, 458, 300, 309, 1177, 380, 362, 264, 7959, 295, 7763, 13, 407, 498, 291], "temperature": 0.0, "avg_logprob": -0.2278811205988345, "compression_ratio": 1.6529680365296804, "no_speech_prob": 3.7265324408508604e-06}, {"id": 71, "seek": 46962, "start": 469.62, "end": 478.18, "text": " if you have like an HTTP task which is task HTTP dot error my data and then you can do", "tokens": [498, 291, 362, 411, 364, 33283, 5633, 597, 307, 5633, 33283, 5893, 6713, 452, 1412, 293, 550, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.2700285322210762, "compression_ratio": 1.6682464454976302, "no_speech_prob": 6.179367346703657e-07}, {"id": 72, "seek": 46962, "start": 478.18, "end": 485.86, "text": " task dot on error and that allows you to do you know you could just say task dot succeed", "tokens": [5633, 5893, 322, 6713, 293, 300, 4045, 291, 281, 360, 291, 458, 291, 727, 445, 584, 5633, 5893, 7754], "temperature": 0.0, "avg_logprob": -0.2700285322210762, "compression_ratio": 1.6682464454976302, "no_speech_prob": 6.179367346703657e-07}, {"id": 73, "seek": 46962, "start": 485.86, "end": 492.1, "text": " in that on error and turn it into some other fallback data for example. Yeah. So you can", "tokens": [294, 300, 322, 6713, 293, 1261, 309, 666, 512, 661, 2100, 3207, 1412, 337, 1365, 13, 865, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.2700285322210762, "compression_ratio": 1.6682464454976302, "no_speech_prob": 6.179367346703657e-07}, {"id": 74, "seek": 46962, "start": 492.1, "end": 497.02, "text": " recover from the from the failure. Exactly. And in some cases that might make sense. In", "tokens": [8114, 490, 264, 490, 264, 7763, 13, 7587, 13, 400, 294, 512, 3331, 300, 1062, 652, 2020, 13, 682], "temperature": 0.0, "avg_logprob": -0.2700285322210762, "compression_ratio": 1.6682464454976302, "no_speech_prob": 6.179367346703657e-07}, {"id": 75, "seek": 49702, "start": 497.02, "end": 501.29999999999995, "text": " some cases you might want to do a follow up task and try something else whatever it might", "tokens": [512, 3331, 291, 1062, 528, 281, 360, 257, 1524, 493, 5633, 293, 853, 746, 1646, 2035, 309, 1062], "temperature": 0.0, "avg_logprob": -0.25076443808419363, "compression_ratio": 1.6844106463878328, "no_speech_prob": 5.626371262223984e-07}, {"id": 76, "seek": 49702, "start": 501.29999999999995, "end": 507.94, "text": " be but it's completely at your disposal. But if you if you were to do task dot succeed", "tokens": [312, 457, 309, 311, 2584, 412, 428, 26400, 13, 583, 498, 291, 498, 291, 645, 281, 360, 5633, 5893, 7754], "temperature": 0.0, "avg_logprob": -0.25076443808419363, "compression_ratio": 1.6844106463878328, "no_speech_prob": 5.626371262223984e-07}, {"id": 77, "seek": 49702, "start": 507.94, "end": 514.9399999999999, "text": " in the on error then you would get you would be able to recover from that failure and then", "tokens": [294, 264, 322, 6713, 550, 291, 576, 483, 291, 576, 312, 1075, 281, 8114, 490, 300, 7763, 293, 550], "temperature": 0.0, "avg_logprob": -0.25076443808419363, "compression_ratio": 1.6844106463878328, "no_speech_prob": 5.626371262223984e-07}, {"id": 78, "seek": 49702, "start": 514.9399999999999, "end": 519.9399999999999, "text": " the types would would reflect that it's not possible for it to fail anymore which is kind", "tokens": [264, 3467, 576, 576, 5031, 300, 309, 311, 406, 1944, 337, 309, 281, 3061, 3602, 597, 307, 733], "temperature": 0.0, "avg_logprob": -0.25076443808419363, "compression_ratio": 1.6844106463878328, "no_speech_prob": 5.626371262223984e-07}, {"id": 79, "seek": 49702, "start": 519.9399999999999, "end": 526.18, "text": " of cool. So you can do the same thing with Elm pages V3 back end tasks. It's it's the", "tokens": [295, 1627, 13, 407, 291, 393, 360, 264, 912, 551, 365, 2699, 76, 7183, 691, 18, 646, 917, 9608, 13, 467, 311, 309, 311, 264], "temperature": 0.0, "avg_logprob": -0.25076443808419363, "compression_ratio": 1.6844106463878328, "no_speech_prob": 5.626371262223984e-07}, {"id": 80, "seek": 52618, "start": 526.18, "end": 534.3399999999999, "text": " same mental model. And if the error type variable is unbound if if you don't have any errors", "tokens": [912, 4973, 2316, 13, 400, 498, 264, 6713, 2010, 7006, 307, 517, 18767, 498, 498, 291, 500, 380, 362, 604, 13603], "temperature": 0.0, "avg_logprob": -0.22031972826141671, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.0845121778402245e-06}, {"id": 81, "seek": 52618, "start": 534.3399999999999, "end": 537.9399999999999, "text": " there then you know it's not possible for it to have an error. So that's back in tasks", "tokens": [456, 550, 291, 458, 309, 311, 406, 1944, 337, 309, 281, 362, 364, 6713, 13, 407, 300, 311, 646, 294, 9608], "temperature": 0.0, "avg_logprob": -0.22031972826141671, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.0845121778402245e-06}, {"id": 82, "seek": 52618, "start": 537.9399999999999, "end": 545.3, "text": " back in tasks are the heart of Elm pages even more so in the in the V3 release that is coming", "tokens": [646, 294, 9608, 366, 264, 1917, 295, 2699, 76, 7183, 754, 544, 370, 294, 264, 294, 264, 691, 18, 4374, 300, 307, 1348], "temperature": 0.0, "avg_logprob": -0.22031972826141671, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.0845121778402245e-06}, {"id": 83, "seek": 52618, "start": 545.3, "end": 553.0999999999999, "text": " up soon. And and they are also the heart of Elm pages scripts. So let's talk about what", "tokens": [493, 2321, 13, 400, 293, 436, 366, 611, 264, 1917, 295, 2699, 76, 7183, 23294, 13, 407, 718, 311, 751, 466, 437], "temperature": 0.0, "avg_logprob": -0.22031972826141671, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.0845121778402245e-06}, {"id": 84, "seek": 55310, "start": 553.1, "end": 560.38, "text": " Elm pages scripts are. I almost forgot about that part. Yeah. If only we had a script that", "tokens": [2699, 76, 7183, 23294, 366, 13, 286, 1920, 5298, 466, 300, 644, 13, 865, 13, 759, 787, 321, 632, 257, 5755, 300], "temperature": 0.0, "avg_logprob": -0.25324330610387463, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.8744828846975e-07}, {"id": 85, "seek": 55310, "start": 560.38, "end": 568.02, "text": " we should follow you know exactly we got we got a little bit off script there. So Elm", "tokens": [321, 820, 1524, 291, 458, 2293, 321, 658, 321, 658, 257, 707, 857, 766, 5755, 456, 13, 407, 2699, 76], "temperature": 0.0, "avg_logprob": -0.25324330610387463, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.8744828846975e-07}, {"id": 86, "seek": 55310, "start": 568.02, "end": 576.26, "text": " pages scripts are really just a way of defining a back end task and executing it from the", "tokens": [7183, 23294, 366, 534, 445, 257, 636, 295, 17827, 257, 646, 917, 5633, 293, 32368, 309, 490, 264], "temperature": 0.0, "avg_logprob": -0.25324330610387463, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.8744828846975e-07}, {"id": 87, "seek": 57626, "start": 576.26, "end": 585.14, "text": " command line. So if you wanted to make an HTTP request and log something to the console.", "tokens": [5622, 1622, 13, 407, 498, 291, 1415, 281, 652, 364, 33283, 5308, 293, 3565, 746, 281, 264, 11076, 13], "temperature": 0.0, "avg_logprob": -0.22323617201585036, "compression_ratio": 1.4944444444444445, "no_speech_prob": 2.8291029252613953e-07}, {"id": 88, "seek": 57626, "start": 585.14, "end": 592.7, "text": " So what you would do is so the minimal setup for Elm pages scripts it actually doesn't", "tokens": [407, 437, 291, 576, 360, 307, 370, 264, 13206, 8657, 337, 2699, 76, 7183, 23294, 309, 767, 1177, 380], "temperature": 0.0, "avg_logprob": -0.22323617201585036, "compression_ratio": 1.4944444444444445, "no_speech_prob": 2.8291029252613953e-07}, {"id": 89, "seek": 57626, "start": 592.7, "end": 598.86, "text": " require any of the Elm pages application folder or anything. You don't need any route modules", "tokens": [3651, 604, 295, 264, 2699, 76, 7183, 3861, 10820, 420, 1340, 13, 509, 500, 380, 643, 604, 7955, 16679], "temperature": 0.0, "avg_logprob": -0.22323617201585036, "compression_ratio": 1.4944444444444445, "no_speech_prob": 2.8291029252613953e-07}, {"id": 90, "seek": 59886, "start": 598.86, "end": 606.22, "text": " defined. You don't need any of the config for Elm pages. All you need is a folder called", "tokens": [7642, 13, 509, 500, 380, 643, 604, 295, 264, 6662, 337, 2699, 76, 7183, 13, 1057, 291, 643, 307, 257, 10820, 1219], "temperature": 0.0, "avg_logprob": -0.2290585156783317, "compression_ratio": 1.8638743455497382, "no_speech_prob": 8.579059453950322e-07}, {"id": 91, "seek": 59886, "start": 606.22, "end": 613.14, "text": " script. So much like for an Elm review project you have a folder called review and it's a", "tokens": [5755, 13, 407, 709, 411, 337, 364, 2699, 76, 3131, 1716, 291, 362, 257, 10820, 1219, 3131, 293, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.2290585156783317, "compression_ratio": 1.8638743455497382, "no_speech_prob": 8.579059453950322e-07}, {"id": 92, "seek": 59886, "start": 613.14, "end": 618.62, "text": " regular Elm project. It has an Elm.JSON Elm pages script is the same thing. So you have", "tokens": [3890, 2699, 76, 1716, 13, 467, 575, 364, 2699, 76, 13, 41, 10388, 2699, 76, 7183, 5755, 307, 264, 912, 551, 13, 407, 291, 362], "temperature": 0.0, "avg_logprob": -0.2290585156783317, "compression_ratio": 1.8638743455497382, "no_speech_prob": 8.579059453950322e-07}, {"id": 93, "seek": 59886, "start": 618.62, "end": 623.86, "text": " a script folder that script folder has to have an Elm.JSON. So it's a little Elm project.", "tokens": [257, 5755, 10820, 300, 5755, 10820, 575, 281, 362, 364, 2699, 76, 13, 41, 10388, 13, 407, 309, 311, 257, 707, 2699, 76, 1716, 13], "temperature": 0.0, "avg_logprob": -0.2290585156783317, "compression_ratio": 1.8638743455497382, "no_speech_prob": 8.579059453950322e-07}, {"id": 94, "seek": 62386, "start": 623.86, "end": 629.62, "text": " It has to have Elm pages as a dependency in that Elm.JSON. And and then what you do is", "tokens": [467, 575, 281, 362, 2699, 76, 7183, 382, 257, 33621, 294, 300, 2699, 76, 13, 41, 10388, 13, 400, 293, 550, 437, 291, 360, 307], "temperature": 0.0, "avg_logprob": -0.23997593929893093, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.1544572089405847e-06}, {"id": 95, "seek": 62386, "start": 629.62, "end": 637.34, "text": " you do Elm pages run hello. And now if you have something in your source directories", "tokens": [291, 360, 2699, 76, 7183, 1190, 7751, 13, 400, 586, 498, 291, 362, 746, 294, 428, 4009, 5391, 530], "temperature": 0.0, "avg_logprob": -0.23997593929893093, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.1544572089405847e-06}, {"id": 96, "seek": 62386, "start": 637.34, "end": 644.6, "text": " like source slash hello dot Elm it's going to go and execute that module. So right. Yeah.", "tokens": [411, 4009, 17330, 7751, 5893, 2699, 76, 309, 311, 516, 281, 352, 293, 14483, 300, 10088, 13, 407, 558, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.23997593929893093, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.1544572089405847e-06}, {"id": 97, "seek": 62386, "start": 644.6, "end": 650.26, "text": " What does it mean to execute an Elm module becomes the question. Yeah. So you mentioned", "tokens": [708, 775, 309, 914, 281, 14483, 364, 2699, 76, 10088, 3643, 264, 1168, 13, 865, 13, 407, 291, 2835], "temperature": 0.0, "avg_logprob": -0.23997593929893093, "compression_ratio": 1.6232558139534883, "no_speech_prob": 1.1544572089405847e-06}, {"id": 98, "seek": 65026, "start": 650.26, "end": 658.46, "text": " it's basically running a back end task. Exactly. So back end task is used to fetch data from", "tokens": [309, 311, 1936, 2614, 257, 646, 917, 5633, 13, 7587, 13, 407, 646, 917, 5633, 307, 1143, 281, 23673, 1412, 490], "temperature": 0.0, "avg_logprob": -0.28741178707200654, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.179358820190828e-07}, {"id": 99, "seek": 65026, "start": 658.46, "end": 666.9, "text": " HTTP or from files so you can grab the data. But then that's it. Right. You can only grab", "tokens": [33283, 420, 490, 7098, 370, 291, 393, 4444, 264, 1412, 13, 583, 550, 300, 311, 309, 13, 1779, 13, 509, 393, 787, 4444], "temperature": 0.0, "avg_logprob": -0.28741178707200654, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.179358820190828e-07}, {"id": 100, "seek": 65026, "start": 666.9, "end": 672.9, "text": " data. You can only fetch things. What else can you do with it. Yeah. Exactly. And that", "tokens": [1412, 13, 509, 393, 787, 23673, 721, 13, 708, 1646, 393, 291, 360, 365, 309, 13, 865, 13, 7587, 13, 400, 300], "temperature": 0.0, "avg_logprob": -0.28741178707200654, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.179358820190828e-07}, {"id": 101, "seek": 65026, "start": 672.9, "end": 678.86, "text": " would be much more the mental model in Elm pages V2. But now there's a little bit more", "tokens": [576, 312, 709, 544, 264, 4973, 2316, 294, 2699, 76, 7183, 691, 17, 13, 583, 586, 456, 311, 257, 707, 857, 544], "temperature": 0.0, "avg_logprob": -0.28741178707200654, "compression_ratio": 1.6108597285067874, "no_speech_prob": 6.179358820190828e-07}, {"id": 102, "seek": 67886, "start": 678.86, "end": 686.1800000000001, "text": " of a notion of being able to do effectful things using back end tasks in V3 because", "tokens": [295, 257, 10710, 295, 885, 1075, 281, 360, 1802, 906, 721, 1228, 646, 917, 9608, 294, 691, 18, 570], "temperature": 0.0, "avg_logprob": -0.22423071231482164, "compression_ratio": 1.6770428015564203, "no_speech_prob": 1.1726326647476526e-06}, {"id": 103, "seek": 67886, "start": 686.1800000000001, "end": 691.62, "text": " if you're doing a server rendered route and responding to a form submission which you", "tokens": [498, 291, 434, 884, 257, 7154, 28748, 7955, 293, 16670, 281, 257, 1254, 23689, 597, 291], "temperature": 0.0, "avg_logprob": -0.22423071231482164, "compression_ratio": 1.6770428015564203, "no_speech_prob": 1.1726326647476526e-06}, {"id": 104, "seek": 67886, "start": 691.62, "end": 695.76, "text": " can do in V3 which we'll talk about in a future episode you might want to do something that", "tokens": [393, 360, 294, 691, 18, 597, 321, 603, 751, 466, 294, 257, 2027, 3500, 291, 1062, 528, 281, 360, 746, 300], "temperature": 0.0, "avg_logprob": -0.22423071231482164, "compression_ratio": 1.6770428015564203, "no_speech_prob": 1.1726326647476526e-06}, {"id": 105, "seek": 67886, "start": 695.76, "end": 699.82, "text": " has an effect on the world. It's not just about shoveling in data to generate static", "tokens": [575, 364, 1802, 322, 264, 1002, 13, 467, 311, 406, 445, 466, 29789, 278, 294, 1412, 281, 8460, 13437], "temperature": 0.0, "avg_logprob": -0.22423071231482164, "compression_ratio": 1.6770428015564203, "no_speech_prob": 1.1726326647476526e-06}, {"id": 106, "seek": 67886, "start": 699.82, "end": 707.1800000000001, "text": " pages. So similarly a back end task is not only about grabbing data. It does let you", "tokens": [7183, 13, 407, 14138, 257, 646, 917, 5633, 307, 406, 787, 466, 23771, 1412, 13, 467, 775, 718, 291], "temperature": 0.0, "avg_logprob": -0.22423071231482164, "compression_ratio": 1.6770428015564203, "no_speech_prob": 1.1726326647476526e-06}, {"id": 107, "seek": 70718, "start": 707.18, "end": 714.4599999999999, "text": " pull in data and map that data back and test that map back and test that. And then. But", "tokens": [2235, 294, 1412, 293, 4471, 300, 1412, 646, 293, 1500, 300, 4471, 646, 293, 1500, 300, 13, 400, 550, 13, 583], "temperature": 0.0, "avg_logprob": -0.35054645538330076, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.505687919234333e-06}, {"id": 108, "seek": 70718, "start": 714.4599999999999, "end": 721.02, "text": " you can also perform effects. So for example there is a back end task in the Elm pages", "tokens": [291, 393, 611, 2042, 5065, 13, 407, 337, 1365, 456, 307, 257, 646, 917, 5633, 294, 264, 2699, 76, 7183], "temperature": 0.0, "avg_logprob": -0.35054645538330076, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.505687919234333e-06}, {"id": 109, "seek": 70718, "start": 721.02, "end": 731.3, "text": " API called script log script log takes a string and it gives you a back end task with no data.", "tokens": [9362, 1219, 5755, 3565, 5755, 3565, 2516, 257, 6798, 293, 309, 2709, 291, 257, 646, 917, 5633, 365, 572, 1412, 13], "temperature": 0.0, "avg_logprob": -0.35054645538330076, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.505687919234333e-06}, {"id": 110, "seek": 73130, "start": 731.3, "end": 737.2199999999999, "text": " All right. Yeah. So that looks like back end task lowercase error because it's not possible", "tokens": [1057, 558, 13, 865, 13, 407, 300, 1542, 411, 646, 917, 5633, 3126, 9765, 6713, 570, 309, 311, 406, 1944], "temperature": 0.0, "avg_logprob": -0.2708467668102634, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.0451412890688516e-05}, {"id": 111, "seek": 73130, "start": 737.2199999999999, "end": 744.9, "text": " for it to error and unit because it doesn't have any data. So the hello world for Elm", "tokens": [337, 309, 281, 6713, 293, 4985, 570, 309, 1177, 380, 362, 604, 1412, 13, 407, 264, 7751, 1002, 337, 2699, 76], "temperature": 0.0, "avg_logprob": -0.2708467668102634, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.0451412890688516e-05}, {"id": 112, "seek": 73130, "start": 744.9, "end": 752.02, "text": " pages script is you have your script folder you have an Elm.JSON you have source slash", "tokens": [7183, 5755, 307, 291, 362, 428, 5755, 10820, 291, 362, 364, 2699, 76, 13, 41, 10388, 291, 362, 4009, 17330], "temperature": 0.0, "avg_logprob": -0.2708467668102634, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.0451412890688516e-05}, {"id": 113, "seek": 73130, "start": 752.02, "end": 759.9799999999999, "text": " hello dot Elm and in your hello dot Elm you expose a function called run. Okay. That is", "tokens": [7751, 5893, 2699, 76, 293, 294, 428, 7751, 5893, 2699, 76, 291, 19219, 257, 2445, 1219, 1190, 13, 1033, 13, 663, 307], "temperature": 0.0, "avg_logprob": -0.2708467668102634, "compression_ratio": 1.644859813084112, "no_speech_prob": 1.0451412890688516e-05}, {"id": 114, "seek": 75998, "start": 759.98, "end": 765.74, "text": " the main function in a way. Exactly. Run is like the main function for an Elm pages script.", "tokens": [264, 2135, 2445, 294, 257, 636, 13, 7587, 13, 8950, 307, 411, 264, 2135, 2445, 337, 364, 2699, 76, 7183, 5755, 13], "temperature": 0.0, "avg_logprob": -0.2433654951012653, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.1189358701813035e-06}, {"id": 115, "seek": 75998, "start": 765.74, "end": 776.02, "text": " You run has the type script and and then you do script dot without CLI options scripts", "tokens": [509, 1190, 575, 264, 2010, 5755, 293, 293, 550, 291, 360, 5755, 5893, 1553, 12855, 40, 3956, 23294], "temperature": 0.0, "avg_logprob": -0.2433654951012653, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.1189358701813035e-06}, {"id": 116, "seek": 75998, "start": 776.02, "end": 782.9, "text": " dot log hello. That's hello world. So that's it. Then you run Elm pages run hello and it", "tokens": [5893, 3565, 7751, 13, 663, 311, 7751, 1002, 13, 407, 300, 311, 309, 13, 1396, 291, 1190, 2699, 76, 7183, 1190, 7751, 293, 309], "temperature": 0.0, "avg_logprob": -0.2433654951012653, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.1189358701813035e-06}, {"id": 117, "seek": 75998, "start": 782.9, "end": 789.1800000000001, "text": " prints hello world. I hope that was clear to listeners. It's always hard to explain", "tokens": [22305, 7751, 1002, 13, 286, 1454, 300, 390, 1850, 281, 23274, 13, 467, 311, 1009, 1152, 281, 2903], "temperature": 0.0, "avg_logprob": -0.2433654951012653, "compression_ratio": 1.6956521739130435, "no_speech_prob": 1.1189358701813035e-06}, {"id": 118, "seek": 78918, "start": 789.18, "end": 794.5799999999999, "text": " code all of this was not that much code. Yeah. And that's sort of the thing I hope people", "tokens": [3089, 439, 295, 341, 390, 406, 300, 709, 3089, 13, 865, 13, 400, 300, 311, 1333, 295, 264, 551, 286, 1454, 561], "temperature": 0.0, "avg_logprob": -0.19920529140515275, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.5559510251914617e-06}, {"id": 119, "seek": 78918, "start": 794.5799999999999, "end": 802.5799999999999, "text": " take away from this is like to write a hello world script and run it is a very small number", "tokens": [747, 1314, 490, 341, 307, 411, 281, 2464, 257, 7751, 1002, 5755, 293, 1190, 309, 307, 257, 588, 1359, 1230], "temperature": 0.0, "avg_logprob": -0.19920529140515275, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.5559510251914617e-06}, {"id": 120, "seek": 78918, "start": 802.5799999999999, "end": 808.62, "text": " of files that you need to create code you need to write and commands you need to run.", "tokens": [295, 7098, 300, 291, 643, 281, 1884, 3089, 291, 643, 281, 2464, 293, 16901, 291, 643, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.19920529140515275, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.5559510251914617e-06}, {"id": 121, "seek": 78918, "start": 808.62, "end": 816.3, "text": " Notice for example that we didn't say Elm pages compile script this and then make an", "tokens": [13428, 337, 1365, 300, 321, 994, 380, 584, 2699, 76, 7183, 31413, 5755, 341, 293, 550, 652, 364], "temperature": 0.0, "avg_logprob": -0.19920529140515275, "compression_ratio": 1.6296296296296295, "no_speech_prob": 3.5559510251914617e-06}, {"id": 122, "seek": 81630, "start": 816.3, "end": 822.42, "text": " index dot JS file and node and run a node script and all of that. And if we had a compiler", "tokens": [8186, 5893, 33063, 3991, 293, 9984, 293, 1190, 257, 9984, 5755, 293, 439, 295, 300, 13, 400, 498, 321, 632, 257, 31958], "temperature": 0.0, "avg_logprob": -0.20726812931529262, "compression_ratio": 1.7301587301587302, "no_speech_prob": 9.27618373225414e-07}, {"id": 123, "seek": 81630, "start": 822.42, "end": 828.74, "text": " error or anything like that in hello dot Elm then running Elm pages run hello would tell", "tokens": [6713, 420, 1340, 411, 300, 294, 7751, 5893, 2699, 76, 550, 2614, 2699, 76, 7183, 1190, 7751, 576, 980], "temperature": 0.0, "avg_logprob": -0.20726812931529262, "compression_ratio": 1.7301587301587302, "no_speech_prob": 9.27618373225414e-07}, {"id": 124, "seek": 81630, "start": 828.74, "end": 834.38, "text": " us about that. So it's it's a very minimal amount of code we will link to an example", "tokens": [505, 466, 300, 13, 407, 309, 311, 309, 311, 257, 588, 13206, 2372, 295, 3089, 321, 486, 2113, 281, 364, 1365], "temperature": 0.0, "avg_logprob": -0.20726812931529262, "compression_ratio": 1.7301587301587302, "no_speech_prob": 9.27618373225414e-07}, {"id": 125, "seek": 81630, "start": 834.38, "end": 839.6999999999999, "text": " of a hello dot Elm will link to a starter repo that gives you like a minimal boiler", "tokens": [295, 257, 7751, 5893, 2699, 76, 486, 2113, 281, 257, 22465, 49040, 300, 2709, 291, 411, 257, 13206, 39228], "temperature": 0.0, "avg_logprob": -0.20726812931529262, "compression_ratio": 1.7301587301587302, "no_speech_prob": 9.27618373225414e-07}, {"id": 126, "seek": 81630, "start": 839.6999999999999, "end": 845.5799999999999, "text": " plate so you can clone it and play around with it. But it's designed to be a very small", "tokens": [5924, 370, 291, 393, 26506, 309, 293, 862, 926, 365, 309, 13, 583, 309, 311, 4761, 281, 312, 257, 588, 1359], "temperature": 0.0, "avg_logprob": -0.20726812931529262, "compression_ratio": 1.7301587301587302, "no_speech_prob": 9.27618373225414e-07}, {"id": 127, "seek": 84558, "start": 845.58, "end": 852.4200000000001, "text": " amount of code. And also the the abstraction of a back end task is designed to be minimal", "tokens": [2372, 295, 3089, 13, 400, 611, 264, 264, 37765, 295, 257, 646, 917, 5633, 307, 4761, 281, 312, 13206], "temperature": 0.0, "avg_logprob": -0.28150828854068294, "compression_ratio": 1.6682242990654206, "no_speech_prob": 3.96695804738556e-06}, {"id": 128, "seek": 84558, "start": 852.4200000000001, "end": 858.22, "text": " in a way too because there's no in it. There's no update. There's no subscriptions like you", "tokens": [294, 257, 636, 886, 570, 456, 311, 572, 294, 309, 13, 821, 311, 572, 5623, 13, 821, 311, 572, 44951, 411, 291], "temperature": 0.0, "avg_logprob": -0.28150828854068294, "compression_ratio": 1.6682242990654206, "no_speech_prob": 3.96695804738556e-06}, {"id": 129, "seek": 84558, "start": 858.22, "end": 865.58, "text": " just log. You don't return a model and then a command with a logging thing. And then you", "tokens": [445, 3565, 13, 509, 500, 380, 2736, 257, 2316, 293, 550, 257, 5622, 365, 257, 27991, 551, 13, 400, 550, 291], "temperature": 0.0, "avg_logprob": -0.28150828854068294, "compression_ratio": 1.6682242990654206, "no_speech_prob": 3.96695804738556e-06}, {"id": 130, "seek": 84558, "start": 865.58, "end": 871.9000000000001, "text": " like if you wanted to make an HTTP request you just do you know back end task dot HTTP", "tokens": [411, 498, 291, 1415, 281, 652, 364, 33283, 5308, 291, 445, 360, 291, 458, 646, 917, 5633, 5893, 33283], "temperature": 0.0, "avg_logprob": -0.28150828854068294, "compression_ratio": 1.6682242990654206, "no_speech_prob": 3.96695804738556e-06}, {"id": 131, "seek": 87190, "start": 871.9, "end": 879.86, "text": " dot get JSON give it a URL give it a JSON decoder and then back end task dot and then", "tokens": [5893, 483, 31828, 976, 309, 257, 12905, 976, 309, 257, 31828, 979, 19866, 293, 550, 646, 917, 5633, 5893, 293, 550], "temperature": 0.0, "avg_logprob": -0.25119010077582465, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.406086878181668e-06}, {"id": 132, "seek": 87190, "start": 879.86, "end": 886.22, "text": " and then you can log some data that you decoded. So right. It's designed to be more like the", "tokens": [293, 550, 291, 393, 3565, 512, 1412, 300, 291, 979, 12340, 13, 407, 558, 13, 467, 311, 4761, 281, 312, 544, 411, 264], "temperature": 0.0, "avg_logprob": -0.25119010077582465, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.406086878181668e-06}, {"id": 133, "seek": 87190, "start": 886.22, "end": 891.5, "text": " abstraction of a back end task lets you do things in a more lightweight way especially", "tokens": [37765, 295, 257, 646, 917, 5633, 6653, 291, 360, 721, 294, 257, 544, 22052, 636, 2318], "temperature": 0.0, "avg_logprob": -0.25119010077582465, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.406086878181668e-06}, {"id": 134, "seek": 87190, "start": 891.5, "end": 896.34, "text": " for this sort of mental model where it's just like just execute this thing. So it back end", "tokens": [337, 341, 1333, 295, 4973, 2316, 689, 309, 311, 445, 411, 445, 14483, 341, 551, 13, 407, 309, 646, 917], "temperature": 0.0, "avg_logprob": -0.25119010077582465, "compression_ratio": 1.679245283018868, "no_speech_prob": 2.406086878181668e-06}, {"id": 135, "seek": 89634, "start": 896.34, "end": 903.94, "text": " task maps very nicely to the idea of a script where it's just execute this thing or fail.", "tokens": [5633, 11317, 588, 9594, 281, 264, 1558, 295, 257, 5755, 689, 309, 311, 445, 14483, 341, 551, 420, 3061, 13], "temperature": 0.0, "avg_logprob": -0.21548492677750125, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.9033617491004406e-06}, {"id": 136, "seek": 89634, "start": 903.94, "end": 909.98, "text": " Do this do this do that. And we're done. Right. Exactly. It's not updating the model. It's", "tokens": [1144, 341, 360, 341, 360, 300, 13, 400, 321, 434, 1096, 13, 1779, 13, 7587, 13, 467, 311, 406, 25113, 264, 2316, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.21548492677750125, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.9033617491004406e-06}, {"id": 137, "seek": 89634, "start": 909.98, "end": 916.38, "text": " not responding to user input. So it's just a sequence of tasks that it's performing.", "tokens": [406, 16670, 281, 4195, 4846, 13, 407, 309, 311, 445, 257, 8310, 295, 9608, 300, 309, 311, 10205, 13], "temperature": 0.0, "avg_logprob": -0.21548492677750125, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.9033617491004406e-06}, {"id": 138, "seek": 89634, "start": 916.38, "end": 923.0600000000001, "text": " Whereas if you had to listen to user input if you had to listen to the user typing into", "tokens": [13813, 498, 291, 632, 281, 2140, 281, 4195, 4846, 498, 291, 632, 281, 2140, 281, 264, 4195, 18444, 666], "temperature": 0.0, "avg_logprob": -0.21548492677750125, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.9033617491004406e-06}, {"id": 139, "seek": 92306, "start": 923.06, "end": 927.9399999999999, "text": " an input field or an on click that would sort of break that mental model of just saying", "tokens": [364, 4846, 2519, 420, 364, 322, 2052, 300, 576, 1333, 295, 1821, 300, 4973, 2316, 295, 445, 1566], "temperature": 0.0, "avg_logprob": -0.234336332841353, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.240682978386758e-06}, {"id": 140, "seek": 92306, "start": 927.9399999999999, "end": 933.9, "text": " there's one back end task it just runs and it finishes or it fails. Right. So is there", "tokens": [456, 311, 472, 646, 917, 5633, 309, 445, 6676, 293, 309, 23615, 420, 309, 18199, 13, 1779, 13, 407, 307, 456], "temperature": 0.0, "avg_logprob": -0.234336332841353, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.240682978386758e-06}, {"id": 141, "seek": 92306, "start": 933.9, "end": 939.6999999999999, "text": " no way to listen to user prompts or could that be added. That is a that is a very good", "tokens": [572, 636, 281, 2140, 281, 4195, 41095, 420, 727, 300, 312, 3869, 13, 663, 307, 257, 300, 307, 257, 588, 665], "temperature": 0.0, "avg_logprob": -0.234336332841353, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.240682978386758e-06}, {"id": 142, "seek": 92306, "start": 939.6999999999999, "end": 945.9, "text": " question. I have thought about that. Wolfgang Schuster has been doing some experiments with", "tokens": [1168, 13, 286, 362, 1194, 466, 300, 13, 16634, 19619, 2065, 8393, 575, 668, 884, 512, 12050, 365], "temperature": 0.0, "avg_logprob": -0.234336332841353, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.240682978386758e-06}, {"id": 143, "seek": 94590, "start": 945.9, "end": 953.34, "text": " some with a sort of Elm Inc project where you can do things like create interactive", "tokens": [512, 365, 257, 1333, 295, 2699, 76, 7779, 1716, 689, 291, 393, 360, 721, 411, 1884, 15141], "temperature": 0.0, "avg_logprob": -0.24959095613456067, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.679695174061635e-06}, {"id": 144, "seek": 94590, "start": 953.34, "end": 958.9, "text": " terminal applications in Elm. And I definitely think it could be interesting potentially", "tokens": [14709, 5821, 294, 2699, 76, 13, 400, 286, 2138, 519, 309, 727, 312, 1880, 7263], "temperature": 0.0, "avg_logprob": -0.24959095613456067, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.679695174061635e-06}, {"id": 145, "seek": 94590, "start": 958.9, "end": 963.78, "text": " to have a way to have an init update things like that if you wanted to do something like", "tokens": [281, 362, 257, 636, 281, 362, 364, 3157, 5623, 721, 411, 300, 498, 291, 1415, 281, 360, 746, 411], "temperature": 0.0, "avg_logprob": -0.24959095613456067, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.679695174061635e-06}, {"id": 146, "seek": 94590, "start": 963.78, "end": 970.78, "text": " that. But it would be possible but it would add complexity. Yeah. But I mean even without", "tokens": [300, 13, 583, 309, 576, 312, 1944, 457, 309, 576, 909, 14024, 13, 865, 13, 583, 286, 914, 754, 1553], "temperature": 0.0, "avg_logprob": -0.24959095613456067, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.679695174061635e-06}, {"id": 147, "seek": 97078, "start": 970.78, "end": 975.98, "text": " having to have an init update and all those things you could probably have a define a", "tokens": [1419, 281, 362, 364, 3157, 5623, 293, 439, 729, 721, 291, 727, 1391, 362, 257, 6964, 257], "temperature": 0.0, "avg_logprob": -0.2950708110158036, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.7718539210327435e-06}, {"id": 148, "seek": 97078, "start": 975.98, "end": 983.26, "text": " back end tasks which succeeds with the user prompts without having this the Elm architecture", "tokens": [646, 917, 9608, 597, 49263, 365, 264, 4195, 41095, 1553, 1419, 341, 264, 2699, 76, 9482], "temperature": 0.0, "avg_logprob": -0.2950708110158036, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.7718539210327435e-06}, {"id": 149, "seek": 97078, "start": 983.26, "end": 989.74, "text": " lifecycle. Exactly. That's the that's the thing is you can and you actually can. I actually", "tokens": [45722, 13, 7587, 13, 663, 311, 264, 300, 311, 264, 551, 307, 291, 393, 293, 291, 767, 393, 13, 286, 767], "temperature": 0.0, "avg_logprob": -0.2950708110158036, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.7718539210327435e-06}, {"id": 150, "seek": 97078, "start": 989.74, "end": 996.02, "text": " haven't tried that specific use case but you can run arbitrary Node.js code using back", "tokens": [2378, 380, 3031, 300, 2685, 764, 1389, 457, 291, 393, 1190, 23211, 38640, 13, 25530, 3089, 1228, 646], "temperature": 0.0, "avg_logprob": -0.2950708110158036, "compression_ratio": 1.63013698630137, "no_speech_prob": 5.7718539210327435e-06}, {"id": 151, "seek": 99602, "start": 996.02, "end": 1002.18, "text": " and task dot custom. We'll link to the back end task dot custom module docs which explains", "tokens": [293, 5633, 5893, 2375, 13, 492, 603, 2113, 281, 264, 646, 917, 5633, 5893, 2375, 10088, 45623, 597, 13948], "temperature": 0.0, "avg_logprob": -0.29721529730435076, "compression_ratio": 1.8153846153846154, "no_speech_prob": 6.048780051060021e-06}, {"id": 152, "seek": 99602, "start": 1002.18, "end": 1010.14, "text": " how to set that up. But essentially you just give it a JSON encoder and a JSON decoder", "tokens": [577, 281, 992, 300, 493, 13, 583, 4476, 291, 445, 976, 309, 257, 31828, 2058, 19866, 293, 257, 31828, 979, 19866], "temperature": 0.0, "avg_logprob": -0.29721529730435076, "compression_ratio": 1.8153846153846154, "no_speech_prob": 6.048780051060021e-06}, {"id": 153, "seek": 99602, "start": 1010.14, "end": 1015.86, "text": " and you can define a custom back end task and that custom back end task could be a wait", "tokens": [293, 291, 393, 6964, 257, 2375, 646, 917, 5633, 293, 300, 2375, 646, 917, 5633, 727, 312, 257, 1699], "temperature": 0.0, "avg_logprob": -0.29721529730435076, "compression_ratio": 1.8153846153846154, "no_speech_prob": 6.048780051060021e-06}, {"id": 154, "seek": 99602, "start": 1015.86, "end": 1022.9399999999999, "text": " user input and you could give it a prompt and you know you could block until you receive", "tokens": [4195, 4846, 293, 291, 727, 976, 309, 257, 12391, 293, 291, 458, 291, 727, 3461, 1826, 291, 4774], "temperature": 0.0, "avg_logprob": -0.29721529730435076, "compression_ratio": 1.8153846153846154, "no_speech_prob": 6.048780051060021e-06}, {"id": 155, "seek": 102294, "start": 1022.94, "end": 1028.8600000000001, "text": " the user input and then return that data. And that would totally work. So. So yeah you", "tokens": [264, 4195, 4846, 293, 550, 2736, 300, 1412, 13, 400, 300, 576, 3879, 589, 13, 407, 13, 407, 1338, 291], "temperature": 0.0, "avg_logprob": -0.26921696133083767, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.5056909887789516e-06}, {"id": 156, "seek": 102294, "start": 1028.8600000000001, "end": 1035.3, "text": " can do that and it supports that simpler mental model of just running a script until you're", "tokens": [393, 360, 300, 293, 309, 9346, 300, 18587, 4973, 2316, 295, 445, 2614, 257, 5755, 1826, 291, 434], "temperature": 0.0, "avg_logprob": -0.26921696133083767, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.5056909887789516e-06}, {"id": 157, "seek": 102294, "start": 1035.3, "end": 1042.38, "text": " done. The thing that's that I find really fun about back end tasks is that like it is", "tokens": [1096, 13, 440, 551, 300, 311, 300, 286, 915, 534, 1019, 466, 646, 917, 9608, 307, 300, 411, 309, 307], "temperature": 0.0, "avg_logprob": -0.26921696133083767, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.5056909887789516e-06}, {"id": 158, "seek": 102294, "start": 1042.38, "end": 1049.38, "text": " this it's a type it's data. It's a description of an effect or of something to achieve and", "tokens": [341, 309, 311, 257, 2010, 309, 311, 1412, 13, 467, 311, 257, 3855, 295, 364, 1802, 420, 295, 746, 281, 4584, 293], "temperature": 0.0, "avg_logprob": -0.26921696133083767, "compression_ratio": 1.6435185185185186, "no_speech_prob": 1.5056909887789516e-06}, {"id": 159, "seek": 104938, "start": 1049.38, "end": 1055.9, "text": " exactly get out of it. Exactly. And so because it's this sort of declarative description", "tokens": [2293, 483, 484, 295, 309, 13, 7587, 13, 400, 370, 570, 309, 311, 341, 1333, 295, 16694, 1166, 3855], "temperature": 0.0, "avg_logprob": -0.25704273310574616, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.565865765471244e-06}, {"id": 160, "seek": 104938, "start": 1055.9, "end": 1062.5800000000002, "text": " of an effect and how to respond to subsequent effects you can use it in a lot of different", "tokens": [295, 364, 1802, 293, 577, 281, 4196, 281, 19962, 5065, 291, 393, 764, 309, 294, 257, 688, 295, 819], "temperature": 0.0, "avg_logprob": -0.25704273310574616, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.565865765471244e-06}, {"id": 161, "seek": 104938, "start": 1062.5800000000002, "end": 1069.46, "text": " places. So like Elm pages scripts is one place you could use it. You know if you wanted to", "tokens": [3190, 13, 407, 411, 2699, 76, 7183, 23294, 307, 472, 1081, 291, 727, 764, 309, 13, 509, 458, 498, 291, 1415, 281], "temperature": 0.0, "avg_logprob": -0.25704273310574616, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.565865765471244e-06}, {"id": 162, "seek": 104938, "start": 1069.46, "end": 1076.8200000000002, "text": " define some sort of runtime where you say oh yeah well you can turn that into you know", "tokens": [6964, 512, 1333, 295, 34474, 689, 291, 584, 1954, 1338, 731, 291, 393, 1261, 300, 666, 291, 458], "temperature": 0.0, "avg_logprob": -0.25704273310574616, "compression_ratio": 1.6604651162790698, "no_speech_prob": 4.565865765471244e-06}, {"id": 163, "seek": 107682, "start": 1076.82, "end": 1082.54, "text": " not an Elm command but something like an Elm command and you return or like you can return", "tokens": [406, 364, 2699, 76, 5622, 457, 746, 411, 364, 2699, 76, 5622, 293, 291, 2736, 420, 411, 291, 393, 2736], "temperature": 0.0, "avg_logprob": -0.2127742323764535, "compression_ratio": 1.7609756097560976, "no_speech_prob": 1.191100295727665e-06}, {"id": 164, "seek": 107682, "start": 1082.54, "end": 1089.1799999999998, "text": " a back end task in your in a tuple with a model change and a back end task or like it's", "tokens": [257, 646, 917, 5633, 294, 428, 294, 257, 2604, 781, 365, 257, 2316, 1319, 293, 257, 646, 917, 5633, 420, 411, 309, 311], "temperature": 0.0, "avg_logprob": -0.2127742323764535, "compression_ratio": 1.7609756097560976, "no_speech_prob": 1.191100295727665e-06}, {"id": 165, "seek": 107682, "start": 1089.1799999999998, "end": 1096.6599999999999, "text": " possible to do that. And I think that sometimes people underestimate what you can model for", "tokens": [1944, 281, 360, 300, 13, 400, 286, 519, 300, 2171, 561, 35826, 437, 291, 393, 2316, 337], "temperature": 0.0, "avg_logprob": -0.2127742323764535, "compression_ratio": 1.7609756097560976, "no_speech_prob": 1.191100295727665e-06}, {"id": 166, "seek": 107682, "start": 1096.6599999999999, "end": 1104.1799999999998, "text": " frameworks to be able to do like effectful things using this pattern of describing effects", "tokens": [29834, 281, 312, 1075, 281, 360, 411, 1802, 906, 721, 1228, 341, 5102, 295, 16141, 5065], "temperature": 0.0, "avg_logprob": -0.2127742323764535, "compression_ratio": 1.7609756097560976, "no_speech_prob": 1.191100295727665e-06}, {"id": 167, "seek": 110418, "start": 1104.18, "end": 1109.78, "text": " as data. I think it's like it's actually a very powerful tool that we can do a lot with", "tokens": [382, 1412, 13, 286, 519, 309, 311, 411, 309, 311, 767, 257, 588, 4005, 2290, 300, 321, 393, 360, 257, 688, 365], "temperature": 0.0, "avg_logprob": -0.1930204603407118, "compression_ratio": 1.6933962264150944, "no_speech_prob": 1.8447932461640448e-06}, {"id": 168, "seek": 110418, "start": 1109.78, "end": 1117.42, "text": " and as framework designers we can put guardrails so it's very very clear what what it's possible", "tokens": [293, 382, 8388, 16196, 321, 393, 829, 6290, 424, 4174, 370, 309, 311, 588, 588, 1850, 437, 437, 309, 311, 1944], "temperature": 0.0, "avg_logprob": -0.1930204603407118, "compression_ratio": 1.6933962264150944, "no_speech_prob": 1.8447932461640448e-06}, {"id": 169, "seek": 110418, "start": 1117.42, "end": 1122.64, "text": " to do using those data types and where they can be used and where they cannot be used.", "tokens": [281, 360, 1228, 729, 1412, 3467, 293, 689, 436, 393, 312, 1143, 293, 689, 436, 2644, 312, 1143, 13], "temperature": 0.0, "avg_logprob": -0.1930204603407118, "compression_ratio": 1.6933962264150944, "no_speech_prob": 1.8447932461640448e-06}, {"id": 170, "seek": 110418, "start": 1122.64, "end": 1129.02, "text": " So you know it's essentially the idea of a managed effect where like calling a back end", "tokens": [407, 291, 458, 309, 311, 4476, 264, 1558, 295, 257, 6453, 1802, 689, 411, 5141, 257, 646, 917], "temperature": 0.0, "avg_logprob": -0.1930204603407118, "compression_ratio": 1.6933962264150944, "no_speech_prob": 1.8447932461640448e-06}, {"id": 171, "seek": 112902, "start": 1129.02, "end": 1135.1, "text": " creating a back end task in Elm pages doesn't do anything. You can create a back end task", "tokens": [4084, 257, 646, 917, 5633, 294, 2699, 76, 7183, 1177, 380, 360, 1340, 13, 509, 393, 1884, 257, 646, 917, 5633], "temperature": 0.0, "avg_logprob": -0.27048548427196817, "compression_ratio": 1.7230769230769232, "no_speech_prob": 5.453254061649204e-07}, {"id": 172, "seek": 112902, "start": 1135.1, "end": 1141.34, "text": " just like you can create a command but when you give it to Elm pages in a place where", "tokens": [445, 411, 291, 393, 1884, 257, 5622, 457, 562, 291, 976, 309, 281, 2699, 76, 7183, 294, 257, 1081, 689], "temperature": 0.0, "avg_logprob": -0.27048548427196817, "compression_ratio": 1.7230769230769232, "no_speech_prob": 5.453254061649204e-07}, {"id": 173, "seek": 112902, "start": 1141.34, "end": 1146.7, "text": " it accepts that type then it lets the framework do something with it. So the sky's the limit", "tokens": [309, 33538, 300, 2010, 550, 309, 6653, 264, 8388, 360, 746, 365, 309, 13, 407, 264, 5443, 311, 264, 4948], "temperature": 0.0, "avg_logprob": -0.27048548427196817, "compression_ratio": 1.7230769230769232, "no_speech_prob": 5.453254061649204e-07}, {"id": 174, "seek": 112902, "start": 1146.7, "end": 1151.86, "text": " with how you build things with that. What I really like about this pattern is that because", "tokens": [365, 577, 291, 1322, 721, 365, 300, 13, 708, 286, 534, 411, 466, 341, 5102, 307, 300, 570], "temperature": 0.0, "avg_logprob": -0.27048548427196817, "compression_ratio": 1.7230769230769232, "no_speech_prob": 5.453254061649204e-07}, {"id": 175, "seek": 112902, "start": 1151.86, "end": 1158.1, "text": " there's an abstraction layer because you make a new API that whose internals are hidden.", "tokens": [456, 311, 364, 37765, 4583, 570, 291, 652, 257, 777, 9362, 300, 6104, 2154, 1124, 366, 7633, 13], "temperature": 0.0, "avg_logprob": -0.27048548427196817, "compression_ratio": 1.7230769230769232, "no_speech_prob": 5.453254061649204e-07}, {"id": 176, "seek": 115810, "start": 1158.1, "end": 1163.78, "text": " I mean I'm guessing they're opaque right. Well that decouples you from how it's implemented", "tokens": [286, 914, 286, 478, 17939, 436, 434, 42687, 558, 13, 1042, 300, 979, 263, 2622, 291, 490, 577, 309, 311, 12270], "temperature": 0.0, "avg_logprob": -0.29442073521989115, "compression_ratio": 1.7766990291262137, "no_speech_prob": 4.0928807720774785e-06}, {"id": 177, "seek": 115810, "start": 1163.78, "end": 1170.1799999999998, "text": " under the hood. So however you implement back end tasks under the hood or how you implement", "tokens": [833, 264, 13376, 13, 407, 4461, 291, 4445, 646, 917, 9608, 833, 264, 13376, 420, 577, 291, 4445], "temperature": 0.0, "avg_logprob": -0.29442073521989115, "compression_ratio": 1.7766990291262137, "no_speech_prob": 4.0928807720774785e-06}, {"id": 178, "seek": 115810, "start": 1170.1799999999998, "end": 1177.74, "text": " logging, reading a file, writing to a file, all those things. I'm guessing they're implemented", "tokens": [27991, 11, 3760, 257, 3991, 11, 3579, 281, 257, 3991, 11, 439, 729, 721, 13, 286, 478, 17939, 436, 434, 12270], "temperature": 0.0, "avg_logprob": -0.29442073521989115, "compression_ratio": 1.7766990291262137, "no_speech_prob": 4.0928807720774785e-06}, {"id": 179, "seek": 115810, "start": 1177.74, "end": 1184.82, "text": " in Node.js at the moment but they could be implemented using a bash script or whatever.", "tokens": [294, 38640, 13, 25530, 412, 264, 1623, 457, 436, 727, 312, 12270, 1228, 257, 46183, 5755, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.29442073521989115, "compression_ratio": 1.7766990291262137, "no_speech_prob": 4.0928807720774785e-06}, {"id": 180, "seek": 118482, "start": 1184.82, "end": 1191.8999999999999, "text": " And all those options are available as long as you don't tie it to a specific implementation", "tokens": [400, 439, 729, 3956, 366, 2435, 382, 938, 382, 291, 500, 380, 7582, 309, 281, 257, 2685, 11420], "temperature": 0.0, "avg_logprob": -0.28750650315057663, "compression_ratio": 1.664179104477612, "no_speech_prob": 4.425470706337364e-06}, {"id": 181, "seek": 118482, "start": 1191.8999999999999, "end": 1196.34, "text": " which because there's an abstraction layer they are not. So that is something that I", "tokens": [597, 570, 456, 311, 364, 37765, 4583, 436, 366, 406, 13, 407, 300, 307, 746, 300, 286], "temperature": 0.0, "avg_logprob": -0.28750650315057663, "compression_ratio": 1.664179104477612, "no_speech_prob": 4.425470706337364e-06}, {"id": 182, "seek": 118482, "start": 1196.34, "end": 1202.6599999999999, "text": " really like is that you're free to implement it however you want, you as the framework", "tokens": [534, 411, 307, 300, 291, 434, 1737, 281, 4445, 309, 4461, 291, 528, 11, 291, 382, 264, 8388], "temperature": 0.0, "avg_logprob": -0.28750650315057663, "compression_ratio": 1.664179104477612, "no_speech_prob": 4.425470706337364e-06}, {"id": 183, "seek": 118482, "start": 1202.6599999999999, "end": 1208.6599999999999, "text": " author. But I'm guessing if you do it through a port a user can do so as well. Although", "tokens": [3793, 13, 583, 286, 478, 17939, 498, 291, 360, 309, 807, 257, 2436, 257, 4195, 393, 360, 370, 382, 731, 13, 5780], "temperature": 0.0, "avg_logprob": -0.28750650315057663, "compression_ratio": 1.664179104477612, "no_speech_prob": 4.425470706337364e-06}, {"id": 184, "seek": 118482, "start": 1208.6599999999999, "end": 1214.62, "text": " it is going to be executed through JavaScript. But it could be done through other means maybe", "tokens": [309, 307, 516, 281, 312, 17577, 807, 15778, 13, 583, 309, 727, 312, 1096, 807, 661, 1355, 1310], "temperature": 0.0, "avg_logprob": -0.28750650315057663, "compression_ratio": 1.664179104477612, "no_speech_prob": 4.425470706337364e-06}, {"id": 185, "seek": 121462, "start": 1214.62, "end": 1221.8999999999999, "text": " Absolutely. Well yeah as you say I mean at the end of the day Elm pages is creating,", "tokens": [7021, 13, 1042, 1338, 382, 291, 584, 286, 914, 412, 264, 917, 295, 264, 786, 2699, 76, 7183, 307, 4084, 11], "temperature": 0.0, "avg_logprob": -0.2513711141503375, "compression_ratio": 1.7401960784313726, "no_speech_prob": 1.8161790649173781e-06}, {"id": 186, "seek": 121462, "start": 1221.8999999999999, "end": 1226.6999999999998, "text": " it's scaffolding up an application around your application. That's sort of what a framework", "tokens": [309, 311, 44094, 278, 493, 364, 3861, 926, 428, 3861, 13, 663, 311, 1333, 295, 437, 257, 8388], "temperature": 0.0, "avg_logprob": -0.2513711141503375, "compression_ratio": 1.7401960784313726, "no_speech_prob": 1.8161790649173781e-06}, {"id": 187, "seek": 121462, "start": 1226.6999999999998, "end": 1235.86, "text": " is. And so it's at the end of the day compiling an Elm application and executing it in this", "tokens": [307, 13, 400, 370, 309, 311, 412, 264, 917, 295, 264, 786, 715, 4883, 364, 2699, 76, 3861, 293, 32368, 309, 294, 341], "temperature": 0.0, "avg_logprob": -0.2513711141503375, "compression_ratio": 1.7401960784313726, "no_speech_prob": 1.8161790649173781e-06}, {"id": 188, "seek": 121462, "start": 1235.86, "end": 1239.8999999999999, "text": " case in Node.js. But it could be executing it in other contexts. It could be executing", "tokens": [1389, 294, 38640, 13, 25530, 13, 583, 309, 727, 312, 32368, 309, 294, 661, 30628, 13, 467, 727, 312, 32368], "temperature": 0.0, "avg_logprob": -0.2513711141503375, "compression_ratio": 1.7401960784313726, "no_speech_prob": 1.8161790649173781e-06}, {"id": 189, "seek": 123990, "start": 1239.9, "end": 1248.3400000000001, "text": " it with Deno or Cloudflare workers or with Bun with different run times. But at the end", "tokens": [309, 365, 413, 5808, 420, 8061, 3423, 543, 5600, 420, 365, 14661, 365, 819, 1190, 1413, 13, 583, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.28040433191991115, "compression_ratio": 1.5855855855855856, "no_speech_prob": 4.637753590941429e-06}, {"id": 190, "seek": 123990, "start": 1248.3400000000001, "end": 1254.7800000000002, "text": " of the day it is using Elm which its way of communicating is through ports. And so it's", "tokens": [295, 264, 786, 309, 307, 1228, 2699, 76, 597, 1080, 636, 295, 17559, 307, 807, 18160, 13, 400, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.28040433191991115, "compression_ratio": 1.5855855855855856, "no_speech_prob": 4.637753590941429e-06}, {"id": 191, "seek": 123990, "start": 1254.7800000000002, "end": 1259.66, "text": " just it's just building that. It's just like a little application. You know just like when", "tokens": [445, 309, 311, 445, 2390, 300, 13, 467, 311, 445, 411, 257, 707, 3861, 13, 509, 458, 445, 411, 562], "temperature": 0.0, "avg_logprob": -0.28040433191991115, "compression_ratio": 1.5855855855855856, "no_speech_prob": 4.637753590941429e-06}, {"id": 192, "seek": 123990, "start": 1259.66, "end": 1266.3400000000001, "text": " we write Elm review and Elm GraphQL command line tools that you npm install somewhere", "tokens": [321, 2464, 2699, 76, 3131, 293, 2699, 76, 21884, 13695, 5622, 1622, 3873, 300, 291, 297, 14395, 3625, 4079], "temperature": 0.0, "avg_logprob": -0.28040433191991115, "compression_ratio": 1.5855855855855856, "no_speech_prob": 4.637753590941429e-06}, {"id": 193, "seek": 126634, "start": 1266.34, "end": 1274.1, "text": " in there where we have compiled Elm code where we take that JS we import that code and run", "tokens": [294, 456, 689, 321, 362, 36548, 2699, 76, 3089, 689, 321, 747, 300, 33063, 321, 974, 300, 3089, 293, 1190], "temperature": 0.0, "avg_logprob": -0.22518417861435439, "compression_ratio": 1.722488038277512, "no_speech_prob": 2.8572758310474455e-06}, {"id": 194, "seek": 126634, "start": 1274.1, "end": 1281.4199999999998, "text": " it set up you know in it the Elm application subscribe to some ports send back some ports.", "tokens": [309, 992, 493, 291, 458, 294, 309, 264, 2699, 76, 3861, 3022, 281, 512, 18160, 2845, 646, 512, 18160, 13], "temperature": 0.0, "avg_logprob": -0.22518417861435439, "compression_ratio": 1.722488038277512, "no_speech_prob": 2.8572758310474455e-06}, {"id": 195, "seek": 126634, "start": 1281.4199999999998, "end": 1285.4599999999998, "text": " So we're communicating to the Elm app through ports and that's that's all that Elm pages", "tokens": [407, 321, 434, 17559, 281, 264, 2699, 76, 724, 807, 18160, 293, 300, 311, 300, 311, 439, 300, 2699, 76, 7183], "temperature": 0.0, "avg_logprob": -0.22518417861435439, "compression_ratio": 1.722488038277512, "no_speech_prob": 2.8572758310474455e-06}, {"id": 196, "seek": 126634, "start": 1285.4599999999998, "end": 1291.6399999999999, "text": " is doing. But it creates a set of abstractions for that that makes it easier for the user", "tokens": [307, 884, 13, 583, 309, 7829, 257, 992, 295, 12649, 626, 337, 300, 300, 1669, 309, 3571, 337, 264, 4195], "temperature": 0.0, "avg_logprob": -0.22518417861435439, "compression_ratio": 1.722488038277512, "no_speech_prob": 2.8572758310474455e-06}, {"id": 197, "seek": 129164, "start": 1291.64, "end": 1298.6200000000001, "text": " to basically execute things in a back end and run a script in a back end context which", "tokens": [281, 1936, 14483, 721, 294, 257, 646, 917, 293, 1190, 257, 5755, 294, 257, 646, 917, 4319, 597], "temperature": 0.0, "avg_logprob": -0.2261486265394423, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.340514240335324e-06}, {"id": 198, "seek": 129164, "start": 1298.6200000000001, "end": 1306.5, "text": " turns out is a very useful thing to do if you're you know making a static site because", "tokens": [4523, 484, 307, 257, 588, 4420, 551, 281, 360, 498, 291, 434, 291, 458, 1455, 257, 13437, 3621, 570], "temperature": 0.0, "avg_logprob": -0.2261486265394423, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.340514240335324e-06}, {"id": 199, "seek": 129164, "start": 1306.5, "end": 1311.24, "text": " you want to read some files and then you want to pull that data in your front end. But that's", "tokens": [291, 528, 281, 1401, 512, 7098, 293, 550, 291, 528, 281, 2235, 300, 1412, 294, 428, 1868, 917, 13, 583, 300, 311], "temperature": 0.0, "avg_logprob": -0.2261486265394423, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.340514240335324e-06}, {"id": 200, "seek": 129164, "start": 1311.24, "end": 1318.1000000000001, "text": " also scripting right. So it does bring up the question like is Elm a good tool for this", "tokens": [611, 5755, 278, 558, 13, 407, 309, 775, 1565, 493, 264, 1168, 411, 307, 2699, 76, 257, 665, 2290, 337, 341], "temperature": 0.0, "avg_logprob": -0.2261486265394423, "compression_ratio": 1.6210045662100456, "no_speech_prob": 3.340514240335324e-06}, {"id": 201, "seek": 131810, "start": 1318.1, "end": 1327.12, "text": " type of task like this kind of back end task. Right. Yeah exactly. Is Elm a good tool for", "tokens": [2010, 295, 5633, 411, 341, 733, 295, 646, 917, 5633, 13, 1779, 13, 865, 2293, 13, 1119, 2699, 76, 257, 665, 2290, 337], "temperature": 0.0, "avg_logprob": -0.20572007056510094, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.4822320508756093e-06}, {"id": 202, "seek": 131810, "start": 1327.12, "end": 1333.1799999999998, "text": " writing a script. Is that a good idea. And I mean of course we're biased. We want to", "tokens": [3579, 257, 5755, 13, 1119, 300, 257, 665, 1558, 13, 400, 286, 914, 295, 1164, 321, 434, 28035, 13, 492, 528, 281], "temperature": 0.0, "avg_logprob": -0.20572007056510094, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.4822320508756093e-06}, {"id": 203, "seek": 131810, "start": 1333.1799999999998, "end": 1340.4599999999998, "text": " do everything in Elm and we write lots and lots of Node.js code so that we can have the", "tokens": [360, 1203, 294, 2699, 76, 293, 321, 2464, 3195, 293, 3195, 295, 38640, 13, 25530, 3089, 370, 300, 321, 393, 362, 264], "temperature": 0.0, "avg_logprob": -0.20572007056510094, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.4822320508756093e-06}, {"id": 204, "seek": 131810, "start": 1340.4599999999998, "end": 1346.4599999999998, "text": " ability to do things only in Elm. But I think it's I think it's quite nice to be able to", "tokens": [3485, 281, 360, 721, 787, 294, 2699, 76, 13, 583, 286, 519, 309, 311, 286, 519, 309, 311, 1596, 1481, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.20572007056510094, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.4822320508756093e-06}, {"id": 205, "seek": 134646, "start": 1346.46, "end": 1353.66, "text": " just operate within the confines of type safe Elm code where you can write a JSON decoder", "tokens": [445, 9651, 1951, 264, 1497, 1652, 295, 2010, 3273, 2699, 76, 3089, 689, 291, 393, 2464, 257, 31828, 979, 19866], "temperature": 0.0, "avg_logprob": -0.21763057499141483, "compression_ratio": 1.6851851851851851, "no_speech_prob": 4.5208935262053274e-07}, {"id": 206, "seek": 134646, "start": 1353.66, "end": 1361.54, "text": " and have have things fail and have this explicitness. But you can write to a file. You can log you", "tokens": [293, 362, 362, 721, 3061, 293, 362, 341, 28021, 6394, 13, 583, 291, 393, 2464, 281, 257, 3991, 13, 509, 393, 3565, 291], "temperature": 0.0, "avg_logprob": -0.21763057499141483, "compression_ratio": 1.6851851851851851, "no_speech_prob": 4.5208935262053274e-07}, {"id": 207, "seek": 134646, "start": 1361.54, "end": 1367.3, "text": " can read files which by the way like writing to a file is like a built in thing in the", "tokens": [393, 1401, 7098, 597, 538, 264, 636, 411, 3579, 281, 257, 3991, 307, 411, 257, 3094, 294, 551, 294, 264], "temperature": 0.0, "avg_logprob": -0.21763057499141483, "compression_ratio": 1.6851851851851851, "no_speech_prob": 4.5208935262053274e-07}, {"id": 208, "seek": 134646, "start": 1367.3, "end": 1373.3, "text": " script module that Elm pages provides. But you can define your own custom back end tasks", "tokens": [5755, 10088, 300, 2699, 76, 7183, 6417, 13, 583, 291, 393, 6964, 428, 1065, 2375, 646, 917, 9608], "temperature": 0.0, "avg_logprob": -0.21763057499141483, "compression_ratio": 1.6851851851851851, "no_speech_prob": 4.5208935262053274e-07}, {"id": 209, "seek": 137330, "start": 1373.3, "end": 1379.58, "text": " as well. So it's just a way of binding Elm code and these back end tasks to a back end.", "tokens": [382, 731, 13, 407, 309, 311, 445, 257, 636, 295, 17359, 2699, 76, 3089, 293, 613, 646, 917, 9608, 281, 257, 646, 917, 13], "temperature": 0.0, "avg_logprob": -0.22552696382156526, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.4823364153926377e-06}, {"id": 210, "seek": 137330, "start": 1379.58, "end": 1384.7, "text": " That's that's really what a back end task is. OK. So you mentioned doing this in Elm", "tokens": [663, 311, 300, 311, 534, 437, 257, 646, 917, 5633, 307, 13, 2264, 13, 407, 291, 2835, 884, 341, 294, 2699, 76], "temperature": 0.0, "avg_logprob": -0.22552696382156526, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.4823364153926377e-06}, {"id": 211, "seek": 137330, "start": 1384.7, "end": 1392.4199999999998, "text": " or doing this in other languages or tools. So yeah. Like does it make sense to write", "tokens": [420, 884, 341, 294, 661, 8650, 420, 3873, 13, 407, 1338, 13, 1743, 775, 309, 652, 2020, 281, 2464], "temperature": 0.0, "avg_logprob": -0.22552696382156526, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.4823364153926377e-06}, {"id": 212, "seek": 137330, "start": 1392.4199999999998, "end": 1400.7, "text": " a script in Elm or in Elm pages or is it sometimes better to do it in JavaScript or Bash or Python", "tokens": [257, 5755, 294, 2699, 76, 420, 294, 2699, 76, 7183, 420, 307, 309, 2171, 1101, 281, 360, 309, 294, 15778, 420, 43068, 420, 15329], "temperature": 0.0, "avg_logprob": -0.22552696382156526, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.4823364153926377e-06}, {"id": 213, "seek": 140070, "start": 1400.7, "end": 1407.18, "text": " or whatever. So you say that there's at the moment logging there's writing a file. That's", "tokens": [420, 2035, 13, 407, 291, 584, 300, 456, 311, 412, 264, 1623, 27991, 456, 311, 3579, 257, 3991, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.2750705155459317, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.668729959827033e-06}, {"id": 214, "seek": 140070, "start": 1407.18, "end": 1413.5800000000002, "text": " not a lot of things that you can do built out of the box but you can do more through", "tokens": [406, 257, 688, 295, 721, 300, 291, 393, 360, 3094, 484, 295, 264, 2424, 457, 291, 393, 360, 544, 807], "temperature": 0.0, "avg_logprob": -0.2750705155459317, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.668729959827033e-06}, {"id": 215, "seek": 140070, "start": 1413.5800000000002, "end": 1420.5, "text": " custom back end tasks using ports. So basically you can probably do anything that a JavaScript", "tokens": [2375, 646, 917, 9608, 1228, 18160, 13, 407, 1936, 291, 393, 1391, 360, 1340, 300, 257, 15778], "temperature": 0.0, "avg_logprob": -0.2750705155459317, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.668729959827033e-06}, {"id": 216, "seek": 140070, "start": 1420.5, "end": 1426.14, "text": " script could do. But is it what are the gains what are the benefits that you have when you", "tokens": [5755, 727, 360, 13, 583, 307, 309, 437, 366, 264, 16823, 437, 366, 264, 5311, 300, 291, 362, 562, 291], "temperature": 0.0, "avg_logprob": -0.2750705155459317, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.668729959827033e-06}, {"id": 217, "seek": 142614, "start": 1426.14, "end": 1432.3000000000002, "text": " do it through Elm pages compared to just running a Node.js script for instance.", "tokens": [360, 309, 807, 2699, 76, 7183, 5347, 281, 445, 2614, 257, 38640, 13, 25530, 5755, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.29086154630814476, "compression_ratio": 1.6009174311926606, "no_speech_prob": 2.8572312658070587e-06}, {"id": 218, "seek": 142614, "start": 1432.3000000000002, "end": 1437.0200000000002, "text": " Exactly. Yeah. Great question. And that's that's exactly the right question I think.", "tokens": [7587, 13, 865, 13, 3769, 1168, 13, 400, 300, 311, 300, 311, 2293, 264, 558, 1168, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.29086154630814476, "compression_ratio": 1.6009174311926606, "no_speech_prob": 2.8572312658070587e-06}, {"id": 219, "seek": 142614, "start": 1437.0200000000002, "end": 1442.9, "text": " So first of all a little bit of background. The motivation for Elm pages scripts and people", "tokens": [407, 700, 295, 439, 257, 707, 857, 295, 3678, 13, 440, 12335, 337, 2699, 76, 7183, 23294, 293, 561], "temperature": 0.0, "avg_logprob": -0.29086154630814476, "compression_ratio": 1.6009174311926606, "no_speech_prob": 2.8572312658070587e-06}, {"id": 220, "seek": 142614, "start": 1442.9, "end": 1449.3400000000001, "text": " might be asking like Elm pages scripts like why what does Elm pages have to do with scripts.", "tokens": [1062, 312, 3365, 411, 2699, 76, 7183, 23294, 411, 983, 437, 775, 2699, 76, 7183, 362, 281, 360, 365, 23294, 13], "temperature": 0.0, "avg_logprob": -0.29086154630814476, "compression_ratio": 1.6009174311926606, "no_speech_prob": 2.8572312658070587e-06}, {"id": 221, "seek": 144934, "start": 1449.34, "end": 1458.58, "text": " Yeah the name don't match. Right. At the moment. So the Elm pages script was born out of this", "tokens": [865, 264, 1315, 500, 380, 2995, 13, 1779, 13, 1711, 264, 1623, 13, 407, 264, 2699, 76, 7183, 5755, 390, 4232, 484, 295, 341], "temperature": 0.0, "avg_logprob": -0.27318658055485906, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.5293808246497065e-06}, {"id": 222, "seek": 144934, "start": 1458.58, "end": 1465.4599999999998, "text": " use case of generating like the scaffolding for a new route. So Elm pages v2 has an Elm", "tokens": [764, 1389, 295, 17746, 411, 264, 44094, 278, 337, 257, 777, 7955, 13, 407, 2699, 76, 7183, 371, 17, 575, 364, 2699, 76], "temperature": 0.0, "avg_logprob": -0.27318658055485906, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.5293808246497065e-06}, {"id": 223, "seek": 144934, "start": 1465.4599999999998, "end": 1473.6599999999999, "text": " pages add command so you can say Elm pages add blog dot slug underscore and it generates", "tokens": [7183, 909, 5622, 370, 291, 393, 584, 2699, 76, 7183, 909, 6968, 5893, 1061, 697, 37556, 293, 309, 23815], "temperature": 0.0, "avg_logprob": -0.27318658055485906, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.5293808246497065e-06}, {"id": 224, "seek": 147366, "start": 1473.66, "end": 1480.46, "text": " something for a route where it's blog slash some dynamics slug. And I wanted to have a", "tokens": [746, 337, 257, 7955, 689, 309, 311, 6968, 17330, 512, 15679, 1061, 697, 13, 400, 286, 1415, 281, 362, 257], "temperature": 0.0, "avg_logprob": -0.2458864183568243, "compression_ratio": 1.453551912568306, "no_speech_prob": 1.7061350945368758e-06}, {"id": 225, "seek": 147366, "start": 1480.46, "end": 1488.02, "text": " way to let users customize that. Ryan has created a nice feature in Elm SPA where you", "tokens": [636, 281, 718, 5022, 19734, 300, 13, 9116, 575, 2942, 257, 1481, 4111, 294, 2699, 76, 318, 10297, 689, 291], "temperature": 0.0, "avg_logprob": -0.2458864183568243, "compression_ratio": 1.453551912568306, "no_speech_prob": 1.7061350945368758e-06}, {"id": 226, "seek": 147366, "start": 1488.02, "end": 1496.7, "text": " can do some templating and create custom commands for for scaffolding new pages. I was really", "tokens": [393, 360, 512, 9100, 990, 293, 1884, 2375, 16901, 337, 337, 44094, 278, 777, 7183, 13, 286, 390, 534], "temperature": 0.0, "avg_logprob": -0.2458864183568243, "compression_ratio": 1.453551912568306, "no_speech_prob": 1.7061350945368758e-06}, {"id": 227, "seek": 149670, "start": 1496.7, "end": 1503.98, "text": " keen on on using Matt's Elm code gen tool for that. And so as I was starting to build", "tokens": [20297, 322, 322, 1228, 7397, 311, 2699, 76, 3089, 1049, 2290, 337, 300, 13, 400, 370, 382, 286, 390, 2891, 281, 1322], "temperature": 0.0, "avg_logprob": -0.22363736079289362, "compression_ratio": 1.59375, "no_speech_prob": 1.1189318911419832e-06}, {"id": 228, "seek": 149670, "start": 1503.98, "end": 1511.42, "text": " that I'm like well it would be really nice if if I could use Elm code gen to create scaffolding", "tokens": [300, 286, 478, 411, 731, 309, 576, 312, 534, 1481, 498, 498, 286, 727, 764, 2699, 76, 3089, 1049, 281, 1884, 44094, 278], "temperature": 0.0, "avg_logprob": -0.22363736079289362, "compression_ratio": 1.59375, "no_speech_prob": 1.1189318911419832e-06}, {"id": 229, "seek": 149670, "start": 1511.42, "end": 1517.5, "text": " for new routes. But I also want to be able to read an environment variable read some", "tokens": [337, 777, 18242, 13, 583, 286, 611, 528, 281, 312, 1075, 281, 1401, 364, 2823, 7006, 1401, 512], "temperature": 0.0, "avg_logprob": -0.22363736079289362, "compression_ratio": 1.59375, "no_speech_prob": 1.1189318911419832e-06}, {"id": 230, "seek": 149670, "start": 1517.5, "end": 1525.98, "text": " configuration from a JSON file maybe get some like JSON data from an API to figure out how", "tokens": [11694, 490, 257, 31828, 3991, 1310, 483, 512, 411, 31828, 1412, 490, 364, 9362, 281, 2573, 484, 577], "temperature": 0.0, "avg_logprob": -0.22363736079289362, "compression_ratio": 1.59375, "no_speech_prob": 1.1189318911419832e-06}, {"id": 231, "seek": 152598, "start": 1525.98, "end": 1532.14, "text": " I'm going to generate my my new routes. And so well that's kind of what back end tasks", "tokens": [286, 478, 516, 281, 8460, 452, 452, 777, 18242, 13, 400, 370, 731, 300, 311, 733, 295, 437, 646, 917, 9608], "temperature": 0.0, "avg_logprob": -0.18532479547821315, "compression_ratio": 1.8608695652173912, "no_speech_prob": 1.3630925423058216e-05}, {"id": 232, "seek": 152598, "start": 1532.14, "end": 1537.34, "text": " let you do. So I wanted I knew for a long time that I wanted to have the ability to", "tokens": [718, 291, 360, 13, 407, 286, 1415, 286, 2586, 337, 257, 938, 565, 300, 286, 1415, 281, 362, 264, 3485, 281], "temperature": 0.0, "avg_logprob": -0.18532479547821315, "compression_ratio": 1.8608695652173912, "no_speech_prob": 1.3630925423058216e-05}, {"id": 233, "seek": 152598, "start": 1537.34, "end": 1543.42, "text": " use back end tasks to scaffold new routes because I just really like this abstraction", "tokens": [764, 646, 917, 9608, 281, 44094, 777, 18242, 570, 286, 445, 534, 411, 341, 37765], "temperature": 0.0, "avg_logprob": -0.18532479547821315, "compression_ratio": 1.8608695652173912, "no_speech_prob": 1.3630925423058216e-05}, {"id": 234, "seek": 152598, "start": 1543.42, "end": 1548.22, "text": " of back end tasks and I want to use it for a lot of things. And then it's like well if", "tokens": [295, 646, 917, 9608, 293, 286, 528, 281, 764, 309, 337, 257, 688, 295, 721, 13, 400, 550, 309, 311, 411, 731, 498], "temperature": 0.0, "avg_logprob": -0.18532479547821315, "compression_ratio": 1.8608695652173912, "no_speech_prob": 1.3630925423058216e-05}, {"id": 235, "seek": 152598, "start": 1548.22, "end": 1553.9, "text": " I can like once I've built that it's like well this is no longer a scaffolding tool.", "tokens": [286, 393, 411, 1564, 286, 600, 3094, 300, 309, 311, 411, 731, 341, 307, 572, 2854, 257, 44094, 278, 2290, 13], "temperature": 0.0, "avg_logprob": -0.18532479547821315, "compression_ratio": 1.8608695652173912, "no_speech_prob": 1.3630925423058216e-05}, {"id": 236, "seek": 155390, "start": 1553.9, "end": 1560.38, "text": " This is just a way to like run back end tasks by like writing a module that defines a back", "tokens": [639, 307, 445, 257, 636, 281, 411, 1190, 646, 917, 9608, 538, 411, 3579, 257, 10088, 300, 23122, 257, 646], "temperature": 0.0, "avg_logprob": -0.2340379442487444, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2602662284043618e-06}, {"id": 237, "seek": 155390, "start": 1560.38, "end": 1566.5400000000002, "text": " end task to run. And OK maybe the special case is it's like writing a file in a specific", "tokens": [917, 5633, 281, 1190, 13, 400, 2264, 1310, 264, 2121, 1389, 307, 309, 311, 411, 3579, 257, 3991, 294, 257, 2685], "temperature": 0.0, "avg_logprob": -0.2340379442487444, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2602662284043618e-06}, {"id": 238, "seek": 155390, "start": 1566.5400000000002, "end": 1572.5400000000002, "text": " format but why not just give a back end task to write a file and then you can use that.", "tokens": [7877, 457, 983, 406, 445, 976, 257, 646, 917, 5633, 281, 2464, 257, 3991, 293, 550, 291, 393, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.2340379442487444, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2602662284043618e-06}, {"id": 239, "seek": 155390, "start": 1572.5400000000002, "end": 1578.3400000000001, "text": " And now it's just on pages scripts. So it's really like Ruby on Rails generators where", "tokens": [400, 586, 309, 311, 445, 322, 7183, 23294, 13, 407, 309, 311, 534, 411, 19907, 322, 48526, 38662, 689], "temperature": 0.0, "avg_logprob": -0.2340379442487444, "compression_ratio": 1.69377990430622, "no_speech_prob": 2.2602662284043618e-06}, {"id": 240, "seek": 157834, "start": 1578.34, "end": 1585.1799999999998, "text": " it's just like that was the main motivation was Ruby on Rails generators are used for", "tokens": [309, 311, 445, 411, 300, 390, 264, 2135, 12335, 390, 19907, 322, 48526, 38662, 366, 1143, 337], "temperature": 0.0, "avg_logprob": -0.23034692945934476, "compression_ratio": 1.755, "no_speech_prob": 3.0894088922650553e-06}, {"id": 241, "seek": 157834, "start": 1585.1799999999998, "end": 1591.78, "text": " if you want to create a new page with a form and then you just it's a tool for very quickly", "tokens": [498, 291, 528, 281, 1884, 257, 777, 3028, 365, 257, 1254, 293, 550, 291, 445, 309, 311, 257, 2290, 337, 588, 2661], "temperature": 0.0, "avg_logprob": -0.23034692945934476, "compression_ratio": 1.755, "no_speech_prob": 3.0894088922650553e-06}, {"id": 242, "seek": 157834, "start": 1591.78, "end": 1597.74, "text": " building up boilerplate. So it's like you know you create a new controller in Rails", "tokens": [2390, 493, 39228, 37008, 13, 407, 309, 311, 411, 291, 458, 291, 1884, 257, 777, 10561, 294, 48526], "temperature": 0.0, "avg_logprob": -0.23034692945934476, "compression_ratio": 1.755, "no_speech_prob": 3.0894088922650553e-06}, {"id": 243, "seek": 157834, "start": 1597.74, "end": 1602.98, "text": " and your template and your template is defining a form and your form has these fields and", "tokens": [293, 428, 12379, 293, 428, 12379, 307, 17827, 257, 1254, 293, 428, 1254, 575, 613, 7909, 293], "temperature": 0.0, "avg_logprob": -0.23034692945934476, "compression_ratio": 1.755, "no_speech_prob": 3.0894088922650553e-06}, {"id": 244, "seek": 160298, "start": 1602.98, "end": 1610.26, "text": " you also want to create you know some some stuff for for working with active record to", "tokens": [291, 611, 528, 281, 1884, 291, 458, 512, 512, 1507, 337, 337, 1364, 365, 4967, 2136, 281], "temperature": 0.0, "avg_logprob": -0.3264344848028504, "compression_ratio": 1.804, "no_speech_prob": 1.7880573750517215e-06}, {"id": 245, "seek": 160298, "start": 1610.26, "end": 1616.7, "text": " define this new user model or whatever. And so people are very productive using Rails", "tokens": [6964, 341, 777, 4195, 2316, 420, 2035, 13, 400, 370, 561, 366, 588, 13304, 1228, 48526], "temperature": 0.0, "avg_logprob": -0.3264344848028504, "compression_ratio": 1.804, "no_speech_prob": 1.7880573750517215e-06}, {"id": 246, "seek": 160298, "start": 1616.7, "end": 1623.22, "text": " generators where they'll say like Rails generate whatever and and you can build custom workflows.", "tokens": [38662, 689, 436, 603, 584, 411, 48526, 8460, 2035, 293, 293, 291, 393, 1322, 2375, 43461, 13], "temperature": 0.0, "avg_logprob": -0.3264344848028504, "compression_ratio": 1.804, "no_speech_prob": 1.7880573750517215e-06}, {"id": 247, "seek": 160298, "start": 1623.22, "end": 1627.58, "text": " You can build custom generators. You can even install custom generators. So and then they", "tokens": [509, 393, 1322, 2375, 38662, 13, 509, 393, 754, 3625, 2375, 38662, 13, 407, 293, 550, 436], "temperature": 0.0, "avg_logprob": -0.3264344848028504, "compression_ratio": 1.804, "no_speech_prob": 1.7880573750517215e-06}, {"id": 248, "seek": 160298, "start": 1627.58, "end": 1632.54, "text": " say that Elm has a lot of boilerplates like we don't write write scripts for that usually.", "tokens": [584, 300, 2699, 76, 575, 257, 688, 295, 39228, 564, 1024, 411, 321, 500, 380, 2464, 2464, 23294, 337, 300, 2673, 13], "temperature": 0.0, "avg_logprob": -0.3264344848028504, "compression_ratio": 1.804, "no_speech_prob": 1.7880573750517215e-06}, {"id": 249, "seek": 163254, "start": 1632.54, "end": 1640.6599999999999, "text": " Right. Right. But I wanted a way to customize template templates for for adding new routes.", "tokens": [1779, 13, 1779, 13, 583, 286, 1415, 257, 636, 281, 19734, 12379, 21165, 337, 337, 5127, 777, 18242, 13], "temperature": 0.0, "avg_logprob": -0.22154177559746635, "compression_ratio": 1.7335907335907337, "no_speech_prob": 1.4823415313003352e-06}, {"id": 250, "seek": 163254, "start": 1640.6599999999999, "end": 1645.1, "text": " And also you know if you want to create a new page and be super productive where you", "tokens": [400, 611, 291, 458, 498, 291, 528, 281, 1884, 257, 777, 3028, 293, 312, 1687, 13304, 689, 291], "temperature": 0.0, "avg_logprob": -0.22154177559746635, "compression_ratio": 1.7335907335907337, "no_speech_prob": 1.4823415313003352e-06}, {"id": 251, "seek": 163254, "start": 1645.1, "end": 1650.5, "text": " can say hey I'm going to make a new form and it has these fields. Why not be able to write", "tokens": [393, 584, 4177, 286, 478, 516, 281, 652, 257, 777, 1254, 293, 309, 575, 613, 7909, 13, 1545, 406, 312, 1075, 281, 2464], "temperature": 0.0, "avg_logprob": -0.22154177559746635, "compression_ratio": 1.7335907335907337, "no_speech_prob": 1.4823415313003352e-06}, {"id": 252, "seek": 163254, "start": 1650.5, "end": 1656.8999999999999, "text": " a custom generator a custom Elm pages script that lets you just template that. And if you", "tokens": [257, 2375, 19265, 257, 2375, 2699, 76, 7183, 5755, 300, 6653, 291, 445, 12379, 300, 13, 400, 498, 291], "temperature": 0.0, "avg_logprob": -0.22154177559746635, "compression_ratio": 1.7335907335907337, "no_speech_prob": 1.4823415313003352e-06}, {"id": 253, "seek": 163254, "start": 1656.8999999999999, "end": 1661.42, "text": " want to read some configuration from something or whatever you want to do why not let users", "tokens": [528, 281, 1401, 512, 11694, 490, 746, 420, 2035, 291, 528, 281, 360, 983, 406, 718, 5022], "temperature": 0.0, "avg_logprob": -0.22154177559746635, "compression_ratio": 1.7335907335907337, "no_speech_prob": 1.4823415313003352e-06}, {"id": 254, "seek": 166142, "start": 1661.42, "end": 1666.98, "text": " do that. So that was the motivation. Now back to the question of like why what what benefit", "tokens": [360, 300, 13, 407, 300, 390, 264, 12335, 13, 823, 646, 281, 264, 1168, 295, 411, 983, 437, 437, 5121], "temperature": 0.0, "avg_logprob": -0.2260625231397021, "compression_ratio": 1.6904761904761905, "no_speech_prob": 7.811384534761601e-07}, {"id": 255, "seek": 166142, "start": 1666.98, "end": 1672.1000000000001, "text": " do you gain by doing this compared to a bash script or a node script. If we look at the", "tokens": [360, 291, 6052, 538, 884, 341, 5347, 281, 257, 46183, 5755, 420, 257, 9984, 5755, 13, 759, 321, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.2260625231397021, "compression_ratio": 1.6904761904761905, "no_speech_prob": 7.811384534761601e-07}, {"id": 256, "seek": 166142, "start": 1672.1000000000001, "end": 1678.8000000000002, "text": " pros and cons between like writing a script in in Elm and writing a script in bash or", "tokens": [6267, 293, 1014, 1296, 411, 3579, 257, 5755, 294, 294, 2699, 76, 293, 3579, 257, 5755, 294, 46183, 420], "temperature": 0.0, "avg_logprob": -0.2260625231397021, "compression_ratio": 1.6904761904761905, "no_speech_prob": 7.811384534761601e-07}, {"id": 257, "seek": 166142, "start": 1678.8000000000002, "end": 1685.3400000000001, "text": " Node.js we can see some pretty pretty obvious pros and cons on either side. So let's look", "tokens": [38640, 13, 25530, 321, 393, 536, 512, 1238, 1238, 6322, 6267, 293, 1014, 322, 2139, 1252, 13, 407, 718, 311, 574], "temperature": 0.0, "avg_logprob": -0.2260625231397021, "compression_ratio": 1.6904761904761905, "no_speech_prob": 7.811384534761601e-07}, {"id": 258, "seek": 168534, "start": 1685.34, "end": 1693.3, "text": " at like writing a vanilla Elm script. If we were to do it on our own we would need to", "tokens": [412, 411, 3579, 257, 17528, 2699, 76, 5755, 13, 759, 321, 645, 281, 360, 309, 322, 527, 1065, 321, 576, 643, 281], "temperature": 0.0, "avg_logprob": -0.21861210275203624, "compression_ratio": 1.7733990147783252, "no_speech_prob": 2.026116590059246e-06}, {"id": 259, "seek": 168534, "start": 1693.3, "end": 1699.1799999999998, "text": " compile we would you know we'd need to like compile some Elm script we would need to take", "tokens": [31413, 321, 576, 291, 458, 321, 1116, 643, 281, 411, 31413, 512, 2699, 76, 5755, 321, 576, 643, 281, 747], "temperature": 0.0, "avg_logprob": -0.21861210275203624, "compression_ratio": 1.7733990147783252, "no_speech_prob": 2.026116590059246e-06}, {"id": 260, "seek": 168534, "start": 1699.1799999999998, "end": 1704.26, "text": " that compiled Elm script and import it into Node.js so we could run it and do some boilerplate", "tokens": [300, 36548, 2699, 76, 5755, 293, 974, 309, 666, 38640, 13, 25530, 370, 321, 727, 1190, 309, 293, 360, 512, 39228, 37008], "temperature": 0.0, "avg_logprob": -0.21861210275203624, "compression_ratio": 1.7733990147783252, "no_speech_prob": 2.026116590059246e-06}, {"id": 261, "seek": 168534, "start": 1704.26, "end": 1709.6599999999999, "text": " around that. And obviously that's not great. And Elm pages scripts takes care of that for", "tokens": [926, 300, 13, 400, 2745, 300, 311, 406, 869, 13, 400, 2699, 76, 7183, 23294, 2516, 1127, 295, 300, 337], "temperature": 0.0, "avg_logprob": -0.21861210275203624, "compression_ratio": 1.7733990147783252, "no_speech_prob": 2.026116590059246e-06}, {"id": 262, "seek": 170966, "start": 1709.66, "end": 1717.5800000000002, "text": " you. So we no longer have to worry about that. But what if we just wanted to grab some HTTP", "tokens": [291, 13, 407, 321, 572, 2854, 362, 281, 3292, 466, 300, 13, 583, 437, 498, 321, 445, 1415, 281, 4444, 512, 33283], "temperature": 0.0, "avg_logprob": -0.3565826416015625, "compression_ratio": 1.7205882352941178, "no_speech_prob": 5.862612852070015e-06}, {"id": 263, "seek": 170966, "start": 1717.5800000000002, "end": 1722.98, "text": " data. Right. If we have to create and update to do that that becomes pretty verbose and", "tokens": [1412, 13, 1779, 13, 759, 321, 362, 281, 1884, 364, 67, 5623, 281, 360, 300, 300, 3643, 1238, 9595, 541, 293], "temperature": 0.0, "avg_logprob": -0.3565826416015625, "compression_ratio": 1.7205882352941178, "no_speech_prob": 5.862612852070015e-06}, {"id": 264, "seek": 170966, "start": 1722.98, "end": 1728.6200000000001, "text": " tedious. So back end tasks make that less tedious because you just do back end tasks", "tokens": [38284, 13, 407, 646, 917, 9608, 652, 300, 1570, 38284, 570, 291, 445, 360, 646, 917, 9608], "temperature": 0.0, "avg_logprob": -0.3565826416015625, "compression_ratio": 1.7205882352941178, "no_speech_prob": 5.862612852070015e-06}, {"id": 265, "seek": 170966, "start": 1728.6200000000001, "end": 1736.22, "text": " dot HTTP dot get JSON URL JSON decoder and then you can do back end tasks dot and then", "tokens": [5893, 33283, 5893, 483, 31828, 12905, 31828, 979, 19866, 293, 550, 291, 393, 360, 646, 917, 9608, 5893, 293, 550], "temperature": 0.0, "avg_logprob": -0.3565826416015625, "compression_ratio": 1.7205882352941178, "no_speech_prob": 5.862612852070015e-06}, {"id": 266, "seek": 173622, "start": 1736.22, "end": 1742.54, "text": " you don't have that boilerplate of init update subscriptions. Exactly. Now the other thing", "tokens": [291, 500, 380, 362, 300, 39228, 37008, 295, 3157, 5623, 44951, 13, 7587, 13, 823, 264, 661, 551], "temperature": 0.0, "avg_logprob": -0.2479950677780878, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.2679239489443717e-06}, {"id": 267, "seek": 173622, "start": 1742.54, "end": 1748.78, "text": " that becomes tedious there is dealing with failure. Right. So in Elm everything is very", "tokens": [300, 3643, 38284, 456, 307, 6260, 365, 7763, 13, 1779, 13, 407, 294, 2699, 76, 1203, 307, 588], "temperature": 0.0, "avg_logprob": -0.2479950677780878, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.2679239489443717e-06}, {"id": 268, "seek": 173622, "start": 1748.78, "end": 1753.8, "text": " explicit when things can go wrong and you have to painstakingly handle every possible", "tokens": [13691, 562, 721, 393, 352, 2085, 293, 291, 362, 281, 1822, 372, 2456, 356, 4813, 633, 1944], "temperature": 0.0, "avg_logprob": -0.2479950677780878, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.2679239489443717e-06}, {"id": 269, "seek": 173622, "start": 1753.8, "end": 1760.74, "text": " error. What if the decoded value does not successfully decode to the format you expected.", "tokens": [6713, 13, 708, 498, 264, 979, 12340, 2158, 775, 406, 10727, 979, 1429, 281, 264, 7877, 291, 5176, 13], "temperature": 0.0, "avg_logprob": -0.2479950677780878, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.2679239489443717e-06}, {"id": 270, "seek": 173622, "start": 1760.74, "end": 1765.02, "text": " What if there's an HTTP error. What if there's a file reading error. All these things you", "tokens": [708, 498, 456, 311, 364, 33283, 6713, 13, 708, 498, 456, 311, 257, 3991, 3760, 6713, 13, 1057, 613, 721, 291], "temperature": 0.0, "avg_logprob": -0.2479950677780878, "compression_ratio": 1.669172932330827, "no_speech_prob": 1.2679239489443717e-06}, {"id": 271, "seek": 176502, "start": 1765.02, "end": 1770.82, "text": " have to painstakingly handle every possible failure. So compare that with writing a script", "tokens": [362, 281, 1822, 372, 2456, 356, 4813, 633, 1944, 7763, 13, 407, 6794, 300, 365, 3579, 257, 5755], "temperature": 0.0, "avg_logprob": -0.20230208247540946, "compression_ratio": 1.634703196347032, "no_speech_prob": 5.896389438930782e-07}, {"id": 272, "seek": 176502, "start": 1770.82, "end": 1778.98, "text": " in bash or node. The challenge is well what what things can fail. So it's very easy to", "tokens": [294, 46183, 420, 9984, 13, 440, 3430, 307, 731, 437, 437, 721, 393, 3061, 13, 407, 309, 311, 588, 1858, 281], "temperature": 0.0, "avg_logprob": -0.20230208247540946, "compression_ratio": 1.634703196347032, "no_speech_prob": 5.896389438930782e-07}, {"id": 273, "seek": 176502, "start": 1778.98, "end": 1784.16, "text": " just run something and let it fail. Right. It just throws an exception. The problem is", "tokens": [445, 1190, 746, 293, 718, 309, 3061, 13, 1779, 13, 467, 445, 19251, 364, 11183, 13, 440, 1154, 307], "temperature": 0.0, "avg_logprob": -0.20230208247540946, "compression_ratio": 1.634703196347032, "no_speech_prob": 5.896389438930782e-07}, {"id": 274, "seek": 176502, "start": 1784.16, "end": 1792.26, "text": " knowing where it might fail and what implicit assumptions there are and what possible runtime", "tokens": [5276, 689, 309, 1062, 3061, 293, 437, 26947, 17695, 456, 366, 293, 437, 1944, 34474], "temperature": 0.0, "avg_logprob": -0.20230208247540946, "compression_ratio": 1.634703196347032, "no_speech_prob": 5.896389438930782e-07}, {"id": 275, "seek": 179226, "start": 1792.26, "end": 1796.26, "text": " errors are lurking there. So if you want to write a quick and dirty script and you just", "tokens": [13603, 366, 35583, 5092, 456, 13, 407, 498, 291, 528, 281, 2464, 257, 1702, 293, 9360, 5755, 293, 291, 445], "temperature": 0.0, "avg_logprob": -0.22760344795558762, "compression_ratio": 1.8264462809917354, "no_speech_prob": 1.5623881211013213e-07}, {"id": 276, "seek": 179226, "start": 1796.26, "end": 1801.86, "text": " say I want to hit this API I want to grab this data I want to map the data a little", "tokens": [584, 286, 528, 281, 2045, 341, 9362, 286, 528, 281, 4444, 341, 1412, 286, 528, 281, 4471, 264, 1412, 257, 707], "temperature": 0.0, "avg_logprob": -0.22760344795558762, "compression_ratio": 1.8264462809917354, "no_speech_prob": 1.5623881211013213e-07}, {"id": 277, "seek": 179226, "start": 1801.86, "end": 1807.98, "text": " bit and I want to write some file or something like that. Right. Then writing a Node.js script", "tokens": [857, 293, 286, 528, 281, 2464, 512, 3991, 420, 746, 411, 300, 13, 1779, 13, 1396, 3579, 257, 38640, 13, 25530, 5755], "temperature": 0.0, "avg_logprob": -0.22760344795558762, "compression_ratio": 1.8264462809917354, "no_speech_prob": 1.5623881211013213e-07}, {"id": 278, "seek": 179226, "start": 1807.98, "end": 1813.0, "text": " is is great for that because it doesn't get in your way with saying hey the errors might", "tokens": [307, 307, 869, 337, 300, 570, 309, 1177, 380, 483, 294, 428, 636, 365, 1566, 4177, 264, 13603, 1062], "temperature": 0.0, "avg_logprob": -0.22760344795558762, "compression_ratio": 1.8264462809917354, "no_speech_prob": 1.5623881211013213e-07}, {"id": 279, "seek": 179226, "start": 1813.0, "end": 1817.58, "text": " be wrong. You just pull off JSON data. It doesn't get in your way with saying hey this", "tokens": [312, 2085, 13, 509, 445, 2235, 766, 31828, 1412, 13, 467, 1177, 380, 483, 294, 428, 636, 365, 1566, 4177, 341], "temperature": 0.0, "avg_logprob": -0.22760344795558762, "compression_ratio": 1.8264462809917354, "no_speech_prob": 1.5623881211013213e-07}, {"id": 280, "seek": 181758, "start": 1817.58, "end": 1824.46, "text": " HTTP request might fail. So that if you're just writing a vanilla Elm file you do have", "tokens": [33283, 5308, 1062, 3061, 13, 407, 300, 498, 291, 434, 445, 3579, 257, 17528, 2699, 76, 3991, 291, 360, 362], "temperature": 0.0, "avg_logprob": -0.25596476149284975, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.9897105352793005e-07}, {"id": 281, "seek": 181758, "start": 1824.46, "end": 1830.6599999999999, "text": " to deal with those cases and that becomes tedious. Elm Elm pages back end tasks try", "tokens": [281, 2028, 365, 729, 3331, 293, 300, 3643, 38284, 13, 2699, 76, 2699, 76, 7183, 646, 917, 9608, 853], "temperature": 0.0, "avg_logprob": -0.25596476149284975, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.9897105352793005e-07}, {"id": 282, "seek": 181758, "start": 1830.6599999999999, "end": 1839.74, "text": " to address that problem. So now with with data sources and Elm pages v2 it was more", "tokens": [281, 2985, 300, 1154, 13, 407, 586, 365, 365, 1412, 7139, 293, 2699, 76, 7183, 371, 17, 309, 390, 544], "temperature": 0.0, "avg_logprob": -0.25596476149284975, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.9897105352793005e-07}, {"id": 283, "seek": 181758, "start": 1839.74, "end": 1844.02, "text": " like we talked about earlier it was more convenient because you can just let things fail but it", "tokens": [411, 321, 2825, 466, 3071, 309, 390, 544, 10851, 570, 291, 393, 445, 718, 721, 3061, 457, 309], "temperature": 0.0, "avg_logprob": -0.25596476149284975, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.9897105352793005e-07}, {"id": 284, "seek": 184402, "start": 1844.02, "end": 1849.22, "text": " was less powerful because you couldn't handle possible failures or see where it was possible", "tokens": [390, 1570, 4005, 570, 291, 2809, 380, 4813, 1944, 20774, 420, 536, 689, 309, 390, 1944], "temperature": 0.0, "avg_logprob": -0.18779504564073352, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.6898768485116307e-07}, {"id": 285, "seek": 184402, "start": 1849.22, "end": 1854.22, "text": " for something to fail. So it was less safe and less powerful in v3 it's more safe and", "tokens": [337, 746, 281, 3061, 13, 407, 309, 390, 1570, 3273, 293, 1570, 4005, 294, 371, 18, 309, 311, 544, 3273, 293], "temperature": 0.0, "avg_logprob": -0.18779504564073352, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.6898768485116307e-07}, {"id": 286, "seek": 184402, "start": 1854.22, "end": 1863.58, "text": " powerful. It's a little a little more verbose. So Elm pages v3 provides a new abstraction", "tokens": [4005, 13, 467, 311, 257, 707, 257, 707, 544, 9595, 541, 13, 407, 2699, 76, 7183, 371, 18, 6417, 257, 777, 37765], "temperature": 0.0, "avg_logprob": -0.18779504564073352, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.6898768485116307e-07}, {"id": 287, "seek": 184402, "start": 1863.58, "end": 1870.06, "text": " called a fatal error. And this is very important for the ergonomics of being able to do a quick", "tokens": [1219, 257, 24069, 6713, 13, 400, 341, 307, 588, 1021, 337, 264, 42735, 29884, 295, 885, 1075, 281, 360, 257, 1702], "temperature": 0.0, "avg_logprob": -0.18779504564073352, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.6898768485116307e-07}, {"id": 288, "seek": 187006, "start": 1870.06, "end": 1878.02, "text": " and dirty task but it tries to achieve a balance between convenience and safety. So the way", "tokens": [293, 9360, 5633, 457, 309, 9898, 281, 4584, 257, 4772, 1296, 19283, 293, 4514, 13, 407, 264, 636], "temperature": 0.0, "avg_logprob": -0.21523782481317935, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.2098603292542975e-06}, {"id": 289, "seek": 187006, "start": 1878.02, "end": 1886.86, "text": " it works is it allows you to just give a fatal error to two Elm pages and it will just stop", "tokens": [309, 1985, 307, 309, 4045, 291, 281, 445, 976, 257, 24069, 6713, 281, 732, 2699, 76, 7183, 293, 309, 486, 445, 1590], "temperature": 0.0, "avg_logprob": -0.21523782481317935, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.2098603292542975e-06}, {"id": 290, "seek": 187006, "start": 1886.86, "end": 1892.58, "text": " and report the error. So in the case of a script it will just print out what went wrong.", "tokens": [293, 2275, 264, 6713, 13, 407, 294, 264, 1389, 295, 257, 5755, 309, 486, 445, 4482, 484, 437, 1437, 2085, 13], "temperature": 0.0, "avg_logprob": -0.21523782481317935, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.2098603292542975e-06}, {"id": 291, "seek": 187006, "start": 1892.58, "end": 1899.1799999999998, "text": " So if you say backend task dot HTTP dot get JSON and it gives you a 404 error what you", "tokens": [407, 498, 291, 584, 38087, 5633, 5893, 33283, 5893, 483, 31828, 293, 309, 2709, 291, 257, 3356, 19, 6713, 437, 291], "temperature": 0.0, "avg_logprob": -0.21523782481317935, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.2098603292542975e-06}, {"id": 292, "seek": 189918, "start": 1899.18, "end": 1907.02, "text": " can do it you can do backend task dot allow fatal and that is going to take your HTTP", "tokens": [393, 360, 309, 291, 393, 360, 38087, 5633, 5893, 2089, 24069, 293, 300, 307, 516, 281, 747, 428, 33283], "temperature": 0.0, "avg_logprob": -0.251434395112187, "compression_ratio": 1.745, "no_speech_prob": 3.256308787058515e-07}, {"id": 293, "seek": 189918, "start": 1907.02, "end": 1915.5800000000002, "text": " backend task and and give you a backend task fatal error your decoded data. So that fatal", "tokens": [38087, 5633, 293, 293, 976, 291, 257, 38087, 5633, 24069, 6713, 428, 979, 12340, 1412, 13, 407, 300, 24069], "temperature": 0.0, "avg_logprob": -0.251434395112187, "compression_ratio": 1.745, "no_speech_prob": 3.256308787058515e-07}, {"id": 294, "seek": 189918, "start": 1915.5800000000002, "end": 1922.78, "text": " error contains the the information that the Elm pages framework needs to print an error", "tokens": [6713, 8306, 264, 264, 1589, 300, 264, 2699, 76, 7183, 8388, 2203, 281, 4482, 364, 6713], "temperature": 0.0, "avg_logprob": -0.251434395112187, "compression_ratio": 1.745, "no_speech_prob": 3.256308787058515e-07}, {"id": 295, "seek": 189918, "start": 1922.78, "end": 1928.8200000000002, "text": " message describing what went wrong. So if you do Elm pages run hello and then you hit", "tokens": [3636, 16141, 437, 1437, 2085, 13, 407, 498, 291, 360, 2699, 76, 7183, 1190, 7751, 293, 550, 291, 2045], "temperature": 0.0, "avg_logprob": -0.251434395112187, "compression_ratio": 1.745, "no_speech_prob": 3.256308787058515e-07}, {"id": 296, "seek": 192882, "start": 1928.82, "end": 1933.54, "text": " your API with allow fatal it's just going to print an error message saying hey I was", "tokens": [428, 9362, 365, 2089, 24069, 309, 311, 445, 516, 281, 4482, 364, 6713, 3636, 1566, 4177, 286, 390], "temperature": 0.0, "avg_logprob": -0.23586380347776947, "compression_ratio": 1.6383928571428572, "no_speech_prob": 6.17936620983528e-07}, {"id": 297, "seek": 192882, "start": 1933.54, "end": 1940.78, "text": " running this HTTP backend task something went wrong. There was a 404 error and because you", "tokens": [2614, 341, 33283, 38087, 5633, 746, 1437, 2085, 13, 821, 390, 257, 3356, 19, 6713, 293, 570, 291], "temperature": 0.0, "avg_logprob": -0.23586380347776947, "compression_ratio": 1.6383928571428572, "no_speech_prob": 6.17936620983528e-07}, {"id": 298, "seek": 192882, "start": 1940.78, "end": 1945.34, "text": " you opted out of handling and recovering from that error.", "tokens": [291, 40768, 484, 295, 13175, 293, 29180, 490, 300, 6713, 13], "temperature": 0.0, "avg_logprob": -0.23586380347776947, "compression_ratio": 1.6383928571428572, "no_speech_prob": 6.17936620983528e-07}, {"id": 299, "seek": 192882, "start": 1945.34, "end": 1948.54, "text": " So what happens if you don't write allow fatal.", "tokens": [407, 437, 2314, 498, 291, 500, 380, 2464, 2089, 24069, 13], "temperature": 0.0, "avg_logprob": -0.23586380347776947, "compression_ratio": 1.6383928571428572, "no_speech_prob": 6.17936620983528e-07}, {"id": 300, "seek": 192882, "start": 1948.54, "end": 1955.1799999999998, "text": " If you don't write allow fatal in the case of backend task dot HTTP dot get JSON then", "tokens": [759, 291, 500, 380, 2464, 2089, 24069, 294, 264, 1389, 295, 38087, 5633, 5893, 33283, 5893, 483, 31828, 550], "temperature": 0.0, "avg_logprob": -0.23586380347776947, "compression_ratio": 1.6383928571428572, "no_speech_prob": 6.17936620983528e-07}, {"id": 301, "seek": 195518, "start": 1955.18, "end": 1962.1000000000001, "text": " the types just won't line up because the error type in that get JSON function returns a backend", "tokens": [264, 3467, 445, 1582, 380, 1622, 493, 570, 264, 6713, 2010, 294, 300, 483, 31828, 2445, 11247, 257, 38087], "temperature": 0.0, "avg_logprob": -0.21114629857680378, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.8553866709680733e-07}, {"id": 302, "seek": 195518, "start": 1962.1000000000001, "end": 1970.94, "text": " task with with the error having a fatal error and a recoverable error data. So if you wanted", "tokens": [5633, 365, 365, 264, 6713, 1419, 257, 24069, 6713, 293, 257, 8114, 712, 6713, 1412, 13, 407, 498, 291, 1415], "temperature": 0.0, "avg_logprob": -0.21114629857680378, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.8553866709680733e-07}, {"id": 303, "seek": 195518, "start": 1970.94, "end": 1977.02, "text": " to recover from it then you can do backend task dot map error and then you can pull off", "tokens": [281, 8114, 490, 309, 550, 291, 393, 360, 38087, 5633, 5893, 4471, 6713, 293, 550, 291, 393, 2235, 766], "temperature": 0.0, "avg_logprob": -0.21114629857680378, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.8553866709680733e-07}, {"id": 304, "seek": 195518, "start": 1977.02, "end": 1983.5800000000002, "text": " that recoverable data from that record which is going to be a nice structured HTTP error", "tokens": [300, 8114, 712, 1412, 490, 300, 2136, 597, 307, 516, 281, 312, 257, 1481, 18519, 33283, 6713], "temperature": 0.0, "avg_logprob": -0.21114629857680378, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.8553866709680733e-07}, {"id": 305, "seek": 198358, "start": 1983.58, "end": 1989.54, "text": " which could be your JSON decoding error it could be bad body timeout. And so if you want", "tokens": [597, 727, 312, 428, 31828, 979, 8616, 6713, 309, 727, 312, 1578, 1772, 565, 346, 13, 400, 370, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.29520582116168476, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.411060323647689e-06}, {"id": 306, "seek": 198358, "start": 1989.54, "end": 1997.1399999999999, "text": " to say if it's a timeout try it again you can do that. You can do backend task on error", "tokens": [281, 584, 498, 309, 311, 257, 565, 346, 853, 309, 797, 291, 393, 360, 300, 13, 509, 393, 360, 38087, 5633, 322, 6713], "temperature": 0.0, "avg_logprob": -0.29520582116168476, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.411060323647689e-06}, {"id": 307, "seek": 198358, "start": 1997.1399999999999, "end": 2005.34, "text": " case error dot recoverable timeout try again or if it is you know whatever else and for", "tokens": [1389, 6713, 5893, 8114, 712, 565, 346, 853, 797, 420, 498, 309, 307, 291, 458, 2035, 1646, 293, 337], "temperature": 0.0, "avg_logprob": -0.29520582116168476, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.411060323647689e-06}, {"id": 308, "seek": 198358, "start": 2005.34, "end": 2010.3799999999999, "text": " all of those different cases that structured data you can choose explicitly how to handle", "tokens": [439, 295, 729, 819, 3331, 300, 18519, 1412, 291, 393, 2826, 20803, 577, 281, 4813], "temperature": 0.0, "avg_logprob": -0.29520582116168476, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.411060323647689e-06}, {"id": 309, "seek": 198358, "start": 2010.3799999999999, "end": 2011.3799999999999, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.29520582116168476, "compression_ratio": 1.6651162790697673, "no_speech_prob": 7.411060323647689e-06}, {"id": 310, "seek": 201138, "start": 2011.38, "end": 2016.98, "text": " So if you don't write allow fatal then you have to do something. And what is that something", "tokens": [407, 498, 291, 500, 380, 2464, 2089, 24069, 550, 291, 362, 281, 360, 746, 13, 400, 437, 307, 300, 746], "temperature": 0.0, "avg_logprob": -0.2795358579985949, "compression_ratio": 1.7766990291262137, "no_speech_prob": 7.934301038403646e-07}, {"id": 311, "seek": 201138, "start": 2016.98, "end": 2018.18, "text": " that you have to do.", "tokens": [300, 291, 362, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.2795358579985949, "compression_ratio": 1.7766990291262137, "no_speech_prob": 7.934301038403646e-07}, {"id": 312, "seek": 201138, "start": 2018.18, "end": 2026.5800000000002, "text": " If you don't. So the at the end of the day the Elm pages expects when you say script", "tokens": [759, 291, 500, 380, 13, 407, 264, 412, 264, 917, 295, 264, 786, 264, 2699, 76, 7183, 33280, 562, 291, 584, 5755], "temperature": 0.0, "avg_logprob": -0.2795358579985949, "compression_ratio": 1.7766990291262137, "no_speech_prob": 7.934301038403646e-07}, {"id": 313, "seek": 201138, "start": 2026.5800000000002, "end": 2032.66, "text": " dot without CLI options and you give it a backend task the type of that backend task", "tokens": [5893, 1553, 12855, 40, 3956, 293, 291, 976, 309, 257, 38087, 5633, 264, 2010, 295, 300, 38087, 5633], "temperature": 0.0, "avg_logprob": -0.2795358579985949, "compression_ratio": 1.7766990291262137, "no_speech_prob": 7.934301038403646e-07}, {"id": 314, "seek": 201138, "start": 2032.66, "end": 2039.72, "text": " needs to be the error type can be a fatal error and the data type needs to be unit.", "tokens": [2203, 281, 312, 264, 6713, 2010, 393, 312, 257, 24069, 6713, 293, 264, 1412, 2010, 2203, 281, 312, 4985, 13], "temperature": 0.0, "avg_logprob": -0.2795358579985949, "compression_ratio": 1.7766990291262137, "no_speech_prob": 7.934301038403646e-07}, {"id": 315, "seek": 203972, "start": 2039.72, "end": 2045.58, "text": " So so at the end of the day you you need to give it either no possibility of an error", "tokens": [407, 370, 412, 264, 917, 295, 264, 786, 291, 291, 643, 281, 976, 309, 2139, 572, 7959, 295, 364, 6713], "temperature": 0.0, "avg_logprob": -0.23690990968184036, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.8738684275376727e-06}, {"id": 316, "seek": 203972, "start": 2045.58, "end": 2054.14, "text": " or a fatal error if anything. So doing allow fatal just throws away that recoverable error", "tokens": [420, 257, 24069, 6713, 498, 1340, 13, 407, 884, 2089, 24069, 445, 19251, 1314, 300, 8114, 712, 6713], "temperature": 0.0, "avg_logprob": -0.23690990968184036, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.8738684275376727e-06}, {"id": 317, "seek": 203972, "start": 2054.14, "end": 2059.98, "text": " data that has the nicely structured error whereas allow fit. Yeah allow fatal just grabs", "tokens": [1412, 300, 575, 264, 9594, 18519, 6713, 9735, 2089, 3318, 13, 865, 2089, 24069, 445, 30028], "temperature": 0.0, "avg_logprob": -0.23690990968184036, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.8738684275376727e-06}, {"id": 318, "seek": 203972, "start": 2059.98, "end": 2066.7, "text": " that fatal error and passes it through. But if you do on error then you can continue with", "tokens": [300, 24069, 6713, 293, 11335, 309, 807, 13, 583, 498, 291, 360, 322, 6713, 550, 291, 393, 2354, 365], "temperature": 0.0, "avg_logprob": -0.23690990968184036, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.8738684275376727e-06}, {"id": 319, "seek": 203972, "start": 2066.7, "end": 2067.7, "text": " something else.", "tokens": [746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.23690990968184036, "compression_ratio": 1.7175925925925926, "no_speech_prob": 1.8738684275376727e-06}, {"id": 320, "seek": 206770, "start": 2067.7, "end": 2073.66, "text": " OK. So you have to transform the error type in a way that will print an error or succeed", "tokens": [2264, 13, 407, 291, 362, 281, 4088, 264, 6713, 2010, 294, 257, 636, 300, 486, 4482, 364, 6713, 420, 7754], "temperature": 0.0, "avg_logprob": -0.2621911479308542, "compression_ratio": 1.7246963562753037, "no_speech_prob": 2.4966570322249027e-07}, {"id": 321, "seek": 206770, "start": 2073.66, "end": 2080.14, "text": " I guess. And if you don't then you have to write to use allow fatal.", "tokens": [286, 2041, 13, 400, 498, 291, 500, 380, 550, 291, 362, 281, 2464, 281, 764, 2089, 24069, 13], "temperature": 0.0, "avg_logprob": -0.2621911479308542, "compression_ratio": 1.7246963562753037, "no_speech_prob": 2.4966570322249027e-07}, {"id": 322, "seek": 206770, "start": 2080.14, "end": 2085.2999999999997, "text": " Right. At the end of the day that's the type of error that you can give it. So you can't", "tokens": [1779, 13, 1711, 264, 917, 295, 264, 786, 300, 311, 264, 2010, 295, 6713, 300, 291, 393, 976, 309, 13, 407, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.2621911479308542, "compression_ratio": 1.7246963562753037, "no_speech_prob": 2.4966570322249027e-07}, {"id": 323, "seek": 206770, "start": 2085.2999999999997, "end": 2091.3399999999997, "text": " you can't just give obviously any error type to Elm pages and have it do something because", "tokens": [291, 393, 380, 445, 976, 2745, 604, 6713, 2010, 281, 2699, 76, 7183, 293, 362, 309, 360, 746, 570], "temperature": 0.0, "avg_logprob": -0.2621911479308542, "compression_ratio": 1.7246963562753037, "no_speech_prob": 2.4966570322249027e-07}, {"id": 324, "seek": 206770, "start": 2091.3399999999997, "end": 2097.22, "text": " Elm doesn't let you have like variable return types for something. It's like so it needs", "tokens": [2699, 76, 1177, 380, 718, 291, 362, 411, 7006, 2736, 3467, 337, 746, 13, 467, 311, 411, 370, 309, 2203], "temperature": 0.0, "avg_logprob": -0.2621911479308542, "compression_ratio": 1.7246963562753037, "no_speech_prob": 2.4966570322249027e-07}, {"id": 325, "seek": 209722, "start": 2097.22, "end": 2105.3399999999997, "text": " to be returning back in task fatal error and you can and unit. And so you could you could", "tokens": [281, 312, 12678, 646, 294, 5633, 24069, 6713, 293, 291, 393, 293, 4985, 13, 400, 370, 291, 727, 291, 727], "temperature": 0.0, "avg_logprob": -0.2852397986820766, "compression_ratio": 1.972972972972973, "no_speech_prob": 2.726462071223068e-06}, {"id": 326, "seek": 209722, "start": 2105.3399999999997, "end": 2112.2599999999998, "text": " define a back end task that has whatever error type just like you know a regular Elm task", "tokens": [6964, 257, 646, 917, 5633, 300, 575, 2035, 6713, 2010, 445, 411, 291, 458, 257, 3890, 2699, 76, 5633], "temperature": 0.0, "avg_logprob": -0.2852397986820766, "compression_ratio": 1.972972972972973, "no_speech_prob": 2.726462071223068e-06}, {"id": 327, "seek": 209722, "start": 2112.2599999999998, "end": 2116.7, "text": " have an error type. You can map that error type you can do task dot map error. You can", "tokens": [362, 364, 6713, 2010, 13, 509, 393, 4471, 300, 6713, 2010, 291, 393, 360, 5633, 5893, 4471, 6713, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.2852397986820766, "compression_ratio": 1.972972972972973, "no_speech_prob": 2.726462071223068e-06}, {"id": 328, "seek": 209722, "start": 2116.7, "end": 2120.4199999999996, "text": " also do back and test that map error. It's the same thing whatever the error type is", "tokens": [611, 360, 646, 293, 1500, 300, 4471, 6713, 13, 467, 311, 264, 912, 551, 2035, 264, 6713, 2010, 307], "temperature": 0.0, "avg_logprob": -0.2852397986820766, "compression_ratio": 1.972972972972973, "no_speech_prob": 2.726462071223068e-06}, {"id": 329, "seek": 209722, "start": 2120.4199999999996, "end": 2125.4599999999996, "text": " along the way doesn't concern Elm pages. You can you can do whatever you want. You can", "tokens": [2051, 264, 636, 1177, 380, 3136, 2699, 76, 7183, 13, 509, 393, 291, 393, 360, 2035, 291, 528, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.2852397986820766, "compression_ratio": 1.972972972972973, "no_speech_prob": 2.726462071223068e-06}, {"id": 330, "seek": 212546, "start": 2125.46, "end": 2131.7400000000002, "text": " have whatever structured error data. It's just that if you have the possibility of a", "tokens": [362, 2035, 18519, 6713, 1412, 13, 467, 311, 445, 300, 498, 291, 362, 264, 7959, 295, 257], "temperature": 0.0, "avg_logprob": -0.23790490350057913, "compression_ratio": 1.6495327102803738, "no_speech_prob": 2.726456386881182e-06}, {"id": 331, "seek": 212546, "start": 2131.7400000000002, "end": 2137.46, "text": " failure you have to turn that error type into a fatal error at the end of the day. Right.", "tokens": [7763, 291, 362, 281, 1261, 300, 6713, 2010, 666, 257, 24069, 6713, 412, 264, 917, 295, 264, 786, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.23790490350057913, "compression_ratio": 1.6495327102803738, "no_speech_prob": 2.726456386881182e-06}, {"id": 332, "seek": 212546, "start": 2137.46, "end": 2146.18, "text": " OK. So basically the fatal error concept in Elm pages is a way of saying hey let's have", "tokens": [2264, 13, 407, 1936, 264, 24069, 6713, 3410, 294, 2699, 76, 7183, 307, 257, 636, 295, 1566, 4177, 718, 311, 362], "temperature": 0.0, "avg_logprob": -0.23790490350057913, "compression_ratio": 1.6495327102803738, "no_speech_prob": 2.726456386881182e-06}, {"id": 333, "seek": 212546, "start": 2146.18, "end": 2153.46, "text": " safety. Let's have a balance between safety and convenience because for the sake of safety", "tokens": [4514, 13, 961, 311, 362, 257, 4772, 1296, 4514, 293, 19283, 570, 337, 264, 9717, 295, 4514], "temperature": 0.0, "avg_logprob": -0.23790490350057913, "compression_ratio": 1.6495327102803738, "no_speech_prob": 2.726456386881182e-06}, {"id": 334, "seek": 215346, "start": 2153.46, "end": 2159.58, "text": " we could. So in the design of this I could have just had a single type variable for the", "tokens": [321, 727, 13, 407, 294, 264, 1715, 295, 341, 286, 727, 362, 445, 632, 257, 2167, 2010, 7006, 337, 264], "temperature": 0.0, "avg_logprob": -0.1754364084314417, "compression_ratio": 1.8464730290456433, "no_speech_prob": 2.6841887574846623e-06}, {"id": 335, "seek": 215346, "start": 2159.58, "end": 2166.14, "text": " data not had an error type variable and just let you say oh yeah it can fail or I want", "tokens": [1412, 406, 632, 364, 6713, 2010, 7006, 293, 445, 718, 291, 584, 1954, 1338, 309, 393, 3061, 420, 286, 528], "temperature": 0.0, "avg_logprob": -0.1754364084314417, "compression_ratio": 1.8464730290456433, "no_speech_prob": 2.6841887574846623e-06}, {"id": 336, "seek": 215346, "start": 2166.14, "end": 2172.86, "text": " to recover from the failure. What I wanted to have was I wanted to make it very explicit", "tokens": [281, 8114, 490, 264, 7763, 13, 708, 286, 1415, 281, 362, 390, 286, 1415, 281, 652, 309, 588, 13691], "temperature": 0.0, "avg_logprob": -0.1754364084314417, "compression_ratio": 1.8464730290456433, "no_speech_prob": 2.6841887574846623e-06}, {"id": 337, "seek": 215346, "start": 2172.86, "end": 2178.42, "text": " where failures happen and if there's no error type variable there's no possibility of failure.", "tokens": [689, 20774, 1051, 293, 498, 456, 311, 572, 6713, 2010, 7006, 456, 311, 572, 7959, 295, 7763, 13], "temperature": 0.0, "avg_logprob": -0.1754364084314417, "compression_ratio": 1.8464730290456433, "no_speech_prob": 2.6841887574846623e-06}, {"id": 338, "seek": 215346, "start": 2178.42, "end": 2182.34, "text": " So you can tell just by looking at the types if it's possible for something to fail or", "tokens": [407, 291, 393, 980, 445, 538, 1237, 412, 264, 3467, 498, 309, 311, 1944, 337, 746, 281, 3061, 420], "temperature": 0.0, "avg_logprob": -0.1754364084314417, "compression_ratio": 1.8464730290456433, "no_speech_prob": 2.6841887574846623e-06}, {"id": 339, "seek": 218234, "start": 2182.34, "end": 2188.46, "text": " not and how it could fail. Now the fatal error type is a very generic failure that doesn't", "tokens": [406, 293, 577, 309, 727, 3061, 13, 823, 264, 24069, 6713, 2010, 307, 257, 588, 19577, 7763, 300, 1177, 380], "temperature": 0.0, "avg_logprob": -0.20674304242404, "compression_ratio": 1.6356877323420074, "no_speech_prob": 1.084512746274413e-06}, {"id": 340, "seek": 218234, "start": 2188.46, "end": 2193.7400000000002, "text": " contain any useful information for you. So it's just saying that's the that's the balance", "tokens": [5304, 604, 4420, 1589, 337, 291, 13, 407, 309, 311, 445, 1566, 300, 311, 264, 300, 311, 264, 4772], "temperature": 0.0, "avg_logprob": -0.20674304242404, "compression_ratio": 1.6356877323420074, "no_speech_prob": 1.084512746274413e-06}, {"id": 341, "seek": 218234, "start": 2193.7400000000002, "end": 2197.46, "text": " between the safety and the convenience. You know it can fail but you can't do anything", "tokens": [1296, 264, 4514, 293, 264, 19283, 13, 509, 458, 309, 393, 3061, 457, 291, 393, 380, 360, 1340], "temperature": 0.0, "avg_logprob": -0.20674304242404, "compression_ratio": 1.6356877323420074, "no_speech_prob": 1.084512746274413e-06}, {"id": 342, "seek": 218234, "start": 2197.46, "end": 2201.7400000000002, "text": " meaningful to recover from it at that point because you have to kind of choose when you", "tokens": [10995, 281, 8114, 490, 309, 412, 300, 935, 570, 291, 362, 281, 733, 295, 2826, 562, 291], "temperature": 0.0, "avg_logprob": -0.20674304242404, "compression_ratio": 1.6356877323420074, "no_speech_prob": 1.084512746274413e-06}, {"id": 343, "seek": 218234, "start": 2201.7400000000002, "end": 2208.5, "text": " get that data. So that's so the core APIs and Elm pages like HTTP reading from files", "tokens": [483, 300, 1412, 13, 407, 300, 311, 370, 264, 4965, 21445, 293, 2699, 76, 7183, 411, 33283, 3760, 490, 7098], "temperature": 0.0, "avg_logprob": -0.20674304242404, "compression_ratio": 1.6356877323420074, "no_speech_prob": 1.084512746274413e-06}, {"id": 344, "seek": 220850, "start": 2208.5, "end": 2213.94, "text": " writing to files things that can fail. They give you these two different bits of data", "tokens": [3579, 281, 7098, 721, 300, 393, 3061, 13, 814, 976, 291, 613, 732, 819, 9239, 295, 1412], "temperature": 0.0, "avg_logprob": -0.20038694601792556, "compression_ratio": 1.7470355731225296, "no_speech_prob": 4.664443906676752e-07}, {"id": 345, "seek": 220850, "start": 2213.94, "end": 2219.26, "text": " where you can choose I want to either recover or let the fatal exception through the fatal", "tokens": [689, 291, 393, 2826, 286, 528, 281, 2139, 8114, 420, 718, 264, 24069, 11183, 807, 264, 24069], "temperature": 0.0, "avg_logprob": -0.20038694601792556, "compression_ratio": 1.7470355731225296, "no_speech_prob": 4.664443906676752e-07}, {"id": 346, "seek": 220850, "start": 2219.26, "end": 2226.5, "text": " error through. So the point of that design is that you can have the convenience of just", "tokens": [6713, 807, 13, 407, 264, 935, 295, 300, 1715, 307, 300, 291, 393, 362, 264, 19283, 295, 445], "temperature": 0.0, "avg_logprob": -0.20038694601792556, "compression_ratio": 1.7470355731225296, "no_speech_prob": 4.664443906676752e-07}, {"id": 347, "seek": 220850, "start": 2226.5, "end": 2232.74, "text": " saying yeah just give this message to the framework and let it fail or you can recover", "tokens": [1566, 1338, 445, 976, 341, 3636, 281, 264, 8388, 293, 718, 309, 3061, 420, 291, 393, 8114], "temperature": 0.0, "avg_logprob": -0.20038694601792556, "compression_ratio": 1.7470355731225296, "no_speech_prob": 4.664443906676752e-07}, {"id": 348, "seek": 220850, "start": 2232.74, "end": 2237.62, "text": " from it. So it's trying to give you an ergonomic way to easily just say I don't care about", "tokens": [490, 309, 13, 407, 309, 311, 1382, 281, 976, 291, 364, 42735, 21401, 636, 281, 3612, 445, 584, 286, 500, 380, 1127, 466], "temperature": 0.0, "avg_logprob": -0.20038694601792556, "compression_ratio": 1.7470355731225296, "no_speech_prob": 4.664443906676752e-07}, {"id": 349, "seek": 223762, "start": 2237.62, "end": 2243.94, "text": " this error or a way to recover from it while knowing explicitly whether failure is possible", "tokens": [341, 6713, 420, 257, 636, 281, 8114, 490, 309, 1339, 5276, 20803, 1968, 7763, 307, 1944], "temperature": 0.0, "avg_logprob": -0.2358358171251085, "compression_ratio": 1.5913043478260869, "no_speech_prob": 7.81147036832408e-07}, {"id": 350, "seek": 223762, "start": 2243.94, "end": 2249.7799999999997, "text": " just based on the types. Right. And at the end of the script if it fails then it's always", "tokens": [445, 2361, 322, 264, 3467, 13, 1779, 13, 400, 412, 264, 917, 295, 264, 5755, 498, 309, 18199, 550, 309, 311, 1009], "temperature": 0.0, "avg_logprob": -0.2358358171251085, "compression_ratio": 1.5913043478260869, "no_speech_prob": 7.81147036832408e-07}, {"id": 351, "seek": 223762, "start": 2249.7799999999997, "end": 2256.1, "text": " going to have some kind of nice error message or a reasonably nice error message I'm hoping.", "tokens": [516, 281, 362, 512, 733, 295, 1481, 6713, 3636, 420, 257, 23551, 1481, 6713, 3636, 286, 478, 7159, 13], "temperature": 0.0, "avg_logprob": -0.2358358171251085, "compression_ratio": 1.5913043478260869, "no_speech_prob": 7.81147036832408e-07}, {"id": 352, "seek": 223762, "start": 2256.1, "end": 2262.54, "text": " Oh yeah. I mean that's a major goal of Elm pages for sure is to you know strive for quality", "tokens": [876, 1338, 13, 286, 914, 300, 311, 257, 2563, 3387, 295, 2699, 76, 7183, 337, 988, 307, 281, 291, 458, 23829, 337, 3125], "temperature": 0.0, "avg_logprob": -0.2358358171251085, "compression_ratio": 1.5913043478260869, "no_speech_prob": 7.81147036832408e-07}, {"id": 353, "seek": 226254, "start": 2262.54, "end": 2268.46, "text": " we expect in Elm community for error messages. OK. So what I like about this is that you", "tokens": [321, 2066, 294, 2699, 76, 1768, 337, 6713, 7897, 13, 2264, 13, 407, 437, 286, 411, 466, 341, 307, 300, 291], "temperature": 0.0, "avg_logprob": -0.24588189806256974, "compression_ratio": 1.5705882352941176, "no_speech_prob": 8.579131076658086e-07}, {"id": 354, "seek": 226254, "start": 2268.46, "end": 2276.3, "text": " said as you say like you can identify what is going to succeed and what is what can fail.", "tokens": [848, 382, 291, 584, 411, 291, 393, 5876, 437, 307, 516, 281, 7754, 293, 437, 307, 437, 393, 3061, 13], "temperature": 0.0, "avg_logprob": -0.24588189806256974, "compression_ratio": 1.5705882352941176, "no_speech_prob": 8.579131076658086e-07}, {"id": 355, "seek": 226254, "start": 2276.3, "end": 2284.1, "text": " So once you so once this script is compiled by Elm pages if it compiles then it's either", "tokens": [407, 1564, 291, 370, 1564, 341, 5755, 307, 36548, 538, 2699, 76, 7183, 498, 309, 715, 4680, 550, 309, 311, 2139], "temperature": 0.0, "avg_logprob": -0.24588189806256974, "compression_ratio": 1.5705882352941176, "no_speech_prob": 8.579131076658086e-07}, {"id": 356, "seek": 228410, "start": 2284.1, "end": 2293.74, "text": " going to fail in the intended way or well in a intended way in a or it's going to succeed", "tokens": [516, 281, 3061, 294, 264, 10226, 636, 420, 731, 294, 257, 10226, 636, 294, 257, 420, 309, 311, 516, 281, 7754], "temperature": 0.0, "avg_logprob": -0.21711786677328387, "compression_ratio": 1.8677248677248677, "no_speech_prob": 3.806921711202449e-07}, {"id": 357, "seek": 228410, "start": 2293.74, "end": 2298.58, "text": " in the intended way. Yeah. But it's never going to fail because of how you wrote the", "tokens": [294, 264, 10226, 636, 13, 865, 13, 583, 309, 311, 1128, 516, 281, 3061, 570, 295, 577, 291, 4114, 264], "temperature": 0.0, "avg_logprob": -0.21711786677328387, "compression_ratio": 1.8677248677248677, "no_speech_prob": 3.806921711202449e-07}, {"id": 358, "seek": 228410, "start": 2298.58, "end": 2304.66, "text": " code. So something that happens a lot to people at least to me but I'm guessing to a lot of", "tokens": [3089, 13, 407, 746, 300, 2314, 257, 688, 281, 561, 412, 1935, 281, 385, 457, 286, 478, 17939, 281, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.21711786677328387, "compression_ratio": 1.8677248677248677, "no_speech_prob": 3.806921711202449e-07}, {"id": 359, "seek": 228410, "start": 2304.66, "end": 2309.14, "text": " people who write scripts is that the script is going to fail because you did something", "tokens": [561, 567, 2464, 23294, 307, 300, 264, 5755, 307, 516, 281, 3061, 570, 291, 630, 746], "temperature": 0.0, "avg_logprob": -0.21711786677328387, "compression_ratio": 1.8677248677248677, "no_speech_prob": 3.806921711202449e-07}, {"id": 360, "seek": 230914, "start": 2309.14, "end": 2314.8599999999997, "text": " stupid in your script like for instance you write a Node.js script and you mistyped a", "tokens": [6631, 294, 428, 5755, 411, 337, 5197, 291, 2464, 257, 38640, 13, 25530, 5755, 293, 291, 3544, 88, 3452, 257], "temperature": 0.0, "avg_logprob": -0.24252149991900007, "compression_ratio": 1.785425101214575, "no_speech_prob": 5.71498333101772e-07}, {"id": 361, "seek": 230914, "start": 2314.8599999999997, "end": 2318.7799999999997, "text": " function name. So that's not going to happen anymore. The only thing the only reason that", "tokens": [2445, 1315, 13, 407, 300, 311, 406, 516, 281, 1051, 3602, 13, 440, 787, 551, 264, 787, 1778, 300], "temperature": 0.0, "avg_logprob": -0.24252149991900007, "compression_ratio": 1.785425101214575, "no_speech_prob": 5.71498333101772e-07}, {"id": 362, "seek": 230914, "start": 2318.7799999999997, "end": 2326.22, "text": " is going to fail is because some operation that's touched the external system like the", "tokens": [307, 516, 281, 3061, 307, 570, 512, 6916, 300, 311, 9828, 264, 8320, 1185, 411, 264], "temperature": 0.0, "avg_logprob": -0.24252149991900007, "compression_ratio": 1.785425101214575, "no_speech_prob": 5.71498333101772e-07}, {"id": 363, "seek": 230914, "start": 2326.22, "end": 2333.5, "text": " file system or made requests across HTTP failed for some reason. But it's never going to fail", "tokens": [3991, 1185, 420, 1027, 12475, 2108, 33283, 7612, 337, 512, 1778, 13, 583, 309, 311, 1128, 516, 281, 3061], "temperature": 0.0, "avg_logprob": -0.24252149991900007, "compression_ratio": 1.785425101214575, "no_speech_prob": 5.71498333101772e-07}, {"id": 364, "seek": 230914, "start": 2333.5, "end": 2338.7799999999997, "text": " because of how you wrote the code. So that is quite nice. So that is one of the plus", "tokens": [570, 295, 577, 291, 4114, 264, 3089, 13, 407, 300, 307, 1596, 1481, 13, 407, 300, 307, 472, 295, 264, 1804], "temperature": 0.0, "avg_logprob": -0.24252149991900007, "compression_ratio": 1.785425101214575, "no_speech_prob": 5.71498333101772e-07}, {"id": 365, "seek": 233878, "start": 2338.78, "end": 2346.1400000000003, "text": " sides that I find in using Elm pages scripts. But do you see other ones compared to writing", "tokens": [4881, 300, 286, 915, 294, 1228, 2699, 76, 7183, 23294, 13, 583, 360, 291, 536, 661, 2306, 5347, 281, 3579], "temperature": 0.0, "avg_logprob": -0.28567985166986304, "compression_ratio": 1.6728971962616823, "no_speech_prob": 5.594130925601348e-06}, {"id": 366, "seek": 233878, "start": 2346.1400000000003, "end": 2352.5400000000004, "text": " because you compared it previously with writing a script in Elm without Elm pages which yeah", "tokens": [570, 291, 5347, 309, 8046, 365, 3579, 257, 5755, 294, 2699, 76, 1553, 2699, 76, 7183, 597, 1338], "temperature": 0.0, "avg_logprob": -0.28567985166986304, "compression_ratio": 1.6728971962616823, "no_speech_prob": 5.594130925601348e-06}, {"id": 367, "seek": 233878, "start": 2352.5400000000004, "end": 2357.1000000000004, "text": " sounds painful. Some people have done it. It's actually not that bad in practice. I", "tokens": [3263, 11697, 13, 2188, 561, 362, 1096, 309, 13, 467, 311, 767, 406, 300, 1578, 294, 3124, 13, 286], "temperature": 0.0, "avg_logprob": -0.28567985166986304, "compression_ratio": 1.6728971962616823, "no_speech_prob": 5.594130925601348e-06}, {"id": 368, "seek": 233878, "start": 2357.1000000000004, "end": 2362.78, "text": " have done so myself obviously. But how does it compare to writing something in JavaScript", "tokens": [362, 1096, 370, 2059, 2745, 13, 583, 577, 775, 309, 6794, 281, 3579, 746, 294, 15778], "temperature": 0.0, "avg_logprob": -0.28567985166986304, "compression_ratio": 1.6728971962616823, "no_speech_prob": 5.594130925601348e-06}, {"id": 369, "seek": 236278, "start": 2362.78, "end": 2368.82, "text": " or in Bash or Perl or Python or whatever. When would you do one of those or when would", "tokens": [420, 294, 43068, 420, 3026, 75, 420, 15329, 420, 2035, 13, 1133, 576, 291, 360, 472, 295, 729, 420, 562, 576], "temperature": 0.0, "avg_logprob": -0.27127623558044434, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.854067578387912e-06}, {"id": 370, "seek": 236278, "start": 2368.82, "end": 2374.98, "text": " you use Elm pages scripts. Right. Yeah. So you know there's there's a tradeoff in that", "tokens": [291, 764, 2699, 76, 7183, 23294, 13, 1779, 13, 865, 13, 407, 291, 458, 456, 311, 456, 311, 257, 4923, 4506, 294, 300], "temperature": 0.0, "avg_logprob": -0.27127623558044434, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.854067578387912e-06}, {"id": 371, "seek": 236278, "start": 2374.98, "end": 2380.6600000000003, "text": " you know again in the context of Bash or Node.js you don't you don't know where possible failures", "tokens": [291, 458, 797, 294, 264, 4319, 295, 43068, 420, 38640, 13, 25530, 291, 500, 380, 291, 500, 380, 458, 689, 1944, 20774], "temperature": 0.0, "avg_logprob": -0.27127623558044434, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.854067578387912e-06}, {"id": 372, "seek": 236278, "start": 2380.6600000000003, "end": 2387.02, "text": " lurk because you know even if you're writing a script in TypeScript you don't know where", "tokens": [35583, 74, 570, 291, 458, 754, 498, 291, 434, 3579, 257, 5755, 294, 15576, 14237, 291, 500, 380, 458, 689], "temperature": 0.0, "avg_logprob": -0.27127623558044434, "compression_ratio": 1.7061611374407584, "no_speech_prob": 6.854067578387912e-06}, {"id": 373, "seek": 238702, "start": 2387.02, "end": 2393.98, "text": " you might have gotten some any data back that is actually leaking possibly incorrect type", "tokens": [291, 1062, 362, 5768, 512, 604, 1412, 646, 300, 307, 767, 32856, 6264, 18424, 2010], "temperature": 0.0, "avg_logprob": -0.2127791037926307, "compression_ratio": 1.4972972972972973, "no_speech_prob": 1.5056659776746528e-06}, {"id": 374, "seek": 238702, "start": 2393.98, "end": 2402.12, "text": " data somewhere. You don't know. So you for me if I'm trying to solve a problem for example", "tokens": [1412, 4079, 13, 509, 500, 380, 458, 13, 407, 291, 337, 385, 498, 286, 478, 1382, 281, 5039, 257, 1154, 337, 1365], "temperature": 0.0, "avg_logprob": -0.2127791037926307, "compression_ratio": 1.4972972972972973, "no_speech_prob": 1.5056659776746528e-06}, {"id": 375, "seek": 238702, "start": 2402.12, "end": 2412.94, "text": " like recently I was trying to I was writing a script for for Elm radio where I can automatically", "tokens": [411, 3938, 286, 390, 1382, 281, 286, 390, 3579, 257, 5755, 337, 337, 2699, 76, 6477, 689, 286, 393, 6772], "temperature": 0.0, "avg_logprob": -0.2127791037926307, "compression_ratio": 1.4972972972972973, "no_speech_prob": 1.5056659776746528e-06}, {"id": 376, "seek": 241294, "start": 2412.94, "end": 2420.94, "text": " apply the right ID3 tags to MP3s that we publish that will apply the right album cover image", "tokens": [3079, 264, 558, 7348, 18, 18632, 281, 14146, 18, 82, 300, 321, 11374, 300, 486, 3079, 264, 558, 6030, 2060, 3256], "temperature": 0.0, "avg_logprob": -0.22333041920381433, "compression_ratio": 1.6334841628959276, "no_speech_prob": 8.579187351642759e-07}, {"id": 377, "seek": 241294, "start": 2420.94, "end": 2426.12, "text": " which you need to do before publishing and the right track information and it helps pull", "tokens": [597, 291, 643, 281, 360, 949, 17832, 293, 264, 558, 2837, 1589, 293, 309, 3665, 2235], "temperature": 0.0, "avg_logprob": -0.22333041920381433, "compression_ratio": 1.6334841628959276, "no_speech_prob": 8.579187351642759e-07}, {"id": 378, "seek": 241294, "start": 2426.12, "end": 2433.7400000000002, "text": " in data from the Notion API so it can get title information and the number of the episode", "tokens": [294, 1412, 490, 264, 1726, 313, 9362, 370, 309, 393, 483, 4876, 1589, 293, 264, 1230, 295, 264, 3500], "temperature": 0.0, "avg_logprob": -0.22333041920381433, "compression_ratio": 1.6334841628959276, "no_speech_prob": 8.579187351642759e-07}, {"id": 379, "seek": 241294, "start": 2433.7400000000002, "end": 2440.7000000000003, "text": " and things like that. And writing it in Node.js I found really frustrating because I even", "tokens": [293, 721, 411, 300, 13, 400, 3579, 309, 294, 38640, 13, 25530, 286, 1352, 534, 16522, 570, 286, 754], "temperature": 0.0, "avg_logprob": -0.22333041920381433, "compression_ratio": 1.6334841628959276, "no_speech_prob": 8.579187351642759e-07}, {"id": 380, "seek": 244070, "start": 2440.7, "end": 2445.74, "text": " though I was even using like an NPM package for hitting the Notion API but there were", "tokens": [1673, 286, 390, 754, 1228, 411, 364, 426, 18819, 7372, 337, 8850, 264, 1726, 313, 9362, 457, 456, 645], "temperature": 0.0, "avg_logprob": -0.2387731019840684, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.3081655652058544e-06}, {"id": 381, "seek": 244070, "start": 2445.74, "end": 2451.66, "text": " all these incorrect assumptions about the format of the JSON data I was getting back", "tokens": [439, 613, 18424, 17695, 466, 264, 7877, 295, 264, 31828, 1412, 286, 390, 1242, 646], "temperature": 0.0, "avg_logprob": -0.2387731019840684, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.3081655652058544e-06}, {"id": 382, "seek": 244070, "start": 2451.66, "end": 2459.62, "text": " even so with this helper package. NPM helper package? Yeah. And you know if I was writing", "tokens": [754, 370, 365, 341, 36133, 7372, 13, 426, 18819, 36133, 7372, 30, 865, 13, 400, 291, 458, 498, 286, 390, 3579], "temperature": 0.0, "avg_logprob": -0.2387731019840684, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.3081655652058544e-06}, {"id": 383, "seek": 244070, "start": 2459.62, "end": 2465.4399999999996, "text": " writing in Elm it would be JSON decoders so I would I would immediately turn it into nicely", "tokens": [3579, 294, 2699, 76, 309, 576, 312, 31828, 979, 378, 433, 370, 286, 576, 286, 576, 4258, 1261, 309, 666, 9594], "temperature": 0.0, "avg_logprob": -0.2387731019840684, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.3081655652058544e-06}, {"id": 384, "seek": 246544, "start": 2465.44, "end": 2470.9, "text": " structured data or an error and and be able to get like a shorter feedback cycle as I", "tokens": [18519, 1412, 420, 364, 6713, 293, 293, 312, 1075, 281, 483, 411, 257, 11639, 5824, 6586, 382, 286], "temperature": 0.0, "avg_logprob": -0.20006205240885416, "compression_ratio": 1.7125984251968505, "no_speech_prob": 5.014558610128006e-06}, {"id": 385, "seek": 246544, "start": 2470.9, "end": 2475.1, "text": " was working on that script instead of just having to like run it a little bit further", "tokens": [390, 1364, 322, 300, 5755, 2602, 295, 445, 1419, 281, 411, 1190, 309, 257, 707, 857, 3052], "temperature": 0.0, "avg_logprob": -0.20006205240885416, "compression_ratio": 1.7125984251968505, "no_speech_prob": 5.014558610128006e-06}, {"id": 386, "seek": 246544, "start": 2475.1, "end": 2480.14, "text": " run it a little bit further it would just tell me that I have decoding errors until", "tokens": [1190, 309, 257, 707, 857, 3052, 309, 576, 445, 980, 385, 300, 286, 362, 979, 8616, 13603, 1826], "temperature": 0.0, "avg_logprob": -0.20006205240885416, "compression_ratio": 1.7125984251968505, "no_speech_prob": 5.014558610128006e-06}, {"id": 387, "seek": 246544, "start": 2480.14, "end": 2486.82, "text": " I've gotten the data format as expected which is my preferred workflow. And also I just", "tokens": [286, 600, 5768, 264, 1412, 7877, 382, 5176, 597, 307, 452, 16494, 20993, 13, 400, 611, 286, 445], "temperature": 0.0, "avg_logprob": -0.20006205240885416, "compression_ratio": 1.7125984251968505, "no_speech_prob": 5.014558610128006e-06}, {"id": 388, "seek": 246544, "start": 2486.82, "end": 2495.42, "text": " I can once like if you write a script in Node.js and then it succeeds you're like okay well", "tokens": [286, 393, 1564, 411, 498, 291, 2464, 257, 5755, 294, 38640, 13, 25530, 293, 550, 309, 49263, 291, 434, 411, 1392, 731], "temperature": 0.0, "avg_logprob": -0.20006205240885416, "compression_ratio": 1.7125984251968505, "no_speech_prob": 5.014558610128006e-06}, {"id": 389, "seek": 249542, "start": 2495.42, "end": 2499.66, "text": " it's possible for this script to succeed but you're not necessarily convinced that it will", "tokens": [309, 311, 1944, 337, 341, 5755, 281, 7754, 457, 291, 434, 406, 4725, 12561, 300, 309, 486], "temperature": 0.0, "avg_logprob": -0.19333247785215024, "compression_ratio": 1.739463601532567, "no_speech_prob": 6.786572726014128e-07}, {"id": 390, "seek": 249542, "start": 2499.66, "end": 2505.54, "text": " succeed for all cases. Whereas like if I if I write the script in Elm I would be much", "tokens": [7754, 337, 439, 3331, 13, 13813, 411, 498, 286, 498, 286, 2464, 264, 5755, 294, 2699, 76, 286, 576, 312, 709], "temperature": 0.0, "avg_logprob": -0.19333247785215024, "compression_ratio": 1.739463601532567, "no_speech_prob": 6.786572726014128e-07}, {"id": 391, "seek": 249542, "start": 2505.54, "end": 2512.2200000000003, "text": " more confident that like oh yeah it it's good now like it's it's handling the expected JSON", "tokens": [544, 6679, 300, 411, 1954, 1338, 309, 309, 311, 665, 586, 411, 309, 311, 309, 311, 13175, 264, 5176, 31828], "temperature": 0.0, "avg_logprob": -0.19333247785215024, "compression_ratio": 1.739463601532567, "no_speech_prob": 6.786572726014128e-07}, {"id": 392, "seek": 249542, "start": 2512.2200000000003, "end": 2517.78, "text": " data I mean maybe the API sends slightly different data formats in different cases but I'm much", "tokens": [1412, 286, 914, 1310, 264, 9362, 14790, 4748, 819, 1412, 25879, 294, 819, 3331, 457, 286, 478, 709], "temperature": 0.0, "avg_logprob": -0.19333247785215024, "compression_ratio": 1.739463601532567, "no_speech_prob": 6.786572726014128e-07}, {"id": 393, "seek": 249542, "start": 2517.78, "end": 2524.1, "text": " more confident that I'm done at that point. So that's that's one thing is that confidence", "tokens": [544, 6679, 300, 286, 478, 1096, 412, 300, 935, 13, 407, 300, 311, 300, 311, 472, 551, 307, 300, 6687], "temperature": 0.0, "avg_logprob": -0.19333247785215024, "compression_ratio": 1.739463601532567, "no_speech_prob": 6.786572726014128e-07}, {"id": 394, "seek": 252410, "start": 2524.1, "end": 2530.02, "text": " which I still want when I'm writing a script like and it's I still want to pull API data", "tokens": [597, 286, 920, 528, 562, 286, 478, 3579, 257, 5755, 411, 293, 309, 311, 286, 920, 528, 281, 2235, 9362, 1412], "temperature": 0.0, "avg_logprob": -0.21929262946633732, "compression_ratio": 1.75, "no_speech_prob": 1.8738712697086157e-06}, {"id": 395, "seek": 252410, "start": 2530.02, "end": 2536.74, "text": " down and have some sanity around that being confident that the data format I'm getting", "tokens": [760, 293, 362, 512, 47892, 926, 300, 885, 6679, 300, 264, 1412, 7877, 286, 478, 1242], "temperature": 0.0, "avg_logprob": -0.21929262946633732, "compression_ratio": 1.75, "no_speech_prob": 1.8738712697086157e-06}, {"id": 396, "seek": 252410, "start": 2536.74, "end": 2541.58, "text": " is right I still want to work with nice types while I'm doing that and and know that the", "tokens": [307, 558, 286, 920, 528, 281, 589, 365, 1481, 3467, 1339, 286, 478, 884, 300, 293, 293, 458, 300, 264], "temperature": 0.0, "avg_logprob": -0.21929262946633732, "compression_ratio": 1.75, "no_speech_prob": 1.8738712697086157e-06}, {"id": 397, "seek": 252410, "start": 2541.58, "end": 2546.3199999999997, "text": " types I'm working with are correct not like half correct mixed in with some anys that", "tokens": [3467, 286, 478, 1364, 365, 366, 3006, 406, 411, 1922, 3006, 7467, 294, 365, 512, 604, 82, 300], "temperature": 0.0, "avg_logprob": -0.21929262946633732, "compression_ratio": 1.75, "no_speech_prob": 1.8738712697086157e-06}, {"id": 398, "seek": 254632, "start": 2546.32, "end": 2555.46, "text": " trickled into my system. I mean you don't have anys in bash. Right. Oh man working with", "tokens": [4282, 1493, 666, 452, 1185, 13, 286, 914, 291, 500, 380, 362, 604, 82, 294, 46183, 13, 1779, 13, 876, 587, 1364, 365], "temperature": 0.0, "avg_logprob": -0.2995452880859375, "compression_ratio": 1.6339285714285714, "no_speech_prob": 2.7264331947662868e-06}, {"id": 399, "seek": 254632, "start": 2555.46, "end": 2562.7000000000003, "text": " the API data responses in bash does not sound fun. I don't even know how you would do that.", "tokens": [264, 9362, 1412, 13019, 294, 46183, 775, 406, 1626, 1019, 13, 286, 500, 380, 754, 458, 577, 291, 576, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.2995452880859375, "compression_ratio": 1.6339285714285714, "no_speech_prob": 2.7264331947662868e-06}, {"id": 400, "seek": 254632, "start": 2562.7000000000003, "end": 2569.38, "text": " Yeah. I would just curl it and yeah pray that it works. JQ or something I don't know there", "tokens": [865, 13, 286, 576, 445, 22591, 309, 293, 1338, 3690, 300, 309, 1985, 13, 508, 48, 420, 746, 286, 500, 380, 458, 456], "temperature": 0.0, "avg_logprob": -0.2995452880859375, "compression_ratio": 1.6339285714285714, "no_speech_prob": 2.7264331947662868e-06}, {"id": 401, "seek": 254632, "start": 2569.38, "end": 2574.9, "text": " yeah there are tools but it's it's not fun you know so it's it's nice to use like a programming", "tokens": [1338, 456, 366, 3873, 457, 309, 311, 309, 311, 406, 1019, 291, 458, 370, 309, 311, 309, 311, 1481, 281, 764, 411, 257, 9410], "temperature": 0.0, "avg_logprob": -0.2995452880859375, "compression_ratio": 1.6339285714285714, "no_speech_prob": 2.7264331947662868e-06}, {"id": 402, "seek": 257490, "start": 2574.9, "end": 2580.7400000000002, "text": " language for that not just a bash script. But yes so like the other thing is if if I", "tokens": [2856, 337, 300, 406, 445, 257, 46183, 5755, 13, 583, 2086, 370, 411, 264, 661, 551, 307, 498, 498, 286], "temperature": 0.0, "avg_logprob": -0.24767698560442244, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.8572819701366825e-06}, {"id": 403, "seek": 257490, "start": 2580.7400000000002, "end": 2586.42, "text": " want to make it more robust to run this script maybe it's like when you write a quick and", "tokens": [528, 281, 652, 309, 544, 13956, 281, 1190, 341, 5755, 1310, 309, 311, 411, 562, 291, 2464, 257, 1702, 293], "temperature": 0.0, "avg_logprob": -0.24767698560442244, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.8572819701366825e-06}, {"id": 404, "seek": 257490, "start": 2586.42, "end": 2591.5, "text": " dirty script you want to just allow failures to just happen right. That would be like in", "tokens": [9360, 5755, 291, 528, 281, 445, 2089, 20774, 281, 445, 1051, 558, 13, 663, 576, 312, 411, 294], "temperature": 0.0, "avg_logprob": -0.24767698560442244, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.8572819701366825e-06}, {"id": 405, "seek": 257490, "start": 2591.5, "end": 2597.6, "text": " Node.js you just don't do a try catch. So with Elm pages back end tasks you do need", "tokens": [38640, 13, 25530, 291, 445, 500, 380, 360, 257, 853, 3745, 13, 407, 365, 2699, 76, 7183, 646, 917, 9608, 291, 360, 643], "temperature": 0.0, "avg_logprob": -0.24767698560442244, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.8572819701366825e-06}, {"id": 406, "seek": 257490, "start": 2597.6, "end": 2603.0, "text": " to be explicit where failures are possible. But at the same time I mean you know you do", "tokens": [281, 312, 13691, 689, 20774, 366, 1944, 13, 583, 412, 264, 912, 565, 286, 914, 291, 458, 291, 360], "temperature": 0.0, "avg_logprob": -0.24767698560442244, "compression_ratio": 1.6539923954372624, "no_speech_prob": 2.8572819701366825e-06}, {"id": 407, "seek": 260300, "start": 2603.0, "end": 2610.06, "text": " your get JSON and then a failure is possible. So the types will not fit together unless", "tokens": [428, 483, 31828, 293, 550, 257, 7763, 307, 1944, 13, 407, 264, 3467, 486, 406, 3318, 1214, 5969], "temperature": 0.0, "avg_logprob": -0.23538199258506845, "compression_ratio": 1.736220472440945, "no_speech_prob": 4.425431143317837e-06}, {"id": 408, "seek": 260300, "start": 2610.06, "end": 2616.3, "text": " you do allow fatal back and test out allow fatal. And yes you do have to write that explicitly", "tokens": [291, 360, 2089, 24069, 646, 293, 1500, 484, 2089, 24069, 13, 400, 2086, 291, 360, 362, 281, 2464, 300, 20803], "temperature": 0.0, "avg_logprob": -0.23538199258506845, "compression_ratio": 1.736220472440945, "no_speech_prob": 4.425431143317837e-06}, {"id": 409, "seek": 260300, "start": 2616.3, "end": 2621.38, "text": " but that's all you do. And if if you just say hey I don't want to deal with any possible", "tokens": [457, 300, 311, 439, 291, 360, 13, 400, 498, 498, 291, 445, 584, 4177, 286, 500, 380, 528, 281, 2028, 365, 604, 1944], "temperature": 0.0, "avg_logprob": -0.23538199258506845, "compression_ratio": 1.736220472440945, "no_speech_prob": 4.425431143317837e-06}, {"id": 410, "seek": 260300, "start": 2621.38, "end": 2625.66, "text": " errors I just want to work with the happy path I expect everything to work. And if if", "tokens": [13603, 286, 445, 528, 281, 589, 365, 264, 2055, 3100, 286, 2066, 1203, 281, 589, 13, 400, 498, 498], "temperature": 0.0, "avg_logprob": -0.23538199258506845, "compression_ratio": 1.736220472440945, "no_speech_prob": 4.425431143317837e-06}, {"id": 411, "seek": 260300, "start": 2625.66, "end": 2630.78, "text": " anything goes wrong just give me an error message right. Then you just any time the", "tokens": [1340, 1709, 2085, 445, 976, 385, 364, 6713, 3636, 558, 13, 1396, 291, 445, 604, 565, 264], "temperature": 0.0, "avg_logprob": -0.23538199258506845, "compression_ratio": 1.736220472440945, "no_speech_prob": 4.425431143317837e-06}, {"id": 412, "seek": 263078, "start": 2630.78, "end": 2636.34, "text": " types tell you to you just do back and test dot allow fatal. And now what you end up with", "tokens": [3467, 980, 291, 281, 291, 445, 360, 646, 293, 1500, 5893, 2089, 24069, 13, 400, 586, 437, 291, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.25132874519594256, "compression_ratio": 1.6542056074766356, "no_speech_prob": 3.927853526874969e-07}, {"id": 413, "seek": 263078, "start": 2636.34, "end": 2642.86, "text": " is yes you had to write allow fatal a handful of places. But for one thing you can look", "tokens": [307, 2086, 291, 632, 281, 2464, 2089, 24069, 257, 16458, 295, 3190, 13, 583, 337, 472, 551, 291, 393, 574], "temperature": 0.0, "avg_logprob": -0.25132874519594256, "compression_ratio": 1.6542056074766356, "no_speech_prob": 3.927853526874969e-07}, {"id": 414, "seek": 263078, "start": 2642.86, "end": 2648.9, "text": " at it and see where can fatal things happen. Right. Yeah. That's nice. Yeah. And if you", "tokens": [412, 309, 293, 536, 689, 393, 24069, 721, 1051, 13, 1779, 13, 865, 13, 663, 311, 1481, 13, 865, 13, 400, 498, 291], "temperature": 0.0, "avg_logprob": -0.25132874519594256, "compression_ratio": 1.6542056074766356, "no_speech_prob": 3.927853526874969e-07}, {"id": 415, "seek": 263078, "start": 2648.9, "end": 2654.6200000000003, "text": " want to recover from it at some point later when you have more time or you want to print", "tokens": [528, 281, 8114, 490, 309, 412, 512, 935, 1780, 562, 291, 362, 544, 565, 420, 291, 528, 281, 4482], "temperature": 0.0, "avg_logprob": -0.25132874519594256, "compression_ratio": 1.6542056074766356, "no_speech_prob": 3.927853526874969e-07}, {"id": 416, "seek": 265462, "start": 2654.62, "end": 2660.94, "text": " out a nicer error message then you know where to look. Exactly. And you know exactly the", "tokens": [484, 257, 22842, 6713, 3636, 550, 291, 458, 689, 281, 574, 13, 7587, 13, 400, 291, 458, 2293, 264], "temperature": 0.0, "avg_logprob": -0.25181821935317095, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.1779265807708725e-05}, {"id": 417, "seek": 265462, "start": 2660.94, "end": 2666.74, "text": " possible failures that can happen. Like it always feels like uncomfortable for me doing", "tokens": [1944, 20774, 300, 393, 1051, 13, 1743, 309, 1009, 3417, 411, 10532, 337, 385, 884], "temperature": 0.0, "avg_logprob": -0.25181821935317095, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.1779265807708725e-05}, {"id": 418, "seek": 265462, "start": 2666.74, "end": 2672.58, "text": " like a try catch in Node.js and then just like expecting the cut exception to be this", "tokens": [411, 257, 853, 3745, 294, 38640, 13, 25530, 293, 550, 445, 411, 9650, 264, 1723, 11183, 281, 312, 341], "temperature": 0.0, "avg_logprob": -0.25181821935317095, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.1779265807708725e-05}, {"id": 419, "seek": 265462, "start": 2672.58, "end": 2679.9, "text": " thing that has this key but then like it might not. And like do I do a try catch within my", "tokens": [551, 300, 575, 341, 2141, 457, 550, 411, 309, 1062, 406, 13, 400, 411, 360, 286, 360, 257, 853, 3745, 1951, 452], "temperature": 0.0, "avg_logprob": -0.25181821935317095, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.1779265807708725e-05}, {"id": 420, "seek": 267990, "start": 2679.9, "end": 2688.34, "text": " catch in case my expectations about the properties on that on that error are incorrect. Like", "tokens": [3745, 294, 1389, 452, 9843, 466, 264, 7221, 322, 300, 322, 300, 6713, 366, 18424, 13, 1743], "temperature": 0.0, "avg_logprob": -0.21017574227374533, "compression_ratio": 1.6711111111111112, "no_speech_prob": 5.422139111033175e-06}, {"id": 421, "seek": 267990, "start": 2688.34, "end": 2694.4, "text": " so it's if you want to do error handling error recovery like you have nice types that let", "tokens": [370, 309, 311, 498, 291, 528, 281, 360, 6713, 13175, 6713, 8597, 411, 291, 362, 1481, 3467, 300, 718], "temperature": 0.0, "avg_logprob": -0.21017574227374533, "compression_ratio": 1.6711111111111112, "no_speech_prob": 5.422139111033175e-06}, {"id": 422, "seek": 267990, "start": 2694.4, "end": 2699.78, "text": " you do that. If you don't it's explicit where you're not doing that. And that's a very intentional", "tokens": [291, 360, 300, 13, 759, 291, 500, 380, 309, 311, 13691, 689, 291, 434, 406, 884, 300, 13, 400, 300, 311, 257, 588, 21935], "temperature": 0.0, "avg_logprob": -0.21017574227374533, "compression_ratio": 1.6711111111111112, "no_speech_prob": 5.422139111033175e-06}, {"id": 423, "seek": 267990, "start": 2699.78, "end": 2706.1800000000003, "text": " design trade off that it's a little less convenient but it feels more safe. So to me that is a", "tokens": [1715, 4923, 766, 300, 309, 311, 257, 707, 1570, 10851, 457, 309, 3417, 544, 3273, 13, 407, 281, 385, 300, 307, 257], "temperature": 0.0, "avg_logprob": -0.21017574227374533, "compression_ratio": 1.6711111111111112, "no_speech_prob": 5.422139111033175e-06}, {"id": 424, "seek": 270618, "start": 2706.18, "end": 2712.18, "text": " trade off I'm willing to make to write allow fatal a few extra places and have that explicit", "tokens": [4923, 766, 286, 478, 4950, 281, 652, 281, 2464, 2089, 24069, 257, 1326, 2857, 3190, 293, 362, 300, 13691], "temperature": 0.0, "avg_logprob": -0.22701481672433707, "compression_ratio": 1.7578125, "no_speech_prob": 3.23774497701379e-06}, {"id": 425, "seek": 270618, "start": 2712.18, "end": 2718.14, "text": " this and know where things can fail. And then if I want to make my script more robust over", "tokens": [341, 293, 458, 689, 721, 393, 3061, 13, 400, 550, 498, 286, 528, 281, 652, 452, 5755, 544, 13956, 670], "temperature": 0.0, "avg_logprob": -0.22701481672433707, "compression_ratio": 1.7578125, "no_speech_prob": 3.23774497701379e-06}, {"id": 426, "seek": 270618, "start": 2718.14, "end": 2723.8599999999997, "text": " time and say like OK this error case I should really have proper error handling for this.", "tokens": [565, 293, 584, 411, 2264, 341, 6713, 1389, 286, 820, 534, 362, 2296, 6713, 13175, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.22701481672433707, "compression_ratio": 1.7578125, "no_speech_prob": 3.23774497701379e-06}, {"id": 427, "seek": 270618, "start": 2723.8599999999997, "end": 2728.5, "text": " This this script that I'm running fails on Sundays because this thing happens and I should", "tokens": [639, 341, 5755, 300, 286, 478, 2614, 18199, 322, 44857, 570, 341, 551, 2314, 293, 286, 820], "temperature": 0.0, "avg_logprob": -0.22701481672433707, "compression_ratio": 1.7578125, "no_speech_prob": 3.23774497701379e-06}, {"id": 428, "seek": 270618, "start": 2728.5, "end": 2735.22, "text": " really clean that up and add proper error handling. You can or if you want to present", "tokens": [534, 2541, 300, 493, 293, 909, 2296, 6713, 13175, 13, 509, 393, 420, 498, 291, 528, 281, 1974], "temperature": 0.0, "avg_logprob": -0.22701481672433707, "compression_ratio": 1.7578125, "no_speech_prob": 3.23774497701379e-06}, {"id": 429, "seek": 273522, "start": 2735.22, "end": 2743.8599999999997, "text": " nicer error message in one case instead of just saying I got this HTTP error you could", "tokens": [22842, 6713, 3636, 294, 472, 1389, 2602, 295, 445, 1566, 286, 658, 341, 33283, 6713, 291, 727], "temperature": 0.0, "avg_logprob": -0.2521032136062096, "compression_ratio": 1.8238341968911918, "no_speech_prob": 6.747924544470152e-06}, {"id": 430, "seek": 273522, "start": 2743.8599999999997, "end": 2751.14, "text": " give a more custom error message. You can do that. You don't like it. No ends. Right.", "tokens": [976, 257, 544, 2375, 6713, 3636, 13, 509, 393, 360, 300, 13, 509, 500, 380, 411, 309, 13, 883, 5314, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.2521032136062096, "compression_ratio": 1.8238341968911918, "no_speech_prob": 6.747924544470152e-06}, {"id": 431, "seek": 273522, "start": 2751.14, "end": 2758.1, "text": " So you could you could say like instead of saying you know and you know or instead of", "tokens": [407, 291, 727, 291, 727, 584, 411, 2602, 295, 1566, 291, 458, 293, 291, 458, 420, 2602, 295], "temperature": 0.0, "avg_logprob": -0.2521032136062096, "compression_ratio": 1.8238341968911918, "no_speech_prob": 6.747924544470152e-06}, {"id": 432, "seek": 273522, "start": 2758.1, "end": 2763.9399999999996, "text": " saying the default can't read file error message that the that it gives you when you get that", "tokens": [1566, 264, 7576, 393, 380, 1401, 3991, 6713, 3636, 300, 264, 300, 309, 2709, 291, 562, 291, 483, 300], "temperature": 0.0, "avg_logprob": -0.2521032136062096, "compression_ratio": 1.8238341968911918, "no_speech_prob": 6.747924544470152e-06}, {"id": 433, "seek": 276394, "start": 2763.94, "end": 2771.26, "text": " fatal error in pages in the core APIs you could have you could turn that into your custom", "tokens": [24069, 6713, 294, 7183, 294, 264, 4965, 21445, 291, 727, 362, 291, 727, 1261, 300, 666, 428, 2375], "temperature": 0.0, "avg_logprob": -0.3139616719792398, "compression_ratio": 1.6116071428571428, "no_speech_prob": 5.714984467886097e-07}, {"id": 434, "seek": 276394, "start": 2771.26, "end": 2775.82, "text": " error with nice error feedback whereas doing that in Node.js it's just going to be a lot", "tokens": [6713, 365, 1481, 6713, 5824, 9735, 884, 300, 294, 38640, 13, 25530, 309, 311, 445, 516, 281, 312, 257, 688], "temperature": 0.0, "avg_logprob": -0.3139616719792398, "compression_ratio": 1.6116071428571428, "no_speech_prob": 5.714984467886097e-07}, {"id": 435, "seek": 276394, "start": 2775.82, "end": 2783.38, "text": " harder. So I just feel like it like the goal of this design is to give you a way to be", "tokens": [6081, 13, 407, 286, 445, 841, 411, 309, 411, 264, 3387, 295, 341, 1715, 307, 281, 976, 291, 257, 636, 281, 312], "temperature": 0.0, "avg_logprob": -0.3139616719792398, "compression_ratio": 1.6116071428571428, "no_speech_prob": 5.714984467886097e-07}, {"id": 436, "seek": 276394, "start": 2783.38, "end": 2791.62, "text": " productive build things up with minimal boilerplate. You write your script hello.elm you expose", "tokens": [13304, 1322, 721, 493, 365, 13206, 39228, 37008, 13, 509, 2464, 428, 5755, 7751, 13, 338, 76, 291, 19219], "temperature": 0.0, "avg_logprob": -0.3139616719792398, "compression_ratio": 1.6116071428571428, "no_speech_prob": 5.714984467886097e-07}, {"id": 437, "seek": 279162, "start": 2791.62, "end": 2797.54, "text": " run its type of script you define a back end task and then you want a quick and dirty script", "tokens": [1190, 1080, 2010, 295, 5755, 291, 6964, 257, 646, 917, 5633, 293, 550, 291, 528, 257, 1702, 293, 9360, 5755], "temperature": 0.0, "avg_logprob": -0.1870791045102206, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.714264936315885e-07}, {"id": 438, "seek": 279162, "start": 2797.54, "end": 2805.7799999999997, "text": " just the happy path you allow fatal. But as you want to deal with more error cases in", "tokens": [445, 264, 2055, 3100, 291, 2089, 24069, 13, 583, 382, 291, 528, 281, 2028, 365, 544, 6713, 3331, 294], "temperature": 0.0, "avg_logprob": -0.1870791045102206, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.714264936315885e-07}, {"id": 439, "seek": 279162, "start": 2805.7799999999997, "end": 2811.74, "text": " a graceful way it gives you the tools to do that and to really maintain it. So it's trying", "tokens": [257, 10042, 906, 636, 309, 2709, 291, 264, 3873, 281, 360, 300, 293, 281, 534, 6909, 309, 13, 407, 309, 311, 1382], "temperature": 0.0, "avg_logprob": -0.1870791045102206, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.714264936315885e-07}, {"id": 440, "seek": 279162, "start": 2811.74, "end": 2819.62, "text": " to give a balance between convenience and maintainability. So I don't know like would", "tokens": [281, 976, 257, 4772, 1296, 19283, 293, 6909, 2310, 13, 407, 286, 500, 380, 458, 411, 576], "temperature": 0.0, "avg_logprob": -0.1870791045102206, "compression_ratio": 1.6210045662100456, "no_speech_prob": 8.714264936315885e-07}, {"id": 441, "seek": 281962, "start": 2819.62, "end": 2826.94, "text": " you use Node.js in some cases instead of using an Elm pages script. I'm sure there are cases", "tokens": [291, 764, 38640, 13, 25530, 294, 512, 3331, 2602, 295, 1228, 364, 2699, 76, 7183, 5755, 13, 286, 478, 988, 456, 366, 3331], "temperature": 0.0, "avg_logprob": -0.2346622385877244, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.9333124328113627e-06}, {"id": 442, "seek": 281962, "start": 2826.94, "end": 2834.3399999999997, "text": " for that but I think if I if I have some little scripts for like helping with the Elm radio", "tokens": [337, 300, 457, 286, 519, 498, 286, 498, 286, 362, 512, 707, 23294, 337, 411, 4315, 365, 264, 2699, 76, 6477], "temperature": 0.0, "avg_logprob": -0.2346622385877244, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.9333124328113627e-06}, {"id": 443, "seek": 281962, "start": 2834.3399999999997, "end": 2839.7799999999997, "text": " publishing process like I want that in an Elm pages script because I want I want that", "tokens": [17832, 1399, 411, 286, 528, 300, 294, 364, 2699, 76, 7183, 5755, 570, 286, 528, 286, 528, 300], "temperature": 0.0, "avg_logprob": -0.2346622385877244, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.9333124328113627e-06}, {"id": 444, "seek": 281962, "start": 2839.7799999999997, "end": 2846.74, "text": " in Elm and I know. Yeah. Yeah. That makes sense. I mean you could just start writing", "tokens": [294, 2699, 76, 293, 286, 458, 13, 865, 13, 865, 13, 663, 1669, 2020, 13, 286, 914, 291, 727, 445, 722, 3579], "temperature": 0.0, "avg_logprob": -0.2346622385877244, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.9333124328113627e-06}, {"id": 445, "seek": 284674, "start": 2846.74, "end": 2854.7799999999997, "text": " a script in Node.js like because you start small you do like one thing because it's a", "tokens": [257, 5755, 294, 38640, 13, 25530, 411, 570, 291, 722, 1359, 291, 360, 411, 472, 551, 570, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.3056906734604433, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.642533218022436e-06}, {"id": 446, "seek": 284674, "start": 2854.7799999999997, "end": 2860.7, "text": " prototype and then well you need one additional thing and then you need another additional", "tokens": [19475, 293, 550, 731, 291, 643, 472, 4497, 551, 293, 550, 291, 643, 1071, 4497], "temperature": 0.0, "avg_logprob": -0.3056906734604433, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.642533218022436e-06}, {"id": 447, "seek": 284674, "start": 2860.7, "end": 2870.9399999999996, "text": " thing and two three seven additional things and. Right. And at some point you think you", "tokens": [551, 293, 732, 1045, 3407, 4497, 721, 293, 13, 1779, 13, 400, 412, 512, 935, 291, 519, 291], "temperature": 0.0, "avg_logprob": -0.3056906734604433, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.642533218022436e-06}, {"id": 448, "seek": 284674, "start": 2870.9399999999996, "end": 2876.7, "text": " should rewrite this in another language. Yeah. Let's rewrite this in bash. This makes a lot", "tokens": [820, 28132, 341, 294, 1071, 2856, 13, 865, 13, 961, 311, 28132, 341, 294, 46183, 13, 639, 1669, 257, 688], "temperature": 0.0, "avg_logprob": -0.3056906734604433, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.642533218022436e-06}, {"id": 449, "seek": 287670, "start": 2876.7, "end": 2885.2599999999998, "text": " more sense. Perfect. Yeah. I would say for me like I feel like when I hit dealing with", "tokens": [544, 2020, 13, 10246, 13, 865, 13, 286, 576, 584, 337, 385, 411, 286, 841, 411, 562, 286, 2045, 6260, 365], "temperature": 0.0, "avg_logprob": -0.19260446151884475, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.6796607269498054e-06}, {"id": 450, "seek": 287670, "start": 2885.2599999999998, "end": 2892.2599999999998, "text": " JSON data in Node.js that's when I really want to just use Elm for that. And I know", "tokens": [31828, 1412, 294, 38640, 13, 25530, 300, 311, 562, 286, 534, 528, 281, 445, 764, 2699, 76, 337, 300, 13, 400, 286, 458], "temperature": 0.0, "avg_logprob": -0.19260446151884475, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.6796607269498054e-06}, {"id": 451, "seek": 287670, "start": 2892.2599999999998, "end": 2896.66, "text": " there are tools like Zod to help you do it in a more Elm way where you're writing things", "tokens": [456, 366, 3873, 411, 1176, 378, 281, 854, 291, 360, 309, 294, 257, 544, 2699, 76, 636, 689, 291, 434, 3579, 721], "temperature": 0.0, "avg_logprob": -0.19260446151884475, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.6796607269498054e-06}, {"id": 452, "seek": 287670, "start": 2896.66, "end": 2902.7, "text": " in the style of a decoder. But I don't know for me I'm I'm going to tend to reach for", "tokens": [294, 264, 3758, 295, 257, 979, 19866, 13, 583, 286, 500, 380, 458, 337, 385, 286, 478, 286, 478, 516, 281, 3928, 281, 2524, 337], "temperature": 0.0, "avg_logprob": -0.19260446151884475, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.6796607269498054e-06}, {"id": 453, "seek": 290270, "start": 2902.7, "end": 2907.66, "text": " Elm to do that type of task. And if you as you say you can you can start something in", "tokens": [2699, 76, 281, 360, 300, 2010, 295, 5633, 13, 400, 498, 291, 382, 291, 584, 291, 393, 291, 393, 722, 746, 294], "temperature": 0.0, "avg_logprob": -0.28597903044327444, "compression_ratio": 1.893162393162393, "no_speech_prob": 1.3925244957135874e-06}, {"id": 454, "seek": 290270, "start": 2907.66, "end": 2914.62, "text": " Node.js you can create custom back end tasks so you can like write whole chunks of Node.js", "tokens": [38640, 13, 25530, 291, 393, 1884, 2375, 646, 917, 9608, 370, 291, 393, 411, 2464, 1379, 24004, 295, 38640, 13, 25530], "temperature": 0.0, "avg_logprob": -0.28597903044327444, "compression_ratio": 1.893162393162393, "no_speech_prob": 1.3925244957135874e-06}, {"id": 455, "seek": 290270, "start": 2914.62, "end": 2921.62, "text": " code in your custom back end task dot TS file and then you can just execute that as a back", "tokens": [3089, 294, 428, 2375, 646, 917, 5633, 5893, 37645, 3991, 293, 550, 291, 393, 445, 14483, 300, 382, 257, 646], "temperature": 0.0, "avg_logprob": -0.28597903044327444, "compression_ratio": 1.893162393162393, "no_speech_prob": 1.3925244957135874e-06}, {"id": 456, "seek": 290270, "start": 2921.62, "end": 2925.98, "text": " end task. Yeah. So you can easily migrate from one to the other is what you're saying.", "tokens": [917, 5633, 13, 865, 13, 407, 291, 393, 3612, 31821, 490, 472, 281, 264, 661, 307, 437, 291, 434, 1566, 13], "temperature": 0.0, "avg_logprob": -0.28597903044327444, "compression_ratio": 1.893162393162393, "no_speech_prob": 1.3925244957135874e-06}, {"id": 457, "seek": 290270, "start": 2925.98, "end": 2932.2999999999997, "text": " Yeah exactly. So the so the custom back end tasks the way you define them you write your", "tokens": [865, 2293, 13, 407, 264, 370, 264, 2375, 646, 917, 9608, 264, 636, 291, 6964, 552, 291, 2464, 428], "temperature": 0.0, "avg_logprob": -0.28597903044327444, "compression_ratio": 1.893162393162393, "no_speech_prob": 1.3925244957135874e-06}, {"id": 458, "seek": 293230, "start": 2932.3, "end": 2938.9, "text": " custom back end task TS file or JS whatever you prefer. It transpiles it using ES build", "tokens": [2375, 646, 917, 5633, 37645, 3991, 420, 33063, 2035, 291, 4382, 13, 467, 7132, 4680, 309, 1228, 12564, 1322], "temperature": 0.0, "avg_logprob": -0.32260517452074133, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.8162108972319402e-06}, {"id": 459, "seek": 293230, "start": 2938.9, "end": 2946.5800000000004, "text": " and and you export async functions and then you do back end task dot custom dot run. You", "tokens": [293, 293, 291, 10725, 382, 34015, 6828, 293, 550, 291, 360, 646, 917, 5633, 5893, 2375, 5893, 1190, 13, 509], "temperature": 0.0, "avg_logprob": -0.32260517452074133, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.8162108972319402e-06}, {"id": 460, "seek": 293230, "start": 2946.5800000000004, "end": 2952.38, "text": " give it the name of the function that you exported from that TypeScript file. You encode", "tokens": [976, 309, 264, 1315, 295, 264, 2445, 300, 291, 42055, 490, 300, 15576, 14237, 3991, 13, 509, 2058, 1429], "temperature": 0.0, "avg_logprob": -0.32260517452074133, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.8162108972319402e-06}, {"id": 461, "seek": 293230, "start": 2952.38, "end": 2959.82, "text": " some JSON data to pass in. You give it a JSON decoder and then you've got a back end task.", "tokens": [512, 31828, 1412, 281, 1320, 294, 13, 509, 976, 309, 257, 31828, 979, 19866, 293, 550, 291, 600, 658, 257, 646, 917, 5633, 13], "temperature": 0.0, "avg_logprob": -0.32260517452074133, "compression_ratio": 1.7281553398058251, "no_speech_prob": 1.8162108972319402e-06}, {"id": 462, "seek": 295982, "start": 2959.82, "end": 2966.54, "text": " And does Elm pages make sure that that port exists both in JavaScript and in Elm before", "tokens": [400, 775, 2699, 76, 7183, 652, 988, 300, 300, 2436, 8198, 1293, 294, 15778, 293, 294, 2699, 76, 949], "temperature": 0.0, "avg_logprob": -0.2315466070687899, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.276299124394427e-07}, {"id": 463, "seek": 295982, "start": 2966.54, "end": 2973.6600000000003, "text": " you run it. So it it doesn't need to make sure the port exists in Elm because it's not", "tokens": [291, 1190, 309, 13, 407, 309, 309, 1177, 380, 643, 281, 652, 988, 264, 2436, 8198, 294, 2699, 76, 570, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.2315466070687899, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.276299124394427e-07}, {"id": 464, "seek": 295982, "start": 2973.6600000000003, "end": 2979.54, "text": " actually it's it it's using it's not defining a port for each of those but it's just calling", "tokens": [767, 309, 311, 309, 309, 311, 1228, 309, 311, 406, 17827, 257, 2436, 337, 1184, 295, 729, 457, 309, 311, 445, 5141], "temperature": 0.0, "avg_logprob": -0.2315466070687899, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.276299124394427e-07}, {"id": 465, "seek": 295982, "start": 2979.54, "end": 2987.06, "text": " your async function. But it does make sure that your your custom back end task TypeScript", "tokens": [428, 382, 34015, 2445, 13, 583, 309, 775, 652, 988, 300, 428, 428, 2375, 646, 917, 5633, 15576, 14237], "temperature": 0.0, "avg_logprob": -0.2315466070687899, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.276299124394427e-07}, {"id": 466, "seek": 298706, "start": 2987.06, "end": 2994.62, "text": " file compiles. If there is a an error in the file you can recover from that as one as the", "tokens": [3991, 715, 4680, 13, 759, 456, 307, 257, 364, 6713, 294, 264, 3991, 291, 393, 8114, 490, 300, 382, 472, 382, 264], "temperature": 0.0, "avg_logprob": -0.2198597625061706, "compression_ratio": 1.661904761904762, "no_speech_prob": 9.721500191517407e-07}, {"id": 467, "seek": 298706, "start": 2994.62, "end": 2999.82, "text": " recoverable error type for that back end task or if you allow fatal it'll print it out in", "tokens": [8114, 712, 6713, 2010, 337, 300, 646, 917, 5633, 420, 498, 291, 2089, 24069, 309, 603, 4482, 309, 484, 294], "temperature": 0.0, "avg_logprob": -0.2198597625061706, "compression_ratio": 1.661904761904762, "no_speech_prob": 9.721500191517407e-07}, {"id": 468, "seek": 298706, "start": 2999.82, "end": 3006.94, "text": " a nice formatted way. Yeah. If you do not have an exported function of the name that", "tokens": [257, 1481, 1254, 32509, 636, 13, 865, 13, 759, 291, 360, 406, 362, 364, 42055, 2445, 295, 264, 1315, 300], "temperature": 0.0, "avg_logprob": -0.2198597625061706, "compression_ratio": 1.661904761904762, "no_speech_prob": 9.721500191517407e-07}, {"id": 469, "seek": 298706, "start": 3006.94, "end": 3012.7799999999997, "text": " you're trying to call it gives you that as part of the structured error type. And if", "tokens": [291, 434, 1382, 281, 818, 309, 2709, 291, 300, 382, 644, 295, 264, 18519, 6713, 2010, 13, 400, 498], "temperature": 0.0, "avg_logprob": -0.2198597625061706, "compression_ratio": 1.661904761904762, "no_speech_prob": 9.721500191517407e-07}, {"id": 470, "seek": 301278, "start": 3012.78, "end": 3017.98, "text": " you export something but it's not a function it even tells you about that. So all of those", "tokens": [291, 10725, 746, 457, 309, 311, 406, 257, 2445, 309, 754, 5112, 291, 466, 300, 13, 407, 439, 295, 729], "temperature": 0.0, "avg_logprob": -0.24915327849211516, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.3287738056533271e-06}, {"id": 471, "seek": 301278, "start": 3017.98, "end": 3023.46, "text": " possible error variants will be automatically printed for you in a nice format if you allow", "tokens": [1944, 6713, 21669, 486, 312, 6772, 13567, 337, 291, 294, 257, 1481, 7877, 498, 291, 2089], "temperature": 0.0, "avg_logprob": -0.24915327849211516, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.3287738056533271e-06}, {"id": 472, "seek": 301278, "start": 3023.46, "end": 3029.1000000000004, "text": " fatal and if you want to recover from it you can even do that like it even has a custom", "tokens": [24069, 293, 498, 291, 528, 281, 8114, 490, 309, 291, 393, 754, 360, 300, 411, 309, 754, 575, 257, 2375], "temperature": 0.0, "avg_logprob": -0.24915327849211516, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.3287738056533271e-06}, {"id": 473, "seek": 301278, "start": 3029.1000000000004, "end": 3035.26, "text": " type with all those possible failure cases for you. Yeah. So you can pat a match on it", "tokens": [2010, 365, 439, 729, 1944, 7763, 3331, 337, 291, 13, 865, 13, 407, 291, 393, 1947, 257, 2995, 322, 309], "temperature": 0.0, "avg_logprob": -0.24915327849211516, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.3287738056533271e-06}, {"id": 474, "seek": 301278, "start": 3035.26, "end": 3040.98, "text": " and print out a nice error message or something. Exactly. And it will also if you throw an", "tokens": [293, 4482, 484, 257, 1481, 6713, 3636, 420, 746, 13, 7587, 13, 400, 309, 486, 611, 498, 291, 3507, 364], "temperature": 0.0, "avg_logprob": -0.24915327849211516, "compression_ratio": 1.7364341085271318, "no_speech_prob": 1.3287738056533271e-06}, {"id": 475, "seek": 304098, "start": 3040.98, "end": 3049.9, "text": " exception in your port data source it will give you if you throw JSON data it will give", "tokens": [11183, 294, 428, 2436, 1412, 4009, 309, 486, 976, 291, 498, 291, 3507, 31828, 1412, 309, 486, 976], "temperature": 0.0, "avg_logprob": -0.26763349420884075, "compression_ratio": 1.497142857142857, "no_speech_prob": 2.601606865937356e-06}, {"id": 476, "seek": 304098, "start": 3049.9, "end": 3059.98, "text": " you that as the error type. OK. So now another topic I do find it a little bit weird that", "tokens": [291, 300, 382, 264, 6713, 2010, 13, 2264, 13, 407, 586, 1071, 4829, 286, 360, 915, 309, 257, 707, 857, 3657, 300], "temperature": 0.0, "avg_logprob": -0.26763349420884075, "compression_ratio": 1.497142857142857, "no_speech_prob": 2.601606865937356e-06}, {"id": 477, "seek": 304098, "start": 3059.98, "end": 3065.38, "text": " to run a script in Elm which I would love to do and I don't mind necessarily writing", "tokens": [281, 1190, 257, 5755, 294, 2699, 76, 597, 286, 576, 959, 281, 360, 293, 286, 500, 380, 1575, 4725, 3579], "temperature": 0.0, "avg_logprob": -0.26763349420884075, "compression_ratio": 1.497142857142857, "no_speech_prob": 2.601606865937356e-06}, {"id": 478, "seek": 306538, "start": 3065.38, "end": 3072.26, "text": " a scripts folder with an Elm JSON file and all those boilerplate things. But do I really", "tokens": [257, 23294, 10820, 365, 364, 2699, 76, 31828, 3991, 293, 439, 729, 39228, 37008, 721, 13, 583, 360, 286, 534], "temperature": 0.0, "avg_logprob": -0.30846744952815597, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.1233151983324206e-06}, {"id": 479, "seek": 306538, "start": 3072.26, "end": 3079.7000000000003, "text": " have to pull in all of Elm pages. Right. Well I mean how do I explain it to my co-worker", "tokens": [362, 281, 2235, 294, 439, 295, 2699, 76, 7183, 13, 1779, 13, 1042, 286, 914, 577, 360, 286, 2903, 309, 281, 452, 598, 12, 49402], "temperature": 0.0, "avg_logprob": -0.30846744952815597, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.1233151983324206e-06}, {"id": 480, "seek": 306538, "start": 3079.7000000000003, "end": 3084.7400000000002, "text": " like oh yeah of course use Elm pages. The name makes a lot of sense. Why not Elm scripts", "tokens": [411, 1954, 1338, 295, 1164, 764, 2699, 76, 7183, 13, 440, 1315, 1669, 257, 688, 295, 2020, 13, 1545, 406, 2699, 76, 23294], "temperature": 0.0, "avg_logprob": -0.30846744952815597, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.1233151983324206e-06}, {"id": 481, "seek": 306538, "start": 3084.7400000000002, "end": 3093.82, "text": " or. Right. No I mean that's fair. So you can you know you can create a script folder in", "tokens": [420, 13, 1779, 13, 883, 286, 914, 300, 311, 3143, 13, 407, 291, 393, 291, 458, 291, 393, 1884, 257, 5755, 10820, 294], "temperature": 0.0, "avg_logprob": -0.30846744952815597, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.1233151983324206e-06}, {"id": 482, "seek": 309382, "start": 3093.82, "end": 3099.7400000000002, "text": " any project doesn't need any of the Elm pages boilerplate. And you know could it someday", "tokens": [604, 1716, 1177, 380, 643, 604, 295, 264, 2699, 76, 7183, 39228, 37008, 13, 400, 291, 458, 727, 309, 19412], "temperature": 0.0, "avg_logprob": -0.22861476520915608, "compression_ratio": 1.5656108597285068, "no_speech_prob": 2.7264181881037075e-06}, {"id": 483, "seek": 309382, "start": 3099.7400000000002, "end": 3107.3, "text": " make sense to have maybe slimmed down version of the NPM package with a different name.", "tokens": [652, 2020, 281, 362, 1310, 25357, 1912, 760, 3037, 295, 264, 426, 18819, 7372, 365, 257, 819, 1315, 13], "temperature": 0.0, "avg_logprob": -0.22861476520915608, "compression_ratio": 1.5656108597285068, "no_speech_prob": 2.7264181881037075e-06}, {"id": 484, "seek": 309382, "start": 3107.3, "end": 3112.34, "text": " Sure. But right now that's not a priority right now it's like I mean right now it is", "tokens": [4894, 13, 583, 558, 586, 300, 311, 406, 257, 9365, 558, 586, 309, 311, 411, 286, 914, 558, 586, 309, 307], "temperature": 0.0, "avg_logprob": -0.22861476520915608, "compression_ratio": 1.5656108597285068, "no_speech_prob": 2.7264181881037075e-06}, {"id": 485, "seek": 309382, "start": 3112.34, "end": 3119.7000000000003, "text": " a tool that's basically Rails generate for Elm pages. So it's a tool for helping Elm", "tokens": [257, 2290, 300, 311, 1936, 48526, 8460, 337, 2699, 76, 7183, 13, 407, 309, 311, 257, 2290, 337, 4315, 2699, 76], "temperature": 0.0, "avg_logprob": -0.22861476520915608, "compression_ratio": 1.5656108597285068, "no_speech_prob": 2.7264181881037075e-06}, {"id": 486, "seek": 311970, "start": 3119.7, "end": 3125.62, "text": " pages users be more productive. And it happens to be usable outside of an Elm pages project.", "tokens": [7183, 5022, 312, 544, 13304, 13, 400, 309, 2314, 281, 312, 29975, 2380, 295, 364, 2699, 76, 7183, 1716, 13], "temperature": 0.0, "avg_logprob": -0.28919280202765213, "compression_ratio": 1.7110266159695817, "no_speech_prob": 2.48247215495212e-06}, {"id": 487, "seek": 311970, "start": 3125.62, "end": 3131.22, "text": " But yeah it's definitely like a little funky. The thing is like the concept of a back end", "tokens": [583, 1338, 309, 311, 2138, 411, 257, 707, 33499, 13, 440, 551, 307, 411, 264, 3410, 295, 257, 646, 917], "temperature": 0.0, "avg_logprob": -0.28919280202765213, "compression_ratio": 1.7110266159695817, "no_speech_prob": 2.48247215495212e-06}, {"id": 488, "seek": 311970, "start": 3131.22, "end": 3138.5, "text": " task is so tied to Elm pages right now. To Elm pages implementation you mean or. Because", "tokens": [5633, 307, 370, 9601, 281, 2699, 76, 7183, 558, 586, 13, 1407, 2699, 76, 7183, 11420, 291, 914, 420, 13, 1436], "temperature": 0.0, "avg_logprob": -0.28919280202765213, "compression_ratio": 1.7110266159695817, "no_speech_prob": 2.48247215495212e-06}, {"id": 489, "seek": 311970, "start": 3138.5, "end": 3143.2999999999997, "text": " as you say it like doesn't have anything to do with Elm pages necessarily. It just happens", "tokens": [382, 291, 584, 309, 411, 1177, 380, 362, 1340, 281, 360, 365, 2699, 76, 7183, 4725, 13, 467, 445, 2314], "temperature": 0.0, "avg_logprob": -0.28919280202765213, "compression_ratio": 1.7110266159695817, "no_speech_prob": 2.48247215495212e-06}, {"id": 490, "seek": 311970, "start": 3143.2999999999997, "end": 3149.58, "text": " to be code that is in that project also for good reasons. The use case of Elm pages but", "tokens": [281, 312, 3089, 300, 307, 294, 300, 1716, 611, 337, 665, 4112, 13, 440, 764, 1389, 295, 2699, 76, 7183, 457], "temperature": 0.0, "avg_logprob": -0.28919280202765213, "compression_ratio": 1.7110266159695817, "no_speech_prob": 2.48247215495212e-06}, {"id": 491, "seek": 314958, "start": 3149.58, "end": 3157.54, "text": " it doesn't have to be. Right. Yeah. The problem is like I've had this also for Elm review", "tokens": [309, 1177, 380, 362, 281, 312, 13, 1779, 13, 865, 13, 440, 1154, 307, 411, 286, 600, 632, 341, 611, 337, 2699, 76, 3131], "temperature": 0.0, "avg_logprob": -0.28307429780351356, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.9022733087913366e-06}, {"id": 492, "seek": 314958, "start": 3157.54, "end": 3165.2599999999998, "text": " is like where do you draw the line. Like does it make sense to have Elm have the scripts", "tokens": [307, 411, 689, 360, 291, 2642, 264, 1622, 13, 1743, 775, 309, 652, 2020, 281, 362, 2699, 76, 362, 264, 23294], "temperature": 0.0, "avg_logprob": -0.28307429780351356, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.9022733087913366e-06}, {"id": 493, "seek": 314958, "start": 3165.2599999999998, "end": 3172.1, "text": " parts in a separate CLI in a separate Elm package called Elm scripts or whatever which", "tokens": [3166, 294, 257, 4994, 12855, 40, 294, 257, 4994, 2699, 76, 7372, 1219, 2699, 76, 23294, 420, 2035, 597], "temperature": 0.0, "avg_logprob": -0.28307429780351356, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.9022733087913366e-06}, {"id": 494, "seek": 314958, "start": 3172.1, "end": 3177.46, "text": " you would then use in Elm pages. But that adds a lot of complexity about how do you", "tokens": [291, 576, 550, 764, 294, 2699, 76, 7183, 13, 583, 300, 10860, 257, 688, 295, 14024, 466, 577, 360, 291], "temperature": 0.0, "avg_logprob": -0.28307429780351356, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.9022733087913366e-06}, {"id": 495, "seek": 317746, "start": 3177.46, "end": 3182.98, "text": " make sure that those are in sync and how do you handle some of the underlying things that", "tokens": [652, 988, 300, 729, 366, 294, 20271, 293, 577, 360, 291, 4813, 512, 295, 264, 14217, 721, 300], "temperature": 0.0, "avg_logprob": -0.24797383854898175, "compression_ratio": 1.7339901477832513, "no_speech_prob": 6.083544690227427e-07}, {"id": 496, "seek": 317746, "start": 3182.98, "end": 3189.02, "text": " have to be written in JavaScript or have to be written to something. So yeah it's a it's", "tokens": [362, 281, 312, 3720, 294, 15778, 420, 362, 281, 312, 3720, 281, 746, 13, 407, 1338, 309, 311, 257, 309, 311], "temperature": 0.0, "avg_logprob": -0.24797383854898175, "compression_ratio": 1.7339901477832513, "no_speech_prob": 6.083544690227427e-07}, {"id": 497, "seek": 317746, "start": 3189.02, "end": 3195.18, "text": " a bit annoying but also yeah. Yeah it feels a little funky but like the fact that you're", "tokens": [257, 857, 11304, 457, 611, 1338, 13, 865, 309, 3417, 257, 707, 33499, 457, 411, 264, 1186, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.24797383854898175, "compression_ratio": 1.7339901477832513, "no_speech_prob": 6.083544690227427e-07}, {"id": 498, "seek": 317746, "start": 3195.18, "end": 3200.6, "text": " calling Elm pages run in a project that is not an Elm pages project is like the main", "tokens": [5141, 2699, 76, 7183, 1190, 294, 257, 1716, 300, 307, 406, 364, 2699, 76, 7183, 1716, 307, 411, 264, 2135], "temperature": 0.0, "avg_logprob": -0.24797383854898175, "compression_ratio": 1.7339901477832513, "no_speech_prob": 6.083544690227427e-07}, {"id": 499, "seek": 320060, "start": 3200.6, "end": 3209.86, "text": " problem. And in that case like make an alias to solve that problem. But like in the future", "tokens": [1154, 13, 400, 294, 300, 1389, 411, 652, 364, 419, 4609, 281, 5039, 300, 1154, 13, 583, 411, 294, 264, 2027], "temperature": 0.0, "avg_logprob": -0.28030698439654184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.280477226828225e-05}, {"id": 500, "seek": 320060, "start": 3209.86, "end": 3216.48, "text": " it definitely could be reasonable to have like you know like something called Elm engine", "tokens": [309, 2138, 727, 312, 10585, 281, 362, 411, 291, 458, 411, 746, 1219, 2699, 76, 2848], "temperature": 0.0, "avg_logprob": -0.28030698439654184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.280477226828225e-05}, {"id": 501, "seek": 320060, "start": 3216.48, "end": 3223.38, "text": " or Elm back end task or Elm back end or something and have a package for that have fatal error", "tokens": [420, 2699, 76, 646, 917, 5633, 420, 2699, 76, 646, 917, 420, 746, 293, 362, 257, 7372, 337, 300, 362, 24069, 6713], "temperature": 0.0, "avg_logprob": -0.28030698439654184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.280477226828225e-05}, {"id": 502, "seek": 320060, "start": 3223.38, "end": 3229.06, "text": " and back end task and the things related to that concept exist there in that separate", "tokens": [293, 646, 917, 5633, 293, 264, 721, 4077, 281, 300, 3410, 2514, 456, 294, 300, 4994], "temperature": 0.0, "avg_logprob": -0.28030698439654184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.280477226828225e-05}, {"id": 503, "seek": 322906, "start": 3229.06, "end": 3235.34, "text": " thing. And then at that point actually it could be could be cool because potentially", "tokens": [551, 13, 400, 550, 412, 300, 935, 767, 309, 727, 312, 727, 312, 1627, 570, 7263], "temperature": 0.0, "avg_logprob": -0.25514588356018064, "compression_ratio": 1.8, "no_speech_prob": 2.0904469693050487e-06}, {"id": 504, "seek": 322906, "start": 3235.34, "end": 3241.42, "text": " I could make it like a standalone thing for resolving a back end task where the code to", "tokens": [286, 727, 652, 309, 411, 257, 37454, 551, 337, 49940, 257, 646, 917, 5633, 689, 264, 3089, 281], "temperature": 0.0, "avg_logprob": -0.25514588356018064, "compression_ratio": 1.8, "no_speech_prob": 2.0904469693050487e-06}, {"id": 505, "seek": 322906, "start": 3241.42, "end": 3247.38, "text": " take something of that back end task type and execute that and then give you the resolved", "tokens": [747, 746, 295, 300, 646, 917, 5633, 2010, 293, 14483, 300, 293, 550, 976, 291, 264, 20772], "temperature": 0.0, "avg_logprob": -0.25514588356018064, "compression_ratio": 1.8, "no_speech_prob": 2.0904469693050487e-06}, {"id": 506, "seek": 322906, "start": 3247.38, "end": 3253.7799999999997, "text": " data could be like split off into something and then Elm review could let you use a back", "tokens": [1412, 727, 312, 411, 7472, 766, 666, 746, 293, 550, 2699, 76, 3131, 727, 718, 291, 764, 257, 646], "temperature": 0.0, "avg_logprob": -0.25514588356018064, "compression_ratio": 1.8, "no_speech_prob": 2.0904469693050487e-06}, {"id": 507, "seek": 325378, "start": 3253.78, "end": 3259.46, "text": " end task in some place or whichever tool. So I mean I would I am interested in like", "tokens": [917, 5633, 294, 512, 1081, 420, 24123, 2290, 13, 407, 286, 914, 286, 576, 286, 669, 3102, 294, 411], "temperature": 0.0, "avg_logprob": -0.2413562018916292, "compression_ratio": 1.6404494382022472, "no_speech_prob": 2.0904246866848553e-06}, {"id": 508, "seek": 325378, "start": 3259.46, "end": 3266.34, "text": " being being able to access arbitrary files. Right. Exactly. Maybe not HTTP but I mean", "tokens": [885, 885, 1075, 281, 2105, 23211, 7098, 13, 1779, 13, 7587, 13, 2704, 406, 33283, 457, 286, 914], "temperature": 0.0, "avg_logprob": -0.2413562018916292, "compression_ratio": 1.6404494382022472, "no_speech_prob": 2.0904246866848553e-06}, {"id": 509, "seek": 325378, "start": 3266.34, "end": 3271.2200000000003, "text": " I could make that limitation. So yeah that could be interesting. It probably wouldn't", "tokens": [286, 727, 652, 300, 27432, 13, 407, 1338, 300, 727, 312, 1880, 13, 467, 1391, 2759, 380], "temperature": 0.0, "avg_logprob": -0.2413562018916292, "compression_ratio": 1.6404494382022472, "no_speech_prob": 2.0904246866848553e-06}, {"id": 510, "seek": 325378, "start": 3271.2200000000003, "end": 3277.38, "text": " work that way but maybe under the hood. Right. So that's yeah it's definitely something that", "tokens": [589, 300, 636, 457, 1310, 833, 264, 13376, 13, 1779, 13, 407, 300, 311, 1338, 309, 311, 2138, 746, 300], "temperature": 0.0, "avg_logprob": -0.2413562018916292, "compression_ratio": 1.6404494382022472, "no_speech_prob": 2.0904246866848553e-06}, {"id": 511, "seek": 325378, "start": 3277.38, "end": 3282.36, "text": " could happen in the future. For now I'm really keen to see like what people build with it", "tokens": [727, 1051, 294, 264, 2027, 13, 1171, 586, 286, 478, 534, 20297, 281, 536, 411, 437, 561, 1322, 365, 309], "temperature": 0.0, "avg_logprob": -0.2413562018916292, "compression_ratio": 1.6404494382022472, "no_speech_prob": 2.0904246866848553e-06}, {"id": 512, "seek": 328236, "start": 3282.36, "end": 3287.1400000000003, "text": " and and go from there. But yeah you could definitely imagine a possible future where", "tokens": [293, 293, 352, 490, 456, 13, 583, 1338, 291, 727, 2138, 3811, 257, 1944, 2027, 689], "temperature": 0.0, "avg_logprob": -0.24450579229390845, "compression_ratio": 1.6616541353383458, "no_speech_prob": 1.6028039908633218e-06}, {"id": 513, "seek": 328236, "start": 3287.1400000000003, "end": 3291.6600000000003, "text": " it's sort of designed to fit into more places and I would love to see people using it for", "tokens": [309, 311, 1333, 295, 4761, 281, 3318, 666, 544, 3190, 293, 286, 576, 959, 281, 536, 561, 1228, 309, 337], "temperature": 0.0, "avg_logprob": -0.24450579229390845, "compression_ratio": 1.6616541353383458, "no_speech_prob": 1.6028039908633218e-06}, {"id": 514, "seek": 328236, "start": 3291.6600000000003, "end": 3298.06, "text": " more types of tasks. Yeah. If you split it off then the only thing that you gain is ergonomics", "tokens": [544, 3467, 295, 9608, 13, 865, 13, 759, 291, 7472, 309, 766, 550, 264, 787, 551, 300, 291, 6052, 307, 42735, 29884], "temperature": 0.0, "avg_logprob": -0.24450579229390845, "compression_ratio": 1.6616541353383458, "no_speech_prob": 1.6028039908633218e-06}, {"id": 515, "seek": 328236, "start": 3298.06, "end": 3304.42, "text": " I'm guessing because it's not going to be necessarily faster. Definitely one use case", "tokens": [286, 478, 17939, 570, 309, 311, 406, 516, 281, 312, 4725, 4663, 13, 12151, 472, 764, 1389], "temperature": 0.0, "avg_logprob": -0.24450579229390845, "compression_ratio": 1.6616541353383458, "no_speech_prob": 1.6028039908633218e-06}, {"id": 516, "seek": 328236, "start": 3304.42, "end": 3310.38, "text": " I see for for these scripts is if you want to work with existing data that you have in", "tokens": [286, 536, 337, 337, 613, 23294, 307, 498, 291, 528, 281, 589, 365, 6741, 1412, 300, 291, 362, 294], "temperature": 0.0, "avg_logprob": -0.24450579229390845, "compression_ratio": 1.6616541353383458, "no_speech_prob": 1.6028039908633218e-06}, {"id": 517, "seek": 331038, "start": 3310.38, "end": 3316.94, "text": " your own pages projects. Right. For instance it's used on the Elm Radio website to fetch", "tokens": [428, 1065, 7183, 4455, 13, 1779, 13, 1171, 5197, 309, 311, 1143, 322, 264, 2699, 76, 17296, 3144, 281, 23673], "temperature": 0.0, "avg_logprob": -0.2798414342543658, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.7330096397927264e-06}, {"id": 518, "seek": 331038, "start": 3316.94, "end": 3325.54, "text": " episodes. Right. Episode data. Well now if you want to generate something you want to", "tokens": [9313, 13, 1779, 13, 19882, 1412, 13, 1042, 586, 498, 291, 528, 281, 8460, 746, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.2798414342543658, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.7330096397927264e-06}, {"id": 519, "seek": 331038, "start": 3325.54, "end": 3332.54, "text": " generate a file containing the list of episodes while you just reuse those same back end tasks.", "tokens": [8460, 257, 3991, 19273, 264, 1329, 295, 9313, 1339, 291, 445, 26225, 729, 912, 646, 917, 9608, 13], "temperature": 0.0, "avg_logprob": -0.2798414342543658, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.7330096397927264e-06}, {"id": 520, "seek": 331038, "start": 3332.54, "end": 3336.98, "text": " So that is really nice I think. Exactly. Yeah. For like generating our transcripts where", "tokens": [407, 300, 307, 534, 1481, 286, 519, 13, 7587, 13, 865, 13, 1171, 411, 17746, 527, 24444, 82, 689], "temperature": 0.0, "avg_logprob": -0.2798414342543658, "compression_ratio": 1.609865470852018, "no_speech_prob": 1.7330096397927264e-06}, {"id": 521, "seek": 333698, "start": 3336.98, "end": 3340.9, "text": " there are a set of things that don't have transcripts yet that could just be an Elm", "tokens": [456, 366, 257, 992, 295, 721, 300, 500, 380, 362, 24444, 82, 1939, 300, 727, 445, 312, 364, 2699, 76], "temperature": 0.0, "avg_logprob": -0.22379850685049635, "compression_ratio": 1.860759493670886, "no_speech_prob": 2.684158289412153e-06}, {"id": 522, "seek": 333698, "start": 3340.9, "end": 3347.14, "text": " pages script because right now there's a back end task that goes and looks at the file system", "tokens": [7183, 5755, 570, 558, 586, 456, 311, 257, 646, 917, 5633, 300, 1709, 293, 1542, 412, 264, 3991, 1185], "temperature": 0.0, "avg_logprob": -0.22379850685049635, "compression_ratio": 1.860759493670886, "no_speech_prob": 2.684158289412153e-06}, {"id": 523, "seek": 333698, "start": 3347.14, "end": 3351.58, "text": " and decodes a bunch of front matter from files and all these things that you can do with", "tokens": [293, 979, 4789, 257, 3840, 295, 1868, 1871, 490, 7098, 293, 439, 613, 721, 300, 291, 393, 360, 365], "temperature": 0.0, "avg_logprob": -0.22379850685049635, "compression_ratio": 1.860759493670886, "no_speech_prob": 2.684158289412153e-06}, {"id": 524, "seek": 333698, "start": 3351.58, "end": 3358.66, "text": " Elm pages back end tasks and it figures out the list of episodes which are which exist", "tokens": [2699, 76, 7183, 646, 917, 9608, 293, 309, 9624, 484, 264, 1329, 295, 9313, 597, 366, 597, 2514], "temperature": 0.0, "avg_logprob": -0.22379850685049635, "compression_ratio": 1.860759493670886, "no_speech_prob": 2.684158289412153e-06}, {"id": 525, "seek": 333698, "start": 3358.66, "end": 3364.7, "text": " but don't yet have transcript data. So we could tie that in in an Elm pages script with", "tokens": [457, 500, 380, 1939, 362, 24444, 1412, 13, 407, 321, 727, 7582, 300, 294, 294, 364, 2699, 76, 7183, 5755, 365], "temperature": 0.0, "avg_logprob": -0.22379850685049635, "compression_ratio": 1.860759493670886, "no_speech_prob": 2.684158289412153e-06}, {"id": 526, "seek": 336470, "start": 3364.7, "end": 3371.2599999999998, "text": " actually just run the script and it goes and executes the transcripts for generating transcripts", "tokens": [767, 445, 1190, 264, 5755, 293, 309, 1709, 293, 4454, 1819, 264, 24444, 82, 337, 17746, 24444, 82], "temperature": 0.0, "avg_logprob": -0.24246331203131027, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.0188027772528585e-06}, {"id": 527, "seek": 336470, "start": 3371.2599999999998, "end": 3376.54, "text": " that you need and moving the files to the appropriate file locations and all that. So", "tokens": [300, 291, 643, 293, 2684, 264, 7098, 281, 264, 6854, 3991, 9253, 293, 439, 300, 13, 407], "temperature": 0.0, "avg_logprob": -0.24246331203131027, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.0188027772528585e-06}, {"id": 528, "seek": 336470, "start": 3376.54, "end": 3383.3399999999997, "text": " you mentioned before that this is mostly used for running scripts on your own computer right.", "tokens": [291, 2835, 949, 300, 341, 307, 5240, 1143, 337, 2614, 23294, 322, 428, 1065, 3820, 558, 13], "temperature": 0.0, "avg_logprob": -0.24246331203131027, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.0188027772528585e-06}, {"id": 529, "seek": 336470, "start": 3383.3399999999997, "end": 3389.22, "text": " I know that Elm pages also has support for serverless or all those kinds of things that", "tokens": [286, 458, 300, 2699, 76, 7183, 611, 575, 1406, 337, 7154, 1832, 420, 439, 729, 3685, 295, 721, 300], "temperature": 0.0, "avg_logprob": -0.24246331203131027, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.0188027772528585e-06}, {"id": 530, "seek": 338922, "start": 3389.22, "end": 3396.4199999999996, "text": " I have to admit I don't understand too much. But would this be usable for serverless things", "tokens": [286, 362, 281, 9796, 286, 500, 380, 1223, 886, 709, 13, 583, 576, 341, 312, 29975, 337, 7154, 1832, 721], "temperature": 0.0, "avg_logprob": -0.23504276275634767, "compression_ratio": 1.5982142857142858, "no_speech_prob": 5.626274059977732e-07}, {"id": 531, "seek": 338922, "start": 3396.4199999999996, "end": 3402.4599999999996, "text": " as well or would that be different parts of Elm pages in which case we will talk about", "tokens": [382, 731, 420, 576, 300, 312, 819, 3166, 295, 2699, 76, 7183, 294, 597, 1389, 321, 486, 751, 466], "temperature": 0.0, "avg_logprob": -0.23504276275634767, "compression_ratio": 1.5982142857142858, "no_speech_prob": 5.626274059977732e-07}, {"id": 532, "seek": 338922, "start": 3402.4599999999996, "end": 3409.4599999999996, "text": " it in a later episode. Yeah. So yes and no. So it wouldn't be script but back end tasks", "tokens": [309, 294, 257, 1780, 3500, 13, 865, 13, 407, 2086, 293, 572, 13, 407, 309, 2759, 380, 312, 5755, 457, 646, 917, 9608], "temperature": 0.0, "avg_logprob": -0.23504276275634767, "compression_ratio": 1.5982142857142858, "no_speech_prob": 5.626274059977732e-07}, {"id": 533, "seek": 338922, "start": 3409.4599999999996, "end": 3415.2599999999998, "text": " can be resolved in serverless functions or on a server and that that's what server rendered", "tokens": [393, 312, 20772, 294, 7154, 1832, 6828, 420, 322, 257, 7154, 293, 300, 300, 311, 437, 7154, 28748], "temperature": 0.0, "avg_logprob": -0.23504276275634767, "compression_ratio": 1.5982142857142858, "no_speech_prob": 5.626274059977732e-07}, {"id": 534, "seek": 341526, "start": 3415.26, "end": 3420.9, "text": " routes are in Elm pages v3. It uses back end tasks you can do the same types of things", "tokens": [18242, 366, 294, 2699, 76, 7183, 371, 18, 13, 467, 4960, 646, 917, 9608, 291, 393, 360, 264, 912, 3467, 295, 721], "temperature": 0.0, "avg_logprob": -0.28617102524329874, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.041483751076157e-06}, {"id": 535, "seek": 341526, "start": 3420.9, "end": 3429.1400000000003, "text": " but for scripts there's no reason why you wouldn't use an Elm pages script in your CI.", "tokens": [457, 337, 23294, 456, 311, 572, 1778, 983, 291, 2759, 380, 764, 364, 2699, 76, 7183, 5755, 294, 428, 37777, 13], "temperature": 0.0, "avg_logprob": -0.28617102524329874, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.041483751076157e-06}, {"id": 536, "seek": 341526, "start": 3429.1400000000003, "end": 3434.5, "text": " So like you should you should absolutely use it like outside of your local machine. And", "tokens": [407, 411, 291, 820, 291, 820, 3122, 764, 309, 411, 2380, 295, 428, 2654, 3479, 13, 400], "temperature": 0.0, "avg_logprob": -0.28617102524329874, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.041483751076157e-06}, {"id": 537, "seek": 341526, "start": 3434.5, "end": 3440.3, "text": " again like it's designed to be relatively easy to write a quick and dirty script that", "tokens": [797, 411, 309, 311, 4761, 281, 312, 7226, 1858, 281, 2464, 257, 1702, 293, 9360, 5755, 300], "temperature": 0.0, "avg_logprob": -0.28617102524329874, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.041483751076157e-06}, {"id": 538, "seek": 344030, "start": 3440.3, "end": 3448.0600000000004, "text": " only handles the happy path and then mature into a script with nice error handling and", "tokens": [787, 18722, 264, 2055, 3100, 293, 550, 14442, 666, 257, 5755, 365, 1481, 6713, 13175, 293], "temperature": 0.0, "avg_logprob": -0.22687750770932152, "compression_ratio": 1.7246376811594204, "no_speech_prob": 8.186231070794747e-07}, {"id": 539, "seek": 344030, "start": 3448.0600000000004, "end": 3454.2200000000003, "text": " and be a really robust script. So like I think it's a great tool for like writing team scripts", "tokens": [293, 312, 257, 534, 13956, 5755, 13, 407, 411, 286, 519, 309, 311, 257, 869, 2290, 337, 411, 3579, 1469, 23294], "temperature": 0.0, "avg_logprob": -0.22687750770932152, "compression_ratio": 1.7246376811594204, "no_speech_prob": 8.186231070794747e-07}, {"id": 540, "seek": 344030, "start": 3454.2200000000003, "end": 3460.3, "text": " and maintaining them and having them on your CI and making them really robust over time.", "tokens": [293, 14916, 552, 293, 1419, 552, 322, 428, 37777, 293, 1455, 552, 534, 13956, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.22687750770932152, "compression_ratio": 1.7246376811594204, "no_speech_prob": 8.186231070794747e-07}, {"id": 541, "seek": 344030, "start": 3460.3, "end": 3465.1000000000004, "text": " So one thing that we haven't touched on yet that I want to make sure we mention is the", "tokens": [407, 472, 551, 300, 321, 2378, 380, 9828, 322, 1939, 300, 286, 528, 281, 652, 988, 321, 2152, 307, 264], "temperature": 0.0, "avg_logprob": -0.22687750770932152, "compression_ratio": 1.7246376811594204, "no_speech_prob": 8.186231070794747e-07}, {"id": 542, "seek": 346510, "start": 3465.1, "end": 3474.42, "text": " CLI options. So Elm pages scripts have the ability to to include a CLI options parser.", "tokens": [12855, 40, 3956, 13, 407, 2699, 76, 7183, 23294, 362, 264, 3485, 281, 281, 4090, 257, 12855, 40, 3956, 21156, 260, 13], "temperature": 0.0, "avg_logprob": -0.24248692609261777, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.9333010641275905e-06}, {"id": 543, "seek": 346510, "start": 3474.42, "end": 3481.7, "text": " So for anyone who hasn't heard the term CLI options it's just the term for you know running", "tokens": [407, 337, 2878, 567, 6132, 380, 2198, 264, 1433, 12855, 40, 3956, 309, 311, 445, 264, 1433, 337, 291, 458, 2614], "temperature": 0.0, "avg_logprob": -0.24248692609261777, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.9333010641275905e-06}, {"id": 544, "seek": 346510, "start": 3481.7, "end": 3489.54, "text": " Elm review dash dash fix dash all that would be a command line option that's specifically", "tokens": [2699, 76, 3131, 8240, 8240, 3191, 8240, 439, 300, 576, 312, 257, 5622, 1622, 3614, 300, 311, 4682], "temperature": 0.0, "avg_logprob": -0.24248692609261777, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.9333010641275905e-06}, {"id": 545, "seek": 348954, "start": 3489.54, "end": 3495.9, "text": " a keyword option but is it right. Sorry that one is called a flag. Yeah that was a what", "tokens": [257, 20428, 3614, 457, 307, 309, 558, 13, 4919, 300, 472, 307, 1219, 257, 7166, 13, 865, 300, 390, 257, 437], "temperature": 0.0, "avg_logprob": -0.3241712894845516, "compression_ratio": 1.803030303030303, "no_speech_prob": 2.0579734609782463e-06}, {"id": 546, "seek": 348954, "start": 3495.9, "end": 3501.1, "text": " was I thinking. Yeah. But yeah all the things that you can provide are options I'm guessing", "tokens": [390, 286, 1953, 13, 865, 13, 583, 1338, 439, 264, 721, 300, 291, 393, 2893, 366, 3956, 286, 478, 17939], "temperature": 0.0, "avg_logprob": -0.3241712894845516, "compression_ratio": 1.803030303030303, "no_speech_prob": 2.0579734609782463e-06}, {"id": 547, "seek": 348954, "start": 3501.1, "end": 3508.02, "text": " and the things that start with a dash or dash dash are flags. Is that it. So the ones that", "tokens": [293, 264, 721, 300, 722, 365, 257, 8240, 420, 8240, 8240, 366, 23265, 13, 1119, 300, 309, 13, 407, 264, 2306, 300], "temperature": 0.0, "avg_logprob": -0.3241712894845516, "compression_ratio": 1.803030303030303, "no_speech_prob": 2.0579734609782463e-06}, {"id": 548, "seek": 348954, "start": 3508.02, "end": 3513.98, "text": " do not that only have a key but not a value are called flags. The ones that have a key", "tokens": [360, 406, 300, 787, 362, 257, 2141, 457, 406, 257, 2158, 366, 1219, 23265, 13, 440, 2306, 300, 362, 257, 2141], "temperature": 0.0, "avg_logprob": -0.3241712894845516, "compression_ratio": 1.803030303030303, "no_speech_prob": 2.0579734609782463e-06}, {"id": 549, "seek": 351398, "start": 3513.98, "end": 3523.54, "text": " and value are. So I looked through a lot of different names for these terms and based", "tokens": [293, 2158, 366, 13, 407, 286, 2956, 807, 257, 688, 295, 819, 5288, 337, 613, 2115, 293, 2361], "temperature": 0.0, "avg_logprob": -0.251895137216853, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.11235031758406e-07}, {"id": 550, "seek": 351398, "start": 3523.54, "end": 3529.62, "text": " on common conventions and the the ones that were widely used and seemed like the most", "tokens": [322, 2689, 33520, 293, 264, 264, 2306, 300, 645, 13371, 1143, 293, 6576, 411, 264, 881], "temperature": 0.0, "avg_logprob": -0.251895137216853, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.11235031758406e-07}, {"id": 551, "seek": 351398, "start": 3529.62, "end": 3537.54, "text": " intuitive I I came up with a little label. So we'll link to my Elm CLI options parser", "tokens": [21769, 286, 286, 1361, 493, 365, 257, 707, 7645, 13, 407, 321, 603, 2113, 281, 452, 2699, 76, 12855, 40, 3956, 21156, 260], "temperature": 0.0, "avg_logprob": -0.251895137216853, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.11235031758406e-07}, {"id": 552, "seek": 351398, "start": 3537.54, "end": 3543.7400000000002, "text": " package. This is actually what Elm CLI or what Elm pages scripts uses to parse command", "tokens": [7372, 13, 639, 307, 767, 437, 2699, 76, 12855, 40, 420, 437, 2699, 76, 7183, 23294, 4960, 281, 48377, 5622], "temperature": 0.0, "avg_logprob": -0.251895137216853, "compression_ratio": 1.5779816513761469, "no_speech_prob": 7.11235031758406e-07}, {"id": 553, "seek": 354374, "start": 3543.74, "end": 3550.1, "text": " line options. But there's a little graphic in there that has little annotations of what", "tokens": [1622, 3956, 13, 583, 456, 311, 257, 707, 14089, 294, 456, 300, 575, 707, 25339, 763, 295, 437], "temperature": 0.0, "avg_logprob": -0.29449618179186254, "compression_ratio": 1.736, "no_speech_prob": 1.9637902823888e-06}, {"id": 554, "seek": 354374, "start": 3550.1, "end": 3556.62, "text": " these parts of a command line call are. But yeah so a flag does not have a value. You", "tokens": [613, 3166, 295, 257, 5622, 1622, 818, 366, 13, 583, 1338, 370, 257, 7166, 775, 406, 362, 257, 2158, 13, 509], "temperature": 0.0, "avg_logprob": -0.29449618179186254, "compression_ratio": 1.736, "no_speech_prob": 1.9637902823888e-06}, {"id": 555, "seek": 354374, "start": 3556.62, "end": 3563.22, "text": " know if you write log dash dash stat. Yeah it's a Boolean in a way. Exactly exactly.", "tokens": [458, 498, 291, 2464, 3565, 8240, 8240, 2219, 13, 865, 309, 311, 257, 23351, 28499, 294, 257, 636, 13, 7587, 2293, 13], "temperature": 0.0, "avg_logprob": -0.29449618179186254, "compression_ratio": 1.736, "no_speech_prob": 1.9637902823888e-06}, {"id": 556, "seek": 354374, "start": 3563.22, "end": 3567.52, "text": " It's going to give you a Boolean and then you have keyword ones. You can mix up the", "tokens": [467, 311, 516, 281, 976, 291, 257, 23351, 28499, 293, 550, 291, 362, 20428, 2306, 13, 509, 393, 2890, 493, 264], "temperature": 0.0, "avg_logprob": -0.29449618179186254, "compression_ratio": 1.736, "no_speech_prob": 1.9637902823888e-06}, {"id": 557, "seek": 354374, "start": 3567.52, "end": 3573.06, "text": " order of those ones and it's order independent. You have positional arguments. You can have", "tokens": [1668, 295, 729, 2306, 293, 309, 311, 1668, 6695, 13, 509, 362, 2535, 304, 12869, 13, 509, 393, 362], "temperature": 0.0, "avg_logprob": -0.29449618179186254, "compression_ratio": 1.736, "no_speech_prob": 1.9637902823888e-06}, {"id": 558, "seek": 357306, "start": 3573.06, "end": 3578.9, "text": " optional positional arguments. So Elm CLI options parser is an Elm package that I built.", "tokens": [17312, 2535, 304, 12869, 13, 407, 2699, 76, 12855, 40, 3956, 21156, 260, 307, 364, 2699, 76, 7372, 300, 286, 3094, 13], "temperature": 0.0, "avg_logprob": -0.22331182418331022, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.873870360213914e-06}, {"id": 559, "seek": 357306, "start": 3578.9, "end": 3586.54, "text": " I use it for Elm GraphQL. I've used it for years in Elm GraphQL and it turns a command", "tokens": [286, 764, 309, 337, 2699, 76, 21884, 13695, 13, 286, 600, 1143, 309, 337, 924, 294, 2699, 76, 21884, 13695, 293, 309, 4523, 257, 5622], "temperature": 0.0, "avg_logprob": -0.22331182418331022, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.873870360213914e-06}, {"id": 560, "seek": 357306, "start": 3586.54, "end": 3594.14, "text": " line command into structured data or an error message that tells you the help options of", "tokens": [1622, 5622, 666, 18519, 1412, 420, 364, 6713, 3636, 300, 5112, 291, 264, 854, 3956, 295], "temperature": 0.0, "avg_logprob": -0.22331182418331022, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.873870360213914e-06}, {"id": 561, "seek": 357306, "start": 3594.14, "end": 3600.82, "text": " what went wrong and why the command was not valid. So in Elm pages scripts if you want", "tokens": [437, 1437, 2085, 293, 983, 264, 5622, 390, 406, 7363, 13, 407, 294, 2699, 76, 7183, 23294, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.22331182418331022, "compression_ratio": 1.6794258373205742, "no_speech_prob": 1.873870360213914e-06}, {"id": 562, "seek": 360082, "start": 3600.82, "end": 3607.5800000000004, "text": " to you can accept command line arguments. So our hello world we said script dot without", "tokens": [281, 291, 393, 3241, 5622, 1622, 12869, 13, 407, 527, 7751, 1002, 321, 848, 5755, 5893, 1553], "temperature": 0.0, "avg_logprob": -0.2579798134424353, "compression_ratio": 2.00561797752809, "no_speech_prob": 1.2679263363679638e-06}, {"id": 563, "seek": 360082, "start": 3607.5800000000004, "end": 3614.1000000000004, "text": " CLI options. But if you and that just takes a back end task and that's it. So script dot", "tokens": [12855, 40, 3956, 13, 583, 498, 291, 293, 300, 445, 2516, 257, 646, 917, 5633, 293, 300, 311, 309, 13, 407, 5755, 5893], "temperature": 0.0, "avg_logprob": -0.2579798134424353, "compression_ratio": 2.00561797752809, "no_speech_prob": 1.2679263363679638e-06}, {"id": 564, "seek": 360082, "start": 3614.1000000000004, "end": 3621.02, "text": " log hello world. That's it. Script dot without CLI options script dot log hello. If you want", "tokens": [3565, 7751, 1002, 13, 663, 311, 309, 13, 15675, 5893, 1553, 12855, 40, 3956, 5755, 5893, 3565, 7751, 13, 759, 291, 528], "temperature": 0.0, "avg_logprob": -0.2579798134424353, "compression_ratio": 2.00561797752809, "no_speech_prob": 1.2679263363679638e-06}, {"id": 565, "seek": 360082, "start": 3621.02, "end": 3627.82, "text": " you can accept CLI options. So that would be script dot with CLI options. Then you give", "tokens": [291, 393, 3241, 12855, 40, 3956, 13, 407, 300, 576, 312, 5755, 5893, 365, 12855, 40, 3956, 13, 1396, 291, 976], "temperature": 0.0, "avg_logprob": -0.2579798134424353, "compression_ratio": 2.00561797752809, "no_speech_prob": 1.2679263363679638e-06}, {"id": 566, "seek": 362782, "start": 3627.82, "end": 3636.7400000000002, "text": " it your CLI options parser and then you receive that parsed data and return a back end task.", "tokens": [309, 428, 12855, 40, 3956, 21156, 260, 293, 550, 291, 4774, 300, 21156, 292, 1412, 293, 2736, 257, 646, 917, 5633, 13], "temperature": 0.0, "avg_logprob": -0.26124652739494075, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.6687661122414283e-06}, {"id": 567, "seek": 362782, "start": 3636.7400000000002, "end": 3643.1400000000003, "text": " So you could you know based on based on a flag do one type of back end task or another.", "tokens": [407, 291, 727, 291, 458, 2361, 322, 2361, 322, 257, 7166, 360, 472, 2010, 295, 646, 917, 5633, 420, 1071, 13], "temperature": 0.0, "avg_logprob": -0.26124652739494075, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.6687661122414283e-06}, {"id": 568, "seek": 362782, "start": 3643.1400000000003, "end": 3650.7000000000003, "text": " Yeah that makes a lot of sense. Yeah. So just another sort of like essentially if you think", "tokens": [865, 300, 1669, 257, 688, 295, 2020, 13, 865, 13, 407, 445, 1071, 1333, 295, 411, 4476, 498, 291, 519], "temperature": 0.0, "avg_logprob": -0.26124652739494075, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.6687661122414283e-06}, {"id": 569, "seek": 362782, "start": 3650.7000000000003, "end": 3656.5, "text": " about it if you if you want to do a simple scripting workflow you know in in bash you", "tokens": [466, 309, 498, 291, 498, 291, 528, 281, 360, 257, 2199, 5755, 278, 20993, 291, 458, 294, 294, 46183, 291], "temperature": 0.0, "avg_logprob": -0.26124652739494075, "compression_ratio": 1.6886792452830188, "no_speech_prob": 3.6687661122414283e-06}, {"id": 570, "seek": 365650, "start": 3656.5, "end": 3664.5, "text": " can just pull off positional arguments in node JS you read a bunch of stack overflow", "tokens": [393, 445, 2235, 766, 2535, 304, 12869, 294, 9984, 33063, 291, 1401, 257, 3840, 295, 8630, 37772], "temperature": 0.0, "avg_logprob": -0.24309477258901127, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.3496885458152974e-06}, {"id": 571, "seek": 365650, "start": 3664.5, "end": 3672.58, "text": " questions until you figure out the right incantation and which like which array index the actual", "tokens": [1651, 1826, 291, 2573, 484, 264, 558, 834, 394, 399, 293, 597, 411, 597, 10225, 8186, 264, 3539], "temperature": 0.0, "avg_logprob": -0.24309477258901127, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.3496885458152974e-06}, {"id": 572, "seek": 365650, "start": 3672.58, "end": 3681.38, "text": " user arguments start at and then where to get those. Yeah you mean until you learn which", "tokens": [4195, 12869, 722, 412, 293, 550, 689, 281, 483, 729, 13, 865, 291, 914, 1826, 291, 1466, 597], "temperature": 0.0, "avg_logprob": -0.24309477258901127, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.3496885458152974e-06}, {"id": 573, "seek": 368138, "start": 3681.38, "end": 3687.78, "text": " command line tool you have to use like the use commander or minimists or no not that", "tokens": [5622, 1622, 2290, 291, 362, 281, 764, 411, 264, 764, 17885, 420, 4464, 1751, 420, 572, 406, 300], "temperature": 0.0, "avg_logprob": -0.38132699330647785, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.812994353007525e-06}, {"id": 574, "seek": 368138, "start": 3687.78, "end": 3693.26, "text": " one because it's deprecated or that other one has this problem with duplicate flags", "tokens": [472, 570, 309, 311, 1367, 13867, 770, 420, 300, 661, 472, 575, 341, 1154, 365, 23976, 23265], "temperature": 0.0, "avg_logprob": -0.38132699330647785, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.812994353007525e-06}, {"id": 575, "seek": 368138, "start": 3693.26, "end": 3702.94, "text": " or exactly that stage to that stage. But stage one is just like oh wait the index zero of", "tokens": [420, 2293, 300, 3233, 281, 300, 3233, 13, 583, 3233, 472, 307, 445, 411, 1954, 1699, 264, 8186, 4018, 295], "temperature": 0.0, "avg_logprob": -0.38132699330647785, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.812994353007525e-06}, {"id": 576, "seek": 368138, "start": 3702.94, "end": 3711.1400000000003, "text": " the arguments of the process dot argv or whatever is like the command that was called and then", "tokens": [264, 12869, 295, 264, 1399, 5893, 3882, 85, 420, 2035, 307, 411, 264, 5622, 300, 390, 1219, 293, 550], "temperature": 0.0, "avg_logprob": -0.38132699330647785, "compression_ratio": 1.721951219512195, "no_speech_prob": 2.812994353007525e-06}, {"id": 577, "seek": 371114, "start": 3711.14, "end": 3719.94, "text": " yeah the second one is whatever. And so yeah after you like finally figure that out and", "tokens": [1338, 264, 1150, 472, 307, 2035, 13, 400, 370, 1338, 934, 291, 411, 2721, 2573, 300, 484, 293], "temperature": 0.0, "avg_logprob": -0.22945267172420727, "compression_ratio": 1.614678899082569, "no_speech_prob": 8.851470738591161e-07}, {"id": 578, "seek": 371114, "start": 3719.94, "end": 3724.58, "text": " you pull a single argument because that's all you need and then you realize oh I actually", "tokens": [291, 2235, 257, 2167, 6770, 570, 300, 311, 439, 291, 643, 293, 550, 291, 4325, 1954, 286, 767], "temperature": 0.0, "avg_logprob": -0.22945267172420727, "compression_ratio": 1.614678899082569, "no_speech_prob": 8.851470738591161e-07}, {"id": 579, "seek": 371114, "start": 3724.58, "end": 3729.02, "text": " need to parse different types of options and then you then you go through and figure out", "tokens": [643, 281, 48377, 819, 3467, 295, 3956, 293, 550, 291, 550, 291, 352, 807, 293, 2573, 484], "temperature": 0.0, "avg_logprob": -0.22945267172420727, "compression_ratio": 1.614678899082569, "no_speech_prob": 8.851470738591161e-07}, {"id": 580, "seek": 371114, "start": 3729.02, "end": 3735.14, "text": " which of the NPM packages is cool for that now. And of course it's like Elm is really", "tokens": [597, 295, 264, 426, 18819, 17401, 307, 1627, 337, 300, 586, 13, 400, 295, 1164, 309, 311, 411, 2699, 76, 307, 534], "temperature": 0.0, "avg_logprob": -0.22945267172420727, "compression_ratio": 1.614678899082569, "no_speech_prob": 8.851470738591161e-07}, {"id": 581, "seek": 373514, "start": 3735.14, "end": 3741.62, "text": " good for turning unstructured data into structured data. It's like parse don't validate is what", "tokens": [665, 337, 6246, 18799, 46847, 1412, 666, 18519, 1412, 13, 467, 311, 411, 48377, 500, 380, 29562, 307, 437], "temperature": 0.0, "avg_logprob": -0.24180984497070312, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.812976845234516e-06}, {"id": 582, "seek": 373514, "start": 3741.62, "end": 3749.06, "text": " makes Elm awesome to me I think among other things. But it really shines there whereas", "tokens": [1669, 2699, 76, 3476, 281, 385, 286, 519, 3654, 661, 721, 13, 583, 309, 534, 28056, 456, 9735], "temperature": 0.0, "avg_logprob": -0.24180984497070312, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.812976845234516e-06}, {"id": 583, "seek": 373514, "start": 3749.06, "end": 3756.42, "text": " if you're using minimist or commander or whatever it's just not as nice to work with massaging", "tokens": [498, 291, 434, 1228, 4464, 468, 420, 17885, 420, 2035, 309, 311, 445, 406, 382, 1481, 281, 589, 365, 2758, 3568], "temperature": 0.0, "avg_logprob": -0.24180984497070312, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.812976845234516e-06}, {"id": 584, "seek": 373514, "start": 3756.42, "end": 3762.8599999999997, "text": " these things into nicely structured data. So yeah I find that that's like a really nice", "tokens": [613, 721, 666, 9594, 18519, 1412, 13, 407, 1338, 286, 915, 300, 300, 311, 411, 257, 534, 1481], "temperature": 0.0, "avg_logprob": -0.24180984497070312, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.812976845234516e-06}, {"id": 585, "seek": 376286, "start": 3762.86, "end": 3769.46, "text": " workflow because in Elm pages script like it comes built in with this tool. You just", "tokens": [20993, 570, 294, 2699, 76, 7183, 5755, 411, 309, 1487, 3094, 294, 365, 341, 2290, 13, 509, 445], "temperature": 0.0, "avg_logprob": -0.20705175399780273, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.4144560509521398e-06}, {"id": 586, "seek": 376286, "start": 3769.46, "end": 3775.26, "text": " define your command line options parser and like you sort of know the data you're going", "tokens": [6964, 428, 5622, 1622, 3956, 21156, 260, 293, 411, 291, 1333, 295, 458, 264, 1412, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.20705175399780273, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.4144560509521398e-06}, {"id": 587, "seek": 376286, "start": 3775.26, "end": 3781.56, "text": " to end up with and it has built it. It's all wired in for you. So it has a baked in opinion", "tokens": [281, 917, 493, 365, 293, 309, 575, 3094, 309, 13, 467, 311, 439, 27415, 294, 337, 291, 13, 407, 309, 575, 257, 19453, 294, 4800], "temperature": 0.0, "avg_logprob": -0.20705175399780273, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.4144560509521398e-06}, {"id": 588, "seek": 376286, "start": 3781.56, "end": 3787.1800000000003, "text": " about that. So again that's the philosophy is like trying to remove friction as much", "tokens": [466, 300, 13, 407, 797, 300, 311, 264, 10675, 307, 411, 1382, 281, 4159, 17710, 382, 709], "temperature": 0.0, "avg_logprob": -0.20705175399780273, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.4144560509521398e-06}, {"id": 589, "seek": 378718, "start": 3787.18, "end": 3794.3799999999997, "text": " as possible while still giving you like tools for doing things in a powerful but safe way.", "tokens": [382, 1944, 1339, 920, 2902, 291, 411, 3873, 337, 884, 721, 294, 257, 4005, 457, 3273, 636, 13], "temperature": 0.0, "avg_logprob": -0.21276242644698531, "compression_ratio": 1.673003802281369, "no_speech_prob": 2.8572983410413144e-06}, {"id": 590, "seek": 378718, "start": 3794.3799999999997, "end": 3800.1, "text": " And I think this this fits in with that where like I don't know I just I feel like it's", "tokens": [400, 286, 519, 341, 341, 9001, 294, 365, 300, 689, 411, 286, 500, 380, 458, 286, 445, 286, 841, 411, 309, 311], "temperature": 0.0, "avg_logprob": -0.21276242644698531, "compression_ratio": 1.673003802281369, "no_speech_prob": 2.8572983410413144e-06}, {"id": 591, "seek": 378718, "start": 3800.1, "end": 3806.3399999999997, "text": " prohibitively expensive to actually figure out how to build command line options parsing", "tokens": [16015, 2187, 356, 5124, 281, 767, 2573, 484, 577, 281, 1322, 5622, 1622, 3956, 21156, 278], "temperature": 0.0, "avg_logprob": -0.21276242644698531, "compression_ratio": 1.673003802281369, "no_speech_prob": 2.8572983410413144e-06}, {"id": 592, "seek": 378718, "start": 3806.3399999999997, "end": 3811.58, "text": " for a quick and dirty script in a lot of cases. But when I'm working with this I don't I don't", "tokens": [337, 257, 1702, 293, 9360, 5755, 294, 257, 688, 295, 3331, 13, 583, 562, 286, 478, 1364, 365, 341, 286, 500, 380, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.21276242644698531, "compression_ratio": 1.673003802281369, "no_speech_prob": 2.8572983410413144e-06}, {"id": 593, "seek": 378718, "start": 3811.58, "end": 3817.14, "text": " feel that I feel like I should ask because there are alternatives to running.", "tokens": [841, 300, 286, 841, 411, 286, 820, 1029, 570, 456, 366, 20478, 281, 2614, 13], "temperature": 0.0, "avg_logprob": -0.21276242644698531, "compression_ratio": 1.673003802281369, "no_speech_prob": 2.8572983410413144e-06}, {"id": 594, "seek": 381714, "start": 3817.14, "end": 3825.3799999999997, "text": " Scripts in Elm. I think the most known one is Elm POSIX. There's also ElmScript which", "tokens": [15675, 82, 294, 2699, 76, 13, 286, 519, 264, 881, 2570, 472, 307, 2699, 76, 430, 4367, 21124, 13, 821, 311, 611, 2699, 76, 14237, 597], "temperature": 0.0, "avg_logprob": -0.3002480899586397, "compression_ratio": 1.5353982300884956, "no_speech_prob": 1.130030614149291e-05}, {"id": 595, "seek": 381714, "start": 3825.3799999999997, "end": 3831.7799999999997, "text": " is a name we've used unknowingly so far. At least I did. So ElmScript from Ian McKenzie", "tokens": [307, 257, 1315, 321, 600, 1143, 517, 15869, 12163, 370, 1400, 13, 1711, 1935, 286, 630, 13, 407, 2699, 76, 14237, 490, 19595, 21765, 32203], "temperature": 0.0, "avg_logprob": -0.3002480899586397, "compression_ratio": 1.5353982300884956, "no_speech_prob": 1.130030614149291e-05}, {"id": 596, "seek": 381714, "start": 3831.7799999999997, "end": 3840.94, "text": " and ElmPOSIX from Albert Dahlin. Have you used those for inspiration? Have you seen", "tokens": [293, 2699, 76, 47, 4367, 21124, 490, 20812, 36977, 5045, 13, 3560, 291, 1143, 729, 337, 10249, 30, 3560, 291, 1612], "temperature": 0.0, "avg_logprob": -0.3002480899586397, "compression_ratio": 1.5353982300884956, "no_speech_prob": 1.130030614149291e-05}, {"id": 597, "seek": 381714, "start": 3840.94, "end": 3846.06, "text": " limitations of those or is it just that well it made sense for Elm pages and this is just", "tokens": [15705, 295, 729, 420, 307, 309, 445, 300, 731, 309, 1027, 2020, 337, 2699, 76, 7183, 293, 341, 307, 445], "temperature": 0.0, "avg_logprob": -0.3002480899586397, "compression_ratio": 1.5353982300884956, "no_speech_prob": 1.130030614149291e-05}, {"id": 598, "seek": 384606, "start": 3846.06, "end": 3849.2999999999997, "text": " an entirely novel approach and API.", "tokens": [364, 7696, 7613, 3109, 293, 9362, 13], "temperature": 0.0, "avg_logprob": -0.26134730320350796, "compression_ratio": 1.6188524590163935, "no_speech_prob": 3.3930343761312542e-06}, {"id": 599, "seek": 384606, "start": 3849.2999999999997, "end": 3855.46, "text": " Right. Yeah I'm I've been aware of those tools but but yeah as you say it's more the latter", "tokens": [1779, 13, 865, 286, 478, 286, 600, 668, 3650, 295, 729, 3873, 457, 457, 1338, 382, 291, 584, 309, 311, 544, 264, 18481], "temperature": 0.0, "avg_logprob": -0.26134730320350796, "compression_ratio": 1.6188524590163935, "no_speech_prob": 3.3930343761312542e-06}, {"id": 600, "seek": 384606, "start": 3855.46, "end": 3861.34, "text": " that it's sort of emerged from the wanting to be able to use back end tasks in different", "tokens": [300, 309, 311, 1333, 295, 20178, 490, 264, 7935, 281, 312, 1075, 281, 764, 646, 917, 9608, 294, 819], "temperature": 0.0, "avg_logprob": -0.26134730320350796, "compression_ratio": 1.6188524590163935, "no_speech_prob": 3.3930343761312542e-06}, {"id": 601, "seek": 384606, "start": 3861.34, "end": 3867.7799999999997, "text": " places. And so rather than looking at what's out there how would I do it differently or", "tokens": [3190, 13, 400, 370, 2831, 813, 1237, 412, 437, 311, 484, 456, 577, 576, 286, 360, 309, 7614, 420], "temperature": 0.0, "avg_logprob": -0.26134730320350796, "compression_ratio": 1.6188524590163935, "no_speech_prob": 3.3930343761312542e-06}, {"id": 602, "seek": 384606, "start": 3867.7799999999997, "end": 3872.9, "text": " do I like the way it's done and then designing based on that it was more just I want to be", "tokens": [360, 286, 411, 264, 636, 309, 311, 1096, 293, 550, 14685, 2361, 322, 300, 309, 390, 544, 445, 286, 528, 281, 312], "temperature": 0.0, "avg_logprob": -0.26134730320350796, "compression_ratio": 1.6188524590163935, "no_speech_prob": 3.3930343761312542e-06}, {"id": 603, "seek": 387290, "start": 3872.9, "end": 3877.14, "text": " able to use back end tasks. What would that look like. But that said like comparing them", "tokens": [1075, 281, 764, 646, 917, 9608, 13, 708, 576, 300, 574, 411, 13, 583, 300, 848, 411, 15763, 552], "temperature": 0.0, "avg_logprob": -0.24741080567076967, "compression_ratio": 1.5633187772925765, "no_speech_prob": 1.1015822565241251e-06}, {"id": 604, "seek": 387290, "start": 3877.14, "end": 3885.86, "text": " like Elm POSIX for example it's it has this IO Monad concept where it makes the tradeoff", "tokens": [411, 2699, 76, 430, 4367, 21124, 337, 1365, 309, 311, 309, 575, 341, 39839, 4713, 345, 3410, 689, 309, 1669, 264, 4923, 4506], "temperature": 0.0, "avg_logprob": -0.24741080567076967, "compression_ratio": 1.5633187772925765, "no_speech_prob": 1.1015822565241251e-06}, {"id": 605, "seek": 387290, "start": 3885.86, "end": 3892.42, "text": " of having a single type variable for the for the resulting data that you get meaning that", "tokens": [295, 1419, 257, 2167, 2010, 7006, 337, 264, 337, 264, 16505, 1412, 300, 291, 483, 3620, 300], "temperature": 0.0, "avg_logprob": -0.24741080567076967, "compression_ratio": 1.5633187772925765, "no_speech_prob": 1.1015822565241251e-06}, {"id": 606, "seek": 387290, "start": 3892.42, "end": 3898.78, "text": " errors are not represented in the type which as we talked about is a it's a tradeoff. It's", "tokens": [13603, 366, 406, 10379, 294, 264, 2010, 597, 382, 321, 2825, 466, 307, 257, 309, 311, 257, 4923, 4506, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.24741080567076967, "compression_ratio": 1.5633187772925765, "no_speech_prob": 1.1015822565241251e-06}, {"id": 607, "seek": 389878, "start": 3898.78, "end": 3904.86, "text": " a tradeoff of convenience versus explicitness of possible failures. So it chooses the tradeoff", "tokens": [257, 4923, 4506, 295, 19283, 5717, 28021, 6394, 295, 1944, 20774, 13, 407, 309, 25963, 264, 4923, 4506], "temperature": 0.0, "avg_logprob": -0.25725599082119494, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.601568894533557e-06}, {"id": 608, "seek": 389878, "start": 3904.86, "end": 3910.94, "text": " of convenience which is totally reasonable tradeoff for a command line tool. And yeah", "tokens": [295, 19283, 597, 307, 3879, 10585, 4923, 4506, 337, 257, 5622, 1622, 2290, 13, 400, 1338], "temperature": 0.0, "avg_logprob": -0.25725599082119494, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.601568894533557e-06}, {"id": 609, "seek": 389878, "start": 3910.94, "end": 3918.6600000000003, "text": " the Elm POSIX standard API has a lot more functions in the toolkit designed at designed", "tokens": [264, 2699, 76, 430, 4367, 21124, 3832, 9362, 575, 257, 688, 544, 6828, 294, 264, 40167, 4761, 412, 4761], "temperature": 0.0, "avg_logprob": -0.25725599082119494, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.601568894533557e-06}, {"id": 610, "seek": 389878, "start": 3918.6600000000003, "end": 3927.7400000000002, "text": " for helping you do sort of scripty tasks like reading the the flags for a file and you know", "tokens": [337, 4315, 291, 360, 1333, 295, 5755, 88, 9608, 411, 3760, 264, 264, 23265, 337, 257, 3991, 293, 291, 458], "temperature": 0.0, "avg_logprob": -0.25725599082119494, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.601568894533557e-06}, {"id": 611, "seek": 392774, "start": 3927.74, "end": 3935.22, "text": " making things writable and things like that. So that's not really the it's it's certainly", "tokens": [1455, 721, 10912, 712, 293, 721, 411, 300, 13, 407, 300, 311, 406, 534, 264, 309, 311, 309, 311, 3297], "temperature": 0.0, "avg_logprob": -0.21418260973553324, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.3496885458152974e-06}, {"id": 612, "seek": 392774, "start": 3935.22, "end": 3942.58, "text": " like a little bit confusing but that's that's not the main purpose of of Elm pages scripts.", "tokens": [411, 257, 707, 857, 13181, 457, 300, 311, 300, 311, 406, 264, 2135, 4334, 295, 295, 2699, 76, 7183, 23294, 13], "temperature": 0.0, "avg_logprob": -0.21418260973553324, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.3496885458152974e-06}, {"id": 613, "seek": 392774, "start": 3942.58, "end": 3946.5, "text": " The main purpose of Elm pages scripts again it's like trying to be like a Rails generator", "tokens": [440, 2135, 4334, 295, 2699, 76, 7183, 23294, 797, 309, 311, 411, 1382, 281, 312, 411, 257, 48526, 19265], "temperature": 0.0, "avg_logprob": -0.21418260973553324, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.3496885458152974e-06}, {"id": 614, "seek": 392774, "start": 3946.5, "end": 3953.58, "text": " type thing and trying to be a toolkit for helping to manage your project again like", "tokens": [2010, 551, 293, 1382, 281, 312, 257, 40167, 337, 4315, 281, 3067, 428, 1716, 797, 411], "temperature": 0.0, "avg_logprob": -0.21418260973553324, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.3496885458152974e-06}, {"id": 615, "seek": 395358, "start": 3953.58, "end": 3959.38, "text": " helping the publishing process for Elm radio dot com. That's like that's the type of thing", "tokens": [4315, 264, 17832, 1399, 337, 2699, 76, 6477, 5893, 395, 13, 663, 311, 411, 300, 311, 264, 2010, 295, 551], "temperature": 0.0, "avg_logprob": -0.23106399112277562, "compression_ratio": 1.5739910313901346, "no_speech_prob": 9.515517376712523e-06}, {"id": 616, "seek": 395358, "start": 3959.38, "end": 3966.7799999999997, "text": " it's designed for. You can do whatever you want to with the custom back end tasks but", "tokens": [309, 311, 4761, 337, 13, 509, 393, 360, 2035, 291, 528, 281, 365, 264, 2375, 646, 917, 9608, 457], "temperature": 0.0, "avg_logprob": -0.23106399112277562, "compression_ratio": 1.5739910313901346, "no_speech_prob": 9.515517376712523e-06}, {"id": 617, "seek": 395358, "start": 3966.7799999999997, "end": 3975.62, "text": " it's not like it wouldn't fit well in Elm pages to have a large API for making directories", "tokens": [309, 311, 406, 411, 309, 2759, 380, 3318, 731, 294, 2699, 76, 7183, 281, 362, 257, 2416, 9362, 337, 1455, 5391, 530], "temperature": 0.0, "avg_logprob": -0.23106399112277562, "compression_ratio": 1.5739910313901346, "no_speech_prob": 9.515517376712523e-06}, {"id": 618, "seek": 395358, "start": 3975.62, "end": 3980.7, "text": " and making files executable and things like that. So that's not what it focuses on.", "tokens": [293, 1455, 7098, 7568, 712, 293, 721, 411, 300, 13, 407, 300, 311, 406, 437, 309, 16109, 322, 13], "temperature": 0.0, "avg_logprob": -0.23106399112277562, "compression_ratio": 1.5739910313901346, "no_speech_prob": 9.515517376712523e-06}, {"id": 619, "seek": 398070, "start": 3980.7, "end": 3986.8999999999996, "text": " I don't know. I think it could make sense at least creating directories. Yeah. If you", "tokens": [286, 500, 380, 458, 13, 286, 519, 309, 727, 652, 2020, 412, 1935, 4084, 5391, 530, 13, 865, 13, 759, 291], "temperature": 0.0, "avg_logprob": -0.24878385792607846, "compression_ratio": 1.6908212560386473, "no_speech_prob": 1.1726395996447536e-06}, {"id": 620, "seek": 398070, "start": 3986.8999999999996, "end": 3994.7, "text": " say like this is not what's Elm pages scripts is meant for then you know that people are", "tokens": [584, 411, 341, 307, 406, 437, 311, 2699, 76, 7183, 23294, 307, 4140, 337, 550, 291, 458, 300, 561, 366], "temperature": 0.0, "avg_logprob": -0.24878385792607846, "compression_ratio": 1.6908212560386473, "no_speech_prob": 1.1726395996447536e-06}, {"id": 621, "seek": 398070, "start": 3994.7, "end": 4001.06, "text": " afraid to use it then in a sense that oh well if this is not what it was meant for then", "tokens": [4638, 281, 764, 309, 550, 294, 257, 2020, 300, 1954, 731, 498, 341, 307, 406, 437, 309, 390, 4140, 337, 550], "temperature": 0.0, "avg_logprob": -0.24878385792607846, "compression_ratio": 1.6908212560386473, "no_speech_prob": 1.1726395996447536e-06}, {"id": 622, "seek": 398070, "start": 4001.06, "end": 4007.14, "text": " I might use it in a way that was unexpected or not meant for and then Dillon is going to", "tokens": [286, 1062, 764, 309, 294, 257, 636, 300, 390, 13106, 420, 406, 4140, 337, 293, 550, 28160, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.24878385792607846, "compression_ratio": 1.6908212560386473, "no_speech_prob": 1.1726395996447536e-06}, {"id": 623, "seek": 400714, "start": 4007.14, "end": 4014.54, "text": " pull the plug and remove those features from me. I mean we've seen this in Elmland so.", "tokens": [2235, 264, 5452, 293, 4159, 729, 4122, 490, 385, 13, 286, 914, 321, 600, 1612, 341, 294, 2699, 76, 1661, 370, 13], "temperature": 0.0, "avg_logprob": -0.22866463912160773, "compression_ratio": 1.5442477876106195, "no_speech_prob": 2.6841878479899606e-06}, {"id": 624, "seek": 400714, "start": 4014.54, "end": 4019.8599999999997, "text": " I don't I don't look at it quite like that. So to me it's more about what exists in the", "tokens": [286, 500, 380, 286, 500, 380, 574, 412, 309, 1596, 411, 300, 13, 407, 281, 385, 309, 311, 544, 466, 437, 8198, 294, 264], "temperature": 0.0, "avg_logprob": -0.22866463912160773, "compression_ratio": 1.5442477876106195, "no_speech_prob": 2.6841878479899606e-06}, {"id": 625, "seek": 400714, "start": 4019.8599999999997, "end": 4026.7799999999997, "text": " standard API because a back end task gives you a way to define a custom back end task", "tokens": [3832, 9362, 570, 257, 646, 917, 5633, 2709, 291, 257, 636, 281, 6964, 257, 2375, 646, 917, 5633], "temperature": 0.0, "avg_logprob": -0.22866463912160773, "compression_ratio": 1.5442477876106195, "no_speech_prob": 2.6841878479899606e-06}, {"id": 626, "seek": 400714, "start": 4026.7799999999997, "end": 4032.66, "text": " which is just JSON in JSON out. You can you can build anything with that. So if you want", "tokens": [597, 307, 445, 31828, 294, 31828, 484, 13, 509, 393, 291, 393, 1322, 1340, 365, 300, 13, 407, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.22866463912160773, "compression_ratio": 1.5442477876106195, "no_speech_prob": 2.6841878479899606e-06}, {"id": 627, "seek": 403266, "start": 4032.66, "end": 4042.22, "text": " to build like the difference is that the standard library in Elm pages does not have a lot of", "tokens": [281, 1322, 411, 264, 2649, 307, 300, 264, 3832, 6405, 294, 2699, 76, 7183, 775, 406, 362, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.19239507913589476, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.340499006299069e-06}, {"id": 628, "seek": 403266, "start": 4042.22, "end": 4048.2599999999998, "text": " functions for that built in. So but so you know maybe that in the future could be an", "tokens": [6828, 337, 300, 3094, 294, 13, 407, 457, 370, 291, 458, 1310, 300, 294, 264, 2027, 727, 312, 364], "temperature": 0.0, "avg_logprob": -0.19239507913589476, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.340499006299069e-06}, {"id": 629, "seek": 403266, "start": 4048.2599999999998, "end": 4054.66, "text": " argument for something like you were describing pulling out a separate thing and maybe having", "tokens": [6770, 337, 746, 411, 291, 645, 16141, 8407, 484, 257, 4994, 551, 293, 1310, 1419], "temperature": 0.0, "avg_logprob": -0.19239507913589476, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.340499006299069e-06}, {"id": 630, "seek": 403266, "start": 4054.66, "end": 4059.66, "text": " like an extended standard library. It's a difficult challenge of like how you package", "tokens": [411, 364, 10913, 3832, 6405, 13, 467, 311, 257, 2252, 3430, 295, 411, 577, 291, 7372], "temperature": 0.0, "avg_logprob": -0.19239507913589476, "compression_ratio": 1.634703196347032, "no_speech_prob": 3.340499006299069e-06}, {"id": 631, "seek": 405966, "start": 4059.66, "end": 4067.8599999999997, "text": " together Elm code and this like back end code for doing these Node.js things. But you know", "tokens": [1214, 2699, 76, 3089, 293, 341, 411, 646, 917, 3089, 337, 884, 613, 38640, 13, 25530, 721, 13, 583, 291, 458], "temperature": 0.0, "avg_logprob": -0.3018279790878296, "compression_ratio": 1.5885167464114833, "no_speech_prob": 9.570779866407975e-07}, {"id": 632, "seek": 405966, "start": 4067.8599999999997, "end": 4073.3399999999997, "text": " but potentially you could kind of have the ability to do these things sort of built in", "tokens": [457, 7263, 291, 727, 733, 295, 362, 264, 3485, 281, 360, 613, 721, 1333, 295, 3094, 294], "temperature": 0.0, "avg_logprob": -0.3018279790878296, "compression_ratio": 1.5885167464114833, "no_speech_prob": 9.570779866407975e-07}, {"id": 633, "seek": 405966, "start": 4073.3399999999997, "end": 4081.14, "text": " somewhere but then not expose the back end task set of functions for for using them by", "tokens": [4079, 457, 550, 406, 19219, 264, 646, 917, 5633, 992, 295, 6828, 337, 337, 1228, 552, 538], "temperature": 0.0, "avg_logprob": -0.3018279790878296, "compression_ratio": 1.5885167464114833, "no_speech_prob": 9.570779866407975e-07}, {"id": 634, "seek": 405966, "start": 4081.14, "end": 4084.8599999999997, "text": " default. There are a number of ways I could imagine that going but.", "tokens": [7576, 13, 821, 366, 257, 1230, 295, 2098, 286, 727, 3811, 300, 516, 457, 13], "temperature": 0.0, "avg_logprob": -0.3018279790878296, "compression_ratio": 1.5885167464114833, "no_speech_prob": 9.570779866407975e-07}, {"id": 635, "seek": 408486, "start": 4084.86, "end": 4092.6600000000003, "text": " So what I'm hearing is Pinky promise I won't remove things. And I mean it's it's just like", "tokens": [407, 437, 286, 478, 4763, 307, 17118, 88, 6228, 286, 1582, 380, 4159, 721, 13, 400, 286, 914, 309, 311, 309, 311, 445, 411], "temperature": 0.0, "avg_logprob": -0.26734776650705644, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.6423720100865467e-06}, {"id": 636, "seek": 408486, "start": 4092.6600000000003, "end": 4100.9400000000005, "text": " a back end task is a general purpose tool. It is not very opinionated. It's like like", "tokens": [257, 646, 917, 5633, 307, 257, 2674, 4334, 2290, 13, 467, 307, 406, 588, 4800, 770, 13, 467, 311, 411, 411], "temperature": 0.0, "avg_logprob": -0.26734776650705644, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.6423720100865467e-06}, {"id": 637, "seek": 408486, "start": 4100.9400000000005, "end": 4108.54, "text": " Elm Elm removed like custom the ability to do user defined custom operators but ports", "tokens": [2699, 76, 2699, 76, 7261, 411, 2375, 264, 3485, 281, 360, 4195, 7642, 2375, 19077, 457, 18160], "temperature": 0.0, "avg_logprob": -0.26734776650705644, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.6423720100865467e-06}, {"id": 638, "seek": 408486, "start": 4108.54, "end": 4112.400000000001, "text": " are there. It's not like oh our ports going to stop letting me do whatever it's like no", "tokens": [366, 456, 13, 467, 311, 406, 411, 1954, 527, 18160, 516, 281, 1590, 8295, 385, 360, 2035, 309, 311, 411, 572], "temperature": 0.0, "avg_logprob": -0.26734776650705644, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.6423720100865467e-06}, {"id": 639, "seek": 411240, "start": 4112.4, "end": 4117.86, "text": " port a port is a port. It's like a general purpose language feature that's like core", "tokens": [2436, 257, 2436, 307, 257, 2436, 13, 467, 311, 411, 257, 2674, 4334, 2856, 4111, 300, 311, 411, 4965], "temperature": 0.0, "avg_logprob": -0.2422211295679996, "compression_ratio": 1.9392265193370166, "no_speech_prob": 2.9944017114758026e-06}, {"id": 640, "seek": 411240, "start": 4117.86, "end": 4122.94, "text": " to the design and it's not going to it's not going to go away. Like it's the same with", "tokens": [281, 264, 1715, 293, 309, 311, 406, 516, 281, 309, 311, 406, 516, 281, 352, 1314, 13, 1743, 309, 311, 264, 912, 365], "temperature": 0.0, "avg_logprob": -0.2422211295679996, "compression_ratio": 1.9392265193370166, "no_speech_prob": 2.9944017114758026e-06}, {"id": 641, "seek": 411240, "start": 4122.94, "end": 4130.259999999999, "text": " a back end task like that's just a core concept in in Elm pages and that's not going to go", "tokens": [257, 646, 917, 5633, 411, 300, 311, 445, 257, 4965, 3410, 294, 294, 2699, 76, 7183, 293, 300, 311, 406, 516, 281, 352], "temperature": 0.0, "avg_logprob": -0.2422211295679996, "compression_ratio": 1.9392265193370166, "no_speech_prob": 2.9944017114758026e-06}, {"id": 642, "seek": 411240, "start": 4130.259999999999, "end": 4136.62, "text": " away and that's not going to change like you can you can define your own back end tasks.", "tokens": [1314, 293, 300, 311, 406, 516, 281, 1319, 411, 291, 393, 291, 393, 6964, 428, 1065, 646, 917, 9608, 13], "temperature": 0.0, "avg_logprob": -0.2422211295679996, "compression_ratio": 1.9392265193370166, "no_speech_prob": 2.9944017114758026e-06}, {"id": 643, "seek": 413662, "start": 4136.62, "end": 4142.58, "text": " Right. Yeah. I mostly wanted people to to know what they can rely on without being afraid", "tokens": [1779, 13, 865, 13, 286, 5240, 1415, 561, 281, 281, 458, 437, 436, 393, 10687, 322, 1553, 885, 4638], "temperature": 0.0, "avg_logprob": -0.2440661702837263, "compression_ratio": 1.5945945945945945, "no_speech_prob": 4.092519702680875e-06}, {"id": 644, "seek": 413662, "start": 4142.58, "end": 4148.7, "text": " of like things getting removed. Right. No it's a it's a great point to set expectations", "tokens": [295, 411, 721, 1242, 7261, 13, 1779, 13, 883, 309, 311, 257, 309, 311, 257, 869, 935, 281, 992, 9843], "temperature": 0.0, "avg_logprob": -0.2440661702837263, "compression_ratio": 1.5945945945945945, "no_speech_prob": 4.092519702680875e-06}, {"id": 645, "seek": 413662, "start": 4148.7, "end": 4154.12, "text": " there and again the expectation is like it's a totally general purpose building block and", "tokens": [456, 293, 797, 264, 14334, 307, 411, 309, 311, 257, 3879, 2674, 4334, 2390, 3461, 293], "temperature": 0.0, "avg_logprob": -0.2440661702837263, "compression_ratio": 1.5945945945945945, "no_speech_prob": 4.092519702680875e-06}, {"id": 646, "seek": 413662, "start": 4154.12, "end": 4162.18, "text": " you can you can run whatever you need to in your back end task. But the core libraries", "tokens": [291, 393, 291, 393, 1190, 2035, 291, 643, 281, 294, 428, 646, 917, 5633, 13, 583, 264, 4965, 15148], "temperature": 0.0, "avg_logprob": -0.2440661702837263, "compression_ratio": 1.5945945945945945, "no_speech_prob": 4.092519702680875e-06}, {"id": 647, "seek": 416218, "start": 4162.18, "end": 4169.8, "text": " might not you might not expect the core standard library to expose functions for doing a wide", "tokens": [1062, 406, 291, 1062, 406, 2066, 264, 4965, 3832, 6405, 281, 19219, 6828, 337, 884, 257, 4874], "temperature": 0.0, "avg_logprob": -0.3289192623562283, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.1907410427957075e-06}, {"id": 648, "seek": 416218, "start": 4169.8, "end": 4174.06, "text": " variety of scripting tasks because that's not the main goal. So that's where I went", "tokens": [5673, 295, 5755, 278, 9608, 570, 300, 311, 406, 264, 2135, 3387, 13, 407, 300, 311, 689, 286, 1437], "temperature": 0.0, "avg_logprob": -0.3289192623562283, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.1907410427957075e-06}, {"id": 649, "seek": 416218, "start": 4174.06, "end": 4180.26, "text": " then yourself. Exactly. And that's not going to change. Yeah. All right. OK. Well we are", "tokens": [550, 1803, 13, 7587, 13, 400, 300, 311, 406, 516, 281, 1319, 13, 865, 13, 1057, 558, 13, 2264, 13, 1042, 321, 366], "temperature": 0.0, "avg_logprob": -0.3289192623562283, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.1907410427957075e-06}, {"id": 650, "seek": 416218, "start": 4180.26, "end": 4186.740000000001, "text": " at the end of the script. If I want to make one final pun where can people try this out", "tokens": [412, 264, 917, 295, 264, 5755, 13, 759, 286, 528, 281, 652, 472, 2572, 4468, 689, 393, 561, 853, 341, 484], "temperature": 0.0, "avg_logprob": -0.3289192623562283, "compression_ratio": 1.5594713656387664, "no_speech_prob": 2.1907410427957075e-06}, {"id": 651, "seek": 418674, "start": 4186.74, "end": 4192.179999999999, "text": " because Elm pages v3 has not been released but you can already try this out. So how can", "tokens": [570, 2699, 76, 7183, 371, 18, 575, 406, 668, 4736, 457, 291, 393, 1217, 853, 341, 484, 13, 407, 577, 393], "temperature": 0.0, "avg_logprob": -0.23489621345033038, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.368735977142933e-06}, {"id": 652, "seek": 418674, "start": 4192.179999999999, "end": 4198.74, "text": " they try it out. How can they help. And what are you looking for. Yeah. So I will link", "tokens": [436, 853, 309, 484, 13, 1012, 393, 436, 854, 13, 400, 437, 366, 291, 1237, 337, 13, 865, 13, 407, 286, 486, 2113], "temperature": 0.0, "avg_logprob": -0.23489621345033038, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.368735977142933e-06}, {"id": 653, "seek": 418674, "start": 4198.74, "end": 4207.62, "text": " to a starter repo both in Elm pages starter repo for v3 as well as a minimal boilerplate", "tokens": [281, 257, 22465, 49040, 1293, 294, 2699, 76, 7183, 22465, 49040, 337, 371, 18, 382, 731, 382, 257, 13206, 39228, 37008], "temperature": 0.0, "avg_logprob": -0.23489621345033038, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.368735977142933e-06}, {"id": 654, "seek": 418674, "start": 4207.62, "end": 4213.54, "text": " branch on that repo that gives you the minimum setup for an Elm pages script and also has", "tokens": [9819, 322, 300, 49040, 300, 2709, 291, 264, 7285, 8657, 337, 364, 2699, 76, 7183, 5755, 293, 611, 575], "temperature": 0.0, "avg_logprob": -0.23489621345033038, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.368735977142933e-06}, {"id": 655, "seek": 421354, "start": 4213.54, "end": 4218.42, "text": " some information about Elm pages scripts and how to run them. I would I would love to hear", "tokens": [512, 1589, 466, 2699, 76, 7183, 23294, 293, 577, 281, 1190, 552, 13, 286, 576, 286, 576, 959, 281, 1568], "temperature": 0.0, "avg_logprob": -0.23139308846515158, "compression_ratio": 1.6511627906976745, "no_speech_prob": 1.653660888223385e-06}, {"id": 656, "seek": 421354, "start": 4218.42, "end": 4224.42, "text": " about what people do with them. I think it's like a pretty general tool and I think I'll", "tokens": [466, 437, 561, 360, 365, 552, 13, 286, 519, 309, 311, 411, 257, 1238, 2674, 2290, 293, 286, 519, 286, 603], "temperature": 0.0, "avg_logprob": -0.23139308846515158, "compression_ratio": 1.6511627906976745, "no_speech_prob": 1.653660888223385e-06}, {"id": 657, "seek": 421354, "start": 4224.42, "end": 4229.1, "text": " be surprised by some of the use cases people find for this. Other than that yeah the Elm", "tokens": [312, 6100, 538, 512, 295, 264, 764, 3331, 561, 915, 337, 341, 13, 5358, 813, 300, 1338, 264, 2699, 76], "temperature": 0.0, "avg_logprob": -0.23139308846515158, "compression_ratio": 1.6511627906976745, "no_speech_prob": 1.653660888223385e-06}, {"id": 658, "seek": 421354, "start": 4229.1, "end": 4236.78, "text": " pages docs on pages v3 docs and and the Elm pages channel on Slack is a great place to", "tokens": [7183, 45623, 322, 7183, 371, 18, 45623, 293, 293, 264, 2699, 76, 7183, 2269, 322, 37211, 307, 257, 869, 1081, 281], "temperature": 0.0, "avg_logprob": -0.23139308846515158, "compression_ratio": 1.6511627906976745, "no_speech_prob": 1.653660888223385e-06}, {"id": 659, "seek": 423678, "start": 4236.78, "end": 4245.0199999999995, "text": " ask questions. Yeah. So people will not use Elm pages they will use Elm pages v3 alpha.", "tokens": [1029, 1651, 13, 865, 13, 407, 561, 486, 406, 764, 2699, 76, 7183, 436, 486, 764, 2699, 76, 7183, 371, 18, 8961, 13], "temperature": 0.0, "avg_logprob": -0.2750013229694772, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.4366349887495744e-06}, {"id": 660, "seek": 423678, "start": 4245.0199999999995, "end": 4251.5, "text": " Was that correct. Yeah. There's a package Elm pages v3 beta in hindsight I probably", "tokens": [3027, 300, 3006, 13, 865, 13, 821, 311, 257, 7372, 2699, 76, 7183, 371, 18, 9861, 294, 44357, 286, 1391], "temperature": 0.0, "avg_logprob": -0.2750013229694772, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.4366349887495744e-06}, {"id": 661, "seek": 423678, "start": 4251.5, "end": 4259.0599999999995, "text": " should have called it like Elm pages pre-release or something. But yeah. So and and hopefully", "tokens": [820, 362, 1219, 309, 411, 2699, 76, 7183, 659, 12, 265, 1122, 420, 746, 13, 583, 1338, 13, 407, 293, 293, 4696], "temperature": 0.0, "avg_logprob": -0.2750013229694772, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.4366349887495744e-06}, {"id": 662, "seek": 423678, "start": 4259.0599999999995, "end": 4263.62, "text": " it won't be too much longer before that's a stable release. So hopefully that that instruction", "tokens": [309, 1582, 380, 312, 886, 709, 2854, 949, 300, 311, 257, 8351, 4374, 13, 407, 4696, 300, 300, 10951], "temperature": 0.0, "avg_logprob": -0.2750013229694772, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.4366349887495744e-06}, {"id": 663, "seek": 426362, "start": 4263.62, "end": 4268.66, "text": " will be irrelevant soon. But but definitely keep an eye on the docs if it says deprecated", "tokens": [486, 312, 28682, 2321, 13, 583, 457, 2138, 1066, 364, 3313, 322, 264, 45623, 498, 309, 1619, 1367, 13867, 770], "temperature": 0.0, "avg_logprob": -0.22047352510340074, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.8129652491770685e-06}, {"id": 664, "seek": 426362, "start": 4268.66, "end": 4274.78, "text": " this is now a stable release then keep an eye out for that. I did want to mention one", "tokens": [341, 307, 586, 257, 8351, 4374, 550, 1066, 364, 3313, 484, 337, 300, 13, 286, 630, 528, 281, 2152, 472], "temperature": 0.0, "avg_logprob": -0.22047352510340074, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.8129652491770685e-06}, {"id": 665, "seek": 426362, "start": 4274.78, "end": 4280.5199999999995, "text": " more quick thing which is so I think one of the really exciting things that that comes", "tokens": [544, 1702, 551, 597, 307, 370, 286, 519, 472, 295, 264, 534, 4670, 721, 300, 300, 1487], "temperature": 0.0, "avg_logprob": -0.22047352510340074, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.8129652491770685e-06}, {"id": 666, "seek": 426362, "start": 4280.5199999999995, "end": 4287.42, "text": " from designing these things you know as as data back end tasks are just a type of data", "tokens": [490, 14685, 613, 721, 291, 458, 382, 382, 1412, 646, 917, 9608, 366, 445, 257, 2010, 295, 1412], "temperature": 0.0, "avg_logprob": -0.22047352510340074, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.8129652491770685e-06}, {"id": 667, "seek": 428742, "start": 4287.42, "end": 4294.9800000000005, "text": " that you pass to a specific place a CLI options parser is just a piece of data. So one interesting", "tokens": [300, 291, 1320, 281, 257, 2685, 1081, 257, 12855, 40, 3956, 21156, 260, 307, 445, 257, 2522, 295, 1412, 13, 407, 472, 1880], "temperature": 0.0, "avg_logprob": -0.20983373035084119, "compression_ratio": 1.6255707762557077, "no_speech_prob": 4.495103439694503e-06}, {"id": 668, "seek": 428742, "start": 4294.9800000000005, "end": 4302.34, "text": " thing that comes from that is because it's data you could turn the validator for CLI", "tokens": [551, 300, 1487, 490, 300, 307, 570, 309, 311, 1412, 291, 727, 1261, 264, 7363, 1639, 337, 12855, 40], "temperature": 0.0, "avg_logprob": -0.20983373035084119, "compression_ratio": 1.6255707762557077, "no_speech_prob": 4.495103439694503e-06}, {"id": 669, "seek": 428742, "start": 4302.34, "end": 4309.24, "text": " options into a web interface that presents input fields for all the different flags.", "tokens": [3956, 666, 257, 3670, 9226, 300, 13533, 4846, 7909, 337, 439, 264, 819, 23265, 13], "temperature": 0.0, "avg_logprob": -0.20983373035084119, "compression_ratio": 1.6255707762557077, "no_speech_prob": 4.495103439694503e-06}, {"id": 670, "seek": 428742, "start": 4309.24, "end": 4315.86, "text": " So this is one thing on my mind that I think could be a cool project is to have like on", "tokens": [407, 341, 307, 472, 551, 322, 452, 1575, 300, 286, 519, 727, 312, 257, 1627, 1716, 307, 281, 362, 411, 322], "temperature": 0.0, "avg_logprob": -0.20983373035084119, "compression_ratio": 1.6255707762557077, "no_speech_prob": 4.495103439694503e-06}, {"id": 671, "seek": 431586, "start": 4315.86, "end": 4321.139999999999, "text": " the Elm pages dev server or to have maybe some separate command whatever it may be some", "tokens": [264, 2699, 76, 7183, 1905, 7154, 420, 281, 362, 1310, 512, 4994, 5622, 2035, 309, 815, 312, 512], "temperature": 0.0, "avg_logprob": -0.225061309471559, "compression_ratio": 1.7832512315270936, "no_speech_prob": 2.7693783977156272e-06}, {"id": 672, "seek": 431586, "start": 4321.139999999999, "end": 4328.099999999999, "text": " way to pop up a web page that you can type in your command and get the validation messages", "tokens": [636, 281, 1665, 493, 257, 3670, 3028, 300, 291, 393, 2010, 294, 428, 5622, 293, 483, 264, 24071, 7897], "temperature": 0.0, "avg_logprob": -0.225061309471559, "compression_ratio": 1.7832512315270936, "no_speech_prob": 2.7693783977156272e-06}, {"id": 673, "seek": 431586, "start": 4328.099999999999, "end": 4334.46, "text": " because Elm CLI options parser lets you define validations for all of the CLI flags and it", "tokens": [570, 2699, 76, 12855, 40, 3956, 21156, 260, 6653, 291, 6964, 7363, 763, 337, 439, 295, 264, 12855, 40, 23265, 293, 309], "temperature": 0.0, "avg_logprob": -0.225061309471559, "compression_ratio": 1.7832512315270936, "no_speech_prob": 2.7693783977156272e-06}, {"id": 674, "seek": 431586, "start": 4334.46, "end": 4340.9, "text": " enumerates all the possible ways you could define you could run the CLI all the sub commands", "tokens": [465, 15583, 1024, 439, 264, 1944, 2098, 291, 727, 6964, 291, 727, 1190, 264, 12855, 40, 439, 264, 1422, 16901], "temperature": 0.0, "avg_logprob": -0.225061309471559, "compression_ratio": 1.7832512315270936, "no_speech_prob": 2.7693783977156272e-06}, {"id": 675, "seek": 434090, "start": 4340.9, "end": 4345.98, "text": " all of the optional keyword arguments all of the required keyword arguments. So you", "tokens": [439, 295, 264, 17312, 20428, 12869, 439, 295, 264, 4739, 20428, 12869, 13, 407, 291], "temperature": 0.0, "avg_logprob": -0.20860386839007386, "compression_ratio": 1.8270042194092826, "no_speech_prob": 1.5294082231775974e-06}, {"id": 676, "seek": 434090, "start": 4345.98, "end": 4350.46, "text": " could have a cool set of like drop downs that tell you exactly what's required and what's", "tokens": [727, 362, 257, 1627, 992, 295, 411, 3270, 21554, 300, 980, 291, 2293, 437, 311, 4739, 293, 437, 311], "temperature": 0.0, "avg_logprob": -0.20860386839007386, "compression_ratio": 1.8270042194092826, "no_speech_prob": 1.5294082231775974e-06}, {"id": 677, "seek": 434090, "start": 4350.46, "end": 4356.259999999999, "text": " optional and give you validation messages in real time as you type them so you can see", "tokens": [17312, 293, 976, 291, 24071, 7897, 294, 957, 565, 382, 291, 2010, 552, 370, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.20860386839007386, "compression_ratio": 1.8270042194092826, "no_speech_prob": 1.5294082231775974e-06}, {"id": 678, "seek": 434090, "start": 4356.259999999999, "end": 4361.259999999999, "text": " before you run it what error message you're going to get and you know before you run it", "tokens": [949, 291, 1190, 309, 437, 6713, 3636, 291, 434, 516, 281, 483, 293, 291, 458, 949, 291, 1190, 309], "temperature": 0.0, "avg_logprob": -0.20860386839007386, "compression_ratio": 1.8270042194092826, "no_speech_prob": 1.5294082231775974e-06}, {"id": 679, "seek": 434090, "start": 4361.259999999999, "end": 4366.299999999999, "text": " if the CLI option parsing will succeed and then you can just hit execute when you're", "tokens": [498, 264, 12855, 40, 3614, 21156, 278, 486, 7754, 293, 550, 291, 393, 445, 2045, 14483, 562, 291, 434], "temperature": 0.0, "avg_logprob": -0.20860386839007386, "compression_ratio": 1.8270042194092826, "no_speech_prob": 1.5294082231775974e-06}, {"id": 680, "seek": 436630, "start": 4366.3, "end": 4371.9800000000005, "text": " done and it can just run it from that running server it can actually execute the command.", "tokens": [1096, 293, 309, 393, 445, 1190, 309, 490, 300, 2614, 7154, 309, 393, 767, 14483, 264, 5622, 13], "temperature": 0.0, "avg_logprob": -0.23399625505719865, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.71088060294278e-06}, {"id": 681, "seek": 436630, "start": 4371.9800000000005, "end": 4378.74, "text": " So that is one thing that I think just again it's like using these tools that are built", "tokens": [407, 300, 307, 472, 551, 300, 286, 519, 445, 797, 309, 311, 411, 1228, 613, 3873, 300, 366, 3094], "temperature": 0.0, "avg_logprob": -0.23399625505719865, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.71088060294278e-06}, {"id": 682, "seek": 436630, "start": 4378.74, "end": 4383.58, "text": " around pure functions and data types that describe these things opens up some really", "tokens": [926, 6075, 6828, 293, 1412, 3467, 300, 6786, 613, 721, 9870, 493, 512, 534], "temperature": 0.0, "avg_logprob": -0.23399625505719865, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.71088060294278e-06}, {"id": 683, "seek": 436630, "start": 4383.58, "end": 4384.58, "text": " cool stuff.", "tokens": [1627, 1507, 13], "temperature": 0.0, "avg_logprob": -0.23399625505719865, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.71088060294278e-06}, {"id": 684, "seek": 436630, "start": 4384.58, "end": 4386.66, "text": " Absolutely. Yeah.", "tokens": [7021, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.23399625505719865, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.71088060294278e-06}, {"id": 685, "seek": 436630, "start": 4386.66, "end": 4391.26, "text": " Another thing on my radar for the future is I think it would be really cool to have a", "tokens": [3996, 551, 322, 452, 16544, 337, 264, 2027, 307, 286, 519, 309, 576, 312, 534, 1627, 281, 362, 257], "temperature": 0.0, "avg_logprob": -0.23399625505719865, "compression_ratio": 1.7027027027027026, "no_speech_prob": 4.71088060294278e-06}, {"id": 686, "seek": 439126, "start": 4391.26, "end": 4396.9800000000005, "text": " command for bundling a script so you could write a script run this bundle command and", "tokens": [5622, 337, 13882, 1688, 257, 5755, 370, 291, 727, 2464, 257, 5755, 1190, 341, 24438, 5622, 293], "temperature": 0.0, "avg_logprob": -0.25454695719592974, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.7880552150018048e-06}, {"id": 687, "seek": 439126, "start": 4396.9800000000005, "end": 4401.820000000001, "text": " get a little optimized script that includes all the command line parsing and everything", "tokens": [483, 257, 707, 26941, 5755, 300, 5974, 439, 264, 5622, 1622, 21156, 278, 293, 1203], "temperature": 0.0, "avg_logprob": -0.25454695719592974, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.7880552150018048e-06}, {"id": 688, "seek": 439126, "start": 4401.820000000001, "end": 4407.34, "text": " just a single self-contained node.js file and then you could publish that as an npm", "tokens": [445, 257, 2167, 2698, 12, 9000, 3563, 9984, 13, 25530, 3991, 293, 550, 291, 727, 11374, 300, 382, 364, 297, 14395], "temperature": 0.0, "avg_logprob": -0.25454695719592974, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.7880552150018048e-06}, {"id": 689, "seek": 439126, "start": 4407.34, "end": 4414.780000000001, "text": " package or include it in your in your path somewhere in your bin folder and run it on", "tokens": [7372, 420, 4090, 309, 294, 428, 294, 428, 3100, 4079, 294, 428, 5171, 10820, 293, 1190, 309, 322], "temperature": 0.0, "avg_logprob": -0.25454695719592974, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.7880552150018048e-06}, {"id": 690, "seek": 439126, "start": 4414.780000000001, "end": 4419.62, "text": " your system. So yeah lots of lots of fun things that this could take different directions", "tokens": [428, 1185, 13, 407, 1338, 3195, 295, 3195, 295, 1019, 721, 300, 341, 727, 747, 819, 11095], "temperature": 0.0, "avg_logprob": -0.25454695719592974, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.7880552150018048e-06}, {"id": 691, "seek": 439126, "start": 4419.62, "end": 4420.62, "text": " in the future.", "tokens": [294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.25454695719592974, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.7880552150018048e-06}, {"id": 692, "seek": 442062, "start": 4420.62, "end": 4423.74, "text": " Yeah I'm excited to hear what people use it for as well.", "tokens": [865, 286, 478, 2919, 281, 1568, 437, 561, 764, 309, 337, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.3761530685424805, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.844908860628493e-06}, {"id": 693, "seek": 442062, "start": 4423.74, "end": 4424.74, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3761530685424805, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.844908860628493e-06}, {"id": 694, "seek": 442062, "start": 4424.74, "end": 4429.42, "text": " Could be fun to use it inside of Elm Review but that would be an additional dependency", "tokens": [7497, 312, 1019, 281, 764, 309, 1854, 295, 2699, 76, 19954, 457, 300, 576, 312, 364, 4497, 33621], "temperature": 0.0, "avg_logprob": -0.3761530685424805, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.844908860628493e-06}, {"id": 695, "seek": 442062, "start": 4429.42, "end": 4432.42, "text": " that I don't think people want.", "tokens": [300, 286, 500, 380, 519, 561, 528, 13], "temperature": 0.0, "avg_logprob": -0.3761530685424805, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.844908860628493e-06}, {"id": 696, "seek": 442062, "start": 4432.42, "end": 4437.74, "text": " Right. Which is why maybe bundling it could could become interesting in some use cases", "tokens": [1779, 13, 3013, 307, 983, 1310, 13882, 1688, 309, 727, 727, 1813, 1880, 294, 512, 764, 3331], "temperature": 0.0, "avg_logprob": -0.3761530685424805, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.844908860628493e-06}, {"id": 697, "seek": 442062, "start": 4437.74, "end": 4439.74, "text": " but that's that's fair.", "tokens": [457, 300, 311, 300, 311, 3143, 13], "temperature": 0.0, "avg_logprob": -0.3761530685424805, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.844908860628493e-06}, {"id": 698, "seek": 442062, "start": 4439.74, "end": 4444.38, "text": " Yeah. Well stay tuned and Jeroen until next time.", "tokens": [865, 13, 1042, 1754, 10870, 293, 508, 2032, 268, 1826, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.3761530685424805, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.844908860628493e-06}, {"id": 699, "seek": 444438, "start": 4444.38, "end": 4453.34, "text": " Until next time.", "tokens": [50364, 9088, 958, 565, 13, 50812], "temperature": 0.0, "avg_logprob": -0.45402434894016813, "compression_ratio": 0.6666666666666666, "no_speech_prob": 1.2061936104146298e-05}], "language": "en"}