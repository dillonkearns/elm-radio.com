{"text": " Hello Jeroen. Hello Dillon. Well, we talked about exploring a new API design for forms a while back, but since then I would say I've perhaps formed some new opinions. Did you? Did you? I just wanted to inform you. Oh, thank you. That's very nice of you. Informally, of course. Okay, how am I supposed to react to this, man? You don't need to mutate at all. It's okay. What? What's that mutation joke coming out of the blue? You just said react, so I felt obliged to say something to defend Elm's honor. So yeah. Well, I guess in my view, this is how people should react. You don't want to start a flame war or even start at the tiniest ember, right? I just want to give people an update. That's all. That's all I'm saying. All right. That sounds good. In it? So yeah, we did. Moving along, we did have an in-depth discussion about designing a new form API. And at the end of this episode, we'll link to the episode in the show notes, you asked me if I was planning to create a standalone version of this form API. At the time, I wasn't sure whether or if I could or would. And a few more people asked me, and it turns out I did it. I haven't quite hit publish. Actually, I might do it before this episode goes live. I think I will, but I did it. I extracted a standalone form API. So all of these ideas we talked about are no longer coupled to Elm Pages. And it turns out it worked out pretty well. Cool. So I'm guessing that simplifies Elm Pages API quite a lot. It removed several modules from the API doc sidebar, which was nice. It's already quite large. Well, I guess you still have to learn them, but they're just in a different package. You have to learn them, but it provides an incremental migration path, which is nice if you wanted to change over from a different framework or vanilla Elm over to Elm Pages. And it also gives you the ability to create little standalone shareable LE snippets because you don't need a framework to use this form API. So yeah, they turned out really nicely. So what did you end up doing to make this work, to extract this to a separate package? How did that impact Elm Pages API? And is your form package influenced in any way with how it will interact with Elm Pages? Yeah, you know, it's interesting. At first, I wasn't sure whether some of these ideas would translate to a standalone tool. But after I've gone through this process, I've gotten a little more clarity on it. And I think they do translate. But really, the core idea is this opinion that form data should be managed for you. And you should defer parsing it into higher level data or errors to a later stage. Because when people are, you know, designing things that prematurely parse, you end up with structured data or like semi structured data in your model. And you what I realized is that by doing that you're coupling to your specific app, because you're coupling to those data types like your, this is one of the really interesting things in in Elm is it kind of makes some of these types of coupling more clear, because you have to be so explicit, you know, not just in what, what state you depend on and things like that, of course, but also what types you depend on. Because you can't just say, Oh, I'll take any type, and I'll check if this type is there. And I'll do these special things with these types. You know, you can have like, you can have a type variable, or you can have a concrete type, but you you can't cast things and you can't check what the type is. And so, so what I what I realized is that when, you know, it's a type of coupling, if you have like a form field, and you have a message for changing that form field, you know, for example, check in date changed, if you have a message for that, and then you have some maybe date, because it parses into a maybe date, you've coupled that to this type in your domain. The reason is a problem is because by coupling to that, you don't have a way to abstract things away from that. So like, you know, when you couple to something, you have to deal with it as a special case. But if you decouple from data, and you just say we have low level data, like all we have is key value pairs, then you just store all of that as a single entity. So this is just form data. It's totally unstructured, it's totally untyped, it's just strings, which at the end of the day, that is like, you know, even if you have a number input field, you can be in a state where it's invalid, you can type 1.2. And when you do one dot, that is a string, which is not a valid number. So you need to be able to represent strings at the end of the day, they're all strings. To kind of summarize the previous episode, like Elm Form is a lot about handling things at the very low level, like keeping things in a low level data state, just pairs of strings as a list. So for each field, you will have a value, and the name of the field, and a list of those is just the form data. So that's what you use as the low level data. And you will never transform it to anything else until you actually try to parse it, which you do when you submit it, basically. When you try to submit it. Yes, and also when fields change, when there's a form field event that changes a field, then you present real time client-side validation errors. So you're basically kind of putting these two pieces together, but they're completely decoupled. So you've got low level form data, and you've got a parser. And those two things, the low level form data doesn't know or care about the parser. So it's completely decoupled from it. It is a completely agnostic data format. That's low level, it's not coupled to domain types. The parser does know about your domain types, it parses into whatever types you want to parse into. But by separating those two pieces, the form API can completely take charge of managing all of your form state, including not just the values for your form fields, but the field, the term I'm using for it is field status, but like whether something has been blurred or changed. So all of that low level form state is completely managed by the API. So the end result is you, so the wiring is you have to use form.init in your model to init it, you have to have a form.model in your model, you have to have a form.message in your message, and you need to have an update clause that delegates to form.update. And then you need to render in your view. And that's it. And now that might sound like, okay, that's just like what we're used to doing with Elm packages. Yeah, that's basically the Elm architecture. Right. Like to call it, right? Exactly. The triplets, the Elm triplets. But this is not the pattern that we're used to using for form packages. The pattern that we're used to for form packages is wiring in messages and model fields for every single field. So this simplifies that. So it is a single form.model value for all of the forms you might have on a given page. So if you have multiple forms that you could submit on a page or even throughout your pages. So this is one of the really cool things. You know, if you have, you can wire up this form state, you know, in your shared model between pages and maintain it in a single place. So that was one of the reasons why I wasn't sure if this pattern would translate from Elm pages into a more general use case was because Elm pages had this built in assumption where it had, so it managed the model for the user, right? So Elm pages as a framework can have its own model and then the user's model sits within its model, right? And so it can manage the form state, which is the state for any form. And then it can have its own message. So the Elm pages framework can have its own message, which knows about these form events. So all you have to do is render a form and it renders with this pages message type, you know, HTML pages message. And then the framework knows how to deal with that message. So you don't, that means that you can have a form with real time client side validations that dynamically manages all the form state without needing to call form.init, form.update, have a message for it. All you do is render your form and the framework just takes care of it. So it's, it almost feels like magic when you're using JavaScript and you just wire up a component and it just knows how to use this global state. But it's actually not magic. And if you look at the time traveling debugger, you can see all these explicit messages that are being received. You can still use the time traveling debugger. It's not magic. It's just a baked in opinion. So the magic is that it's decoupled from your type because it doesn't need to know what type your form parses into to take ownership over that form state because it knows what form state looks like. It's just key value strings and blurred, changed, et cetera. Right. So it feels magic because you don't have to wire it in yourself. And the reason why you don't have to wire it yourself is because the types are known because there are those low level data, right? So it's a list of string tuples. Whereas if my form was something that I made myself with the custom types and custom updates, custom messages and so on, then I would have to wire it in somehow. Right. Exactly. In this case, I just have to tell you where it is and that's it. Basically right. In this case, in an Elm pages v3 application still in beta, you say, you know, you say render HTML for the form and you give it a form ID. And that form ID is the unique key that it's going to manage in the dictionary of the form state for all forms in the app. I'm guessing you have an ID so that two forms with the same fields don't conflict. Like if you have a first name in two different pages, they're not using the same data under that. Exactly. So that, you know, that is a constraint of how this package operates is it does operate under the assumption that you're using, you're giving unique form IDs. Actually, I think this would be a really good review rule to check that you have unique form IDs. I was even thinking like, so you can check one, one cool way to check this. It would be kind of interesting to check that like you could by convention check that the declaration name of a form matches the string that you use for the unique identifier. But you still need the ability to make something unique if you render like a delete button for every item or or a quantity, you know, quantity field for every item in the cart. Those each need to have a unique form ID. So in that case, but it would be possible to just scan the page with Elm review and say this needs to have a unique form ID. And if you do, for example, list dot map for each item in the cart, you should use something from that list dot map in your unique ID to to ensure that it's unique. Sounds like react where they tell you, hey, don't use the index as the key for this in this loop. I'm slightly confused here is the form ID for the field or for the form. I thought it was for the form. So that you differentiate between two forms on different pages or different sections of the page. The the form ID is for the form. And then you also have to give a name for each field that's unique within that form. Okay, so you would like to have something that checks that the form IDs are unique, and that for each form, the names are unique. Right. And that would be a best practice just in general for accessibility to have it. Yeah, for accessibility purposes to have a name for each form field is a best practice. In the case of the form, it actually doesn't matter if you have a unique ID if you're only rendering one form on the page. But that's just one requirement that this API has. I think it's a fairly reasonable one. But it would be nice to have either the framework or Elmer view check for whenever you misuse that. Yeah, and that would be very nice. And it's a bit hard to tell which one is more suited because each one will have its own pitfalls and things that it can check and things that it can't check. So a bit hard, but yeah, definitely that would be useful. Yeah, so originally, I wasn't sure if that same pattern would translate outside of Elm pages or if it would even be interesting because this whole core value proposition was manage low level state so that you can simplify the wiring. So you have this magic trick that it's still regular Elm code, but you can just say, render form, give it a unique ID. And now you have real time client side validations for this form parser you defined. And it's like giving you the errors on the fly and you haven't added anything to your model or, you know, all you did is render something in your view. It turns out, I think it's still pretty useful. One of the reasons I think it's useful is because I extracted it out in such a way that you can do that same pattern in your own application if you want to. So you know, I mean, other frameworks could integrate this in the same sort of pattern that Elm pages does. But in your own code base, you could have a single sort of shared model between pages that has all form state and you could have like a shared message. I'm not sure if people use this pattern very commonly. I'm not sure I had seen it before I kind of came up with this for Elm pages. Have you seen this before Jeroen? Of having a sort of framework message and wrapping the page specific messages in the sort of app message? I don't know if I've seen it in packages. I definitely use it at work on something Elm SPA like. I was going to wonder like, do I do that for Elm Review? But no, Elm Review doesn't use messages at all. But at least not in the API. So no, I do use this in a work project, but I don't know if I've seen it in packages. Nice. Yeah. So then, for example, in your code base or other code bases that use that same pattern, you can get the same ergonomics and the same sort of magic trick that Elm pages does where you just create an app level message and each page doesn't have to worry about wiring up that form state. And I have to say, I used to find it to be a bit of a nightmare working with forms in Elm. I used to kind of dread wiring up every single message every time. And this is a big relief. I think, correct me if I'm wrong, but the reason why it works so well for you is because you're using code generation to wire that up partially. Right. Yeah. So that's what I'm doing at work as well. And that only works if you know how something will look right. You can only code generate things that you're aware of. So as you said, if something is opinionated or there is some convention around the practice, then you can make nice tools around it or automate boilerplate writing. So yeah, doing something conventional or standard or really helps with having a nice experience. But I think that's partially the reason why things like Ruby took off because they had so many things around convention. You correct me if I'm right, because I don't know Ruby at all. But since everyone did things in a specific way, people can make tools that assumed or presumed that things were done that way. And then you can make very nice tools. That's also kind of what we do with Elm in a lot of ways. We know that none of our functions will have mutations and all those kinds of things. And we play with that. The only limitation is that we still need to resolve the types in a way that makes everything work together. But apart from that, we kind of do the same with our tooling. And that's why we have a lot of awesome tools. Right. Yes. The thing with Ruby is that it has a lot of capacity for metaprogramming and patching in things. And I think that is one of the things that made it successful. I mean, Rails was sort of built on a lot of those features, being able to use method missing and just patch in methods that made things ergonomic and gave you conventions for being very productive. And I think that's been sort of like a challenge in Elm is like when we have to be so explicit and this programming language is not, it's the opposite of dynamic. It's very static, which means you often have to be very explicit, which sometimes means boilerplate. And so the one downside with having one opinionated way of doing things that you have to abide by is that you're limited in how you do things. So if you do that with your form package, if you have an opinion in a way of managing that form, then you better hope that it's good. Right. Right. Yeah. Which you seem to think it is. I haven't played with it yet. Yeah. We'll see later on. Yeah. Well, you, yeah, you could share your thoughts once you try it out. And we kind of talked about this in our original episode where we talked about exploring this initial API design. But when you have, so choosing what you have opinions on and choosing what you couple to choosing what assumptions to make, right. If you, so this form API has an opinion that form data is a thing. Form fields are a thing. They are input or text area elements and they have key value pairs. That I mean, that is like, that's not my opinion. That's the browser's opinion. So I think that that's a pretty solid foundation for an API to build on. Because you're working with conventions defined by WWC, I think. World Wide Web Consortium. Yeah. Some MDN. You're conforming to MDN. Exactly. That's right. Yes, exactly. So by doing that, this has been something that I've been really thinking about a lot recently is sort of how do we fill that gap where like exactly this sort of sweet spot that Rails and Ruby were able to fit into where you get to use this magic, which makes things very sleek and you can give an amazing demo because you're just like, hey, I just put this thing here and all these things happen. It just works. You don't have to like, okay, I wire this in here. And I have to have a message that responds to this thing. And oh, the types aren't right for this. They didn't line up here. So we have to change that. Right. It's like, that's not very sexy. Right. But you can get a lot closer to that magic when you bake in these assumptions. Yeah. The one thing that I've quite liked with that I've very much liked with all the element implementations where they do code generation and something quite like magic in a way. I'm mostly thinking of Elm SPA where they generate some of the files to do the boilerplate between the main application and all the different pages. The nice thing with that is that you can look at the boilerplate and you can see how things are done. And in Elm SPA's case, you can even overwrite how the boilerplate writing is done, wiring is done. I'm guessing with Elm Pages you won't see that. Right. Yeah. Elm Pages has a specific, I mean, it tries to, you know, give you the right extension points to customize things, but it is like, it generates its main.elm, which does its thing and in that particular part. So it's more like Elm SPA gives you a sort of customizable main.elm, I believe, and Elm Pages gives you points to customize that it pulls into the generated main.elm, but main.elm is not customizable. And there are some limitations, but there are some conveniences with that. Yeah. And it doesn't have to be important to look at how things are implemented under the hood. It really depends on how much do you need it to understand the whole thing. Like for instance, Elm Review has the same mechanism where you give it a list of rules that you config and Elm Review pulls that in and we're good. No one needs to understand how things are wired in into main. They just need to understand what a rule is, what it does, and that's it. Right. Exactly. I mean, that's the power of a good abstraction. Of course, if it's possible to abstract out a certain detail completely, then that's ideal. Yeah. Now I'm curious, will people be able to see the Elm files that you've generated in the Elm stuff folder? If people are really curious on how things work under the hood? Oh yeah. Yeah. You can see all of the code that's generated. Absolutely. Yeah. It's there on the file system. Yeah. I wonder if the editor picks it up, like all the connections. Oh, this function is used in this generated file. Because that would help with understanding. That could get people lost maybe in some cases, but maybe, I don't know. That's a good question. Yeah. It might not, because it does create a separate folder that copies over the Elm.json. Yeah. I've definitely thought about these kinds of considerations. It gets interesting creating an intuitive experience where the editor gives you what you're expecting there. I actually think that it won't, because I don't think it does that for Elm Review either. You can't see where the config that you've defined is used. Right. Right. Yeah. Exactly. Yeah. So we talked about decoupling from the domain-specific data types for having a separate form parser and a separate sort of low-level data type. We haven't yet touched on using that low-level data type for submissions. And this is actually another one of the key pieces that Elm Pages uses for this magic is if you're able to do code sharing, then you can send that low-level data. So what typically will happen with a front-end application these days is you'll have some form data. You manage the form inputs programmatically through JavaScript somehow. And on submit, you intercept the on submit, and then you end up encoding some JSON objects that you send up to an API or something. And you can do that using my Elm Form API. I have a with on submit, and you can use that to... You can even write your sort of form parser in such a way that you can take your form field inputs, and you could even parse it into a JSON object if you wanted to, for example. You could parse it into a tuple with a JSON object and a nice Elm type, which you can use to show optimistic UI, to show the in-progress creating item that you've got. There are all sorts of things you could do. It's quite flexible in that regard, but that works. But that's an extra layer of glue. You know, Lamdara talks a lot about... You know, Mario has given some great talks about this philosophy in Lamdara of removing glue layers. And I think he's done a brilliant job with that. And I believe that this actually removes an additional layer of glue that Lamdara still has in it without using this approach. Because if you think about it, it's a lot of glue to turn these... You know, to do all this managing of form input data and sort of parsing it at various stages. It turns into a lot of glue. And then when you want to send that data, that you're turning this sort of unstructured form data into structured data that you can send up to the server, even with Lamdara, when you can say send to backend, you still... There's an additional challenge that you can't trust that user input because... Because someone can just craft that same request to the backend. Exactly. So they could take the underlying bytes that are sent, they could hack on that, and they could say, okay, well, you know, this represents an Elm type, which is a valid username, which valid usernames must be this many characters and can't have an at or whatever. So that's a nice opaque type, right? It's impossible for that type to be created unless it goes through that code path, right? Except with Lamdara wire, when you do send to backend, it just has a bytes decoder. And magically, that opaque type comes into existence from those bytes. And if you were to hack those bytes and give it a string that's not a valid username, so you actually can't trust it. And, you know, if you look at the meetdown code, Martin Stewart pointed me to some code in that code base, which is a Lamdara app, really, really cool service. And he has a few helpers that sort of have security around that. So he's actually considered that, you know, you know how much we love opaque types and getting guarantees around that, but then you sort of lose those guarantees and you have to revalidate all of your opaque type constraints. Right. And again, only at the edges, but still you have to do it. Yeah. Which, yeah. Right. It starts to feel quite messy. And then you're like, wait a minute, can I really rely on this? And now you're dealing with opaque types that you can't trust. Right? The whole point is guarantees. So that's like, that's the stuff of our nightmares. Right. Yep. Literally. I mean, if you can't trust opaque types, then who can you trust? Exactly. Exactly. Think of the children. So there's the problem. How might this form API help with that? Well, I'm glad you asked. Hey, Dillon, how can your form API, how can it help with that? Excellent question, Jeroen. Good. The answer is you defer parsing that data until you receive it on the server. So now you, with LambdaRacend to backend, what are you sending? You're sending low level key value pairs of strings. Which is the underlying, I mean, this is, as far as the browser is concerned, that's the only thing that exists. Right. That's what form data is. It's low value, low level key value pairs keyed off of the field names. So you can submit those. You don't even need to write an encoder or anything for the submission. You just let it submit those. You can use send to backend to send that low level data. Now in your LambdaRacend backend, when you receive the low level data, now you run your parser. If you want to, you can run that. I mean, you run that parser on the front end. You know, you render with that sort of parsing logic. So you get your client side validations in real time as you type. But then it reruns those same validations because you're using the exact same parser on the backend. Now that's your source of truth. So now the source of truth doesn't come from data that's serialized from the front end to the backend. You've got low level data as the thing you're sending to the backend. And your source of truth is your code sharing of your form between the front end and the backend. So now you can trust your opaque types because you're running them in a trusted environment in the backend. And I actually did a small example with a really very simple little LambdaRacend toy app I have, but I tried it out and it works great. You just run your form parser on the backend and get your data and check if it's valid or invalid and it works beautifully. Yeah. So people can still hack the bytes that are sent, but the backend will just refuse it because, hey, this is not a opaque type. And I'm guessing they will have to handle that somehow. But that's not a lot of work. Do you mean the opaque type of the, what they're sending is key value strings? Yeah. What I mean is they get the backend gets the low level data and they parse it into an opaque type or an error, a result, a failing result. And then they need to handle the successful case and the error case. Yes. And that's running through backend code. So that can be trusted. So the thing is like conceptually, any user, trusted or not, has the ability to send data to the server. Right. And it's the server's job to then take that unverified, untrusted data and decide whether to trust it and to do something with it or ignore it. Yeah. And it should probably never really trust it anyway. Right. Exactly. So by deferring, parsing it from low level to structured data until it gets to the server, you are following that conceptual mental model of not trusting it because you're just like, hey, you can send me key value pairs, right? Like you can go to, you know, I could open up some, you know, web tools and make a submission to Amazon, to some Amazon forum and type in the key value pairs I want to. And I could try to like put some unwanted values in there, but you know, and they can decide what to do with that in return. But it's the job of the server to decide what to trust because the client can't be trusted. The client can be anything. And we don't really care what the client is. Like the client could be Vim or it could be a script or it could be a curl command. And we don't really know, like we can kind of try to put some things through JavaScript onto the page to put like a, you know, a little hidden field to try to prevent that. But we can't really know that we can trust the client because the client can do anything. But the server we control. It's kind of weird that we have both the sentence, the client can't be trusted and the client is king. But try to merge those together. I don't know if I've heard that one. Yeah. I've heard that one before. You haven't heard that one? No. It's mostly for restaurants or. I've heard the customer is king. Customer is always right. Oh, maybe that's just a. It's French because customer is client. That makes sense. That makes sense. That's fair. So that's a French English joke. Hmm. Interesting. It's always a bit hard to figure out who can you tell those jokes that mix two languages. I have some jokes that I can only tell my family because they're like French and Dutch. And it's a mix of those. There's some good ones, but you can only tell them to specific groups of people. And it's such a shame. We can tell the Elm React jokes here, but that's about it. So because we're dealing with low level data, you don't have a lot of guarantees around it to start with. So for instance, how do you make sure that in a form that I'm going to send the backend that I have not forgotten a field at the backend expects? Like you know, usually we try to create a nice opaque type and that opaque type in it, it expects something of a specific shape. Like if it's Jason, then you parse it and you expect it to be a specific shape. If it does, then you get an okay. And otherwise you get an error. And that is always a bit tricky to get right in the sense that if you forget it, then you're likely not going to notice it unless you write specific tests for that, which I personally don't, but I probably should. So how do you avoid that problem with your form API? Now do you mean like a required field? Yeah, let's say there's a required field that is the first name and you forgot to add a field for that because you know, you and me, we both like guarantees and all those things. And we like things type check. And I would like to check that this first name is never forgotten. So how do I ensure that? Is there something in your form or is it just something that you have to write tests for? Yeah. So you can, you can write a required form field. And if that field is absent, then that validation will fail. So your form will just be parsed as invalid and you'll get a client side, you know, you'll get a client side validation. You can forget to render a form field. The form fields we discussed in the last episode on this, that it uses this sort of Elm codec like pattern where there's a pipeline where you sort of pipe through adding each field in the form, and then you get those in a Lambda and you, you use that to parse into the data type and you use that to render your view with all the form fields. You could forget to render a form field. And if it's, you know, if it's optional and you don't do any validation on it, then that that is something that will pass that validation because it's just like, yeah, there's no, this field isn't there and that would be valid. So that is something to look out for. Again, an Elm review rule would be a great, great way to check that you've used every field in your, in your view. I think the unused parameters rule would mostly work, but you probably should not. It could be used still because you need to use it in two places. Oh, cause you use, you use your form fields in your, in your parser definition. You use them in two places. You use them in your view to render a custom, custom view, however you want to render it. And you also use it to do dependent validations, to combine together all of your fields into a data type. Okay. So in practice, would you write tests for that? Yeah, I think, I think it's a great thing to write tests for. Yeah, absolutely. Yeah. That would be a really good thing for using Elm program test for. Yeah, that would actually work quite well. Yeah. So we talked about sending this low level form data with Lambdaera and I was pretty excited with how nicely this worked. And again, if you wanted to, you could even take it a step further and have these app messages where the form messages don't need to be wired in for every single page, right? You could have a single application wide message for your form messages and you just render your, so you could get that same magic that Elm pages sort of bakes in. You could bake that into your own framework or there could be a Lambdaera specific framework that bakes that in. So there's all sorts of opportunities for sort of getting that magic there, which is, which is pretty cool. But yeah, I was pretty excited with how this, this pattern still works with Lambdaera. And to me, this, this takes this like Lambdaera philosophy of like removing glue and removes a big piece, right? Cause you're like, wow, I can build an app with no glue. That's amazing. So you're saying I can like send data to the backend with no glue and receive it from the backend with no glue and it's all real time data. That's amazing. It's like, yeah, no, no glue. Okay. Well, what if I want to like send up some data, like input some data and send it up? It's like, okay, you write a message and you, it's like, oh, okay. That's, that sounds like a lot of, a lot of glue. So I feel like, I feel that this removes a last remaining piece of Lambdaera glue if you, if you use this pattern in Lambdaera. You're removing glue while keeping things working. Yes. In a way that is predictable. Right. Now this gets to an interesting question in the case of Lambdaera. So, you know, Lambdaera also has this really interesting guarantee that you can migrate data and it's like a guaranteed safe migration because you have to account for all of your data and how you, even how you would deal with messages that are coming into the server from an old version. Yeah. So when you say migration or migrating messages and models, it's when a old client talks to a new backend or the other way around or in all those directions that are possible, it migrates things in a way that they can communicate safely. Right exactly. And you can migrate your front end model and to new versions of the app and all this. Right. So that's one of the big value propositions of Lambdaera. So then you take this form data, which maybe let's say before it was more structured form data, you had a check-in date was in your front end model and your checkout date and your first name and last name. And now, and you had a send to backend, which took that and you had a specific message for that. And now you lose the migration of all of that type safe data that knew exactly what the shape is and what you had in the front end and all of that. Right. So this is something I've been thinking about a little bit. I think that let's say that you, let's say that you add a new field and it's a required field. Now you are going to, if the user's on that page, now suddenly there's a new field, but there's also going to be a client side validation that says you're missing a required field. Wait, just so we're clear, the client is on the old version or the new version with a new field? Well, if the client migrates to the new version. Okay. I hope I'm understanding how it works correctly. Maybe it does it not migrate. I think it migrates. I could be wrong on that. I have to check, but whatever it does, the source of truth is the form parser and the backend checks that source of truth and the backend is able to give validation errors as well. So, I think it lines up. I might be missing a detail about how this works, but I believe it's the paradigm still works out for at least for cases where you're adding a field. If you're removing a field, of course, it's all kind of messy. There's no simple, neat way to migrate a form as somebody is entering data into a form. I think with the Lemdara, if you have a frontend in V1 and it sends a message to your backend in V2, then that message will be, that sent to backend message will be migrated. Because your data is low level, you will not have a migration. Although I'm guessing you could write a migration, even when the types don't change. If you wanted to create an introduce a new variance name, just to be explicit about that. Yeah, you absolutely could. That's a good point. If you wanted to, you still could. But in this case, you would probably say, hey, you're missing this required field. Just refresh the page because you can't migrate. You can't add missing data to form, right? Right. Exactly. Yeah. And I believe it, looking at the Lemdara docs, I think it does migrate the frontend model as well. So you're going to get an update where it's showing you the new form fields. Yeah. Then it's mostly an issue about when do you get the update and when is the, how does the deployment work? Right. But at the end of the day, I think it's still like, so my bottom line is I feel like you don't lose a whole lot by using the lower level form data to send it to the backend because at the end of the day, you're going to need to handle the possibility of an invalid form being sent to the backend. There's no avoiding that because clients can send anything. Even if it's only supposed to send through Lemdara, you can still hack it and send bytes over the wire. It's like, that's just how servers work. You can send data to them. And so I think that this approach works really well where you're just saying the source of truth, the gatekeeper is the form parser and you can code share that to show the client side validations and turn it into structured data on the backend or get validations, which we can send back to the client and say, Hey, you must have bypassed something, but you gave me something invalid, right? Or you can also do backend specific validations. That's also possible with this design. But I think it's a sound approach because at the end of the day, you still need to handle the possibility of receiving invalid data on the backend. And if there's a migration and suddenly the front end just migrates to something where now there's a missing required field, you send it to the backend. It doesn't matter that you didn't do a migration. The new version of the code will handle that validation and then you'll also see that validation error on the front end. So I think it's a sound approach. Maybe some Lambdaero nerd will give me a corner case. I would be very interested to hear if there's a corner case where it really isn't as safe, but I think it works pretty well. So as far as I can tell, this works pretty well when the backend expects the same low level data that you're using in the front end. So that works very well on pages. I can work very well in Lambdaero as well. How does that work with REST APIs or GraphQL? Do you just keep that low level data in the front end? And then when you try to submit, you need to parse that and make the REST or GraphQL request, but you don't have that same technique that you use on the backend about getting low level data. Right. Yeah, that's a great question. So and at first when I was extracting this to a standalone API, I wasn't sure if that would pan out and if the pattern would apply well to a use case where it's front end only Elm because we're talking about full stack on pages v3 and Lambdaero Elm. But actually, I think it does work quite nicely. And one of the patterns I hinted at earlier is that so in your form definition, you take all of your form fields and combine them and you can add additional validations and dependent validations between the form fields. And then you tell it how to parse that successfully or unsuccessfully into structured data. So you could parse it into a record just like a JSON decoder. You can parse into a record, but you could parse it into any data. And as you're parsing it, you're basically telling it, you know, so the most common pattern will have will be to have like a type alias for a record and to use that record constructor, you know, just like in a decoder, you say decode dot succeed user, and then you just keep adding on these decoders to applicatively add data to that constructor. But that record constructor is just a function. So you could just as well say a function that takes this value and this value and turns it into these values. And you can put it into a record if you want. But what if instead of putting it into a record, you created a tuple with a JSON encode object and the structured data if you wanted some structured data to show in a pending UI to show this is the item that we're creating right now. But you can also construct, you know, you could even construct like an API request payload thing that maybe there's a custom type for which endpoint you're sending to and maybe there's a JSON encode value for the payload to send. Or you could build up, you know, an Elm GraphQL input object or whatever it is. So like, because you have the data right there, you have all of the fields directly to combine into whatever data type you want. So, so when you render the form, you can say, with on submit, and, and then you can either get the valid or invalid data. And with the valid data, you have the successfully parsed data type. So we've talked about low level data with forms. Do you see any other places where we could use this technique? I'm thinking maybe HTML bytes or I mean, we use it for JSON to some extent, maybe? No, it's not the same thing, right? This is a very good question. I'm trying to think now. I have noticed as a general principle, with API design, sometimes when I have a type variable in an API I'm designing, I'll try to sort of squeeze out the type variable by distilling things down to lower level data. So that's sort of like something I look for when I'm designing APIs. But but yeah, that's a good question. Like what are the pain points from because I really feel like, you know, I might sound like a broken record now, but it just feels like the way we were working with forms in Elm before was so tedious. And we worked with forms that way for so long. And it like it wasn't it wasn't good. I don't know. I mean, we were clueless and now we are glueless. Let's just say it's been a formative experience. We needed a reform. We can all agree. I almost feel like I have just made one pun and you're like, okay, now I can throw them all. Never never look a pun artist directly in the eye. Your kids are never going to look you in the eye. It's too dangerous. It's a great question. Can you think of anything that might benefit from low level data? I don't have anything in mind. No, no. But I'm sure the listener will tell us if they come up with something. Please do. Actually, there's one use case. Simon Liddell, he made a Elm URL package. I don't remember the exact name. Elm app URL. Elm app URL. And that is basically the URL as a string. Let's keep it as a string. Let's pattern match on it as a string. Or as a list of strings, but still pretty low level. So yeah, I think that works instead of having a parser with a type and yeah, URL. Good point. Yeah, actually, you know, this technique of defunctionalization too, in a way is a form of this. So defunctionalization being instead of having like closures of things to execute, you turn it into data, just like a message is defunctionalization. Instead of having like an on click function, you have a message. And each and the handling for each message will do a different thing. Right. And so I think there are certain ways to kind of combine this idea of defunctionalization and decoupling to lower level data. For example, an HTTP call. You could so like Mario and I were discussing with the Elm pages back end task API, it doesn't use Elm HTTP, it has its own back end task HTTP API. So how could you take a request, but the request is this sort of opaque thing that you can't do anything with. So if you extracted that out to its parts and had the request object be just a data type, maybe that would make it more shareable. Yeah, because that would be a common denominator. Yeah, I'm not sure it's exactly the same principle, though, because like, there's still, it still knows about some of the types in your specific use case, whereas we're talking about not knowing about anything like form data doesn't know about anything. Yeah, I'm gonna have to think on this a little bit. But but yeah, listeners, please do tweet at us or message us in the Elm Radio Slack channel if you think of anything. Don't talk about it too hard, because I don't want to delay the publishing of this package. Good point. So there is one thing I feel I should mention, which this package works with Elm CSS. And it works with Elm HTML. It does not work with Elm UI, unfortunately. The reason is because this package has strong opinions about form fields. And Elm UI doesn't expose a way to natively render form fields, except by dropping into HTML. But there's no way to render semantic form fields. So like, this library, this package assumes form field events, like native HTML form field events. And it also just doubles down on trying to use forms for accessibility and wrap things in form elements and that sort of thing. So that's just sort of a baked in opinion. I would love to see Elm UI expose some way to render form components, but at the moment, it doesn't expose that. So it was quite an interesting experience, sort of. When I was building this, I first started with an experiment to extract it to a standalone package. I just took this Elm package starter repo that I have, and I just copied the relevant code in there, and then without having the specific Elm pages dependencies, and then I just saw what was read. It was really, really interesting. Like a few of those things, like those were the points that I needed to abstract away. And what I ended up doing was really like doing an inversion of control in a lot of those places, which turned out to be, I think, a really just a nice way to design it overall. For example, like the original API just sort of reached in and grabbed state from the sort of Elm application state, which it knew about. But then when you decouple it and it doesn't know what Elm application state is, now you have to pass that in. So you say, is it submitting this form? And then what I ended up with is I ended up creating like a little adapter module. It's just like using the adapter pattern. So it's a very thin layer that, so instead of calling form.renderHTML in an Elm pages app, you can call pages.form.renderHTML. And whereas the standalone package takes a record where you give it submitting equals and then however you want to manage that, you can say model.submitting in Elm pages, it knows whether it's submitting because it handles submitting for you. So when you say submit, Elm pages takes the low level form data. It has, it keeps track of the state in its own framework level model of the in progress form submissions. And it knows how, given its model, how to say whether a field is being submitted. That was, so it was pretty cool. Like I basically just said like, oh, all these things that depend on the Elm pages application state. Okay. I'll invert that. Submitting equals some Boolean. And then in the adapter, I took all that code that had the logic that knew about the Elm pages application state to deduce whether it was submitting or not. And just gave that as the value for submitting equals all that logic I extracted that knew about Elm pages. So it's pretty cool. It was a lot cleaner than I expected. When you started, did you think it would not be possible? I kind of did. I also like had a few places where it was coupled to the backend task API, because you could do backend specific validations. If you wanted to, for example, check that a username is unique or try sending an email to check if an email is valid or whatever on the backend. But it turned out I was able to extract that out into this API where you combine together a set of form definitions, and it will try running the parser on any of those. And it turned out it was a very light wrapper to be able to parse into a form. Basically what it turned into is, so instead of the form parsing into a user record, or the form parsing into a JSON encode value, what it ended up being is just the form can parse into a backend task. If it fails to parse into a backend task, that means that the client-side validation has failed. If it succeeds in parsing into a backend task, now you can continue doing backend validations. And I ended up splitting off this thing that you can render to kind of give server state. So you can give error messages from the backend if you want to. I was actually not expecting it to work out. Maybe not even at all. So it was a pleasant surprise. But now that I did it that way, I think it's really nice also to be able to just look at the docs separately, to learn this tool separately, for people to be able to talk about this package, not just like, oh, it's this Elm Pages specific thing. It's the same API, if somebody's using Elm Pages or somebody's using Lambda or somebody's using Elm SBA, they can all talk about the same package. Yeah. I think it might add a bit of confusion if you're used to neither, because then, oh, what is this form? Oh, well, this is in that other package. Oh, I didn't see that one. But apart from that, sounds like a clear win to me. Especially if people start using it in other places, and then they start using Elm Pages, then the learning curve is a lot smaller. So that's nice. Right. Yeah. Yeah. One of the tricky things, design decisions that I dealt with here was, there's this config. So I have this options type, which when you render a form, you give it options, which has like, it must have the form ID, the unique ID for the form. But it can also have a form method. So a form can be get or post. This is like, most front-end only applications aren't going to care about this, because they're not progressively enhancing a form, whereas Elm Pages uses progressive enhancement to say... So I have Elm Pages examples where I use get form submissions, because when you do a get form submission, it appends query parameters to your URL and reloads the page. So for example, if you're doing a search query, that can be a really nice way to do that. And you can progressively enhance that. So instead of doing a hard page load, you can do an XHR request. That's great. But so anyway, the Elm Form API has this baked in opinion that a form method is a thing that exists. And most people will probably ignore it, but it does have that baked in, even though probably Elm Pages apps will be the only ones to use it. But a Lambdaera app could use it too, if it wanted to, or a front-end only app. So I left it in there because it seemed like a reasonable opinion that that exists, since it does exist in the browser. But yeah, so I created this options record and use a little builder pattern to build up these options. And I share that options record between the Elm Pages package and the standalone package. So I'm guessing that means that whenever you need to do a change in the form package, you will also need to update that in Elm Pages. So you will need to bump them together to keep them in sync. What I ended up doing is I just basically in that little facade module that I have in the Elm Pages API, pages.form, what I did is one of the arguments is the standalone packages options. Got you. Okay. And you transform that to the options of Elm Pages. Right, exactly. So I supplemented. So the Elm Pages options are a superset of that. So I said, you know what, I'm just going to, so that they can share as much as possible, I'm going to share this options type between the two. Even though there are a few additional options that you can pass into the Elm Pages one. So I think that worked out pretty nicely, but that was definitely a subtle design decision there. All right. So if people want to know more about Elm Form and Elm Pages, or your form package, did you even call it Elm Form? What is the name? Did you announce it during this whole episode? No, I somewhat was leaving room for the possibility that I changed my mind about what to call it. But yeah, I think for now I'm calling it Elm Form. I think Elm Form, I don't think I need to get too clever about it. It does kind of like represent a, I think, pretty different philosophy around how to deal with forms in Elm. So I definitely have considered like, does that deserve a different name? But yeah, I'm just calling it Elm Form. Not Elm Awesome Form Meatballs or something. That sounds delicious. It does sound delicious. Yeah so Elm Form. So you know, we'll link to the package docs. Hopefully by the time this is live, we'll link to live package docs, not the preview docs. And I'll try to get some nice ELE examples there, because that's one of the cool things we can get ELE examples. But there is a nice examples folder in the repo, which I'll link to. And yeah, I'll also link to a few examples of it in action in Lambdaera and in Elm Pages. So you can sort of compare the Elm Pages one. It can get pretty sophisticated because you can do all these use cases like dealing with in-flight submissions and parsing that data to get optimistic UI and things like that. But yeah, so I would start there. And if you have any feedback, let Dillon know. Yes. And Jeroen, until next time. Until next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " Hello Jeroen.", "tokens": [2425, 508, 2032, 268, 13], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 1, "seek": 0, "start": 2.0, "end": 4.0, "text": " Hello Dillon.", "tokens": [2425, 28160, 13], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 2, "seek": 0, "start": 4.0, "end": 12.200000000000001, "text": " Well, we talked about exploring a new API design for forms a while back, but since then", "tokens": [1042, 11, 321, 2825, 466, 12736, 257, 777, 9362, 1715, 337, 6422, 257, 1339, 646, 11, 457, 1670, 550], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 3, "seek": 0, "start": 12.200000000000001, "end": 15.68, "text": " I would say I've perhaps formed some new opinions.", "tokens": [286, 576, 584, 286, 600, 4317, 8693, 512, 777, 11819, 13], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 4, "seek": 0, "start": 15.68, "end": 16.68, "text": " Did you?", "tokens": [2589, 291, 30], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 5, "seek": 0, "start": 16.68, "end": 17.68, "text": " Did you?", "tokens": [2589, 291, 30], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 6, "seek": 0, "start": 17.68, "end": 19.68, "text": " I just wanted to inform you.", "tokens": [286, 445, 1415, 281, 1356, 291, 13], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 7, "seek": 0, "start": 19.68, "end": 20.68, "text": " Oh, thank you.", "tokens": [876, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 8, "seek": 0, "start": 20.68, "end": 22.68, "text": " That's very nice of you.", "tokens": [663, 311, 588, 1481, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 9, "seek": 0, "start": 22.68, "end": 23.68, "text": " Informally, of course.", "tokens": [34301, 379, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 10, "seek": 0, "start": 23.68, "end": 26.68, "text": " Okay, how am I supposed to react to this, man?", "tokens": [1033, 11, 577, 669, 286, 3442, 281, 4515, 281, 341, 11, 587, 30], "temperature": 0.0, "avg_logprob": -0.38040862893158534, "compression_ratio": 1.4636363636363636, "no_speech_prob": 0.07433469593524933}, {"id": 11, "seek": 2668, "start": 26.68, "end": 32.32, "text": " You don't need to mutate at all.", "tokens": [509, 500, 380, 643, 281, 5839, 473, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.32171523571014404, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.00020019565999973565}, {"id": 12, "seek": 2668, "start": 32.32, "end": 33.32, "text": " It's okay.", "tokens": [467, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.32171523571014404, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.00020019565999973565}, {"id": 13, "seek": 2668, "start": 33.32, "end": 34.32, "text": " What?", "tokens": [708, 30], "temperature": 0.0, "avg_logprob": -0.32171523571014404, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.00020019565999973565}, {"id": 14, "seek": 2668, "start": 34.32, "end": 37.88, "text": " What's that mutation joke coming out of the blue?", "tokens": [708, 311, 300, 27960, 7647, 1348, 484, 295, 264, 3344, 30], "temperature": 0.0, "avg_logprob": -0.32171523571014404, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.00020019565999973565}, {"id": 15, "seek": 2668, "start": 37.88, "end": 42.6, "text": " You just said react, so I felt obliged to say something to defend Elm's honor.", "tokens": [509, 445, 848, 4515, 11, 370, 286, 2762, 47194, 281, 584, 746, 281, 8602, 2699, 76, 311, 5968, 13], "temperature": 0.0, "avg_logprob": -0.32171523571014404, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.00020019565999973565}, {"id": 16, "seek": 2668, "start": 42.6, "end": 43.6, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.32171523571014404, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.00020019565999973565}, {"id": 17, "seek": 2668, "start": 43.6, "end": 52.8, "text": " Well, I guess in my view, this is how people should react.", "tokens": [1042, 11, 286, 2041, 294, 452, 1910, 11, 341, 307, 577, 561, 820, 4515, 13], "temperature": 0.0, "avg_logprob": -0.32171523571014404, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.00020019565999973565}, {"id": 18, "seek": 5280, "start": 52.8, "end": 58.28, "text": " You don't want to start a flame war or even start at the tiniest ember, right?", "tokens": [509, 500, 380, 528, 281, 722, 257, 13287, 1516, 420, 754, 722, 412, 264, 256, 3812, 377, 846, 607, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 19, "seek": 5280, "start": 58.28, "end": 60.239999999999995, "text": " I just want to give people an update.", "tokens": [286, 445, 528, 281, 976, 561, 364, 5623, 13], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 20, "seek": 5280, "start": 60.239999999999995, "end": 61.239999999999995, "text": " That's all.", "tokens": [663, 311, 439, 13], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 21, "seek": 5280, "start": 61.239999999999995, "end": 62.239999999999995, "text": " That's all I'm saying.", "tokens": [663, 311, 439, 286, 478, 1566, 13], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 22, "seek": 5280, "start": 62.239999999999995, "end": 63.239999999999995, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 23, "seek": 5280, "start": 63.239999999999995, "end": 64.24, "text": " That sounds good.", "tokens": [663, 3263, 665, 13], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 24, "seek": 5280, "start": 64.24, "end": 65.24, "text": " In it?", "tokens": [682, 309, 30], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 25, "seek": 5280, "start": 65.24, "end": 66.24, "text": " So yeah, we did.", "tokens": [407, 1338, 11, 321, 630, 13], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 26, "seek": 5280, "start": 66.24, "end": 80.24, "text": " Moving along, we did have an in-depth discussion about designing a new form API.", "tokens": [14242, 2051, 11, 321, 630, 362, 364, 294, 12, 25478, 5017, 466, 14685, 257, 777, 1254, 9362, 13], "temperature": 0.0, "avg_logprob": -0.2799901564915975, "compression_ratio": 1.5, "no_speech_prob": 3.071264654863626e-05}, {"id": 27, "seek": 8024, "start": 80.24, "end": 85.36, "text": " And at the end of this episode, we'll link to the episode in the show notes, you asked", "tokens": [400, 412, 264, 917, 295, 341, 3500, 11, 321, 603, 2113, 281, 264, 3500, 294, 264, 855, 5570, 11, 291, 2351], "temperature": 0.0, "avg_logprob": -0.24328516969586364, "compression_ratio": 1.6126126126126126, "no_speech_prob": 3.6118069601798197e-06}, {"id": 28, "seek": 8024, "start": 85.36, "end": 89.83999999999999, "text": " me if I was planning to create a standalone version of this form API.", "tokens": [385, 498, 286, 390, 5038, 281, 1884, 257, 37454, 3037, 295, 341, 1254, 9362, 13], "temperature": 0.0, "avg_logprob": -0.24328516969586364, "compression_ratio": 1.6126126126126126, "no_speech_prob": 3.6118069601798197e-06}, {"id": 29, "seek": 8024, "start": 89.83999999999999, "end": 96.47999999999999, "text": " At the time, I wasn't sure whether or if I could or would.", "tokens": [1711, 264, 565, 11, 286, 2067, 380, 988, 1968, 420, 498, 286, 727, 420, 576, 13], "temperature": 0.0, "avg_logprob": -0.24328516969586364, "compression_ratio": 1.6126126126126126, "no_speech_prob": 3.6118069601798197e-06}, {"id": 30, "seek": 8024, "start": 96.47999999999999, "end": 102.91999999999999, "text": " And a few more people asked me, and it turns out I did it.", "tokens": [400, 257, 1326, 544, 561, 2351, 385, 11, 293, 309, 4523, 484, 286, 630, 309, 13], "temperature": 0.0, "avg_logprob": -0.24328516969586364, "compression_ratio": 1.6126126126126126, "no_speech_prob": 3.6118069601798197e-06}, {"id": 31, "seek": 8024, "start": 102.91999999999999, "end": 104.36, "text": " I haven't quite hit publish.", "tokens": [286, 2378, 380, 1596, 2045, 11374, 13], "temperature": 0.0, "avg_logprob": -0.24328516969586364, "compression_ratio": 1.6126126126126126, "no_speech_prob": 3.6118069601798197e-06}, {"id": 32, "seek": 8024, "start": 104.36, "end": 107.28, "text": " Actually, I might do it before this episode goes live.", "tokens": [5135, 11, 286, 1062, 360, 309, 949, 341, 3500, 1709, 1621, 13], "temperature": 0.0, "avg_logprob": -0.24328516969586364, "compression_ratio": 1.6126126126126126, "no_speech_prob": 3.6118069601798197e-06}, {"id": 33, "seek": 10728, "start": 107.28, "end": 110.28, "text": " I think I will, but I did it.", "tokens": [286, 519, 286, 486, 11, 457, 286, 630, 309, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 34, "seek": 10728, "start": 110.28, "end": 112.2, "text": " I extracted a standalone form API.", "tokens": [286, 34086, 257, 37454, 1254, 9362, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 35, "seek": 10728, "start": 112.2, "end": 118.56, "text": " So all of these ideas we talked about are no longer coupled to Elm Pages.", "tokens": [407, 439, 295, 613, 3487, 321, 2825, 466, 366, 572, 2854, 29482, 281, 2699, 76, 430, 1660, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 36, "seek": 10728, "start": 118.56, "end": 120.72, "text": " And it turns out it worked out pretty well.", "tokens": [400, 309, 4523, 484, 309, 2732, 484, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 37, "seek": 10728, "start": 120.72, "end": 121.72, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 38, "seek": 10728, "start": 121.72, "end": 125.56, "text": " So I'm guessing that simplifies Elm Pages API quite a lot.", "tokens": [407, 286, 478, 17939, 300, 6883, 11221, 2699, 76, 430, 1660, 9362, 1596, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 39, "seek": 10728, "start": 125.56, "end": 131.16, "text": " It removed several modules from the API doc sidebar, which was nice.", "tokens": [467, 7261, 2940, 16679, 490, 264, 9362, 3211, 1252, 5356, 11, 597, 390, 1481, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 40, "seek": 10728, "start": 131.16, "end": 132.48, "text": " It's already quite large.", "tokens": [467, 311, 1217, 1596, 2416, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 41, "seek": 10728, "start": 132.48, "end": 137.24, "text": " Well, I guess you still have to learn them, but they're just in a different package.", "tokens": [1042, 11, 286, 2041, 291, 920, 362, 281, 1466, 552, 11, 457, 436, 434, 445, 294, 257, 819, 7372, 13], "temperature": 0.0, "avg_logprob": -0.24719999682518742, "compression_ratio": 1.5415162454873645, "no_speech_prob": 1.8266062795646576e-07}, {"id": 42, "seek": 13724, "start": 137.24, "end": 142.76000000000002, "text": " You have to learn them, but it provides an incremental migration path, which is nice", "tokens": [509, 362, 281, 1466, 552, 11, 457, 309, 6417, 364, 35759, 17011, 3100, 11, 597, 307, 1481], "temperature": 0.0, "avg_logprob": -0.2045122666792436, "compression_ratio": 1.5618374558303887, "no_speech_prob": 2.601568894533557e-06}, {"id": 43, "seek": 13724, "start": 142.76000000000002, "end": 150.52, "text": " if you wanted to change over from a different framework or vanilla Elm over to Elm Pages.", "tokens": [498, 291, 1415, 281, 1319, 670, 490, 257, 819, 8388, 420, 17528, 2699, 76, 670, 281, 2699, 76, 430, 1660, 13], "temperature": 0.0, "avg_logprob": -0.2045122666792436, "compression_ratio": 1.5618374558303887, "no_speech_prob": 2.601568894533557e-06}, {"id": 44, "seek": 13724, "start": 150.52, "end": 154.96, "text": " And it also gives you the ability to create little standalone shareable LE snippets because", "tokens": [400, 309, 611, 2709, 291, 264, 3485, 281, 1884, 707, 37454, 2073, 712, 11378, 35623, 1385, 570], "temperature": 0.0, "avg_logprob": -0.2045122666792436, "compression_ratio": 1.5618374558303887, "no_speech_prob": 2.601568894533557e-06}, {"id": 45, "seek": 13724, "start": 154.96, "end": 157.96, "text": " you don't need a framework to use this form API.", "tokens": [291, 500, 380, 643, 257, 8388, 281, 764, 341, 1254, 9362, 13], "temperature": 0.0, "avg_logprob": -0.2045122666792436, "compression_ratio": 1.5618374558303887, "no_speech_prob": 2.601568894533557e-06}, {"id": 46, "seek": 13724, "start": 157.96, "end": 160.4, "text": " So yeah, they turned out really nicely.", "tokens": [407, 1338, 11, 436, 3574, 484, 534, 9594, 13], "temperature": 0.0, "avg_logprob": -0.2045122666792436, "compression_ratio": 1.5618374558303887, "no_speech_prob": 2.601568894533557e-06}, {"id": 47, "seek": 13724, "start": 160.4, "end": 166.20000000000002, "text": " So what did you end up doing to make this work, to extract this to a separate package?", "tokens": [407, 437, 630, 291, 917, 493, 884, 281, 652, 341, 589, 11, 281, 8947, 341, 281, 257, 4994, 7372, 30], "temperature": 0.0, "avg_logprob": -0.2045122666792436, "compression_ratio": 1.5618374558303887, "no_speech_prob": 2.601568894533557e-06}, {"id": 48, "seek": 16620, "start": 166.2, "end": 170.16, "text": " How did that impact Elm Pages API?", "tokens": [1012, 630, 300, 2712, 2699, 76, 430, 1660, 9362, 30], "temperature": 0.0, "avg_logprob": -0.24360255805813535, "compression_ratio": 1.5296610169491525, "no_speech_prob": 1.0029950772150187e-06}, {"id": 49, "seek": 16620, "start": 170.16, "end": 177.88, "text": " And is your form package influenced in any way with how it will interact with Elm Pages?", "tokens": [400, 307, 428, 1254, 7372, 15269, 294, 604, 636, 365, 577, 309, 486, 4648, 365, 2699, 76, 430, 1660, 30], "temperature": 0.0, "avg_logprob": -0.24360255805813535, "compression_ratio": 1.5296610169491525, "no_speech_prob": 1.0029950772150187e-06}, {"id": 50, "seek": 16620, "start": 177.88, "end": 179.88, "text": " Yeah, you know, it's interesting.", "tokens": [865, 11, 291, 458, 11, 309, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.24360255805813535, "compression_ratio": 1.5296610169491525, "no_speech_prob": 1.0029950772150187e-06}, {"id": 51, "seek": 16620, "start": 179.88, "end": 185.6, "text": " At first, I wasn't sure whether some of these ideas would translate to a standalone tool.", "tokens": [1711, 700, 11, 286, 2067, 380, 988, 1968, 512, 295, 613, 3487, 576, 13799, 281, 257, 37454, 2290, 13], "temperature": 0.0, "avg_logprob": -0.24360255805813535, "compression_ratio": 1.5296610169491525, "no_speech_prob": 1.0029950772150187e-06}, {"id": 52, "seek": 16620, "start": 185.6, "end": 189.88, "text": " But after I've gone through this process, I've gotten a little more clarity on it.", "tokens": [583, 934, 286, 600, 2780, 807, 341, 1399, 11, 286, 600, 5768, 257, 707, 544, 16992, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.24360255805813535, "compression_ratio": 1.5296610169491525, "no_speech_prob": 1.0029950772150187e-06}, {"id": 53, "seek": 16620, "start": 189.88, "end": 191.32, "text": " And I think they do translate.", "tokens": [400, 286, 519, 436, 360, 13799, 13], "temperature": 0.0, "avg_logprob": -0.24360255805813535, "compression_ratio": 1.5296610169491525, "no_speech_prob": 1.0029950772150187e-06}, {"id": 54, "seek": 19132, "start": 191.32, "end": 200.79999999999998, "text": " But really, the core idea is this opinion that form data should be managed for you.", "tokens": [583, 534, 11, 264, 4965, 1558, 307, 341, 4800, 300, 1254, 1412, 820, 312, 6453, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.21544654745804637, "compression_ratio": 1.6224489795918366, "no_speech_prob": 1.0616000508889556e-05}, {"id": 55, "seek": 19132, "start": 200.79999999999998, "end": 207.88, "text": " And you should defer parsing it into higher level data or errors to a later stage.", "tokens": [400, 291, 820, 25704, 21156, 278, 309, 666, 2946, 1496, 1412, 420, 13603, 281, 257, 1780, 3233, 13], "temperature": 0.0, "avg_logprob": -0.21544654745804637, "compression_ratio": 1.6224489795918366, "no_speech_prob": 1.0616000508889556e-05}, {"id": 56, "seek": 19132, "start": 207.88, "end": 213.28, "text": " Because when people are, you know, designing things that prematurely parse, you end up", "tokens": [1436, 562, 561, 366, 11, 291, 458, 11, 14685, 721, 300, 34877, 356, 48377, 11, 291, 917, 493], "temperature": 0.0, "avg_logprob": -0.21544654745804637, "compression_ratio": 1.6224489795918366, "no_speech_prob": 1.0616000508889556e-05}, {"id": 57, "seek": 19132, "start": 213.28, "end": 218.35999999999999, "text": " with structured data or like semi structured data in your model.", "tokens": [365, 18519, 1412, 420, 411, 12909, 18519, 1412, 294, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21544654745804637, "compression_ratio": 1.6224489795918366, "no_speech_prob": 1.0616000508889556e-05}, {"id": 58, "seek": 21836, "start": 218.36, "end": 227.20000000000002, "text": " And you what I realized is that by doing that you're coupling to your specific app, because", "tokens": [400, 291, 437, 286, 5334, 307, 300, 538, 884, 300, 291, 434, 37447, 281, 428, 2685, 724, 11, 570], "temperature": 0.0, "avg_logprob": -0.2436043665959285, "compression_ratio": 1.8078602620087336, "no_speech_prob": 1.0783232937683351e-05}, {"id": 59, "seek": 21836, "start": 227.20000000000002, "end": 231.52, "text": " you're coupling to those data types like your, this is one of the really interesting things", "tokens": [291, 434, 37447, 281, 729, 1412, 3467, 411, 428, 11, 341, 307, 472, 295, 264, 534, 1880, 721], "temperature": 0.0, "avg_logprob": -0.2436043665959285, "compression_ratio": 1.8078602620087336, "no_speech_prob": 1.0783232937683351e-05}, {"id": 60, "seek": 21836, "start": 231.52, "end": 236.76000000000002, "text": " in in Elm is it kind of makes some of these types of coupling more clear, because you", "tokens": [294, 294, 2699, 76, 307, 309, 733, 295, 1669, 512, 295, 613, 3467, 295, 37447, 544, 1850, 11, 570, 291], "temperature": 0.0, "avg_logprob": -0.2436043665959285, "compression_ratio": 1.8078602620087336, "no_speech_prob": 1.0783232937683351e-05}, {"id": 61, "seek": 21836, "start": 236.76000000000002, "end": 241.88000000000002, "text": " have to be so explicit, you know, not just in what, what state you depend on and things", "tokens": [362, 281, 312, 370, 13691, 11, 291, 458, 11, 406, 445, 294, 437, 11, 437, 1785, 291, 5672, 322, 293, 721], "temperature": 0.0, "avg_logprob": -0.2436043665959285, "compression_ratio": 1.8078602620087336, "no_speech_prob": 1.0783232937683351e-05}, {"id": 62, "seek": 21836, "start": 241.88000000000002, "end": 246.0, "text": " like that, of course, but also what types you depend on.", "tokens": [411, 300, 11, 295, 1164, 11, 457, 611, 437, 3467, 291, 5672, 322, 13], "temperature": 0.0, "avg_logprob": -0.2436043665959285, "compression_ratio": 1.8078602620087336, "no_speech_prob": 1.0783232937683351e-05}, {"id": 63, "seek": 24600, "start": 246.0, "end": 250.12, "text": " Because you can't just say, Oh, I'll take any type, and I'll check if this type is there.", "tokens": [1436, 291, 393, 380, 445, 584, 11, 876, 11, 286, 603, 747, 604, 2010, 11, 293, 286, 603, 1520, 498, 341, 2010, 307, 456, 13], "temperature": 0.0, "avg_logprob": -0.21621918678283691, "compression_ratio": 1.8557692307692308, "no_speech_prob": 4.222794814268127e-06}, {"id": 64, "seek": 24600, "start": 250.12, "end": 252.86, "text": " And I'll do these special things with these types.", "tokens": [400, 286, 603, 360, 613, 2121, 721, 365, 613, 3467, 13], "temperature": 0.0, "avg_logprob": -0.21621918678283691, "compression_ratio": 1.8557692307692308, "no_speech_prob": 4.222794814268127e-06}, {"id": 65, "seek": 24600, "start": 252.86, "end": 257.84, "text": " You know, you can have like, you can have a type variable, or you can have a concrete", "tokens": [509, 458, 11, 291, 393, 362, 411, 11, 291, 393, 362, 257, 2010, 7006, 11, 420, 291, 393, 362, 257, 9859], "temperature": 0.0, "avg_logprob": -0.21621918678283691, "compression_ratio": 1.8557692307692308, "no_speech_prob": 4.222794814268127e-06}, {"id": 66, "seek": 24600, "start": 257.84, "end": 264.52, "text": " type, but you you can't cast things and you can't check what the type is.", "tokens": [2010, 11, 457, 291, 291, 393, 380, 4193, 721, 293, 291, 393, 380, 1520, 437, 264, 2010, 307, 13], "temperature": 0.0, "avg_logprob": -0.21621918678283691, "compression_ratio": 1.8557692307692308, "no_speech_prob": 4.222794814268127e-06}, {"id": 67, "seek": 24600, "start": 264.52, "end": 273.28, "text": " And so, so what I what I realized is that when, you know, it's a type of coupling, if", "tokens": [400, 370, 11, 370, 437, 286, 437, 286, 5334, 307, 300, 562, 11, 291, 458, 11, 309, 311, 257, 2010, 295, 37447, 11, 498], "temperature": 0.0, "avg_logprob": -0.21621918678283691, "compression_ratio": 1.8557692307692308, "no_speech_prob": 4.222794814268127e-06}, {"id": 68, "seek": 27328, "start": 273.28, "end": 278.52, "text": " you have like a form field, and you have a message for changing that form field, you", "tokens": [291, 362, 411, 257, 1254, 2519, 11, 293, 291, 362, 257, 3636, 337, 4473, 300, 1254, 2519, 11, 291], "temperature": 0.0, "avg_logprob": -0.20580249566298264, "compression_ratio": 1.835680751173709, "no_speech_prob": 8.990949140752491e-07}, {"id": 69, "seek": 27328, "start": 278.52, "end": 285.91999999999996, "text": " know, for example, check in date changed, if you have a message for that, and then you", "tokens": [458, 11, 337, 1365, 11, 1520, 294, 4002, 3105, 11, 498, 291, 362, 257, 3636, 337, 300, 11, 293, 550, 291], "temperature": 0.0, "avg_logprob": -0.20580249566298264, "compression_ratio": 1.835680751173709, "no_speech_prob": 8.990949140752491e-07}, {"id": 70, "seek": 27328, "start": 285.91999999999996, "end": 292.64, "text": " have some maybe date, because it parses into a maybe date, you've coupled that to this", "tokens": [362, 512, 1310, 4002, 11, 570, 309, 21156, 279, 666, 257, 1310, 4002, 11, 291, 600, 29482, 300, 281, 341], "temperature": 0.0, "avg_logprob": -0.20580249566298264, "compression_ratio": 1.835680751173709, "no_speech_prob": 8.990949140752491e-07}, {"id": 71, "seek": 27328, "start": 292.64, "end": 294.28, "text": " type in your domain.", "tokens": [2010, 294, 428, 9274, 13], "temperature": 0.0, "avg_logprob": -0.20580249566298264, "compression_ratio": 1.835680751173709, "no_speech_prob": 8.990949140752491e-07}, {"id": 72, "seek": 27328, "start": 294.28, "end": 300.71999999999997, "text": " The reason is a problem is because by coupling to that, you don't have a way to abstract", "tokens": [440, 1778, 307, 257, 1154, 307, 570, 538, 37447, 281, 300, 11, 291, 500, 380, 362, 257, 636, 281, 12649], "temperature": 0.0, "avg_logprob": -0.20580249566298264, "compression_ratio": 1.835680751173709, "no_speech_prob": 8.990949140752491e-07}, {"id": 73, "seek": 27328, "start": 300.71999999999997, "end": 303.08, "text": " things away from that.", "tokens": [721, 1314, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.20580249566298264, "compression_ratio": 1.835680751173709, "no_speech_prob": 8.990949140752491e-07}, {"id": 74, "seek": 30308, "start": 303.08, "end": 310.64, "text": " So like, you know, when you couple to something, you have to deal with it as a special case.", "tokens": [407, 411, 11, 291, 458, 11, 562, 291, 1916, 281, 746, 11, 291, 362, 281, 2028, 365, 309, 382, 257, 2121, 1389, 13], "temperature": 0.0, "avg_logprob": -0.19708804364474314, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4285434190242086e-05}, {"id": 75, "seek": 30308, "start": 310.64, "end": 314.91999999999996, "text": " But if you decouple from data, and you just say we have low level data, like all we have", "tokens": [583, 498, 291, 979, 263, 781, 490, 1412, 11, 293, 291, 445, 584, 321, 362, 2295, 1496, 1412, 11, 411, 439, 321, 362], "temperature": 0.0, "avg_logprob": -0.19708804364474314, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4285434190242086e-05}, {"id": 76, "seek": 30308, "start": 314.91999999999996, "end": 320.71999999999997, "text": " is key value pairs, then you just store all of that as a single entity.", "tokens": [307, 2141, 2158, 15494, 11, 550, 291, 445, 3531, 439, 295, 300, 382, 257, 2167, 13977, 13], "temperature": 0.0, "avg_logprob": -0.19708804364474314, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4285434190242086e-05}, {"id": 77, "seek": 30308, "start": 320.71999999999997, "end": 322.71999999999997, "text": " So this is just form data.", "tokens": [407, 341, 307, 445, 1254, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19708804364474314, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4285434190242086e-05}, {"id": 78, "seek": 30308, "start": 322.71999999999997, "end": 326.91999999999996, "text": " It's totally unstructured, it's totally untyped, it's just strings, which at the end of the", "tokens": [467, 311, 3879, 18799, 46847, 11, 309, 311, 3879, 517, 874, 3452, 11, 309, 311, 445, 13985, 11, 597, 412, 264, 917, 295, 264], "temperature": 0.0, "avg_logprob": -0.19708804364474314, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.4285434190242086e-05}, {"id": 79, "seek": 32692, "start": 326.92, "end": 334.0, "text": " day, that is like, you know, even if you have a number input field, you can be in a state", "tokens": [786, 11, 300, 307, 411, 11, 291, 458, 11, 754, 498, 291, 362, 257, 1230, 4846, 2519, 11, 291, 393, 312, 294, 257, 1785], "temperature": 0.0, "avg_logprob": -0.19605006811753758, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.9550775404204614e-06}, {"id": 80, "seek": 32692, "start": 334.0, "end": 337.92, "text": " where it's invalid, you can type 1.2.", "tokens": [689, 309, 311, 34702, 11, 291, 393, 2010, 502, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.19605006811753758, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.9550775404204614e-06}, {"id": 81, "seek": 32692, "start": 337.92, "end": 343.24, "text": " And when you do one dot, that is a string, which is not a valid number.", "tokens": [400, 562, 291, 360, 472, 5893, 11, 300, 307, 257, 6798, 11, 597, 307, 406, 257, 7363, 1230, 13], "temperature": 0.0, "avg_logprob": -0.19605006811753758, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.9550775404204614e-06}, {"id": 82, "seek": 32692, "start": 343.24, "end": 348.16, "text": " So you need to be able to represent strings at the end of the day, they're all strings.", "tokens": [407, 291, 643, 281, 312, 1075, 281, 2906, 13985, 412, 264, 917, 295, 264, 786, 11, 436, 434, 439, 13985, 13], "temperature": 0.0, "avg_logprob": -0.19605006811753758, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.9550775404204614e-06}, {"id": 83, "seek": 32692, "start": 348.16, "end": 353.8, "text": " To kind of summarize the previous episode, like Elm Form is a lot about handling things", "tokens": [1407, 733, 295, 20858, 264, 3894, 3500, 11, 411, 2699, 76, 10126, 307, 257, 688, 466, 13175, 721], "temperature": 0.0, "avg_logprob": -0.19605006811753758, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.9550775404204614e-06}, {"id": 84, "seek": 35380, "start": 353.8, "end": 362.84000000000003, "text": " at the very low level, like keeping things in a low level data state, just pairs of strings", "tokens": [412, 264, 588, 2295, 1496, 11, 411, 5145, 721, 294, 257, 2295, 1496, 1412, 1785, 11, 445, 15494, 295, 13985], "temperature": 0.0, "avg_logprob": -0.24529767773815037, "compression_ratio": 1.703883495145631, "no_speech_prob": 2.947973825939698e-06}, {"id": 85, "seek": 35380, "start": 362.84000000000003, "end": 363.84000000000003, "text": " as a list.", "tokens": [382, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.24529767773815037, "compression_ratio": 1.703883495145631, "no_speech_prob": 2.947973825939698e-06}, {"id": 86, "seek": 35380, "start": 363.84000000000003, "end": 369.52000000000004, "text": " So for each field, you will have a value, and the name of the field, and a list of those", "tokens": [407, 337, 1184, 2519, 11, 291, 486, 362, 257, 2158, 11, 293, 264, 1315, 295, 264, 2519, 11, 293, 257, 1329, 295, 729], "temperature": 0.0, "avg_logprob": -0.24529767773815037, "compression_ratio": 1.703883495145631, "no_speech_prob": 2.947973825939698e-06}, {"id": 87, "seek": 35380, "start": 369.52000000000004, "end": 372.64, "text": " is just the form data.", "tokens": [307, 445, 264, 1254, 1412, 13], "temperature": 0.0, "avg_logprob": -0.24529767773815037, "compression_ratio": 1.703883495145631, "no_speech_prob": 2.947973825939698e-06}, {"id": 88, "seek": 35380, "start": 372.64, "end": 375.8, "text": " So that's what you use as the low level data.", "tokens": [407, 300, 311, 437, 291, 764, 382, 264, 2295, 1496, 1412, 13], "temperature": 0.0, "avg_logprob": -0.24529767773815037, "compression_ratio": 1.703883495145631, "no_speech_prob": 2.947973825939698e-06}, {"id": 89, "seek": 35380, "start": 375.8, "end": 381.6, "text": " And you will never transform it to anything else until you actually try to parse it, which", "tokens": [400, 291, 486, 1128, 4088, 309, 281, 1340, 1646, 1826, 291, 767, 853, 281, 48377, 309, 11, 597], "temperature": 0.0, "avg_logprob": -0.24529767773815037, "compression_ratio": 1.703883495145631, "no_speech_prob": 2.947973825939698e-06}, {"id": 90, "seek": 38160, "start": 381.6, "end": 385.04, "text": " you do when you submit it, basically.", "tokens": [291, 360, 562, 291, 10315, 309, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.2909737777709961, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.860382432525512e-06}, {"id": 91, "seek": 38160, "start": 385.04, "end": 387.0, "text": " When you try to submit it.", "tokens": [1133, 291, 853, 281, 10315, 309, 13], "temperature": 0.0, "avg_logprob": -0.2909737777709961, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.860382432525512e-06}, {"id": 92, "seek": 38160, "start": 387.0, "end": 395.32000000000005, "text": " Yes, and also when fields change, when there's a form field event that changes a field, then", "tokens": [1079, 11, 293, 611, 562, 7909, 1319, 11, 562, 456, 311, 257, 1254, 2519, 2280, 300, 2962, 257, 2519, 11, 550], "temperature": 0.0, "avg_logprob": -0.2909737777709961, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.860382432525512e-06}, {"id": 93, "seek": 38160, "start": 395.32000000000005, "end": 398.24, "text": " you present real time client-side validation errors.", "tokens": [291, 1974, 957, 565, 6423, 12, 1812, 24071, 13603, 13], "temperature": 0.0, "avg_logprob": -0.2909737777709961, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.860382432525512e-06}, {"id": 94, "seek": 38160, "start": 398.24, "end": 404.72, "text": " So you're basically kind of putting these two pieces together, but they're completely", "tokens": [407, 291, 434, 1936, 733, 295, 3372, 613, 732, 3755, 1214, 11, 457, 436, 434, 2584], "temperature": 0.0, "avg_logprob": -0.2909737777709961, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.860382432525512e-06}, {"id": 95, "seek": 38160, "start": 404.72, "end": 405.72, "text": " decoupled.", "tokens": [979, 263, 15551, 13], "temperature": 0.0, "avg_logprob": -0.2909737777709961, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.860382432525512e-06}, {"id": 96, "seek": 38160, "start": 405.72, "end": 409.92, "text": " So you've got low level form data, and you've got a parser.", "tokens": [407, 291, 600, 658, 2295, 1496, 1254, 1412, 11, 293, 291, 600, 658, 257, 21156, 260, 13], "temperature": 0.0, "avg_logprob": -0.2909737777709961, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.860382432525512e-06}, {"id": 97, "seek": 40992, "start": 409.92, "end": 415.92, "text": " And those two things, the low level form data doesn't know or care about the parser.", "tokens": [400, 729, 732, 721, 11, 264, 2295, 1496, 1254, 1412, 1177, 380, 458, 420, 1127, 466, 264, 21156, 260, 13], "temperature": 0.0, "avg_logprob": -0.22276348933995327, "compression_ratio": 1.759825327510917, "no_speech_prob": 3.80700498681108e-07}, {"id": 98, "seek": 40992, "start": 415.92, "end": 418.28000000000003, "text": " So it's completely decoupled from it.", "tokens": [407, 309, 311, 2584, 979, 263, 15551, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.22276348933995327, "compression_ratio": 1.759825327510917, "no_speech_prob": 3.80700498681108e-07}, {"id": 99, "seek": 40992, "start": 418.28000000000003, "end": 421.16, "text": " It is a completely agnostic data format.", "tokens": [467, 307, 257, 2584, 623, 77, 19634, 1412, 7877, 13], "temperature": 0.0, "avg_logprob": -0.22276348933995327, "compression_ratio": 1.759825327510917, "no_speech_prob": 3.80700498681108e-07}, {"id": 100, "seek": 40992, "start": 421.16, "end": 423.88, "text": " That's low level, it's not coupled to domain types.", "tokens": [663, 311, 2295, 1496, 11, 309, 311, 406, 29482, 281, 9274, 3467, 13], "temperature": 0.0, "avg_logprob": -0.22276348933995327, "compression_ratio": 1.759825327510917, "no_speech_prob": 3.80700498681108e-07}, {"id": 101, "seek": 40992, "start": 423.88, "end": 428.12, "text": " The parser does know about your domain types, it parses into whatever types you want to", "tokens": [440, 21156, 260, 775, 458, 466, 428, 9274, 3467, 11, 309, 21156, 279, 666, 2035, 3467, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.22276348933995327, "compression_ratio": 1.759825327510917, "no_speech_prob": 3.80700498681108e-07}, {"id": 102, "seek": 40992, "start": 428.12, "end": 429.68, "text": " parse into.", "tokens": [48377, 666, 13], "temperature": 0.0, "avg_logprob": -0.22276348933995327, "compression_ratio": 1.759825327510917, "no_speech_prob": 3.80700498681108e-07}, {"id": 103, "seek": 40992, "start": 429.68, "end": 436.08000000000004, "text": " But by separating those two pieces, the form API can completely take charge of managing", "tokens": [583, 538, 29279, 729, 732, 3755, 11, 264, 1254, 9362, 393, 2584, 747, 4602, 295, 11642], "temperature": 0.0, "avg_logprob": -0.22276348933995327, "compression_ratio": 1.759825327510917, "no_speech_prob": 3.80700498681108e-07}, {"id": 104, "seek": 43608, "start": 436.08, "end": 444.64, "text": " all of your form state, including not just the values for your form fields, but the field,", "tokens": [439, 295, 428, 1254, 1785, 11, 3009, 406, 445, 264, 4190, 337, 428, 1254, 7909, 11, 457, 264, 2519, 11], "temperature": 0.0, "avg_logprob": -0.25095587510329026, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1610711680987151e-07}, {"id": 105, "seek": 43608, "start": 444.64, "end": 448.8, "text": " the term I'm using for it is field status, but like whether something has been blurred", "tokens": [264, 1433, 286, 478, 1228, 337, 309, 307, 2519, 6558, 11, 457, 411, 1968, 746, 575, 668, 43525], "temperature": 0.0, "avg_logprob": -0.25095587510329026, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1610711680987151e-07}, {"id": 106, "seek": 43608, "start": 448.8, "end": 450.8, "text": " or changed.", "tokens": [420, 3105, 13], "temperature": 0.0, "avg_logprob": -0.25095587510329026, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1610711680987151e-07}, {"id": 107, "seek": 43608, "start": 450.8, "end": 455.36, "text": " So all of that low level form state is completely managed by the API.", "tokens": [407, 439, 295, 300, 2295, 1496, 1254, 1785, 307, 2584, 6453, 538, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.25095587510329026, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1610711680987151e-07}, {"id": 108, "seek": 43608, "start": 455.36, "end": 465.96, "text": " So the end result is you, so the wiring is you have to use form.init in your model to", "tokens": [407, 264, 917, 1874, 307, 291, 11, 370, 264, 27520, 307, 291, 362, 281, 764, 1254, 13, 259, 270, 294, 428, 2316, 281], "temperature": 0.0, "avg_logprob": -0.25095587510329026, "compression_ratio": 1.6586538461538463, "no_speech_prob": 1.1610711680987151e-07}, {"id": 109, "seek": 46596, "start": 465.96, "end": 475.12, "text": " init it, you have to have a form.model in your model, you have to have a form.message", "tokens": [3157, 309, 11, 291, 362, 281, 362, 257, 1254, 13, 8014, 338, 294, 428, 2316, 11, 291, 362, 281, 362, 257, 1254, 13, 76, 442, 609], "temperature": 0.0, "avg_logprob": -0.21681506224352903, "compression_ratio": 1.8314606741573034, "no_speech_prob": 5.989234068692895e-07}, {"id": 110, "seek": 46596, "start": 475.12, "end": 483.67999999999995, "text": " in your message, and you need to have an update clause that delegates to form.update.", "tokens": [294, 428, 3636, 11, 293, 291, 643, 281, 362, 364, 5623, 25925, 300, 45756, 281, 1254, 13, 1010, 17393, 13], "temperature": 0.0, "avg_logprob": -0.21681506224352903, "compression_ratio": 1.8314606741573034, "no_speech_prob": 5.989234068692895e-07}, {"id": 111, "seek": 46596, "start": 483.67999999999995, "end": 486.82, "text": " And then you need to render in your view.", "tokens": [400, 550, 291, 643, 281, 15529, 294, 428, 1910, 13], "temperature": 0.0, "avg_logprob": -0.21681506224352903, "compression_ratio": 1.8314606741573034, "no_speech_prob": 5.989234068692895e-07}, {"id": 112, "seek": 46596, "start": 486.82, "end": 487.82, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.21681506224352903, "compression_ratio": 1.8314606741573034, "no_speech_prob": 5.989234068692895e-07}, {"id": 113, "seek": 46596, "start": 487.82, "end": 493.24, "text": " And now that might sound like, okay, that's just like what we're used to doing with Elm", "tokens": [400, 586, 300, 1062, 1626, 411, 11, 1392, 11, 300, 311, 445, 411, 437, 321, 434, 1143, 281, 884, 365, 2699, 76], "temperature": 0.0, "avg_logprob": -0.21681506224352903, "compression_ratio": 1.8314606741573034, "no_speech_prob": 5.989234068692895e-07}, {"id": 114, "seek": 46596, "start": 493.24, "end": 494.24, "text": " packages.", "tokens": [17401, 13], "temperature": 0.0, "avg_logprob": -0.21681506224352903, "compression_ratio": 1.8314606741573034, "no_speech_prob": 5.989234068692895e-07}, {"id": 115, "seek": 49424, "start": 494.24, "end": 496.16, "text": " Yeah, that's basically the Elm architecture.", "tokens": [865, 11, 300, 311, 1936, 264, 2699, 76, 9482, 13], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 116, "seek": 49424, "start": 496.16, "end": 497.16, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 117, "seek": 49424, "start": 497.16, "end": 499.16, "text": " Like to call it, right?", "tokens": [1743, 281, 818, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 118, "seek": 49424, "start": 499.16, "end": 500.16, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 119, "seek": 49424, "start": 500.16, "end": 502.72, "text": " The triplets, the Elm triplets.", "tokens": [440, 1376, 31023, 11, 264, 2699, 76, 1376, 31023, 13], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 120, "seek": 49424, "start": 502.72, "end": 507.40000000000003, "text": " But this is not the pattern that we're used to using for form packages.", "tokens": [583, 341, 307, 406, 264, 5102, 300, 321, 434, 1143, 281, 1228, 337, 1254, 17401, 13], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 121, "seek": 49424, "start": 507.40000000000003, "end": 514.44, "text": " The pattern that we're used to for form packages is wiring in messages and model fields for", "tokens": [440, 5102, 300, 321, 434, 1143, 281, 337, 1254, 17401, 307, 27520, 294, 7897, 293, 2316, 7909, 337], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 122, "seek": 49424, "start": 514.44, "end": 516.2, "text": " every single field.", "tokens": [633, 2167, 2519, 13], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 123, "seek": 49424, "start": 516.2, "end": 519.04, "text": " So this simplifies that.", "tokens": [407, 341, 6883, 11221, 300, 13], "temperature": 0.0, "avg_logprob": -0.29083276045949835, "compression_ratio": 1.675257731958763, "no_speech_prob": 2.273248327355759e-07}, {"id": 124, "seek": 51904, "start": 519.04, "end": 527.1999999999999, "text": " So it is a single form.model value for all of the forms you might have on a given page.", "tokens": [407, 309, 307, 257, 2167, 1254, 13, 8014, 338, 2158, 337, 439, 295, 264, 6422, 291, 1062, 362, 322, 257, 2212, 3028, 13], "temperature": 0.0, "avg_logprob": -0.23276692628860474, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.414472308169934e-06}, {"id": 125, "seek": 51904, "start": 527.1999999999999, "end": 533.0799999999999, "text": " So if you have multiple forms that you could submit on a page or even throughout your pages.", "tokens": [407, 498, 291, 362, 3866, 6422, 300, 291, 727, 10315, 322, 257, 3028, 420, 754, 3710, 428, 7183, 13], "temperature": 0.0, "avg_logprob": -0.23276692628860474, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.414472308169934e-06}, {"id": 126, "seek": 51904, "start": 533.0799999999999, "end": 534.8, "text": " So this is one of the really cool things.", "tokens": [407, 341, 307, 472, 295, 264, 534, 1627, 721, 13], "temperature": 0.0, "avg_logprob": -0.23276692628860474, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.414472308169934e-06}, {"id": 127, "seek": 51904, "start": 534.8, "end": 541.16, "text": " You know, if you have, you can wire up this form state, you know, in your shared model", "tokens": [509, 458, 11, 498, 291, 362, 11, 291, 393, 6234, 493, 341, 1254, 1785, 11, 291, 458, 11, 294, 428, 5507, 2316], "temperature": 0.0, "avg_logprob": -0.23276692628860474, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.414472308169934e-06}, {"id": 128, "seek": 51904, "start": 541.16, "end": 544.64, "text": " between pages and maintain it in a single place.", "tokens": [1296, 7183, 293, 6909, 309, 294, 257, 2167, 1081, 13], "temperature": 0.0, "avg_logprob": -0.23276692628860474, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.414472308169934e-06}, {"id": 129, "seek": 54464, "start": 544.64, "end": 549.52, "text": " So that was one of the reasons why I wasn't sure if this pattern would translate from", "tokens": [407, 300, 390, 472, 295, 264, 4112, 983, 286, 2067, 380, 988, 498, 341, 5102, 576, 13799, 490], "temperature": 0.0, "avg_logprob": -0.2695702970697639, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.9750480362290546e-07}, {"id": 130, "seek": 54464, "start": 549.52, "end": 556.92, "text": " Elm pages into a more general use case was because Elm pages had this built in assumption", "tokens": [2699, 76, 7183, 666, 257, 544, 2674, 764, 1389, 390, 570, 2699, 76, 7183, 632, 341, 3094, 294, 15302], "temperature": 0.0, "avg_logprob": -0.2695702970697639, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.9750480362290546e-07}, {"id": 131, "seek": 54464, "start": 556.92, "end": 562.04, "text": " where it had, so it managed the model for the user, right?", "tokens": [689, 309, 632, 11, 370, 309, 6453, 264, 2316, 337, 264, 4195, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2695702970697639, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.9750480362290546e-07}, {"id": 132, "seek": 54464, "start": 562.04, "end": 568.84, "text": " So Elm pages as a framework can have its own model and then the user's model sits within", "tokens": [407, 2699, 76, 7183, 382, 257, 8388, 393, 362, 1080, 1065, 2316, 293, 550, 264, 4195, 311, 2316, 12696, 1951], "temperature": 0.0, "avg_logprob": -0.2695702970697639, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.9750480362290546e-07}, {"id": 133, "seek": 54464, "start": 568.84, "end": 569.84, "text": " its model, right?", "tokens": [1080, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2695702970697639, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.9750480362290546e-07}, {"id": 134, "seek": 56984, "start": 569.84, "end": 576.2800000000001, "text": " And so it can manage the form state, which is the state for any form.", "tokens": [400, 370, 309, 393, 3067, 264, 1254, 1785, 11, 597, 307, 264, 1785, 337, 604, 1254, 13], "temperature": 0.0, "avg_logprob": -0.20513952994833187, "compression_ratio": 1.883248730964467, "no_speech_prob": 5.989259079797193e-07}, {"id": 135, "seek": 56984, "start": 576.2800000000001, "end": 579.1800000000001, "text": " And then it can have its own message.", "tokens": [400, 550, 309, 393, 362, 1080, 1065, 3636, 13], "temperature": 0.0, "avg_logprob": -0.20513952994833187, "compression_ratio": 1.883248730964467, "no_speech_prob": 5.989259079797193e-07}, {"id": 136, "seek": 56984, "start": 579.1800000000001, "end": 584.88, "text": " So the Elm pages framework can have its own message, which knows about these form events.", "tokens": [407, 264, 2699, 76, 7183, 8388, 393, 362, 1080, 1065, 3636, 11, 597, 3255, 466, 613, 1254, 3931, 13], "temperature": 0.0, "avg_logprob": -0.20513952994833187, "compression_ratio": 1.883248730964467, "no_speech_prob": 5.989259079797193e-07}, {"id": 137, "seek": 56984, "start": 584.88, "end": 591.36, "text": " So all you have to do is render a form and it renders with this pages message type, you", "tokens": [407, 439, 291, 362, 281, 360, 307, 15529, 257, 1254, 293, 309, 6125, 433, 365, 341, 7183, 3636, 2010, 11, 291], "temperature": 0.0, "avg_logprob": -0.20513952994833187, "compression_ratio": 1.883248730964467, "no_speech_prob": 5.989259079797193e-07}, {"id": 138, "seek": 56984, "start": 591.36, "end": 593.64, "text": " know, HTML pages message.", "tokens": [458, 11, 17995, 7183, 3636, 13], "temperature": 0.0, "avg_logprob": -0.20513952994833187, "compression_ratio": 1.883248730964467, "no_speech_prob": 5.989259079797193e-07}, {"id": 139, "seek": 56984, "start": 593.64, "end": 596.6, "text": " And then the framework knows how to deal with that message.", "tokens": [400, 550, 264, 8388, 3255, 577, 281, 2028, 365, 300, 3636, 13], "temperature": 0.0, "avg_logprob": -0.20513952994833187, "compression_ratio": 1.883248730964467, "no_speech_prob": 5.989259079797193e-07}, {"id": 140, "seek": 59660, "start": 596.6, "end": 603.12, "text": " So you don't, that means that you can have a form with real time client side validations", "tokens": [407, 291, 500, 380, 11, 300, 1355, 300, 291, 393, 362, 257, 1254, 365, 957, 565, 6423, 1252, 7363, 763], "temperature": 0.0, "avg_logprob": -0.2328702069203788, "compression_ratio": 1.6509803921568627, "no_speech_prob": 3.6898671851304243e-07}, {"id": 141, "seek": 59660, "start": 603.12, "end": 610.88, "text": " that dynamically manages all the form state without needing to call form.init, form.update,", "tokens": [300, 43492, 22489, 439, 264, 1254, 1785, 1553, 18006, 281, 818, 1254, 13, 259, 270, 11, 1254, 13, 1010, 17393, 11], "temperature": 0.0, "avg_logprob": -0.2328702069203788, "compression_ratio": 1.6509803921568627, "no_speech_prob": 3.6898671851304243e-07}, {"id": 142, "seek": 59660, "start": 610.88, "end": 612.4, "text": " have a message for it.", "tokens": [362, 257, 3636, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.2328702069203788, "compression_ratio": 1.6509803921568627, "no_speech_prob": 3.6898671851304243e-07}, {"id": 143, "seek": 59660, "start": 612.4, "end": 615.6800000000001, "text": " All you do is render your form and the framework just takes care of it.", "tokens": [1057, 291, 360, 307, 15529, 428, 1254, 293, 264, 8388, 445, 2516, 1127, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.2328702069203788, "compression_ratio": 1.6509803921568627, "no_speech_prob": 3.6898671851304243e-07}, {"id": 144, "seek": 59660, "start": 615.6800000000001, "end": 621.84, "text": " So it's, it almost feels like magic when you're using JavaScript and you just wire up a component", "tokens": [407, 309, 311, 11, 309, 1920, 3417, 411, 5585, 562, 291, 434, 1228, 15778, 293, 291, 445, 6234, 493, 257, 6542], "temperature": 0.0, "avg_logprob": -0.2328702069203788, "compression_ratio": 1.6509803921568627, "no_speech_prob": 3.6898671851304243e-07}, {"id": 145, "seek": 59660, "start": 621.84, "end": 625.44, "text": " and it just knows how to use this global state.", "tokens": [293, 309, 445, 3255, 577, 281, 764, 341, 4338, 1785, 13], "temperature": 0.0, "avg_logprob": -0.2328702069203788, "compression_ratio": 1.6509803921568627, "no_speech_prob": 3.6898671851304243e-07}, {"id": 146, "seek": 62544, "start": 625.44, "end": 627.24, "text": " But it's actually not magic.", "tokens": [583, 309, 311, 767, 406, 5585, 13], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 147, "seek": 62544, "start": 627.24, "end": 631.8800000000001, "text": " And if you look at the time traveling debugger, you can see all these explicit messages that", "tokens": [400, 498, 291, 574, 412, 264, 565, 9712, 24083, 1321, 11, 291, 393, 536, 439, 613, 13691, 7897, 300], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 148, "seek": 62544, "start": 631.8800000000001, "end": 632.8800000000001, "text": " are being received.", "tokens": [366, 885, 4613, 13], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 149, "seek": 62544, "start": 632.8800000000001, "end": 635.2800000000001, "text": " You can still use the time traveling debugger.", "tokens": [509, 393, 920, 764, 264, 565, 9712, 24083, 1321, 13], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 150, "seek": 62544, "start": 635.2800000000001, "end": 636.2800000000001, "text": " It's not magic.", "tokens": [467, 311, 406, 5585, 13], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 151, "seek": 62544, "start": 636.2800000000001, "end": 637.98, "text": " It's just a baked in opinion.", "tokens": [467, 311, 445, 257, 19453, 294, 4800, 13], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 152, "seek": 62544, "start": 637.98, "end": 645.5600000000001, "text": " So the magic is that it's decoupled from your type because it doesn't need to know what", "tokens": [407, 264, 5585, 307, 300, 309, 311, 979, 263, 15551, 490, 428, 2010, 570, 309, 1177, 380, 643, 281, 458, 437], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 153, "seek": 62544, "start": 645.5600000000001, "end": 651.6, "text": " type your form parses into to take ownership over that form state because it knows what", "tokens": [2010, 428, 1254, 21156, 279, 666, 281, 747, 15279, 670, 300, 1254, 1785, 570, 309, 3255, 437], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 154, "seek": 62544, "start": 651.6, "end": 652.94, "text": " form state looks like.", "tokens": [1254, 1785, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.19302553966127592, "compression_ratio": 1.7673469387755103, "no_speech_prob": 5.989257374494628e-07}, {"id": 155, "seek": 65294, "start": 652.94, "end": 657.5200000000001, "text": " It's just key value strings and blurred, changed, et cetera.", "tokens": [467, 311, 445, 2141, 2158, 13985, 293, 43525, 11, 3105, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.2570422089215621, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.7378890144500474e-07}, {"id": 156, "seek": 65294, "start": 657.5200000000001, "end": 658.5200000000001, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2570422089215621, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.7378890144500474e-07}, {"id": 157, "seek": 65294, "start": 658.5200000000001, "end": 662.4000000000001, "text": " So it feels magic because you don't have to wire it in yourself.", "tokens": [407, 309, 3417, 5585, 570, 291, 500, 380, 362, 281, 6234, 309, 294, 1803, 13], "temperature": 0.0, "avg_logprob": -0.2570422089215621, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.7378890144500474e-07}, {"id": 158, "seek": 65294, "start": 662.4000000000001, "end": 668.48, "text": " And the reason why you don't have to wire it yourself is because the types are known", "tokens": [400, 264, 1778, 983, 291, 500, 380, 362, 281, 6234, 309, 1803, 307, 570, 264, 3467, 366, 2570], "temperature": 0.0, "avg_logprob": -0.2570422089215621, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.7378890144500474e-07}, {"id": 159, "seek": 65294, "start": 668.48, "end": 671.6800000000001, "text": " because there are those low level data, right?", "tokens": [570, 456, 366, 729, 2295, 1496, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2570422089215621, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.7378890144500474e-07}, {"id": 160, "seek": 65294, "start": 671.6800000000001, "end": 675.72, "text": " So it's a list of string tuples.", "tokens": [407, 309, 311, 257, 1329, 295, 6798, 2604, 2622, 13], "temperature": 0.0, "avg_logprob": -0.2570422089215621, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.7378890144500474e-07}, {"id": 161, "seek": 65294, "start": 675.72, "end": 682.9200000000001, "text": " Whereas if my form was something that I made myself with the custom types and custom updates,", "tokens": [13813, 498, 452, 1254, 390, 746, 300, 286, 1027, 2059, 365, 264, 2375, 3467, 293, 2375, 9205, 11], "temperature": 0.0, "avg_logprob": -0.2570422089215621, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.7378890144500474e-07}, {"id": 162, "seek": 68292, "start": 682.92, "end": 688.0799999999999, "text": " custom messages and so on, then I would have to wire it in somehow.", "tokens": [2375, 7897, 293, 370, 322, 11, 550, 286, 576, 362, 281, 6234, 309, 294, 6063, 13], "temperature": 0.0, "avg_logprob": -0.31975451936113075, "compression_ratio": 1.52, "no_speech_prob": 4.888274247605295e-07}, {"id": 163, "seek": 68292, "start": 688.0799999999999, "end": 689.0799999999999, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.31975451936113075, "compression_ratio": 1.52, "no_speech_prob": 4.888274247605295e-07}, {"id": 164, "seek": 68292, "start": 689.0799999999999, "end": 690.0799999999999, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.31975451936113075, "compression_ratio": 1.52, "no_speech_prob": 4.888274247605295e-07}, {"id": 165, "seek": 68292, "start": 690.0799999999999, "end": 694.4, "text": " In this case, I just have to tell you where it is and that's it.", "tokens": [682, 341, 1389, 11, 286, 445, 362, 281, 980, 291, 689, 309, 307, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.31975451936113075, "compression_ratio": 1.52, "no_speech_prob": 4.888274247605295e-07}, {"id": 166, "seek": 68292, "start": 694.4, "end": 695.52, "text": " Basically right.", "tokens": [8537, 558, 13], "temperature": 0.0, "avg_logprob": -0.31975451936113075, "compression_ratio": 1.52, "no_speech_prob": 4.888274247605295e-07}, {"id": 167, "seek": 68292, "start": 695.52, "end": 704.4799999999999, "text": " In this case, in an Elm pages v3 application still in beta, you say, you know, you say", "tokens": [682, 341, 1389, 11, 294, 364, 2699, 76, 7183, 371, 18, 3861, 920, 294, 9861, 11, 291, 584, 11, 291, 458, 11, 291, 584], "temperature": 0.0, "avg_logprob": -0.31975451936113075, "compression_ratio": 1.52, "no_speech_prob": 4.888274247605295e-07}, {"id": 168, "seek": 68292, "start": 704.4799999999999, "end": 708.52, "text": " render HTML for the form and you give it a form ID.", "tokens": [15529, 17995, 337, 264, 1254, 293, 291, 976, 309, 257, 1254, 7348, 13], "temperature": 0.0, "avg_logprob": -0.31975451936113075, "compression_ratio": 1.52, "no_speech_prob": 4.888274247605295e-07}, {"id": 169, "seek": 70852, "start": 708.52, "end": 714.76, "text": " And that form ID is the unique key that it's going to manage in the dictionary of the form", "tokens": [400, 300, 1254, 7348, 307, 264, 3845, 2141, 300, 309, 311, 516, 281, 3067, 294, 264, 25890, 295, 264, 1254], "temperature": 0.0, "avg_logprob": -0.26399221960103736, "compression_ratio": 1.6528925619834711, "no_speech_prob": 9.276303103433747e-07}, {"id": 170, "seek": 70852, "start": 714.76, "end": 717.4399999999999, "text": " state for all forms in the app.", "tokens": [1785, 337, 439, 6422, 294, 264, 724, 13], "temperature": 0.0, "avg_logprob": -0.26399221960103736, "compression_ratio": 1.6528925619834711, "no_speech_prob": 9.276303103433747e-07}, {"id": 171, "seek": 70852, "start": 717.4399999999999, "end": 723.56, "text": " I'm guessing you have an ID so that two forms with the same fields don't conflict.", "tokens": [286, 478, 17939, 291, 362, 364, 7348, 370, 300, 732, 6422, 365, 264, 912, 7909, 500, 380, 6596, 13], "temperature": 0.0, "avg_logprob": -0.26399221960103736, "compression_ratio": 1.6528925619834711, "no_speech_prob": 9.276303103433747e-07}, {"id": 172, "seek": 70852, "start": 723.56, "end": 728.8, "text": " Like if you have a first name in two different pages, they're not using the same data under", "tokens": [1743, 498, 291, 362, 257, 700, 1315, 294, 732, 819, 7183, 11, 436, 434, 406, 1228, 264, 912, 1412, 833], "temperature": 0.0, "avg_logprob": -0.26399221960103736, "compression_ratio": 1.6528925619834711, "no_speech_prob": 9.276303103433747e-07}, {"id": 173, "seek": 70852, "start": 728.8, "end": 729.8, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.26399221960103736, "compression_ratio": 1.6528925619834711, "no_speech_prob": 9.276303103433747e-07}, {"id": 174, "seek": 70852, "start": 729.8, "end": 730.8, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.26399221960103736, "compression_ratio": 1.6528925619834711, "no_speech_prob": 9.276303103433747e-07}, {"id": 175, "seek": 70852, "start": 730.8, "end": 735.68, "text": " So that, you know, that is a constraint of how this package operates is it does operate", "tokens": [407, 300, 11, 291, 458, 11, 300, 307, 257, 25534, 295, 577, 341, 7372, 22577, 307, 309, 775, 9651], "temperature": 0.0, "avg_logprob": -0.26399221960103736, "compression_ratio": 1.6528925619834711, "no_speech_prob": 9.276303103433747e-07}, {"id": 176, "seek": 73568, "start": 735.68, "end": 741.28, "text": " under the assumption that you're using, you're giving unique form IDs.", "tokens": [833, 264, 15302, 300, 291, 434, 1228, 11, 291, 434, 2902, 3845, 1254, 48212, 13], "temperature": 0.0, "avg_logprob": -0.3211010070074172, "compression_ratio": 1.7486910994764397, "no_speech_prob": 3.6898291000397876e-07}, {"id": 177, "seek": 73568, "start": 741.28, "end": 746.8399999999999, "text": " Actually, I think this would be a really good review rule to check that you have unique", "tokens": [5135, 11, 286, 519, 341, 576, 312, 257, 534, 665, 3131, 4978, 281, 1520, 300, 291, 362, 3845], "temperature": 0.0, "avg_logprob": -0.3211010070074172, "compression_ratio": 1.7486910994764397, "no_speech_prob": 3.6898291000397876e-07}, {"id": 178, "seek": 73568, "start": 746.8399999999999, "end": 747.8399999999999, "text": " form IDs.", "tokens": [1254, 48212, 13], "temperature": 0.0, "avg_logprob": -0.3211010070074172, "compression_ratio": 1.7486910994764397, "no_speech_prob": 3.6898291000397876e-07}, {"id": 179, "seek": 73568, "start": 747.8399999999999, "end": 754.2399999999999, "text": " I was even thinking like, so you can check one, one cool way to check this.", "tokens": [286, 390, 754, 1953, 411, 11, 370, 291, 393, 1520, 472, 11, 472, 1627, 636, 281, 1520, 341, 13], "temperature": 0.0, "avg_logprob": -0.3211010070074172, "compression_ratio": 1.7486910994764397, "no_speech_prob": 3.6898291000397876e-07}, {"id": 180, "seek": 73568, "start": 754.2399999999999, "end": 760.3199999999999, "text": " It would be kind of interesting to check that like you could by convention check that the", "tokens": [467, 576, 312, 733, 295, 1880, 281, 1520, 300, 411, 291, 727, 538, 10286, 1520, 300, 264], "temperature": 0.0, "avg_logprob": -0.3211010070074172, "compression_ratio": 1.7486910994764397, "no_speech_prob": 3.6898291000397876e-07}, {"id": 181, "seek": 76032, "start": 760.32, "end": 766.6800000000001, "text": " declaration name of a form matches the string that you use for the unique identifier.", "tokens": [27606, 1315, 295, 257, 1254, 10676, 264, 6798, 300, 291, 764, 337, 264, 3845, 45690, 13], "temperature": 0.0, "avg_logprob": -0.23482071725945725, "compression_ratio": 1.6302521008403361, "no_speech_prob": 5.043464739173942e-07}, {"id": 182, "seek": 76032, "start": 766.6800000000001, "end": 772.4000000000001, "text": " But you still need the ability to make something unique if you render like a delete button", "tokens": [583, 291, 920, 643, 264, 3485, 281, 652, 746, 3845, 498, 291, 15529, 411, 257, 12097, 2960], "temperature": 0.0, "avg_logprob": -0.23482071725945725, "compression_ratio": 1.6302521008403361, "no_speech_prob": 5.043464739173942e-07}, {"id": 183, "seek": 76032, "start": 772.4000000000001, "end": 781.6, "text": " for every item or or a quantity, you know, quantity field for every item in the cart.", "tokens": [337, 633, 3174, 420, 420, 257, 11275, 11, 291, 458, 11, 11275, 2519, 337, 633, 3174, 294, 264, 5467, 13], "temperature": 0.0, "avg_logprob": -0.23482071725945725, "compression_ratio": 1.6302521008403361, "no_speech_prob": 5.043464739173942e-07}, {"id": 184, "seek": 76032, "start": 781.6, "end": 783.9000000000001, "text": " Those each need to have a unique form ID.", "tokens": [3950, 1184, 643, 281, 362, 257, 3845, 1254, 7348, 13], "temperature": 0.0, "avg_logprob": -0.23482071725945725, "compression_ratio": 1.6302521008403361, "no_speech_prob": 5.043464739173942e-07}, {"id": 185, "seek": 76032, "start": 783.9000000000001, "end": 789.96, "text": " So in that case, but it would be possible to just scan the page with Elm review and", "tokens": [407, 294, 300, 1389, 11, 457, 309, 576, 312, 1944, 281, 445, 11049, 264, 3028, 365, 2699, 76, 3131, 293], "temperature": 0.0, "avg_logprob": -0.23482071725945725, "compression_ratio": 1.6302521008403361, "no_speech_prob": 5.043464739173942e-07}, {"id": 186, "seek": 78996, "start": 789.96, "end": 792.44, "text": " say this needs to have a unique form ID.", "tokens": [584, 341, 2203, 281, 362, 257, 3845, 1254, 7348, 13], "temperature": 0.0, "avg_logprob": -0.2768072831003289, "compression_ratio": 1.7100840336134453, "no_speech_prob": 8.579164614275214e-07}, {"id": 187, "seek": 78996, "start": 792.44, "end": 798.24, "text": " And if you do, for example, list dot map for each item in the cart, you should use something", "tokens": [400, 498, 291, 360, 11, 337, 1365, 11, 1329, 5893, 4471, 337, 1184, 3174, 294, 264, 5467, 11, 291, 820, 764, 746], "temperature": 0.0, "avg_logprob": -0.2768072831003289, "compression_ratio": 1.7100840336134453, "no_speech_prob": 8.579164614275214e-07}, {"id": 188, "seek": 78996, "start": 798.24, "end": 804.12, "text": " from that list dot map in your unique ID to to ensure that it's unique.", "tokens": [490, 300, 1329, 5893, 4471, 294, 428, 3845, 7348, 281, 281, 5586, 300, 309, 311, 3845, 13], "temperature": 0.0, "avg_logprob": -0.2768072831003289, "compression_ratio": 1.7100840336134453, "no_speech_prob": 8.579164614275214e-07}, {"id": 189, "seek": 78996, "start": 804.12, "end": 809.48, "text": " Sounds like react where they tell you, hey, don't use the index as the key for this in", "tokens": [14576, 411, 4515, 689, 436, 980, 291, 11, 4177, 11, 500, 380, 764, 264, 8186, 382, 264, 2141, 337, 341, 294], "temperature": 0.0, "avg_logprob": -0.2768072831003289, "compression_ratio": 1.7100840336134453, "no_speech_prob": 8.579164614275214e-07}, {"id": 190, "seek": 78996, "start": 809.48, "end": 810.48, "text": " this loop.", "tokens": [341, 6367, 13], "temperature": 0.0, "avg_logprob": -0.2768072831003289, "compression_ratio": 1.7100840336134453, "no_speech_prob": 8.579164614275214e-07}, {"id": 191, "seek": 78996, "start": 810.48, "end": 817.6, "text": " I'm slightly confused here is the form ID for the field or for the form.", "tokens": [286, 478, 4748, 9019, 510, 307, 264, 1254, 7348, 337, 264, 2519, 420, 337, 264, 1254, 13], "temperature": 0.0, "avg_logprob": -0.2768072831003289, "compression_ratio": 1.7100840336134453, "no_speech_prob": 8.579164614275214e-07}, {"id": 192, "seek": 78996, "start": 817.6, "end": 819.44, "text": " I thought it was for the form.", "tokens": [286, 1194, 309, 390, 337, 264, 1254, 13], "temperature": 0.0, "avg_logprob": -0.2768072831003289, "compression_ratio": 1.7100840336134453, "no_speech_prob": 8.579164614275214e-07}, {"id": 193, "seek": 81944, "start": 819.44, "end": 824.5600000000001, "text": " So that you differentiate between two forms on different pages or different sections of", "tokens": [407, 300, 291, 23203, 1296, 732, 6422, 322, 819, 7183, 420, 819, 10863, 295], "temperature": 0.0, "avg_logprob": -0.305231177288553, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.0188049373027752e-06}, {"id": 194, "seek": 81944, "start": 824.5600000000001, "end": 825.5600000000001, "text": " the page.", "tokens": [264, 3028, 13], "temperature": 0.0, "avg_logprob": -0.305231177288553, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.0188049373027752e-06}, {"id": 195, "seek": 81944, "start": 825.5600000000001, "end": 829.32, "text": " The the form ID is for the form.", "tokens": [440, 264, 1254, 7348, 307, 337, 264, 1254, 13], "temperature": 0.0, "avg_logprob": -0.305231177288553, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.0188049373027752e-06}, {"id": 196, "seek": 81944, "start": 829.32, "end": 836.6400000000001, "text": " And then you also have to give a name for each field that's unique within that form.", "tokens": [400, 550, 291, 611, 362, 281, 976, 257, 1315, 337, 1184, 2519, 300, 311, 3845, 1951, 300, 1254, 13], "temperature": 0.0, "avg_logprob": -0.305231177288553, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.0188049373027752e-06}, {"id": 197, "seek": 81944, "start": 836.6400000000001, "end": 842.5200000000001, "text": " Okay, so you would like to have something that checks that the form IDs are unique,", "tokens": [1033, 11, 370, 291, 576, 411, 281, 362, 746, 300, 13834, 300, 264, 1254, 48212, 366, 3845, 11], "temperature": 0.0, "avg_logprob": -0.305231177288553, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.0188049373027752e-06}, {"id": 198, "seek": 81944, "start": 842.5200000000001, "end": 845.6800000000001, "text": " and that for each form, the names are unique.", "tokens": [293, 300, 337, 1184, 1254, 11, 264, 5288, 366, 3845, 13], "temperature": 0.0, "avg_logprob": -0.305231177288553, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.0188049373027752e-06}, {"id": 199, "seek": 81944, "start": 845.6800000000001, "end": 846.72, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.305231177288553, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.0188049373027752e-06}, {"id": 200, "seek": 84672, "start": 846.72, "end": 852.72, "text": " And that would be a best practice just in general for accessibility to have it.", "tokens": [400, 300, 576, 312, 257, 1151, 3124, 445, 294, 2674, 337, 15002, 281, 362, 309, 13], "temperature": 0.0, "avg_logprob": -0.2758521640423647, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.2751881683925603e-07}, {"id": 201, "seek": 84672, "start": 852.72, "end": 858.5600000000001, "text": " Yeah, for accessibility purposes to have a name for each form field is a best practice.", "tokens": [865, 11, 337, 15002, 9932, 281, 362, 257, 1315, 337, 1184, 1254, 2519, 307, 257, 1151, 3124, 13], "temperature": 0.0, "avg_logprob": -0.2758521640423647, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.2751881683925603e-07}, {"id": 202, "seek": 84672, "start": 858.5600000000001, "end": 864.0400000000001, "text": " In the case of the form, it actually doesn't matter if you have a unique ID if you're only", "tokens": [682, 264, 1389, 295, 264, 1254, 11, 309, 767, 1177, 380, 1871, 498, 291, 362, 257, 3845, 7348, 498, 291, 434, 787], "temperature": 0.0, "avg_logprob": -0.2758521640423647, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.2751881683925603e-07}, {"id": 203, "seek": 84672, "start": 864.0400000000001, "end": 865.84, "text": " rendering one form on the page.", "tokens": [22407, 472, 1254, 322, 264, 3028, 13], "temperature": 0.0, "avg_logprob": -0.2758521640423647, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.2751881683925603e-07}, {"id": 204, "seek": 84672, "start": 865.84, "end": 870.2, "text": " But that's just one requirement that this API has.", "tokens": [583, 300, 311, 445, 472, 11695, 300, 341, 9362, 575, 13], "temperature": 0.0, "avg_logprob": -0.2758521640423647, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.2751881683925603e-07}, {"id": 205, "seek": 84672, "start": 870.2, "end": 872.12, "text": " I think it's a fairly reasonable one.", "tokens": [286, 519, 309, 311, 257, 6457, 10585, 472, 13], "temperature": 0.0, "avg_logprob": -0.2758521640423647, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.2751881683925603e-07}, {"id": 206, "seek": 87212, "start": 872.12, "end": 880.8, "text": " But it would be nice to have either the framework or Elmer view check for whenever you misuse", "tokens": [583, 309, 576, 312, 1481, 281, 362, 2139, 264, 8388, 420, 2699, 936, 1910, 1520, 337, 5699, 291, 3346, 438], "temperature": 0.0, "avg_logprob": -0.33300209045410156, "compression_ratio": 1.6908212560386473, "no_speech_prob": 5.804980105494906e-07}, {"id": 207, "seek": 87212, "start": 880.8, "end": 881.8, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.33300209045410156, "compression_ratio": 1.6908212560386473, "no_speech_prob": 5.804980105494906e-07}, {"id": 208, "seek": 87212, "start": 881.8, "end": 882.8, "text": " Yeah, and that would be very nice.", "tokens": [865, 11, 293, 300, 576, 312, 588, 1481, 13], "temperature": 0.0, "avg_logprob": -0.33300209045410156, "compression_ratio": 1.6908212560386473, "no_speech_prob": 5.804980105494906e-07}, {"id": 209, "seek": 87212, "start": 882.8, "end": 888.72, "text": " And it's a bit hard to tell which one is more suited because each one will have its own", "tokens": [400, 309, 311, 257, 857, 1152, 281, 980, 597, 472, 307, 544, 24736, 570, 1184, 472, 486, 362, 1080, 1065], "temperature": 0.0, "avg_logprob": -0.33300209045410156, "compression_ratio": 1.6908212560386473, "no_speech_prob": 5.804980105494906e-07}, {"id": 210, "seek": 87212, "start": 888.72, "end": 892.84, "text": " pitfalls and things that it can check and things that it can't check.", "tokens": [10147, 18542, 293, 721, 300, 309, 393, 1520, 293, 721, 300, 309, 393, 380, 1520, 13], "temperature": 0.0, "avg_logprob": -0.33300209045410156, "compression_ratio": 1.6908212560386473, "no_speech_prob": 5.804980105494906e-07}, {"id": 211, "seek": 87212, "start": 892.84, "end": 896.0, "text": " So a bit hard, but yeah, definitely that would be useful.", "tokens": [407, 257, 857, 1152, 11, 457, 1338, 11, 2138, 300, 576, 312, 4420, 13], "temperature": 0.0, "avg_logprob": -0.33300209045410156, "compression_ratio": 1.6908212560386473, "no_speech_prob": 5.804980105494906e-07}, {"id": 212, "seek": 89600, "start": 896.0, "end": 904.92, "text": " Yeah, so originally, I wasn't sure if that same pattern would translate outside of Elm", "tokens": [865, 11, 370, 7993, 11, 286, 2067, 380, 988, 498, 300, 912, 5102, 576, 13799, 2380, 295, 2699, 76], "temperature": 0.0, "avg_logprob": -0.25506469530937, "compression_ratio": 1.5258215962441315, "no_speech_prob": 6.893537261021265e-07}, {"id": 213, "seek": 89600, "start": 904.92, "end": 911.76, "text": " pages or if it would even be interesting because this whole core value proposition was manage", "tokens": [7183, 420, 498, 309, 576, 754, 312, 1880, 570, 341, 1379, 4965, 2158, 24830, 390, 3067], "temperature": 0.0, "avg_logprob": -0.25506469530937, "compression_ratio": 1.5258215962441315, "no_speech_prob": 6.893537261021265e-07}, {"id": 214, "seek": 89600, "start": 911.76, "end": 915.84, "text": " low level state so that you can simplify the wiring.", "tokens": [2295, 1496, 1785, 370, 300, 291, 393, 20460, 264, 27520, 13], "temperature": 0.0, "avg_logprob": -0.25506469530937, "compression_ratio": 1.5258215962441315, "no_speech_prob": 6.893537261021265e-07}, {"id": 215, "seek": 89600, "start": 915.84, "end": 923.64, "text": " So you have this magic trick that it's still regular Elm code, but you can just say, render", "tokens": [407, 291, 362, 341, 5585, 4282, 300, 309, 311, 920, 3890, 2699, 76, 3089, 11, 457, 291, 393, 445, 584, 11, 15529], "temperature": 0.0, "avg_logprob": -0.25506469530937, "compression_ratio": 1.5258215962441315, "no_speech_prob": 6.893537261021265e-07}, {"id": 216, "seek": 92364, "start": 923.64, "end": 926.52, "text": " form, give it a unique ID.", "tokens": [1254, 11, 976, 309, 257, 3845, 7348, 13], "temperature": 0.0, "avg_logprob": -0.23875347030497043, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.5008367831323994e-06}, {"id": 217, "seek": 92364, "start": 926.52, "end": 931.64, "text": " And now you have real time client side validations for this form parser you defined.", "tokens": [400, 586, 291, 362, 957, 565, 6423, 1252, 7363, 763, 337, 341, 1254, 21156, 260, 291, 7642, 13], "temperature": 0.0, "avg_logprob": -0.23875347030497043, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.5008367831323994e-06}, {"id": 218, "seek": 92364, "start": 931.64, "end": 936.3199999999999, "text": " And it's like giving you the errors on the fly and you haven't added anything to your", "tokens": [400, 309, 311, 411, 2902, 291, 264, 13603, 322, 264, 3603, 293, 291, 2378, 380, 3869, 1340, 281, 428], "temperature": 0.0, "avg_logprob": -0.23875347030497043, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.5008367831323994e-06}, {"id": 219, "seek": 92364, "start": 936.3199999999999, "end": 940.52, "text": " model or, you know, all you did is render something in your view.", "tokens": [2316, 420, 11, 291, 458, 11, 439, 291, 630, 307, 15529, 746, 294, 428, 1910, 13], "temperature": 0.0, "avg_logprob": -0.23875347030497043, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.5008367831323994e-06}, {"id": 220, "seek": 92364, "start": 940.52, "end": 943.64, "text": " It turns out, I think it's still pretty useful.", "tokens": [467, 4523, 484, 11, 286, 519, 309, 311, 920, 1238, 4420, 13], "temperature": 0.0, "avg_logprob": -0.23875347030497043, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.5008367831323994e-06}, {"id": 221, "seek": 92364, "start": 943.64, "end": 949.72, "text": " One of the reasons I think it's useful is because I extracted it out in such a way that", "tokens": [1485, 295, 264, 4112, 286, 519, 309, 311, 4420, 307, 570, 286, 34086, 309, 484, 294, 1270, 257, 636, 300], "temperature": 0.0, "avg_logprob": -0.23875347030497043, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.5008367831323994e-06}, {"id": 222, "seek": 94972, "start": 949.72, "end": 954.48, "text": " you can do that same pattern in your own application if you want to.", "tokens": [291, 393, 360, 300, 912, 5102, 294, 428, 1065, 3861, 498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.24131582201141671, "compression_ratio": 1.7022222222222223, "no_speech_prob": 7.002136612754839e-07}, {"id": 223, "seek": 94972, "start": 954.48, "end": 960.72, "text": " So you know, I mean, other frameworks could integrate this in the same sort of pattern", "tokens": [407, 291, 458, 11, 286, 914, 11, 661, 29834, 727, 13365, 341, 294, 264, 912, 1333, 295, 5102], "temperature": 0.0, "avg_logprob": -0.24131582201141671, "compression_ratio": 1.7022222222222223, "no_speech_prob": 7.002136612754839e-07}, {"id": 224, "seek": 94972, "start": 960.72, "end": 962.48, "text": " that Elm pages does.", "tokens": [300, 2699, 76, 7183, 775, 13], "temperature": 0.0, "avg_logprob": -0.24131582201141671, "compression_ratio": 1.7022222222222223, "no_speech_prob": 7.002136612754839e-07}, {"id": 225, "seek": 94972, "start": 962.48, "end": 968.88, "text": " But in your own code base, you could have a single sort of shared model between pages", "tokens": [583, 294, 428, 1065, 3089, 3096, 11, 291, 727, 362, 257, 2167, 1333, 295, 5507, 2316, 1296, 7183], "temperature": 0.0, "avg_logprob": -0.24131582201141671, "compression_ratio": 1.7022222222222223, "no_speech_prob": 7.002136612754839e-07}, {"id": 226, "seek": 94972, "start": 968.88, "end": 973.48, "text": " that has all form state and you could have like a shared message.", "tokens": [300, 575, 439, 1254, 1785, 293, 291, 727, 362, 411, 257, 5507, 3636, 13], "temperature": 0.0, "avg_logprob": -0.24131582201141671, "compression_ratio": 1.7022222222222223, "no_speech_prob": 7.002136612754839e-07}, {"id": 227, "seek": 94972, "start": 973.48, "end": 976.0, "text": " I'm not sure if people use this pattern very commonly.", "tokens": [286, 478, 406, 988, 498, 561, 764, 341, 5102, 588, 12719, 13], "temperature": 0.0, "avg_logprob": -0.24131582201141671, "compression_ratio": 1.7022222222222223, "no_speech_prob": 7.002136612754839e-07}, {"id": 228, "seek": 97600, "start": 976.0, "end": 980.6, "text": " I'm not sure I had seen it before I kind of came up with this for Elm pages.", "tokens": [286, 478, 406, 988, 286, 632, 1612, 309, 949, 286, 733, 295, 1361, 493, 365, 341, 337, 2699, 76, 7183, 13], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 229, "seek": 97600, "start": 980.6, "end": 982.28, "text": " Have you seen this before Jeroen?", "tokens": [3560, 291, 1612, 341, 949, 508, 2032, 268, 30], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 230, "seek": 97600, "start": 982.28, "end": 990.96, "text": " Of having a sort of framework message and wrapping the page specific messages in the", "tokens": [2720, 1419, 257, 1333, 295, 8388, 3636, 293, 21993, 264, 3028, 2685, 7897, 294, 264], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 231, "seek": 97600, "start": 990.96, "end": 991.96, "text": " sort of app message?", "tokens": [1333, 295, 724, 3636, 30], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 232, "seek": 97600, "start": 991.96, "end": 994.28, "text": " I don't know if I've seen it in packages.", "tokens": [286, 500, 380, 458, 498, 286, 600, 1612, 309, 294, 17401, 13], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 233, "seek": 97600, "start": 994.28, "end": 998.04, "text": " I definitely use it at work on something Elm SPA like.", "tokens": [286, 2138, 764, 309, 412, 589, 322, 746, 2699, 76, 8420, 32, 411, 13], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 234, "seek": 97600, "start": 998.04, "end": 1000.52, "text": " I was going to wonder like, do I do that for Elm Review?", "tokens": [286, 390, 516, 281, 2441, 411, 11, 360, 286, 360, 300, 337, 2699, 76, 19954, 30], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 235, "seek": 97600, "start": 1000.52, "end": 1003.44, "text": " But no, Elm Review doesn't use messages at all.", "tokens": [583, 572, 11, 2699, 76, 19954, 1177, 380, 764, 7897, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 236, "seek": 97600, "start": 1003.44, "end": 1005.52, "text": " But at least not in the API.", "tokens": [583, 412, 1935, 406, 294, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.3028735684272938, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.8162035075874883e-06}, {"id": 237, "seek": 100552, "start": 1005.52, "end": 1013.84, "text": " So no, I do use this in a work project, but I don't know if I've seen it in packages.", "tokens": [407, 572, 11, 286, 360, 764, 341, 294, 257, 589, 1716, 11, 457, 286, 500, 380, 458, 498, 286, 600, 1612, 309, 294, 17401, 13], "temperature": 0.0, "avg_logprob": -0.2374916643199354, "compression_ratio": 1.5598290598290598, "no_speech_prob": 4.289139269531006e-06}, {"id": 238, "seek": 100552, "start": 1013.84, "end": 1014.84, "text": " Nice.", "tokens": [5490, 13], "temperature": 0.0, "avg_logprob": -0.2374916643199354, "compression_ratio": 1.5598290598290598, "no_speech_prob": 4.289139269531006e-06}, {"id": 239, "seek": 100552, "start": 1014.84, "end": 1015.84, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2374916643199354, "compression_ratio": 1.5598290598290598, "no_speech_prob": 4.289139269531006e-06}, {"id": 240, "seek": 100552, "start": 1015.84, "end": 1020.96, "text": " So then, for example, in your code base or other code bases that use that same pattern,", "tokens": [407, 550, 11, 337, 1365, 11, 294, 428, 3089, 3096, 420, 661, 3089, 17949, 300, 764, 300, 912, 5102, 11], "temperature": 0.0, "avg_logprob": -0.2374916643199354, "compression_ratio": 1.5598290598290598, "no_speech_prob": 4.289139269531006e-06}, {"id": 241, "seek": 100552, "start": 1020.96, "end": 1026.6, "text": " you can get the same ergonomics and the same sort of magic trick that Elm pages does where", "tokens": [291, 393, 483, 264, 912, 42735, 29884, 293, 264, 912, 1333, 295, 5585, 4282, 300, 2699, 76, 7183, 775, 689], "temperature": 0.0, "avg_logprob": -0.2374916643199354, "compression_ratio": 1.5598290598290598, "no_speech_prob": 4.289139269531006e-06}, {"id": 242, "seek": 100552, "start": 1026.6, "end": 1034.84, "text": " you just create an app level message and each page doesn't have to worry about wiring up", "tokens": [291, 445, 1884, 364, 724, 1496, 3636, 293, 1184, 3028, 1177, 380, 362, 281, 3292, 466, 27520, 493], "temperature": 0.0, "avg_logprob": -0.2374916643199354, "compression_ratio": 1.5598290598290598, "no_speech_prob": 4.289139269531006e-06}, {"id": 243, "seek": 103484, "start": 1034.84, "end": 1035.84, "text": " that form state.", "tokens": [300, 1254, 1785, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 244, "seek": 103484, "start": 1035.84, "end": 1041.36, "text": " And I have to say, I used to find it to be a bit of a nightmare working with forms in", "tokens": [400, 286, 362, 281, 584, 11, 286, 1143, 281, 915, 309, 281, 312, 257, 857, 295, 257, 18724, 1364, 365, 6422, 294], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 245, "seek": 103484, "start": 1041.36, "end": 1042.36, "text": " Elm.", "tokens": [2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 246, "seek": 103484, "start": 1042.36, "end": 1046.12, "text": " I used to kind of dread wiring up every single message every time.", "tokens": [286, 1143, 281, 733, 295, 22236, 27520, 493, 633, 2167, 3636, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 247, "seek": 103484, "start": 1046.12, "end": 1047.8799999999999, "text": " And this is a big relief.", "tokens": [400, 341, 307, 257, 955, 10915, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 248, "seek": 103484, "start": 1047.8799999999999, "end": 1054.8799999999999, "text": " I think, correct me if I'm wrong, but the reason why it works so well for you is because", "tokens": [286, 519, 11, 3006, 385, 498, 286, 478, 2085, 11, 457, 264, 1778, 983, 309, 1985, 370, 731, 337, 291, 307, 570], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 249, "seek": 103484, "start": 1054.8799999999999, "end": 1059.1999999999998, "text": " you're using code generation to wire that up partially.", "tokens": [291, 434, 1228, 3089, 5125, 281, 6234, 300, 493, 18886, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 250, "seek": 103484, "start": 1059.1999999999998, "end": 1060.1999999999998, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 251, "seek": 103484, "start": 1060.1999999999998, "end": 1061.1999999999998, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 252, "seek": 103484, "start": 1061.1999999999998, "end": 1063.24, "text": " So that's what I'm doing at work as well.", "tokens": [407, 300, 311, 437, 286, 478, 884, 412, 589, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.26842259178476885, "compression_ratio": 1.6, "no_speech_prob": 5.1737520152528305e-06}, {"id": 253, "seek": 106324, "start": 1063.24, "end": 1067.84, "text": " And that only works if you know how something will look right.", "tokens": [400, 300, 787, 1985, 498, 291, 458, 577, 746, 486, 574, 558, 13], "temperature": 0.0, "avg_logprob": -0.27903124929844647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.70616931427503e-06}, {"id": 254, "seek": 106324, "start": 1067.84, "end": 1071.08, "text": " You can only code generate things that you're aware of.", "tokens": [509, 393, 787, 3089, 8460, 721, 300, 291, 434, 3650, 295, 13], "temperature": 0.0, "avg_logprob": -0.27903124929844647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.70616931427503e-06}, {"id": 255, "seek": 106324, "start": 1071.08, "end": 1077.88, "text": " So as you said, if something is opinionated or there is some convention around the practice,", "tokens": [407, 382, 291, 848, 11, 498, 746, 307, 4800, 770, 420, 456, 307, 512, 10286, 926, 264, 3124, 11], "temperature": 0.0, "avg_logprob": -0.27903124929844647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.70616931427503e-06}, {"id": 256, "seek": 106324, "start": 1077.88, "end": 1084.28, "text": " then you can make nice tools around it or automate boilerplate writing.", "tokens": [550, 291, 393, 652, 1481, 3873, 926, 309, 420, 31605, 39228, 37008, 3579, 13], "temperature": 0.0, "avg_logprob": -0.27903124929844647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.70616931427503e-06}, {"id": 257, "seek": 106324, "start": 1084.28, "end": 1090.4, "text": " So yeah, doing something conventional or standard or really helps with having a nice experience.", "tokens": [407, 1338, 11, 884, 746, 16011, 420, 3832, 420, 534, 3665, 365, 1419, 257, 1481, 1752, 13], "temperature": 0.0, "avg_logprob": -0.27903124929844647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.70616931427503e-06}, {"id": 258, "seek": 109040, "start": 1090.4, "end": 1095.8400000000001, "text": " But I think that's partially the reason why things like Ruby took off because they had", "tokens": [583, 286, 519, 300, 311, 18886, 264, 1778, 983, 721, 411, 19907, 1890, 766, 570, 436, 632], "temperature": 0.0, "avg_logprob": -0.24988252145272713, "compression_ratio": 1.632, "no_speech_prob": 9.931050470868286e-08}, {"id": 259, "seek": 109040, "start": 1095.8400000000001, "end": 1097.88, "text": " so many things around convention.", "tokens": [370, 867, 721, 926, 10286, 13], "temperature": 0.0, "avg_logprob": -0.24988252145272713, "compression_ratio": 1.632, "no_speech_prob": 9.931050470868286e-08}, {"id": 260, "seek": 109040, "start": 1097.88, "end": 1101.48, "text": " You correct me if I'm right, because I don't know Ruby at all.", "tokens": [509, 3006, 385, 498, 286, 478, 558, 11, 570, 286, 500, 380, 458, 19907, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.24988252145272713, "compression_ratio": 1.632, "no_speech_prob": 9.931050470868286e-08}, {"id": 261, "seek": 109040, "start": 1101.48, "end": 1107.5600000000002, "text": " But since everyone did things in a specific way, people can make tools that assumed or", "tokens": [583, 1670, 1518, 630, 721, 294, 257, 2685, 636, 11, 561, 393, 652, 3873, 300, 15895, 420], "temperature": 0.0, "avg_logprob": -0.24988252145272713, "compression_ratio": 1.632, "no_speech_prob": 9.931050470868286e-08}, {"id": 262, "seek": 109040, "start": 1107.5600000000002, "end": 1110.6000000000001, "text": " presumed that things were done that way.", "tokens": [18028, 292, 300, 721, 645, 1096, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.24988252145272713, "compression_ratio": 1.632, "no_speech_prob": 9.931050470868286e-08}, {"id": 263, "seek": 109040, "start": 1110.6000000000001, "end": 1113.44, "text": " And then you can make very nice tools.", "tokens": [400, 550, 291, 393, 652, 588, 1481, 3873, 13], "temperature": 0.0, "avg_logprob": -0.24988252145272713, "compression_ratio": 1.632, "no_speech_prob": 9.931050470868286e-08}, {"id": 264, "seek": 109040, "start": 1113.44, "end": 1117.8000000000002, "text": " That's also kind of what we do with Elm in a lot of ways.", "tokens": [663, 311, 611, 733, 295, 437, 321, 360, 365, 2699, 76, 294, 257, 688, 295, 2098, 13], "temperature": 0.0, "avg_logprob": -0.24988252145272713, "compression_ratio": 1.632, "no_speech_prob": 9.931050470868286e-08}, {"id": 265, "seek": 111780, "start": 1117.8, "end": 1126.0, "text": " We know that none of our functions will have mutations and all those kinds of things.", "tokens": [492, 458, 300, 6022, 295, 527, 6828, 486, 362, 29243, 293, 439, 729, 3685, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 266, "seek": 111780, "start": 1126.0, "end": 1127.0, "text": " And we play with that.", "tokens": [400, 321, 862, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 267, "seek": 111780, "start": 1127.0, "end": 1132.24, "text": " The only limitation is that we still need to resolve the types in a way that makes everything", "tokens": [440, 787, 27432, 307, 300, 321, 920, 643, 281, 14151, 264, 3467, 294, 257, 636, 300, 1669, 1203], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 268, "seek": 111780, "start": 1132.24, "end": 1133.24, "text": " work together.", "tokens": [589, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 269, "seek": 111780, "start": 1133.24, "end": 1138.2, "text": " But apart from that, we kind of do the same with our tooling.", "tokens": [583, 4936, 490, 300, 11, 321, 733, 295, 360, 264, 912, 365, 527, 46593, 13], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 270, "seek": 111780, "start": 1138.2, "end": 1141.6, "text": " And that's why we have a lot of awesome tools.", "tokens": [400, 300, 311, 983, 321, 362, 257, 688, 295, 3476, 3873, 13], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 271, "seek": 111780, "start": 1141.6, "end": 1142.9199999999998, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 272, "seek": 111780, "start": 1142.9199999999998, "end": 1144.1599999999999, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2332836479269048, "compression_ratio": 1.6018957345971565, "no_speech_prob": 2.561248948040884e-06}, {"id": 273, "seek": 114416, "start": 1144.16, "end": 1150.8400000000001, "text": " The thing with Ruby is that it has a lot of capacity for metaprogramming and patching", "tokens": [440, 551, 365, 19907, 307, 300, 309, 575, 257, 688, 295, 6042, 337, 1131, 569, 340, 1342, 2810, 293, 9972, 278], "temperature": 0.0, "avg_logprob": -0.22419856942218283, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.7880254290503217e-06}, {"id": 274, "seek": 114416, "start": 1150.8400000000001, "end": 1151.8400000000001, "text": " in things.", "tokens": [294, 721, 13], "temperature": 0.0, "avg_logprob": -0.22419856942218283, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.7880254290503217e-06}, {"id": 275, "seek": 114416, "start": 1151.8400000000001, "end": 1155.3200000000002, "text": " And I think that is one of the things that made it successful.", "tokens": [400, 286, 519, 300, 307, 472, 295, 264, 721, 300, 1027, 309, 4406, 13], "temperature": 0.0, "avg_logprob": -0.22419856942218283, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.7880254290503217e-06}, {"id": 276, "seek": 114416, "start": 1155.3200000000002, "end": 1161.0800000000002, "text": " I mean, Rails was sort of built on a lot of those features, being able to use method missing", "tokens": [286, 914, 11, 48526, 390, 1333, 295, 3094, 322, 257, 688, 295, 729, 4122, 11, 885, 1075, 281, 764, 3170, 5361], "temperature": 0.0, "avg_logprob": -0.22419856942218283, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.7880254290503217e-06}, {"id": 277, "seek": 114416, "start": 1161.0800000000002, "end": 1169.0, "text": " and just patch in methods that made things ergonomic and gave you conventions for being", "tokens": [293, 445, 9972, 294, 7150, 300, 1027, 721, 42735, 21401, 293, 2729, 291, 33520, 337, 885], "temperature": 0.0, "avg_logprob": -0.22419856942218283, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.7880254290503217e-06}, {"id": 278, "seek": 114416, "start": 1169.0, "end": 1170.1200000000001, "text": " very productive.", "tokens": [588, 13304, 13], "temperature": 0.0, "avg_logprob": -0.22419856942218283, "compression_ratio": 1.6682242990654206, "no_speech_prob": 1.7880254290503217e-06}, {"id": 279, "seek": 117012, "start": 1170.12, "end": 1176.08, "text": " And I think that's been sort of like a challenge in Elm is like when we have to be so explicit", "tokens": [400, 286, 519, 300, 311, 668, 1333, 295, 411, 257, 3430, 294, 2699, 76, 307, 411, 562, 321, 362, 281, 312, 370, 13691], "temperature": 0.0, "avg_logprob": -0.22334193257452215, "compression_ratio": 1.6846473029045643, "no_speech_prob": 5.014317139284685e-06}, {"id": 280, "seek": 117012, "start": 1176.08, "end": 1180.36, "text": " and this programming language is not, it's the opposite of dynamic.", "tokens": [293, 341, 9410, 2856, 307, 406, 11, 309, 311, 264, 6182, 295, 8546, 13], "temperature": 0.0, "avg_logprob": -0.22334193257452215, "compression_ratio": 1.6846473029045643, "no_speech_prob": 5.014317139284685e-06}, {"id": 281, "seek": 117012, "start": 1180.36, "end": 1184.6799999999998, "text": " It's very static, which means you often have to be very explicit, which sometimes means", "tokens": [467, 311, 588, 13437, 11, 597, 1355, 291, 2049, 362, 281, 312, 588, 13691, 11, 597, 2171, 1355], "temperature": 0.0, "avg_logprob": -0.22334193257452215, "compression_ratio": 1.6846473029045643, "no_speech_prob": 5.014317139284685e-06}, {"id": 282, "seek": 117012, "start": 1184.6799999999998, "end": 1185.6799999999998, "text": " boilerplate.", "tokens": [39228, 37008, 13], "temperature": 0.0, "avg_logprob": -0.22334193257452215, "compression_ratio": 1.6846473029045643, "no_speech_prob": 5.014317139284685e-06}, {"id": 283, "seek": 117012, "start": 1185.6799999999998, "end": 1193.56, "text": " And so the one downside with having one opinionated way of doing things that you have to abide", "tokens": [400, 370, 264, 472, 25060, 365, 1419, 472, 4800, 770, 636, 295, 884, 721, 300, 291, 362, 281, 39663], "temperature": 0.0, "avg_logprob": -0.22334193257452215, "compression_ratio": 1.6846473029045643, "no_speech_prob": 5.014317139284685e-06}, {"id": 284, "seek": 117012, "start": 1193.56, "end": 1197.4799999999998, "text": " by is that you're limited in how you do things.", "tokens": [538, 307, 300, 291, 434, 5567, 294, 577, 291, 360, 721, 13], "temperature": 0.0, "avg_logprob": -0.22334193257452215, "compression_ratio": 1.6846473029045643, "no_speech_prob": 5.014317139284685e-06}, {"id": 285, "seek": 119748, "start": 1197.48, "end": 1204.1200000000001, "text": " So if you do that with your form package, if you have an opinion in a way of managing", "tokens": [407, 498, 291, 360, 300, 365, 428, 1254, 7372, 11, 498, 291, 362, 364, 4800, 294, 257, 636, 295, 11642], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 286, "seek": 119748, "start": 1204.1200000000001, "end": 1207.56, "text": " that form, then you better hope that it's good.", "tokens": [300, 1254, 11, 550, 291, 1101, 1454, 300, 309, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 287, "seek": 119748, "start": 1207.56, "end": 1208.56, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 288, "seek": 119748, "start": 1208.56, "end": 1209.56, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 289, "seek": 119748, "start": 1209.56, "end": 1210.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 290, "seek": 119748, "start": 1210.56, "end": 1211.56, "text": " Which you seem to think it is.", "tokens": [3013, 291, 1643, 281, 519, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 291, "seek": 119748, "start": 1211.56, "end": 1213.3600000000001, "text": " I haven't played with it yet.", "tokens": [286, 2378, 380, 3737, 365, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 292, "seek": 119748, "start": 1213.3600000000001, "end": 1214.3600000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 293, "seek": 119748, "start": 1214.3600000000001, "end": 1215.68, "text": " We'll see later on.", "tokens": [492, 603, 536, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 294, "seek": 119748, "start": 1215.68, "end": 1216.68, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 295, "seek": 119748, "start": 1216.68, "end": 1219.56, "text": " Well, you, yeah, you could share your thoughts once you try it out.", "tokens": [1042, 11, 291, 11, 1338, 11, 291, 727, 2073, 428, 4598, 1564, 291, 853, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 296, "seek": 119748, "start": 1219.56, "end": 1223.84, "text": " And we kind of talked about this in our original episode where we talked about exploring this", "tokens": [400, 321, 733, 295, 2825, 466, 341, 294, 527, 3380, 3500, 689, 321, 2825, 466, 12736, 341], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 297, "seek": 119748, "start": 1223.84, "end": 1226.24, "text": " initial API design.", "tokens": [5883, 9362, 1715, 13], "temperature": 0.0, "avg_logprob": -0.28183937072753906, "compression_ratio": 1.68503937007874, "no_speech_prob": 3.9896377757031587e-07}, {"id": 298, "seek": 122624, "start": 1226.24, "end": 1232.68, "text": " But when you have, so choosing what you have opinions on and choosing what you couple to", "tokens": [583, 562, 291, 362, 11, 370, 10875, 437, 291, 362, 11819, 322, 293, 10875, 437, 291, 1916, 281], "temperature": 0.0, "avg_logprob": -0.2704017345721905, "compression_ratio": 1.7, "no_speech_prob": 3.3596532489355013e-07}, {"id": 299, "seek": 122624, "start": 1232.68, "end": 1234.56, "text": " choosing what assumptions to make, right.", "tokens": [10875, 437, 17695, 281, 652, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.2704017345721905, "compression_ratio": 1.7, "no_speech_prob": 3.3596532489355013e-07}, {"id": 300, "seek": 122624, "start": 1234.56, "end": 1241.64, "text": " If you, so this form API has an opinion that form data is a thing.", "tokens": [759, 291, 11, 370, 341, 1254, 9362, 575, 364, 4800, 300, 1254, 1412, 307, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.2704017345721905, "compression_ratio": 1.7, "no_speech_prob": 3.3596532489355013e-07}, {"id": 301, "seek": 122624, "start": 1241.64, "end": 1243.96, "text": " Form fields are a thing.", "tokens": [10126, 7909, 366, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.2704017345721905, "compression_ratio": 1.7, "no_speech_prob": 3.3596532489355013e-07}, {"id": 302, "seek": 122624, "start": 1243.96, "end": 1249.92, "text": " They are input or text area elements and they have key value pairs.", "tokens": [814, 366, 4846, 420, 2487, 1859, 4959, 293, 436, 362, 2141, 2158, 15494, 13], "temperature": 0.0, "avg_logprob": -0.2704017345721905, "compression_ratio": 1.7, "no_speech_prob": 3.3596532489355013e-07}, {"id": 303, "seek": 122624, "start": 1249.92, "end": 1255.0, "text": " That I mean, that is like, that's not my opinion.", "tokens": [663, 286, 914, 11, 300, 307, 411, 11, 300, 311, 406, 452, 4800, 13], "temperature": 0.0, "avg_logprob": -0.2704017345721905, "compression_ratio": 1.7, "no_speech_prob": 3.3596532489355013e-07}, {"id": 304, "seek": 125500, "start": 1255.0, "end": 1257.04, "text": " That's the browser's opinion.", "tokens": [663, 311, 264, 11185, 311, 4800, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 305, "seek": 125500, "start": 1257.04, "end": 1262.72, "text": " So I think that that's a pretty solid foundation for an API to build on.", "tokens": [407, 286, 519, 300, 300, 311, 257, 1238, 5100, 7030, 337, 364, 9362, 281, 1322, 322, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 306, "seek": 125500, "start": 1262.72, "end": 1270.28, "text": " Because you're working with conventions defined by WWC, I think.", "tokens": [1436, 291, 434, 1364, 365, 33520, 7642, 538, 12040, 34, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 307, "seek": 125500, "start": 1270.28, "end": 1273.84, "text": " World Wide Web Consortium.", "tokens": [3937, 42543, 9573, 31719, 2197, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 308, "seek": 125500, "start": 1273.84, "end": 1275.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 309, "seek": 125500, "start": 1275.2, "end": 1276.56, "text": " Some MDN.", "tokens": [2188, 22521, 45, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 310, "seek": 125500, "start": 1276.56, "end": 1278.08, "text": " You're conforming to MDN.", "tokens": [509, 434, 18975, 278, 281, 22521, 45, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 311, "seek": 125500, "start": 1278.08, "end": 1279.08, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 312, "seek": 125500, "start": 1279.08, "end": 1280.08, "text": " That's right.", "tokens": [663, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 313, "seek": 125500, "start": 1280.08, "end": 1282.0, "text": " Yes, exactly.", "tokens": [1079, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.36430430942111547, "compression_ratio": 1.4, "no_speech_prob": 2.964896452795074e-07}, {"id": 314, "seek": 128200, "start": 1282.0, "end": 1288.88, "text": " So by doing that, this has been something that I've been really thinking about a lot", "tokens": [407, 538, 884, 300, 11, 341, 575, 668, 746, 300, 286, 600, 668, 534, 1953, 466, 257, 688], "temperature": 0.0, "avg_logprob": -0.2364363670349121, "compression_ratio": 1.7174721189591078, "no_speech_prob": 2.684083483472932e-06}, {"id": 315, "seek": 128200, "start": 1288.88, "end": 1294.96, "text": " recently is sort of how do we fill that gap where like exactly this sort of sweet spot", "tokens": [3938, 307, 1333, 295, 577, 360, 321, 2836, 300, 7417, 689, 411, 2293, 341, 1333, 295, 3844, 4008], "temperature": 0.0, "avg_logprob": -0.2364363670349121, "compression_ratio": 1.7174721189591078, "no_speech_prob": 2.684083483472932e-06}, {"id": 316, "seek": 128200, "start": 1294.96, "end": 1301.78, "text": " that Rails and Ruby were able to fit into where you get to use this magic, which makes", "tokens": [300, 48526, 293, 19907, 645, 1075, 281, 3318, 666, 689, 291, 483, 281, 764, 341, 5585, 11, 597, 1669], "temperature": 0.0, "avg_logprob": -0.2364363670349121, "compression_ratio": 1.7174721189591078, "no_speech_prob": 2.684083483472932e-06}, {"id": 317, "seek": 128200, "start": 1301.78, "end": 1306.24, "text": " things very sleek and you can give an amazing demo because you're just like, hey, I just", "tokens": [721, 588, 43464, 293, 291, 393, 976, 364, 2243, 10723, 570, 291, 434, 445, 411, 11, 4177, 11, 286, 445], "temperature": 0.0, "avg_logprob": -0.2364363670349121, "compression_ratio": 1.7174721189591078, "no_speech_prob": 2.684083483472932e-06}, {"id": 318, "seek": 128200, "start": 1306.24, "end": 1308.28, "text": " put this thing here and all these things happen.", "tokens": [829, 341, 551, 510, 293, 439, 613, 721, 1051, 13], "temperature": 0.0, "avg_logprob": -0.2364363670349121, "compression_ratio": 1.7174721189591078, "no_speech_prob": 2.684083483472932e-06}, {"id": 319, "seek": 128200, "start": 1308.28, "end": 1309.28, "text": " It just works.", "tokens": [467, 445, 1985, 13], "temperature": 0.0, "avg_logprob": -0.2364363670349121, "compression_ratio": 1.7174721189591078, "no_speech_prob": 2.684083483472932e-06}, {"id": 320, "seek": 128200, "start": 1309.28, "end": 1311.56, "text": " You don't have to like, okay, I wire this in here.", "tokens": [509, 500, 380, 362, 281, 411, 11, 1392, 11, 286, 6234, 341, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.2364363670349121, "compression_ratio": 1.7174721189591078, "no_speech_prob": 2.684083483472932e-06}, {"id": 321, "seek": 131156, "start": 1311.56, "end": 1314.44, "text": " And I have to have a message that responds to this thing.", "tokens": [400, 286, 362, 281, 362, 257, 3636, 300, 27331, 281, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 322, "seek": 131156, "start": 1314.44, "end": 1316.56, "text": " And oh, the types aren't right for this.", "tokens": [400, 1954, 11, 264, 3467, 3212, 380, 558, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 323, "seek": 131156, "start": 1316.56, "end": 1317.56, "text": " They didn't line up here.", "tokens": [814, 994, 380, 1622, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 324, "seek": 131156, "start": 1317.56, "end": 1318.56, "text": " So we have to change that.", "tokens": [407, 321, 362, 281, 1319, 300, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 325, "seek": 131156, "start": 1318.56, "end": 1319.56, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 326, "seek": 131156, "start": 1319.56, "end": 1321.52, "text": " It's like, that's not very sexy.", "tokens": [467, 311, 411, 11, 300, 311, 406, 588, 13701, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 327, "seek": 131156, "start": 1321.52, "end": 1322.52, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 328, "seek": 131156, "start": 1322.52, "end": 1328.48, "text": " But you can get a lot closer to that magic when you bake in these assumptions.", "tokens": [583, 291, 393, 483, 257, 688, 4966, 281, 300, 5585, 562, 291, 16562, 294, 613, 17695, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 329, "seek": 131156, "start": 1328.48, "end": 1329.48, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 330, "seek": 131156, "start": 1329.48, "end": 1334.3999999999999, "text": " The one thing that I've quite liked with that I've very much liked with all the element", "tokens": [440, 472, 551, 300, 286, 600, 1596, 4501, 365, 300, 286, 600, 588, 709, 4501, 365, 439, 264, 4478], "temperature": 0.0, "avg_logprob": -0.28838014602661133, "compression_ratio": 1.6415929203539823, "no_speech_prob": 1.1911012052223668e-06}, {"id": 331, "seek": 133440, "start": 1334.4, "end": 1341.5600000000002, "text": " implementations where they do code generation and something quite like magic in a way.", "tokens": [4445, 763, 689, 436, 360, 3089, 5125, 293, 746, 1596, 411, 5585, 294, 257, 636, 13], "temperature": 0.0, "avg_logprob": -0.29886038710431356, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.521508577046916e-06}, {"id": 332, "seek": 133440, "start": 1341.5600000000002, "end": 1347.8400000000001, "text": " I'm mostly thinking of Elm SPA where they generate some of the files to do the boilerplate", "tokens": [286, 478, 5240, 1953, 295, 2699, 76, 8420, 32, 689, 436, 8460, 512, 295, 264, 7098, 281, 360, 264, 39228, 37008], "temperature": 0.0, "avg_logprob": -0.29886038710431356, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.521508577046916e-06}, {"id": 333, "seek": 133440, "start": 1347.8400000000001, "end": 1351.1200000000001, "text": " between the main application and all the different pages.", "tokens": [1296, 264, 2135, 3861, 293, 439, 264, 819, 7183, 13], "temperature": 0.0, "avg_logprob": -0.29886038710431356, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.521508577046916e-06}, {"id": 334, "seek": 133440, "start": 1351.1200000000001, "end": 1355.48, "text": " The nice thing with that is that you can look at the boilerplate and you can see how things", "tokens": [440, 1481, 551, 365, 300, 307, 300, 291, 393, 574, 412, 264, 39228, 37008, 293, 291, 393, 536, 577, 721], "temperature": 0.0, "avg_logprob": -0.29886038710431356, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.521508577046916e-06}, {"id": 335, "seek": 133440, "start": 1355.48, "end": 1357.4, "text": " are done.", "tokens": [366, 1096, 13], "temperature": 0.0, "avg_logprob": -0.29886038710431356, "compression_ratio": 1.6600985221674878, "no_speech_prob": 2.521508577046916e-06}, {"id": 336, "seek": 135740, "start": 1357.4, "end": 1364.8400000000001, "text": " And in Elm SPA's case, you can even overwrite how the boilerplate writing is done, wiring", "tokens": [400, 294, 2699, 76, 8420, 32, 311, 1389, 11, 291, 393, 754, 670, 21561, 577, 264, 39228, 37008, 3579, 307, 1096, 11, 27520], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 337, "seek": 135740, "start": 1364.8400000000001, "end": 1365.8400000000001, "text": " is done.", "tokens": [307, 1096, 13], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 338, "seek": 135740, "start": 1365.8400000000001, "end": 1369.3600000000001, "text": " I'm guessing with Elm Pages you won't see that.", "tokens": [286, 478, 17939, 365, 2699, 76, 430, 1660, 291, 1582, 380, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 339, "seek": 135740, "start": 1369.3600000000001, "end": 1370.3600000000001, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 340, "seek": 135740, "start": 1370.3600000000001, "end": 1371.3600000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 341, "seek": 135740, "start": 1371.3600000000001, "end": 1376.1200000000001, "text": " Elm Pages has a specific, I mean, it tries to, you know, give you the right extension", "tokens": [2699, 76, 430, 1660, 575, 257, 2685, 11, 286, 914, 11, 309, 9898, 281, 11, 291, 458, 11, 976, 291, 264, 558, 10320], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 342, "seek": 135740, "start": 1376.1200000000001, "end": 1383.2, "text": " points to customize things, but it is like, it generates its main.elm, which does its", "tokens": [2793, 281, 19734, 721, 11, 457, 309, 307, 411, 11, 309, 23815, 1080, 2135, 13, 338, 76, 11, 597, 775, 1080], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 343, "seek": 135740, "start": 1383.2, "end": 1386.16, "text": " thing and in that particular part.", "tokens": [551, 293, 294, 300, 1729, 644, 13], "temperature": 0.0, "avg_logprob": -0.27033135743267767, "compression_ratio": 1.5775862068965518, "no_speech_prob": 2.156745495085488e-06}, {"id": 344, "seek": 138616, "start": 1386.16, "end": 1395.6000000000001, "text": " So it's more like Elm SPA gives you a sort of customizable main.elm, I believe, and Elm", "tokens": [407, 309, 311, 544, 411, 2699, 76, 8420, 32, 2709, 291, 257, 1333, 295, 47922, 2135, 13, 338, 76, 11, 286, 1697, 11, 293, 2699, 76], "temperature": 0.0, "avg_logprob": -0.2507827906932646, "compression_ratio": 1.6488888888888888, "no_speech_prob": 4.3817428263537295e-07}, {"id": 345, "seek": 138616, "start": 1395.6000000000001, "end": 1402.4, "text": " Pages gives you points to customize that it pulls into the generated main.elm, but main.elm", "tokens": [430, 1660, 2709, 291, 2793, 281, 19734, 300, 309, 16982, 666, 264, 10833, 2135, 13, 338, 76, 11, 457, 2135, 13, 338, 76], "temperature": 0.0, "avg_logprob": -0.2507827906932646, "compression_ratio": 1.6488888888888888, "no_speech_prob": 4.3817428263537295e-07}, {"id": 346, "seek": 138616, "start": 1402.4, "end": 1403.88, "text": " is not customizable.", "tokens": [307, 406, 47922, 13], "temperature": 0.0, "avg_logprob": -0.2507827906932646, "compression_ratio": 1.6488888888888888, "no_speech_prob": 4.3817428263537295e-07}, {"id": 347, "seek": 138616, "start": 1403.88, "end": 1407.4, "text": " And there are some limitations, but there are some conveniences with that.", "tokens": [400, 456, 366, 512, 15705, 11, 457, 456, 366, 512, 7158, 14004, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.2507827906932646, "compression_ratio": 1.6488888888888888, "no_speech_prob": 4.3817428263537295e-07}, {"id": 348, "seek": 138616, "start": 1407.4, "end": 1408.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2507827906932646, "compression_ratio": 1.6488888888888888, "no_speech_prob": 4.3817428263537295e-07}, {"id": 349, "seek": 138616, "start": 1408.4, "end": 1413.52, "text": " And it doesn't have to be important to look at how things are implemented under the hood.", "tokens": [400, 309, 1177, 380, 362, 281, 312, 1021, 281, 574, 412, 577, 721, 366, 12270, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.2507827906932646, "compression_ratio": 1.6488888888888888, "no_speech_prob": 4.3817428263537295e-07}, {"id": 350, "seek": 141352, "start": 1413.52, "end": 1417.6399999999999, "text": " It really depends on how much do you need it to understand the whole thing.", "tokens": [467, 534, 5946, 322, 577, 709, 360, 291, 643, 309, 281, 1223, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 351, "seek": 141352, "start": 1417.6399999999999, "end": 1422.6, "text": " Like for instance, Elm Review has the same mechanism where you give it a list of rules", "tokens": [1743, 337, 5197, 11, 2699, 76, 19954, 575, 264, 912, 7513, 689, 291, 976, 309, 257, 1329, 295, 4474], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 352, "seek": 141352, "start": 1422.6, "end": 1427.8799999999999, "text": " that you config and Elm Review pulls that in and we're good.", "tokens": [300, 291, 416, 20646, 293, 2699, 76, 19954, 16982, 300, 294, 293, 321, 434, 665, 13], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 353, "seek": 141352, "start": 1427.8799999999999, "end": 1431.84, "text": " No one needs to understand how things are wired in into main.", "tokens": [883, 472, 2203, 281, 1223, 577, 721, 366, 27415, 294, 666, 2135, 13], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 354, "seek": 141352, "start": 1431.84, "end": 1436.36, "text": " They just need to understand what a rule is, what it does, and that's it.", "tokens": [814, 445, 643, 281, 1223, 437, 257, 4978, 307, 11, 437, 309, 775, 11, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 355, "seek": 141352, "start": 1436.36, "end": 1437.36, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 356, "seek": 141352, "start": 1437.36, "end": 1438.36, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 357, "seek": 141352, "start": 1438.36, "end": 1441.02, "text": " I mean, that's the power of a good abstraction.", "tokens": [286, 914, 11, 300, 311, 264, 1347, 295, 257, 665, 37765, 13], "temperature": 0.0, "avg_logprob": -0.26000065273708767, "compression_ratio": 1.6719367588932805, "no_speech_prob": 9.721351261759992e-07}, {"id": 358, "seek": 144102, "start": 1441.02, "end": 1448.52, "text": " Of course, if it's possible to abstract out a certain detail completely, then that's ideal.", "tokens": [2720, 1164, 11, 498, 309, 311, 1944, 281, 12649, 484, 257, 1629, 2607, 2584, 11, 550, 300, 311, 7157, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 359, "seek": 144102, "start": 1448.52, "end": 1449.52, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 360, "seek": 144102, "start": 1449.52, "end": 1456.44, "text": " Now I'm curious, will people be able to see the Elm files that you've generated in the", "tokens": [823, 286, 478, 6369, 11, 486, 561, 312, 1075, 281, 536, 264, 2699, 76, 7098, 300, 291, 600, 10833, 294, 264], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 361, "seek": 144102, "start": 1456.44, "end": 1458.28, "text": " Elm stuff folder?", "tokens": [2699, 76, 1507, 10820, 30], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 362, "seek": 144102, "start": 1458.28, "end": 1460.96, "text": " If people are really curious on how things work under the hood?", "tokens": [759, 561, 366, 534, 6369, 322, 577, 721, 589, 833, 264, 13376, 30], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 363, "seek": 144102, "start": 1460.96, "end": 1461.96, "text": " Oh yeah.", "tokens": [876, 1338, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 364, "seek": 144102, "start": 1461.96, "end": 1462.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 365, "seek": 144102, "start": 1462.96, "end": 1464.96, "text": " You can see all of the code that's generated.", "tokens": [509, 393, 536, 439, 295, 264, 3089, 300, 311, 10833, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 366, "seek": 144102, "start": 1464.96, "end": 1465.96, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 367, "seek": 144102, "start": 1465.96, "end": 1466.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 368, "seek": 144102, "start": 1466.96, "end": 1467.96, "text": " It's there on the file system.", "tokens": [467, 311, 456, 322, 264, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 369, "seek": 144102, "start": 1467.96, "end": 1468.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27622998881543803, "compression_ratio": 1.6324786324786325, "no_speech_prob": 8.851482107274933e-07}, {"id": 370, "seek": 146896, "start": 1468.96, "end": 1471.2, "text": " I wonder if the editor picks it up, like all the connections.", "tokens": [286, 2441, 498, 264, 9839, 16137, 309, 493, 11, 411, 439, 264, 9271, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 371, "seek": 146896, "start": 1471.2, "end": 1474.64, "text": " Oh, this function is used in this generated file.", "tokens": [876, 11, 341, 2445, 307, 1143, 294, 341, 10833, 3991, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 372, "seek": 146896, "start": 1474.64, "end": 1478.24, "text": " Because that would help with understanding.", "tokens": [1436, 300, 576, 854, 365, 3701, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 373, "seek": 146896, "start": 1478.24, "end": 1482.52, "text": " That could get people lost maybe in some cases, but maybe, I don't know.", "tokens": [663, 727, 483, 561, 2731, 1310, 294, 512, 3331, 11, 457, 1310, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 374, "seek": 146896, "start": 1482.52, "end": 1483.52, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 375, "seek": 146896, "start": 1483.52, "end": 1484.52, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 376, "seek": 146896, "start": 1484.52, "end": 1491.1200000000001, "text": " It might not, because it does create a separate folder that copies over the Elm.json.", "tokens": [467, 1062, 406, 11, 570, 309, 775, 1884, 257, 4994, 10820, 300, 14341, 670, 264, 2699, 76, 13, 73, 3015, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 377, "seek": 146896, "start": 1491.1200000000001, "end": 1492.1200000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 378, "seek": 146896, "start": 1492.1200000000001, "end": 1495.52, "text": " I've definitely thought about these kinds of considerations.", "tokens": [286, 600, 2138, 1194, 466, 613, 3685, 295, 24070, 13], "temperature": 0.0, "avg_logprob": -0.37531143051009996, "compression_ratio": 1.5930232558139534, "no_speech_prob": 8.186293030121305e-07}, {"id": 379, "seek": 149552, "start": 1495.52, "end": 1500.2, "text": " It gets interesting creating an intuitive experience where the editor gives you what", "tokens": [467, 2170, 1880, 4084, 364, 21769, 1752, 689, 264, 9839, 2709, 291, 437], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 380, "seek": 149552, "start": 1500.2, "end": 1501.68, "text": " you're expecting there.", "tokens": [291, 434, 9650, 456, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 381, "seek": 149552, "start": 1501.68, "end": 1507.8799999999999, "text": " I actually think that it won't, because I don't think it does that for Elm Review either.", "tokens": [286, 767, 519, 300, 309, 1582, 380, 11, 570, 286, 500, 380, 519, 309, 775, 300, 337, 2699, 76, 19954, 2139, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 382, "seek": 149552, "start": 1507.8799999999999, "end": 1511.72, "text": " You can't see where the config that you've defined is used.", "tokens": [509, 393, 380, 536, 689, 264, 6662, 300, 291, 600, 7642, 307, 1143, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 383, "seek": 149552, "start": 1511.72, "end": 1512.72, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 384, "seek": 149552, "start": 1512.72, "end": 1513.72, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 385, "seek": 149552, "start": 1513.72, "end": 1514.72, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 386, "seek": 149552, "start": 1514.72, "end": 1515.72, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 387, "seek": 149552, "start": 1515.72, "end": 1516.72, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 388, "seek": 149552, "start": 1516.72, "end": 1523.92, "text": " So we talked about decoupling from the domain-specific data types for having a separate form parser", "tokens": [407, 321, 2825, 466, 979, 263, 11970, 490, 264, 9274, 12, 29258, 1412, 3467, 337, 1419, 257, 4994, 1254, 21156, 260], "temperature": 0.0, "avg_logprob": -0.25603365675311224, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.632667358033359e-07}, {"id": 389, "seek": 152392, "start": 1523.92, "end": 1527.3400000000001, "text": " and a separate sort of low-level data type.", "tokens": [293, 257, 4994, 1333, 295, 2295, 12, 12418, 1412, 2010, 13], "temperature": 0.0, "avg_logprob": -0.20229852826971756, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2603128400078276e-06}, {"id": 390, "seek": 152392, "start": 1527.3400000000001, "end": 1534.0, "text": " We haven't yet touched on using that low-level data type for submissions.", "tokens": [492, 2378, 380, 1939, 9828, 322, 1228, 300, 2295, 12, 12418, 1412, 2010, 337, 40429, 13], "temperature": 0.0, "avg_logprob": -0.20229852826971756, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2603128400078276e-06}, {"id": 391, "seek": 152392, "start": 1534.0, "end": 1540.1200000000001, "text": " And this is actually another one of the key pieces that Elm Pages uses for this magic", "tokens": [400, 341, 307, 767, 1071, 472, 295, 264, 2141, 3755, 300, 2699, 76, 430, 1660, 4960, 337, 341, 5585], "temperature": 0.0, "avg_logprob": -0.20229852826971756, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2603128400078276e-06}, {"id": 392, "seek": 152392, "start": 1540.1200000000001, "end": 1547.0, "text": " is if you're able to do code sharing, then you can send that low-level data.", "tokens": [307, 498, 291, 434, 1075, 281, 360, 3089, 5414, 11, 550, 291, 393, 2845, 300, 2295, 12, 12418, 1412, 13], "temperature": 0.0, "avg_logprob": -0.20229852826971756, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.2603128400078276e-06}, {"id": 393, "seek": 154700, "start": 1547.0, "end": 1554.48, "text": " So what typically will happen with a front-end application these days is you'll have some", "tokens": [407, 437, 5850, 486, 1051, 365, 257, 1868, 12, 521, 3861, 613, 1708, 307, 291, 603, 362, 512], "temperature": 0.0, "avg_logprob": -0.26269570712385504, "compression_ratio": 1.5491071428571428, "no_speech_prob": 7.571102855763456e-07}, {"id": 394, "seek": 154700, "start": 1554.48, "end": 1555.72, "text": " form data.", "tokens": [1254, 1412, 13], "temperature": 0.0, "avg_logprob": -0.26269570712385504, "compression_ratio": 1.5491071428571428, "no_speech_prob": 7.571102855763456e-07}, {"id": 395, "seek": 154700, "start": 1555.72, "end": 1561.8, "text": " You manage the form inputs programmatically through JavaScript somehow.", "tokens": [509, 3067, 264, 1254, 15743, 37648, 5030, 807, 15778, 6063, 13], "temperature": 0.0, "avg_logprob": -0.26269570712385504, "compression_ratio": 1.5491071428571428, "no_speech_prob": 7.571102855763456e-07}, {"id": 396, "seek": 154700, "start": 1561.8, "end": 1570.4, "text": " And on submit, you intercept the on submit, and then you end up encoding some JSON objects", "tokens": [400, 322, 10315, 11, 291, 24700, 264, 322, 10315, 11, 293, 550, 291, 917, 493, 43430, 512, 31828, 6565], "temperature": 0.0, "avg_logprob": -0.26269570712385504, "compression_ratio": 1.5491071428571428, "no_speech_prob": 7.571102855763456e-07}, {"id": 397, "seek": 154700, "start": 1570.4, "end": 1572.68, "text": " that you send up to an API or something.", "tokens": [300, 291, 2845, 493, 281, 364, 9362, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.26269570712385504, "compression_ratio": 1.5491071428571428, "no_speech_prob": 7.571102855763456e-07}, {"id": 398, "seek": 154700, "start": 1572.68, "end": 1575.52, "text": " And you can do that using my Elm Form API.", "tokens": [400, 291, 393, 360, 300, 1228, 452, 2699, 76, 10126, 9362, 13], "temperature": 0.0, "avg_logprob": -0.26269570712385504, "compression_ratio": 1.5491071428571428, "no_speech_prob": 7.571102855763456e-07}, {"id": 399, "seek": 157552, "start": 1575.52, "end": 1581.68, "text": " I have a with on submit, and you can use that to...", "tokens": [286, 362, 257, 365, 322, 10315, 11, 293, 291, 393, 764, 300, 281, 485], "temperature": 0.0, "avg_logprob": -0.2287878496893521, "compression_ratio": 1.7119565217391304, "no_speech_prob": 2.56127259490313e-06}, {"id": 400, "seek": 157552, "start": 1581.68, "end": 1589.6399999999999, "text": " You can even write your sort of form parser in such a way that you can take your form", "tokens": [509, 393, 754, 2464, 428, 1333, 295, 1254, 21156, 260, 294, 1270, 257, 636, 300, 291, 393, 747, 428, 1254], "temperature": 0.0, "avg_logprob": -0.2287878496893521, "compression_ratio": 1.7119565217391304, "no_speech_prob": 2.56127259490313e-06}, {"id": 401, "seek": 157552, "start": 1589.6399999999999, "end": 1595.04, "text": " field inputs, and you could even parse it into a JSON object if you wanted to, for example.", "tokens": [2519, 15743, 11, 293, 291, 727, 754, 48377, 309, 666, 257, 31828, 2657, 498, 291, 1415, 281, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.2287878496893521, "compression_ratio": 1.7119565217391304, "no_speech_prob": 2.56127259490313e-06}, {"id": 402, "seek": 157552, "start": 1595.04, "end": 1601.8799999999999, "text": " You could parse it into a tuple with a JSON object and a nice Elm type, which you can", "tokens": [509, 727, 48377, 309, 666, 257, 2604, 781, 365, 257, 31828, 2657, 293, 257, 1481, 2699, 76, 2010, 11, 597, 291, 393], "temperature": 0.0, "avg_logprob": -0.2287878496893521, "compression_ratio": 1.7119565217391304, "no_speech_prob": 2.56127259490313e-06}, {"id": 403, "seek": 160188, "start": 1601.88, "end": 1608.0400000000002, "text": " use to show optimistic UI, to show the in-progress creating item that you've got.", "tokens": [764, 281, 855, 19397, 15682, 11, 281, 855, 264, 294, 12, 4318, 3091, 4084, 3174, 300, 291, 600, 658, 13], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 404, "seek": 160188, "start": 1608.0400000000002, "end": 1610.72, "text": " There are all sorts of things you could do.", "tokens": [821, 366, 439, 7527, 295, 721, 291, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 405, "seek": 160188, "start": 1610.72, "end": 1613.72, "text": " It's quite flexible in that regard, but that works.", "tokens": [467, 311, 1596, 11358, 294, 300, 3843, 11, 457, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 406, "seek": 160188, "start": 1613.72, "end": 1616.64, "text": " But that's an extra layer of glue.", "tokens": [583, 300, 311, 364, 2857, 4583, 295, 8998, 13], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 407, "seek": 160188, "start": 1616.64, "end": 1618.96, "text": " You know, Lamdara talks a lot about...", "tokens": [509, 458, 11, 18825, 67, 2419, 6686, 257, 688, 466, 485], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 408, "seek": 160188, "start": 1618.96, "end": 1623.64, "text": " You know, Mario has given some great talks about this philosophy in Lamdara of removing", "tokens": [509, 458, 11, 9343, 575, 2212, 512, 869, 6686, 466, 341, 10675, 294, 18825, 67, 2419, 295, 12720], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 409, "seek": 160188, "start": 1623.64, "end": 1624.8400000000001, "text": " glue layers.", "tokens": [8998, 7914, 13], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 410, "seek": 160188, "start": 1624.8400000000001, "end": 1627.0, "text": " And I think he's done a brilliant job with that.", "tokens": [400, 286, 519, 415, 311, 1096, 257, 10248, 1691, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.2523969970973192, "compression_ratio": 1.572549019607843, "no_speech_prob": 1.9033672060686513e-06}, {"id": 411, "seek": 162700, "start": 1627.0, "end": 1633.44, "text": " And I believe that this actually removes an additional layer of glue that Lamdara still", "tokens": [400, 286, 1697, 300, 341, 767, 30445, 364, 4497, 4583, 295, 8998, 300, 18825, 67, 2419, 920], "temperature": 0.0, "avg_logprob": -0.23118534266391647, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.766837370581925e-06}, {"id": 412, "seek": 162700, "start": 1633.44, "end": 1635.6, "text": " has in it without using this approach.", "tokens": [575, 294, 309, 1553, 1228, 341, 3109, 13], "temperature": 0.0, "avg_logprob": -0.23118534266391647, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.766837370581925e-06}, {"id": 413, "seek": 162700, "start": 1635.6, "end": 1641.6, "text": " Because if you think about it, it's a lot of glue to turn these...", "tokens": [1436, 498, 291, 519, 466, 309, 11, 309, 311, 257, 688, 295, 8998, 281, 1261, 613, 485], "temperature": 0.0, "avg_logprob": -0.23118534266391647, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.766837370581925e-06}, {"id": 414, "seek": 162700, "start": 1641.6, "end": 1647.8, "text": " You know, to do all this managing of form input data and sort of parsing it at various", "tokens": [509, 458, 11, 281, 360, 439, 341, 11642, 295, 1254, 4846, 1412, 293, 1333, 295, 21156, 278, 309, 412, 3683], "temperature": 0.0, "avg_logprob": -0.23118534266391647, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.766837370581925e-06}, {"id": 415, "seek": 162700, "start": 1647.8, "end": 1648.8, "text": " stages.", "tokens": [10232, 13], "temperature": 0.0, "avg_logprob": -0.23118534266391647, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.766837370581925e-06}, {"id": 416, "seek": 162700, "start": 1648.8, "end": 1650.24, "text": " It turns into a lot of glue.", "tokens": [467, 4523, 666, 257, 688, 295, 8998, 13], "temperature": 0.0, "avg_logprob": -0.23118534266391647, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.766837370581925e-06}, {"id": 417, "seek": 162700, "start": 1650.24, "end": 1655.08, "text": " And then when you want to send that data, that you're turning this sort of unstructured", "tokens": [400, 550, 562, 291, 528, 281, 2845, 300, 1412, 11, 300, 291, 434, 6246, 341, 1333, 295, 18799, 46847], "temperature": 0.0, "avg_logprob": -0.23118534266391647, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.766837370581925e-06}, {"id": 418, "seek": 165508, "start": 1655.08, "end": 1659.9199999999998, "text": " form data into structured data that you can send up to the server, even with Lamdara,", "tokens": [1254, 1412, 666, 18519, 1412, 300, 291, 393, 2845, 493, 281, 264, 7154, 11, 754, 365, 18825, 67, 2419, 11], "temperature": 0.0, "avg_logprob": -0.2768545711741728, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.3081590850561042e-06}, {"id": 419, "seek": 165508, "start": 1659.9199999999998, "end": 1663.72, "text": " when you can say send to backend, you still...", "tokens": [562, 291, 393, 584, 2845, 281, 38087, 11, 291, 920, 485], "temperature": 0.0, "avg_logprob": -0.2768545711741728, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.3081590850561042e-06}, {"id": 420, "seek": 165508, "start": 1663.72, "end": 1668.3, "text": " There's an additional challenge that you can't trust that user input because...", "tokens": [821, 311, 364, 4497, 3430, 300, 291, 393, 380, 3361, 300, 4195, 4846, 570, 485], "temperature": 0.0, "avg_logprob": -0.2768545711741728, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.3081590850561042e-06}, {"id": 421, "seek": 165508, "start": 1668.3, "end": 1672.84, "text": " Because someone can just craft that same request to the backend.", "tokens": [1436, 1580, 393, 445, 8448, 300, 912, 5308, 281, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.2768545711741728, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.3081590850561042e-06}, {"id": 422, "seek": 165508, "start": 1672.84, "end": 1673.84, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.2768545711741728, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.3081590850561042e-06}, {"id": 423, "seek": 165508, "start": 1673.84, "end": 1678.04, "text": " So they could take the underlying bytes that are sent, they could hack on that, and they", "tokens": [407, 436, 727, 747, 264, 14217, 36088, 300, 366, 2279, 11, 436, 727, 10339, 322, 300, 11, 293, 436], "temperature": 0.0, "avg_logprob": -0.2768545711741728, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.3081590850561042e-06}, {"id": 424, "seek": 165508, "start": 1678.04, "end": 1684.3999999999999, "text": " could say, okay, well, you know, this represents an Elm type, which is a valid username, which", "tokens": [727, 584, 11, 1392, 11, 731, 11, 291, 458, 11, 341, 8855, 364, 2699, 76, 2010, 11, 597, 307, 257, 7363, 30351, 11, 597], "temperature": 0.0, "avg_logprob": -0.2768545711741728, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.3081590850561042e-06}, {"id": 425, "seek": 168440, "start": 1684.4, "end": 1689.8400000000001, "text": " valid usernames must be this many characters and can't have an at or whatever.", "tokens": [7363, 505, 1248, 1632, 1633, 312, 341, 867, 4342, 293, 393, 380, 362, 364, 412, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.21132899087572854, "compression_ratio": 1.6830985915492958, "no_speech_prob": 3.1561069135932485e-07}, {"id": 426, "seek": 168440, "start": 1689.8400000000001, "end": 1691.46, "text": " So that's a nice opaque type, right?", "tokens": [407, 300, 311, 257, 1481, 42687, 2010, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21132899087572854, "compression_ratio": 1.6830985915492958, "no_speech_prob": 3.1561069135932485e-07}, {"id": 427, "seek": 168440, "start": 1691.46, "end": 1697.0400000000002, "text": " It's impossible for that type to be created unless it goes through that code path, right?", "tokens": [467, 311, 6243, 337, 300, 2010, 281, 312, 2942, 5969, 309, 1709, 807, 300, 3089, 3100, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21132899087572854, "compression_ratio": 1.6830985915492958, "no_speech_prob": 3.1561069135932485e-07}, {"id": 428, "seek": 168440, "start": 1697.0400000000002, "end": 1703.0400000000002, "text": " Except with Lamdara wire, when you do send to backend, it just has a bytes decoder.", "tokens": [16192, 365, 18825, 67, 2419, 6234, 11, 562, 291, 360, 2845, 281, 38087, 11, 309, 445, 575, 257, 36088, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.21132899087572854, "compression_ratio": 1.6830985915492958, "no_speech_prob": 3.1561069135932485e-07}, {"id": 429, "seek": 168440, "start": 1703.0400000000002, "end": 1707.92, "text": " And magically, that opaque type comes into existence from those bytes.", "tokens": [400, 39763, 11, 300, 42687, 2010, 1487, 666, 9123, 490, 729, 36088, 13], "temperature": 0.0, "avg_logprob": -0.21132899087572854, "compression_ratio": 1.6830985915492958, "no_speech_prob": 3.1561069135932485e-07}, {"id": 430, "seek": 168440, "start": 1707.92, "end": 1712.68, "text": " And if you were to hack those bytes and give it a string that's not a valid username, so", "tokens": [400, 498, 291, 645, 281, 10339, 729, 36088, 293, 976, 309, 257, 6798, 300, 311, 406, 257, 7363, 30351, 11, 370], "temperature": 0.0, "avg_logprob": -0.21132899087572854, "compression_ratio": 1.6830985915492958, "no_speech_prob": 3.1561069135932485e-07}, {"id": 431, "seek": 168440, "start": 1712.68, "end": 1713.92, "text": " you actually can't trust it.", "tokens": [291, 767, 393, 380, 3361, 309, 13], "temperature": 0.0, "avg_logprob": -0.21132899087572854, "compression_ratio": 1.6830985915492958, "no_speech_prob": 3.1561069135932485e-07}, {"id": 432, "seek": 171392, "start": 1713.92, "end": 1718.0800000000002, "text": " And, you know, if you look at the meetdown code, Martin Stewart pointed me to some code", "tokens": [400, 11, 291, 458, 11, 498, 291, 574, 412, 264, 1677, 5093, 3089, 11, 9184, 25951, 10932, 385, 281, 512, 3089], "temperature": 0.0, "avg_logprob": -0.23261947631835939, "compression_ratio": 1.7509578544061302, "no_speech_prob": 4.356864792498527e-06}, {"id": 433, "seek": 171392, "start": 1718.0800000000002, "end": 1723.72, "text": " in that code base, which is a Lamdara app, really, really cool service.", "tokens": [294, 300, 3089, 3096, 11, 597, 307, 257, 18825, 67, 2419, 724, 11, 534, 11, 534, 1627, 2643, 13], "temperature": 0.0, "avg_logprob": -0.23261947631835939, "compression_ratio": 1.7509578544061302, "no_speech_prob": 4.356864792498527e-06}, {"id": 434, "seek": 171392, "start": 1723.72, "end": 1730.3600000000001, "text": " And he has a few helpers that sort of have security around that.", "tokens": [400, 415, 575, 257, 1326, 854, 433, 300, 1333, 295, 362, 3825, 926, 300, 13], "temperature": 0.0, "avg_logprob": -0.23261947631835939, "compression_ratio": 1.7509578544061302, "no_speech_prob": 4.356864792498527e-06}, {"id": 435, "seek": 171392, "start": 1730.3600000000001, "end": 1735.5, "text": " So he's actually considered that, you know, you know how much we love opaque types and", "tokens": [407, 415, 311, 767, 4888, 300, 11, 291, 458, 11, 291, 458, 577, 709, 321, 959, 42687, 3467, 293], "temperature": 0.0, "avg_logprob": -0.23261947631835939, "compression_ratio": 1.7509578544061302, "no_speech_prob": 4.356864792498527e-06}, {"id": 436, "seek": 171392, "start": 1735.5, "end": 1738.48, "text": " getting guarantees around that, but then you sort of lose those guarantees and you have", "tokens": [1242, 32567, 926, 300, 11, 457, 550, 291, 1333, 295, 3624, 729, 32567, 293, 291, 362], "temperature": 0.0, "avg_logprob": -0.23261947631835939, "compression_ratio": 1.7509578544061302, "no_speech_prob": 4.356864792498527e-06}, {"id": 437, "seek": 171392, "start": 1738.48, "end": 1741.8000000000002, "text": " to revalidate all of your opaque type constraints.", "tokens": [281, 319, 3337, 327, 473, 439, 295, 428, 42687, 2010, 18491, 13], "temperature": 0.0, "avg_logprob": -0.23261947631835939, "compression_ratio": 1.7509578544061302, "no_speech_prob": 4.356864792498527e-06}, {"id": 438, "seek": 171392, "start": 1741.8000000000002, "end": 1742.8000000000002, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.23261947631835939, "compression_ratio": 1.7509578544061302, "no_speech_prob": 4.356864792498527e-06}, {"id": 439, "seek": 174280, "start": 1742.8, "end": 1747.28, "text": " And again, only at the edges, but still you have to do it.", "tokens": [400, 797, 11, 787, 412, 264, 8819, 11, 457, 920, 291, 362, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 440, "seek": 174280, "start": 1747.28, "end": 1748.28, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 441, "seek": 174280, "start": 1748.28, "end": 1749.28, "text": " Which, yeah.", "tokens": [3013, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 442, "seek": 174280, "start": 1749.28, "end": 1750.28, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 443, "seek": 174280, "start": 1750.28, "end": 1751.96, "text": " It starts to feel quite messy.", "tokens": [467, 3719, 281, 841, 1596, 16191, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 444, "seek": 174280, "start": 1751.96, "end": 1755.1599999999999, "text": " And then you're like, wait a minute, can I really rely on this?", "tokens": [400, 550, 291, 434, 411, 11, 1699, 257, 3456, 11, 393, 286, 534, 10687, 322, 341, 30], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 445, "seek": 174280, "start": 1755.1599999999999, "end": 1759.48, "text": " And now you're dealing with opaque types that you can't trust.", "tokens": [400, 586, 291, 434, 6260, 365, 42687, 3467, 300, 291, 393, 380, 3361, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 446, "seek": 174280, "start": 1759.48, "end": 1760.52, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 447, "seek": 174280, "start": 1760.52, "end": 1762.36, "text": " The whole point is guarantees.", "tokens": [440, 1379, 935, 307, 32567, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 448, "seek": 174280, "start": 1762.36, "end": 1765.56, "text": " So that's like, that's the stuff of our nightmares.", "tokens": [407, 300, 311, 411, 11, 300, 311, 264, 1507, 295, 527, 36911, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 449, "seek": 174280, "start": 1765.56, "end": 1766.56, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 450, "seek": 174280, "start": 1766.56, "end": 1767.56, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 451, "seek": 174280, "start": 1767.56, "end": 1768.56, "text": " Literally.", "tokens": [23768, 13], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 452, "seek": 174280, "start": 1768.56, "end": 1772.24, "text": " I mean, if you can't trust opaque types, then who can you trust?", "tokens": [286, 914, 11, 498, 291, 393, 380, 3361, 42687, 3467, 11, 550, 567, 393, 291, 3361, 30], "temperature": 0.0, "avg_logprob": -0.3055744451635024, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.7330055470665684e-06}, {"id": 453, "seek": 177224, "start": 1772.24, "end": 1773.24, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 454, "seek": 177224, "start": 1773.24, "end": 1774.24, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 455, "seek": 177224, "start": 1774.24, "end": 1776.16, "text": " Think of the children.", "tokens": [6557, 295, 264, 2227, 13], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 456, "seek": 177224, "start": 1776.16, "end": 1782.6200000000001, "text": " So there's the problem.", "tokens": [407, 456, 311, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 457, "seek": 177224, "start": 1782.6200000000001, "end": 1784.68, "text": " How might this form API help with that?", "tokens": [1012, 1062, 341, 1254, 9362, 854, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 458, "seek": 177224, "start": 1784.68, "end": 1786.48, "text": " Well, I'm glad you asked.", "tokens": [1042, 11, 286, 478, 5404, 291, 2351, 13], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 459, "seek": 177224, "start": 1786.48, "end": 1792.44, "text": " Hey, Dillon, how can your form API, how can it help with that?", "tokens": [1911, 11, 28160, 11, 577, 393, 428, 1254, 9362, 11, 577, 393, 309, 854, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 460, "seek": 177224, "start": 1792.44, "end": 1794.44, "text": " Excellent question, Jeroen.", "tokens": [16723, 1168, 11, 508, 2032, 268, 13], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 461, "seek": 177224, "start": 1794.44, "end": 1795.44, "text": " Good.", "tokens": [2205, 13], "temperature": 0.0, "avg_logprob": -0.3865654774201222, "compression_ratio": 1.4037267080745341, "no_speech_prob": 4.860385615756968e-06}, {"id": 462, "seek": 179544, "start": 1795.44, "end": 1802.88, "text": " The answer is you defer parsing that data until you receive it on the server.", "tokens": [440, 1867, 307, 291, 25704, 21156, 278, 300, 1412, 1826, 291, 4774, 309, 322, 264, 7154, 13], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 463, "seek": 179544, "start": 1802.88, "end": 1807.4, "text": " So now you, with LambdaRacend to backend, what are you sending?", "tokens": [407, 586, 291, 11, 365, 45691, 49, 617, 273, 281, 38087, 11, 437, 366, 291, 7750, 30], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 464, "seek": 179544, "start": 1807.4, "end": 1811.72, "text": " You're sending low level key value pairs of strings.", "tokens": [509, 434, 7750, 2295, 1496, 2141, 2158, 15494, 295, 13985, 13], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 465, "seek": 179544, "start": 1811.72, "end": 1816.28, "text": " Which is the underlying, I mean, this is, as far as the browser is concerned, that's", "tokens": [3013, 307, 264, 14217, 11, 286, 914, 11, 341, 307, 11, 382, 1400, 382, 264, 11185, 307, 5922, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 466, "seek": 179544, "start": 1816.28, "end": 1817.6000000000001, "text": " the only thing that exists.", "tokens": [264, 787, 551, 300, 8198, 13], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 467, "seek": 179544, "start": 1817.6000000000001, "end": 1818.6000000000001, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 468, "seek": 179544, "start": 1818.6000000000001, "end": 1819.6000000000001, "text": " That's what form data is.", "tokens": [663, 311, 437, 1254, 1412, 307, 13], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 469, "seek": 179544, "start": 1819.6000000000001, "end": 1824.96, "text": " It's low value, low level key value pairs keyed off of the field names.", "tokens": [467, 311, 2295, 2158, 11, 2295, 1496, 2141, 2158, 15494, 2141, 292, 766, 295, 264, 2519, 5288, 13], "temperature": 0.0, "avg_logprob": -0.29604153755383616, "compression_ratio": 1.6546184738955823, "no_speech_prob": 9.874528359432588e-07}, {"id": 470, "seek": 182496, "start": 1824.96, "end": 1827.68, "text": " So you can submit those.", "tokens": [407, 291, 393, 10315, 729, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 471, "seek": 182496, "start": 1827.68, "end": 1831.24, "text": " You don't even need to write an encoder or anything for the submission.", "tokens": [509, 500, 380, 754, 643, 281, 2464, 364, 2058, 19866, 420, 1340, 337, 264, 23689, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 472, "seek": 182496, "start": 1831.24, "end": 1832.8, "text": " You just let it submit those.", "tokens": [509, 445, 718, 309, 10315, 729, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 473, "seek": 182496, "start": 1832.8, "end": 1835.64, "text": " You can use send to backend to send that low level data.", "tokens": [509, 393, 764, 2845, 281, 38087, 281, 2845, 300, 2295, 1496, 1412, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 474, "seek": 182496, "start": 1835.64, "end": 1840.76, "text": " Now in your LambdaRacend backend, when you receive the low level data, now you run your", "tokens": [823, 294, 428, 45691, 49, 617, 273, 38087, 11, 562, 291, 4774, 264, 2295, 1496, 1412, 11, 586, 291, 1190, 428], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 475, "seek": 182496, "start": 1840.76, "end": 1841.88, "text": " parser.", "tokens": [21156, 260, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 476, "seek": 182496, "start": 1841.88, "end": 1843.48, "text": " If you want to, you can run that.", "tokens": [759, 291, 528, 281, 11, 291, 393, 1190, 300, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 477, "seek": 182496, "start": 1843.48, "end": 1846.3600000000001, "text": " I mean, you run that parser on the front end.", "tokens": [286, 914, 11, 291, 1190, 300, 21156, 260, 322, 264, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 478, "seek": 182496, "start": 1846.3600000000001, "end": 1849.46, "text": " You know, you render with that sort of parsing logic.", "tokens": [509, 458, 11, 291, 15529, 365, 300, 1333, 295, 21156, 278, 9952, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 479, "seek": 182496, "start": 1849.46, "end": 1852.56, "text": " So you get your client side validations in real time as you type.", "tokens": [407, 291, 483, 428, 6423, 1252, 7363, 763, 294, 957, 565, 382, 291, 2010, 13], "temperature": 0.0, "avg_logprob": -0.22639623134256268, "compression_ratio": 1.8143939393939394, "no_speech_prob": 1.5056846223160392e-06}, {"id": 480, "seek": 185256, "start": 1852.56, "end": 1856.3999999999999, "text": " But then it reruns those same validations because you're using the exact same parser", "tokens": [583, 550, 309, 43819, 26684, 729, 912, 7363, 763, 570, 291, 434, 1228, 264, 1900, 912, 21156, 260], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 481, "seek": 185256, "start": 1856.3999999999999, "end": 1857.56, "text": " on the backend.", "tokens": [322, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 482, "seek": 185256, "start": 1857.56, "end": 1858.9199999999998, "text": " Now that's your source of truth.", "tokens": [823, 300, 311, 428, 4009, 295, 3494, 13], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 483, "seek": 185256, "start": 1858.9199999999998, "end": 1864.8, "text": " So now the source of truth doesn't come from data that's serialized from the front end", "tokens": [407, 586, 264, 4009, 295, 3494, 1177, 380, 808, 490, 1412, 300, 311, 17436, 1602, 490, 264, 1868, 917], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 484, "seek": 185256, "start": 1864.8, "end": 1865.9199999999998, "text": " to the backend.", "tokens": [281, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 485, "seek": 185256, "start": 1865.9199999999998, "end": 1870.72, "text": " You've got low level data as the thing you're sending to the backend.", "tokens": [509, 600, 658, 2295, 1496, 1412, 382, 264, 551, 291, 434, 7750, 281, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 486, "seek": 185256, "start": 1870.72, "end": 1875.36, "text": " And your source of truth is your code sharing of your form between the front end and the", "tokens": [400, 428, 4009, 295, 3494, 307, 428, 3089, 5414, 295, 428, 1254, 1296, 264, 1868, 917, 293, 264], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 487, "seek": 185256, "start": 1875.36, "end": 1876.36, "text": " backend.", "tokens": [38087, 13], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 488, "seek": 185256, "start": 1876.36, "end": 1879.48, "text": " So now you can trust your opaque types because you're running them in a trusted environment", "tokens": [407, 586, 291, 393, 3361, 428, 999, 64, 80, 622, 3467, 570, 291, 434, 2614, 552, 294, 257, 16034, 2823], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 489, "seek": 185256, "start": 1879.48, "end": 1880.48, "text": " in the backend.", "tokens": [294, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.21340387132432725, "compression_ratio": 2.0, "no_speech_prob": 5.203565365263785e-07}, {"id": 490, "seek": 188048, "start": 1880.48, "end": 1886.92, "text": " And I actually did a small example with a really very simple little LambdaRacend toy", "tokens": [400, 286, 767, 630, 257, 1359, 1365, 365, 257, 534, 588, 2199, 707, 45691, 49, 617, 273, 12058], "temperature": 0.0, "avg_logprob": -0.27884278207455043, "compression_ratio": 1.6131687242798354, "no_speech_prob": 9.72141720012587e-07}, {"id": 491, "seek": 188048, "start": 1886.92, "end": 1891.16, "text": " app I have, but I tried it out and it works great.", "tokens": [724, 286, 362, 11, 457, 286, 3031, 309, 484, 293, 309, 1985, 869, 13], "temperature": 0.0, "avg_logprob": -0.27884278207455043, "compression_ratio": 1.6131687242798354, "no_speech_prob": 9.72141720012587e-07}, {"id": 492, "seek": 188048, "start": 1891.16, "end": 1896.52, "text": " You just run your form parser on the backend and get your data and check if it's valid", "tokens": [509, 445, 1190, 428, 1254, 21156, 260, 322, 264, 38087, 293, 483, 428, 1412, 293, 1520, 498, 309, 311, 7363], "temperature": 0.0, "avg_logprob": -0.27884278207455043, "compression_ratio": 1.6131687242798354, "no_speech_prob": 9.72141720012587e-07}, {"id": 493, "seek": 188048, "start": 1896.52, "end": 1898.96, "text": " or invalid and it works beautifully.", "tokens": [420, 34702, 293, 309, 1985, 16525, 13], "temperature": 0.0, "avg_logprob": -0.27884278207455043, "compression_ratio": 1.6131687242798354, "no_speech_prob": 9.72141720012587e-07}, {"id": 494, "seek": 188048, "start": 1898.96, "end": 1899.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27884278207455043, "compression_ratio": 1.6131687242798354, "no_speech_prob": 9.72141720012587e-07}, {"id": 495, "seek": 188048, "start": 1899.96, "end": 1905.48, "text": " So people can still hack the bytes that are sent, but the backend will just refuse it", "tokens": [407, 561, 393, 920, 10339, 264, 36088, 300, 366, 2279, 11, 457, 264, 38087, 486, 445, 16791, 309], "temperature": 0.0, "avg_logprob": -0.27884278207455043, "compression_ratio": 1.6131687242798354, "no_speech_prob": 9.72141720012587e-07}, {"id": 496, "seek": 188048, "start": 1905.48, "end": 1907.76, "text": " because, hey, this is not a opaque type.", "tokens": [570, 11, 4177, 11, 341, 307, 406, 257, 42687, 2010, 13], "temperature": 0.0, "avg_logprob": -0.27884278207455043, "compression_ratio": 1.6131687242798354, "no_speech_prob": 9.72141720012587e-07}, {"id": 497, "seek": 190776, "start": 1907.76, "end": 1911.16, "text": " And I'm guessing they will have to handle that somehow.", "tokens": [400, 286, 478, 17939, 436, 486, 362, 281, 4813, 300, 6063, 13], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 498, "seek": 190776, "start": 1911.16, "end": 1913.78, "text": " But that's not a lot of work.", "tokens": [583, 300, 311, 406, 257, 688, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 499, "seek": 190776, "start": 1913.78, "end": 1919.04, "text": " Do you mean the opaque type of the, what they're sending is key value strings?", "tokens": [1144, 291, 914, 264, 42687, 2010, 295, 264, 11, 437, 436, 434, 7750, 307, 2141, 2158, 13985, 30], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 500, "seek": 190776, "start": 1919.04, "end": 1920.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 501, "seek": 190776, "start": 1920.04, "end": 1925.32, "text": " What I mean is they get the backend gets the low level data and they parse it into an opaque", "tokens": [708, 286, 914, 307, 436, 483, 264, 38087, 2170, 264, 2295, 1496, 1412, 293, 436, 48377, 309, 666, 364, 42687], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 502, "seek": 190776, "start": 1925.32, "end": 1931.08, "text": " type or an error, a result, a failing result.", "tokens": [2010, 420, 364, 6713, 11, 257, 1874, 11, 257, 18223, 1874, 13], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 503, "seek": 190776, "start": 1931.08, "end": 1935.16, "text": " And then they need to handle the successful case and the error case.", "tokens": [400, 550, 436, 643, 281, 4813, 264, 4406, 1389, 293, 264, 6713, 1389, 13], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 504, "seek": 190776, "start": 1935.16, "end": 1936.16, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 505, "seek": 190776, "start": 1936.16, "end": 1937.46, "text": " And that's running through backend code.", "tokens": [400, 300, 311, 2614, 807, 38087, 3089, 13], "temperature": 0.0, "avg_logprob": -0.29402483426607573, "compression_ratio": 1.6892430278884463, "no_speech_prob": 5.896393986404291e-07}, {"id": 506, "seek": 193746, "start": 1937.46, "end": 1939.56, "text": " So that can be trusted.", "tokens": [407, 300, 393, 312, 16034, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 507, "seek": 193746, "start": 1939.56, "end": 1949.28, "text": " So the thing is like conceptually, any user, trusted or not, has the ability to send data", "tokens": [407, 264, 551, 307, 411, 3410, 671, 11, 604, 4195, 11, 16034, 420, 406, 11, 575, 264, 3485, 281, 2845, 1412], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 508, "seek": 193746, "start": 1949.28, "end": 1950.28, "text": " to the server.", "tokens": [281, 264, 7154, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 509, "seek": 193746, "start": 1950.28, "end": 1951.28, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 510, "seek": 193746, "start": 1951.28, "end": 1957.18, "text": " And it's the server's job to then take that unverified, untrusted data and decide whether", "tokens": [400, 309, 311, 264, 7154, 311, 1691, 281, 550, 747, 300, 517, 331, 2587, 11, 1701, 81, 6589, 1412, 293, 4536, 1968], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 511, "seek": 193746, "start": 1957.18, "end": 1961.24, "text": " to trust it and to do something with it or ignore it.", "tokens": [281, 3361, 309, 293, 281, 360, 746, 365, 309, 420, 11200, 309, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 512, "seek": 193746, "start": 1961.24, "end": 1962.24, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 513, "seek": 193746, "start": 1962.24, "end": 1965.44, "text": " And it should probably never really trust it anyway.", "tokens": [400, 309, 820, 1391, 1128, 534, 3361, 309, 4033, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 514, "seek": 193746, "start": 1965.44, "end": 1966.44, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 515, "seek": 193746, "start": 1966.44, "end": 1967.44, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.21974333277288474, "compression_ratio": 1.6619718309859155, "no_speech_prob": 9.57079009822337e-07}, {"id": 516, "seek": 196744, "start": 1967.44, "end": 1975.68, "text": " So by deferring, parsing it from low level to structured data until it gets to the server,", "tokens": [407, 538, 25704, 2937, 11, 21156, 278, 309, 490, 2295, 1496, 281, 18519, 1412, 1826, 309, 2170, 281, 264, 7154, 11], "temperature": 0.0, "avg_logprob": -0.2762021337236677, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.1910916555279982e-06}, {"id": 517, "seek": 196744, "start": 1975.68, "end": 1981.24, "text": " you are following that conceptual mental model of not trusting it because you're just like,", "tokens": [291, 366, 3480, 300, 24106, 4973, 2316, 295, 406, 28235, 309, 570, 291, 434, 445, 411, 11], "temperature": 0.0, "avg_logprob": -0.2762021337236677, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.1910916555279982e-06}, {"id": 518, "seek": 196744, "start": 1981.24, "end": 1983.92, "text": " hey, you can send me key value pairs, right?", "tokens": [4177, 11, 291, 393, 2845, 385, 2141, 2158, 15494, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2762021337236677, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.1910916555279982e-06}, {"id": 519, "seek": 196744, "start": 1983.92, "end": 1991.56, "text": " Like you can go to, you know, I could open up some, you know, web tools and make a submission", "tokens": [1743, 291, 393, 352, 281, 11, 291, 458, 11, 286, 727, 1269, 493, 512, 11, 291, 458, 11, 3670, 3873, 293, 652, 257, 23689], "temperature": 0.0, "avg_logprob": -0.2762021337236677, "compression_ratio": 1.5432692307692308, "no_speech_prob": 1.1910916555279982e-06}, {"id": 520, "seek": 199156, "start": 1991.56, "end": 1998.1599999999999, "text": " to Amazon, to some Amazon forum and type in the key value pairs I want to.", "tokens": [281, 6795, 11, 281, 512, 6795, 17542, 293, 2010, 294, 264, 2141, 2158, 15494, 286, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.2470303050807265, "compression_ratio": 1.7991967871485943, "no_speech_prob": 3.0415073979384033e-06}, {"id": 521, "seek": 199156, "start": 1998.1599999999999, "end": 2003.48, "text": " And I could try to like put some unwanted values in there, but you know, and they can", "tokens": [400, 286, 727, 853, 281, 411, 829, 512, 33745, 4190, 294, 456, 11, 457, 291, 458, 11, 293, 436, 393], "temperature": 0.0, "avg_logprob": -0.2470303050807265, "compression_ratio": 1.7991967871485943, "no_speech_prob": 3.0415073979384033e-06}, {"id": 522, "seek": 199156, "start": 2003.48, "end": 2005.48, "text": " decide what to do with that in return.", "tokens": [4536, 437, 281, 360, 365, 300, 294, 2736, 13], "temperature": 0.0, "avg_logprob": -0.2470303050807265, "compression_ratio": 1.7991967871485943, "no_speech_prob": 3.0415073979384033e-06}, {"id": 523, "seek": 199156, "start": 2005.48, "end": 2010.9199999999998, "text": " But it's the job of the server to decide what to trust because the client can't be trusted.", "tokens": [583, 309, 311, 264, 1691, 295, 264, 7154, 281, 4536, 437, 281, 3361, 570, 264, 6423, 393, 380, 312, 16034, 13], "temperature": 0.0, "avg_logprob": -0.2470303050807265, "compression_ratio": 1.7991967871485943, "no_speech_prob": 3.0415073979384033e-06}, {"id": 524, "seek": 199156, "start": 2010.9199999999998, "end": 2013.8799999999999, "text": " The client can be anything.", "tokens": [440, 6423, 393, 312, 1340, 13], "temperature": 0.0, "avg_logprob": -0.2470303050807265, "compression_ratio": 1.7991967871485943, "no_speech_prob": 3.0415073979384033e-06}, {"id": 525, "seek": 199156, "start": 2013.8799999999999, "end": 2015.72, "text": " And we don't really care what the client is.", "tokens": [400, 321, 500, 380, 534, 1127, 437, 264, 6423, 307, 13], "temperature": 0.0, "avg_logprob": -0.2470303050807265, "compression_ratio": 1.7991967871485943, "no_speech_prob": 3.0415073979384033e-06}, {"id": 526, "seek": 199156, "start": 2015.72, "end": 2021.2, "text": " Like the client could be Vim or it could be a script or it could be a curl command.", "tokens": [1743, 264, 6423, 727, 312, 691, 332, 420, 309, 727, 312, 257, 5755, 420, 309, 727, 312, 257, 22591, 5622, 13], "temperature": 0.0, "avg_logprob": -0.2470303050807265, "compression_ratio": 1.7991967871485943, "no_speech_prob": 3.0415073979384033e-06}, {"id": 527, "seek": 202120, "start": 2021.2, "end": 2026.88, "text": " And we don't really know, like we can kind of try to put some things through JavaScript", "tokens": [400, 321, 500, 380, 534, 458, 11, 411, 321, 393, 733, 295, 853, 281, 829, 512, 721, 807, 15778], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 528, "seek": 202120, "start": 2026.88, "end": 2032.4, "text": " onto the page to put like a, you know, a little hidden field to try to prevent that.", "tokens": [3911, 264, 3028, 281, 829, 411, 257, 11, 291, 458, 11, 257, 707, 7633, 2519, 281, 853, 281, 4871, 300, 13], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 529, "seek": 202120, "start": 2032.4, "end": 2039.0, "text": " But we can't really know that we can trust the client because the client can do anything.", "tokens": [583, 321, 393, 380, 534, 458, 300, 321, 393, 3361, 264, 6423, 570, 264, 6423, 393, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 530, "seek": 202120, "start": 2039.0, "end": 2040.0, "text": " But the server we control.", "tokens": [583, 264, 7154, 321, 1969, 13], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 531, "seek": 202120, "start": 2040.0, "end": 2045.76, "text": " It's kind of weird that we have both the sentence, the client can't be trusted and the client", "tokens": [467, 311, 733, 295, 3657, 300, 321, 362, 1293, 264, 8174, 11, 264, 6423, 393, 380, 312, 16034, 293, 264, 6423], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 532, "seek": 202120, "start": 2045.76, "end": 2046.76, "text": " is king.", "tokens": [307, 4867, 13], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 533, "seek": 202120, "start": 2046.76, "end": 2047.76, "text": " But try to merge those together.", "tokens": [583, 853, 281, 22183, 729, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 534, "seek": 202120, "start": 2047.76, "end": 2048.76, "text": " I don't know if I've heard that one.", "tokens": [286, 500, 380, 458, 498, 286, 600, 2198, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 535, "seek": 202120, "start": 2048.76, "end": 2049.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2988494163335756, "compression_ratio": 1.8645418326693226, "no_speech_prob": 9.570794645696878e-07}, {"id": 536, "seek": 204976, "start": 2049.76, "end": 2051.2000000000003, "text": " I've heard that one before.", "tokens": [286, 600, 2198, 300, 472, 949, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 537, "seek": 204976, "start": 2051.2000000000003, "end": 2052.6400000000003, "text": " You haven't heard that one?", "tokens": [509, 2378, 380, 2198, 300, 472, 30], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 538, "seek": 204976, "start": 2052.6400000000003, "end": 2053.6400000000003, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 539, "seek": 204976, "start": 2053.6400000000003, "end": 2056.1600000000003, "text": " It's mostly for restaurants or.", "tokens": [467, 311, 5240, 337, 11486, 420, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 540, "seek": 204976, "start": 2056.1600000000003, "end": 2058.2000000000003, "text": " I've heard the customer is king.", "tokens": [286, 600, 2198, 264, 5474, 307, 4867, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 541, "seek": 204976, "start": 2058.2000000000003, "end": 2059.2000000000003, "text": " Customer is always right.", "tokens": [37168, 307, 1009, 558, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 542, "seek": 204976, "start": 2059.2000000000003, "end": 2061.32, "text": " Oh, maybe that's just a.", "tokens": [876, 11, 1310, 300, 311, 445, 257, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 543, "seek": 204976, "start": 2061.32, "end": 2065.96, "text": " It's French because customer is client.", "tokens": [467, 311, 5522, 570, 5474, 307, 6423, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 544, "seek": 204976, "start": 2065.96, "end": 2066.96, "text": " That makes sense.", "tokens": [663, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 545, "seek": 204976, "start": 2066.96, "end": 2067.96, "text": " That makes sense.", "tokens": [663, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 546, "seek": 204976, "start": 2067.96, "end": 2068.96, "text": " That's fair.", "tokens": [663, 311, 3143, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 547, "seek": 204976, "start": 2068.96, "end": 2069.96, "text": " So that's a French English joke.", "tokens": [407, 300, 311, 257, 5522, 3669, 7647, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 548, "seek": 204976, "start": 2069.96, "end": 2070.96, "text": " Hmm.", "tokens": [8239, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 549, "seek": 204976, "start": 2070.96, "end": 2071.96, "text": " Interesting.", "tokens": [14711, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 550, "seek": 204976, "start": 2071.96, "end": 2078.8, "text": " It's always a bit hard to figure out who can you tell those jokes that mix two languages.", "tokens": [467, 311, 1009, 257, 857, 1152, 281, 2573, 484, 567, 393, 291, 980, 729, 14439, 300, 2890, 732, 8650, 13], "temperature": 0.0, "avg_logprob": -0.3559533357620239, "compression_ratio": 1.7161016949152543, "no_speech_prob": 2.3454079212115175e-07}, {"id": 551, "seek": 207880, "start": 2078.8, "end": 2084.1200000000003, "text": " I have some jokes that I can only tell my family because they're like French and Dutch.", "tokens": [286, 362, 512, 14439, 300, 286, 393, 787, 980, 452, 1605, 570, 436, 434, 411, 5522, 293, 15719, 13], "temperature": 0.0, "avg_logprob": -0.27846481181957106, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.9333333511895034e-06}, {"id": 552, "seek": 207880, "start": 2084.1200000000003, "end": 2087.1200000000003, "text": " And it's a mix of those.", "tokens": [400, 309, 311, 257, 2890, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.27846481181957106, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.9333333511895034e-06}, {"id": 553, "seek": 207880, "start": 2087.1200000000003, "end": 2092.1600000000003, "text": " There's some good ones, but you can only tell them to specific groups of people.", "tokens": [821, 311, 512, 665, 2306, 11, 457, 291, 393, 787, 980, 552, 281, 2685, 3935, 295, 561, 13], "temperature": 0.0, "avg_logprob": -0.27846481181957106, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.9333333511895034e-06}, {"id": 554, "seek": 207880, "start": 2092.1600000000003, "end": 2093.84, "text": " And it's such a shame.", "tokens": [400, 309, 311, 1270, 257, 10069, 13], "temperature": 0.0, "avg_logprob": -0.27846481181957106, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.9333333511895034e-06}, {"id": 555, "seek": 207880, "start": 2093.84, "end": 2098.88, "text": " We can tell the Elm React jokes here, but that's about it.", "tokens": [492, 393, 980, 264, 2699, 76, 30644, 14439, 510, 11, 457, 300, 311, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.27846481181957106, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.9333333511895034e-06}, {"id": 556, "seek": 207880, "start": 2098.88, "end": 2103.46, "text": " So because we're dealing with low level data, you don't have a lot of guarantees around", "tokens": [407, 570, 321, 434, 6260, 365, 2295, 1496, 1412, 11, 291, 500, 380, 362, 257, 688, 295, 32567, 926], "temperature": 0.0, "avg_logprob": -0.27846481181957106, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.9333333511895034e-06}, {"id": 557, "seek": 207880, "start": 2103.46, "end": 2105.32, "text": " it to start with.", "tokens": [309, 281, 722, 365, 13], "temperature": 0.0, "avg_logprob": -0.27846481181957106, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.9333333511895034e-06}, {"id": 558, "seek": 210532, "start": 2105.32, "end": 2111.32, "text": " So for instance, how do you make sure that in a form that I'm going to send the backend", "tokens": [407, 337, 5197, 11, 577, 360, 291, 652, 988, 300, 294, 257, 1254, 300, 286, 478, 516, 281, 2845, 264, 38087], "temperature": 0.0, "avg_logprob": -0.3200537698310718, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.2482322517826105e-06}, {"id": 559, "seek": 210532, "start": 2111.32, "end": 2116.48, "text": " that I have not forgotten a field at the backend expects?", "tokens": [300, 286, 362, 406, 11832, 257, 2519, 412, 264, 38087, 33280, 30], "temperature": 0.0, "avg_logprob": -0.3200537698310718, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.2482322517826105e-06}, {"id": 560, "seek": 210532, "start": 2116.48, "end": 2122.28, "text": " Like you know, usually we try to create a nice opaque type and that opaque type in it,", "tokens": [1743, 291, 458, 11, 2673, 321, 853, 281, 1884, 257, 1481, 42687, 2010, 293, 300, 42687, 2010, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.3200537698310718, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.2482322517826105e-06}, {"id": 561, "seek": 210532, "start": 2122.28, "end": 2125.6000000000004, "text": " it expects something of a specific shape.", "tokens": [309, 33280, 746, 295, 257, 2685, 3909, 13], "temperature": 0.0, "avg_logprob": -0.3200537698310718, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.2482322517826105e-06}, {"id": 562, "seek": 210532, "start": 2125.6000000000004, "end": 2130.6000000000004, "text": " Like if it's Jason, then you parse it and you expect it to be a specific shape.", "tokens": [1743, 498, 309, 311, 11181, 11, 550, 291, 48377, 309, 293, 291, 2066, 309, 281, 312, 257, 2685, 3909, 13], "temperature": 0.0, "avg_logprob": -0.3200537698310718, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.2482322517826105e-06}, {"id": 563, "seek": 210532, "start": 2130.6000000000004, "end": 2132.6400000000003, "text": " If it does, then you get an okay.", "tokens": [759, 309, 775, 11, 550, 291, 483, 364, 1392, 13], "temperature": 0.0, "avg_logprob": -0.3200537698310718, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.2482322517826105e-06}, {"id": 564, "seek": 210532, "start": 2132.6400000000003, "end": 2134.56, "text": " And otherwise you get an error.", "tokens": [400, 5911, 291, 483, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.3200537698310718, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.2482322517826105e-06}, {"id": 565, "seek": 213456, "start": 2134.56, "end": 2140.56, "text": " And that is always a bit tricky to get right in the sense that if you forget it, then you're", "tokens": [400, 300, 307, 1009, 257, 857, 12414, 281, 483, 558, 294, 264, 2020, 300, 498, 291, 2870, 309, 11, 550, 291, 434], "temperature": 0.0, "avg_logprob": -0.26288876950162127, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.7880335008158e-06}, {"id": 566, "seek": 213456, "start": 2140.56, "end": 2146.4, "text": " likely not going to notice it unless you write specific tests for that, which I personally", "tokens": [3700, 406, 516, 281, 3449, 309, 5969, 291, 2464, 2685, 6921, 337, 300, 11, 597, 286, 5665], "temperature": 0.0, "avg_logprob": -0.26288876950162127, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.7880335008158e-06}, {"id": 567, "seek": 213456, "start": 2146.4, "end": 2149.04, "text": " don't, but I probably should.", "tokens": [500, 380, 11, 457, 286, 1391, 820, 13], "temperature": 0.0, "avg_logprob": -0.26288876950162127, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.7880335008158e-06}, {"id": 568, "seek": 213456, "start": 2149.04, "end": 2153.32, "text": " So how do you avoid that problem with your form API?", "tokens": [407, 577, 360, 291, 5042, 300, 1154, 365, 428, 1254, 9362, 30], "temperature": 0.0, "avg_logprob": -0.26288876950162127, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.7880335008158e-06}, {"id": 569, "seek": 213456, "start": 2153.32, "end": 2156.52, "text": " Now do you mean like a required field?", "tokens": [823, 360, 291, 914, 411, 257, 4739, 2519, 30], "temperature": 0.0, "avg_logprob": -0.26288876950162127, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.7880335008158e-06}, {"id": 570, "seek": 213456, "start": 2156.52, "end": 2163.0, "text": " Yeah, let's say there's a required field that is the first name and you forgot to add a", "tokens": [865, 11, 718, 311, 584, 456, 311, 257, 4739, 2519, 300, 307, 264, 700, 1315, 293, 291, 5298, 281, 909, 257], "temperature": 0.0, "avg_logprob": -0.26288876950162127, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.7880335008158e-06}, {"id": 571, "seek": 216300, "start": 2163.0, "end": 2170.32, "text": " field for that because you know, you and me, we both like guarantees and all those things.", "tokens": [2519, 337, 300, 570, 291, 458, 11, 291, 293, 385, 11, 321, 1293, 411, 32567, 293, 439, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 572, "seek": 216300, "start": 2170.32, "end": 2172.8, "text": " And we like things type check.", "tokens": [400, 321, 411, 721, 2010, 1520, 13], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 573, "seek": 216300, "start": 2172.8, "end": 2176.88, "text": " And I would like to check that this first name is never forgotten.", "tokens": [400, 286, 576, 411, 281, 1520, 300, 341, 700, 1315, 307, 1128, 11832, 13], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 574, "seek": 216300, "start": 2176.88, "end": 2178.92, "text": " So how do I ensure that?", "tokens": [407, 577, 360, 286, 5586, 300, 30], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 575, "seek": 216300, "start": 2178.92, "end": 2182.48, "text": " Is there something in your form or is it just something that you have to write tests for?", "tokens": [1119, 456, 746, 294, 428, 1254, 420, 307, 309, 445, 746, 300, 291, 362, 281, 2464, 6921, 337, 30], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 576, "seek": 216300, "start": 2182.48, "end": 2183.48, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 577, "seek": 216300, "start": 2183.48, "end": 2187.12, "text": " So you can, you can write a required form field.", "tokens": [407, 291, 393, 11, 291, 393, 2464, 257, 4739, 1254, 2519, 13], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 578, "seek": 216300, "start": 2187.12, "end": 2190.84, "text": " And if that field is absent, then that validation will fail.", "tokens": [400, 498, 300, 2519, 307, 25185, 11, 550, 300, 24071, 486, 3061, 13], "temperature": 0.0, "avg_logprob": -0.24709595952715194, "compression_ratio": 1.7385892116182573, "no_speech_prob": 2.7693806714523816e-06}, {"id": 579, "seek": 219084, "start": 2190.84, "end": 2195.88, "text": " So your form will just be parsed as invalid and you'll get a client side, you know, you'll", "tokens": [407, 428, 1254, 486, 445, 312, 21156, 292, 382, 34702, 293, 291, 603, 483, 257, 6423, 1252, 11, 291, 458, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.2353795988369832, "compression_ratio": 1.7530364372469636, "no_speech_prob": 4.5209171162241546e-07}, {"id": 580, "seek": 219084, "start": 2195.88, "end": 2197.76, "text": " get a client side validation.", "tokens": [483, 257, 6423, 1252, 24071, 13], "temperature": 0.0, "avg_logprob": -0.2353795988369832, "compression_ratio": 1.7530364372469636, "no_speech_prob": 4.5209171162241546e-07}, {"id": 581, "seek": 219084, "start": 2197.76, "end": 2202.04, "text": " You can forget to render a form field.", "tokens": [509, 393, 2870, 281, 15529, 257, 1254, 2519, 13], "temperature": 0.0, "avg_logprob": -0.2353795988369832, "compression_ratio": 1.7530364372469636, "no_speech_prob": 4.5209171162241546e-07}, {"id": 582, "seek": 219084, "start": 2202.04, "end": 2207.88, "text": " The form fields we discussed in the last episode on this, that it uses this sort of Elm codec", "tokens": [440, 1254, 7909, 321, 7152, 294, 264, 1036, 3500, 322, 341, 11, 300, 309, 4960, 341, 1333, 295, 2699, 76, 3089, 66], "temperature": 0.0, "avg_logprob": -0.2353795988369832, "compression_ratio": 1.7530364372469636, "no_speech_prob": 4.5209171162241546e-07}, {"id": 583, "seek": 219084, "start": 2207.88, "end": 2214.08, "text": " like pattern where there's a pipeline where you sort of pipe through adding each field", "tokens": [411, 5102, 689, 456, 311, 257, 15517, 689, 291, 1333, 295, 11240, 807, 5127, 1184, 2519], "temperature": 0.0, "avg_logprob": -0.2353795988369832, "compression_ratio": 1.7530364372469636, "no_speech_prob": 4.5209171162241546e-07}, {"id": 584, "seek": 219084, "start": 2214.08, "end": 2219.92, "text": " in the form, and then you get those in a Lambda and you, you use that to parse into the data", "tokens": [294, 264, 1254, 11, 293, 550, 291, 483, 729, 294, 257, 45691, 293, 291, 11, 291, 764, 300, 281, 48377, 666, 264, 1412], "temperature": 0.0, "avg_logprob": -0.2353795988369832, "compression_ratio": 1.7530364372469636, "no_speech_prob": 4.5209171162241546e-07}, {"id": 585, "seek": 221992, "start": 2219.92, "end": 2224.84, "text": " type and you use that to render your view with all the form fields.", "tokens": [2010, 293, 291, 764, 300, 281, 15529, 428, 1910, 365, 439, 264, 1254, 7909, 13], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 586, "seek": 221992, "start": 2224.84, "end": 2228.32, "text": " You could forget to render a form field.", "tokens": [509, 727, 2870, 281, 15529, 257, 1254, 2519, 13], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 587, "seek": 221992, "start": 2228.32, "end": 2233.04, "text": " And if it's, you know, if it's optional and you don't do any validation on it, then that", "tokens": [400, 498, 309, 311, 11, 291, 458, 11, 498, 309, 311, 17312, 293, 291, 500, 380, 360, 604, 24071, 322, 309, 11, 550, 300], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 588, "seek": 221992, "start": 2233.04, "end": 2239.0, "text": " that is something that will pass that validation because it's just like, yeah, there's no,", "tokens": [300, 307, 746, 300, 486, 1320, 300, 24071, 570, 309, 311, 445, 411, 11, 1338, 11, 456, 311, 572, 11], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 589, "seek": 221992, "start": 2239.0, "end": 2241.88, "text": " this field isn't there and that would be valid.", "tokens": [341, 2519, 1943, 380, 456, 293, 300, 576, 312, 7363, 13], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 590, "seek": 221992, "start": 2241.88, "end": 2243.56, "text": " So that is something to look out for.", "tokens": [407, 300, 307, 746, 281, 574, 484, 337, 13], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 591, "seek": 221992, "start": 2243.56, "end": 2247.7200000000003, "text": " Again, an Elm review rule would be a great, great way to check that you've used every", "tokens": [3764, 11, 364, 2699, 76, 3131, 4978, 576, 312, 257, 869, 11, 869, 636, 281, 1520, 300, 291, 600, 1143, 633], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 592, "seek": 221992, "start": 2247.7200000000003, "end": 2249.4, "text": " field in your, in your view.", "tokens": [2519, 294, 428, 11, 294, 428, 1910, 13], "temperature": 0.0, "avg_logprob": -0.2216528574625651, "compression_ratio": 1.8593155893536122, "no_speech_prob": 1.2679138308158144e-06}, {"id": 593, "seek": 224940, "start": 2249.4, "end": 2255.28, "text": " I think the unused parameters rule would mostly work, but you probably should not.", "tokens": [286, 519, 264, 44383, 9834, 4978, 576, 5240, 589, 11, 457, 291, 1391, 820, 406, 13], "temperature": 0.0, "avg_logprob": -0.25811878665462956, "compression_ratio": 1.7875647668393781, "no_speech_prob": 2.1023940632858285e-07}, {"id": 594, "seek": 224940, "start": 2255.28, "end": 2260.32, "text": " It could be used still because you need to use it in two places.", "tokens": [467, 727, 312, 1143, 920, 570, 291, 643, 281, 764, 309, 294, 732, 3190, 13], "temperature": 0.0, "avg_logprob": -0.25811878665462956, "compression_ratio": 1.7875647668393781, "no_speech_prob": 2.1023940632858285e-07}, {"id": 595, "seek": 224940, "start": 2260.32, "end": 2266.96, "text": " Oh, cause you use, you use your form fields in your, in your parser definition.", "tokens": [876, 11, 3082, 291, 764, 11, 291, 764, 428, 1254, 7909, 294, 428, 11, 294, 428, 21156, 260, 7123, 13], "temperature": 0.0, "avg_logprob": -0.25811878665462956, "compression_ratio": 1.7875647668393781, "no_speech_prob": 2.1023940632858285e-07}, {"id": 596, "seek": 224940, "start": 2266.96, "end": 2268.56, "text": " You use them in two places.", "tokens": [509, 764, 552, 294, 732, 3190, 13], "temperature": 0.0, "avg_logprob": -0.25811878665462956, "compression_ratio": 1.7875647668393781, "no_speech_prob": 2.1023940632858285e-07}, {"id": 597, "seek": 224940, "start": 2268.56, "end": 2273.82, "text": " You use them in your view to render a custom, custom view, however you want to render it.", "tokens": [509, 764, 552, 294, 428, 1910, 281, 15529, 257, 2375, 11, 2375, 1910, 11, 4461, 291, 528, 281, 15529, 309, 13], "temperature": 0.0, "avg_logprob": -0.25811878665462956, "compression_ratio": 1.7875647668393781, "no_speech_prob": 2.1023940632858285e-07}, {"id": 598, "seek": 227382, "start": 2273.82, "end": 2280.54, "text": " And you also use it to do dependent validations, to combine together all of your fields into", "tokens": [400, 291, 611, 764, 309, 281, 360, 12334, 7363, 763, 11, 281, 10432, 1214, 439, 295, 428, 7909, 666], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 599, "seek": 227382, "start": 2280.54, "end": 2281.8, "text": " a data type.", "tokens": [257, 1412, 2010, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 600, "seek": 227382, "start": 2281.8, "end": 2282.8, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 601, "seek": 227382, "start": 2282.8, "end": 2285.36, "text": " So in practice, would you write tests for that?", "tokens": [407, 294, 3124, 11, 576, 291, 2464, 6921, 337, 300, 30], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 602, "seek": 227382, "start": 2285.36, "end": 2288.92, "text": " Yeah, I think, I think it's a great thing to write tests for.", "tokens": [865, 11, 286, 519, 11, 286, 519, 309, 311, 257, 869, 551, 281, 2464, 6921, 337, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 603, "seek": 227382, "start": 2288.92, "end": 2289.92, "text": " Yeah, absolutely.", "tokens": [865, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 604, "seek": 227382, "start": 2289.92, "end": 2290.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 605, "seek": 227382, "start": 2290.92, "end": 2294.1600000000003, "text": " That would be a really good thing for using Elm program test for.", "tokens": [663, 576, 312, 257, 534, 665, 551, 337, 1228, 2699, 76, 1461, 1500, 337, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 606, "seek": 227382, "start": 2294.1600000000003, "end": 2296.6400000000003, "text": " Yeah, that would actually work quite well.", "tokens": [865, 11, 300, 576, 767, 589, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 607, "seek": 227382, "start": 2296.6400000000003, "end": 2297.6400000000003, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 608, "seek": 227382, "start": 2297.6400000000003, "end": 2302.78, "text": " So we talked about sending this low level form data with Lambdaera and I was pretty", "tokens": [407, 321, 2825, 466, 7750, 341, 2295, 1496, 1254, 1412, 365, 45691, 1663, 293, 286, 390, 1238], "temperature": 0.0, "avg_logprob": -0.2817902640690879, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.2678555094680632e-06}, {"id": 609, "seek": 230278, "start": 2302.78, "end": 2305.2400000000002, "text": " excited with how nicely this worked.", "tokens": [2919, 365, 577, 9594, 341, 2732, 13], "temperature": 0.0, "avg_logprob": -0.23105894724527995, "compression_ratio": 1.8007380073800738, "no_speech_prob": 1.994651256609359e-06}, {"id": 610, "seek": 230278, "start": 2305.2400000000002, "end": 2309.6400000000003, "text": " And again, if you wanted to, you could even take it a step further and have these app", "tokens": [400, 797, 11, 498, 291, 1415, 281, 11, 291, 727, 754, 747, 309, 257, 1823, 3052, 293, 362, 613, 724], "temperature": 0.0, "avg_logprob": -0.23105894724527995, "compression_ratio": 1.8007380073800738, "no_speech_prob": 1.994651256609359e-06}, {"id": 611, "seek": 230278, "start": 2309.6400000000003, "end": 2315.48, "text": " messages where the form messages don't need to be wired in for every single page, right?", "tokens": [7897, 689, 264, 1254, 7897, 500, 380, 643, 281, 312, 27415, 294, 337, 633, 2167, 3028, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23105894724527995, "compression_ratio": 1.8007380073800738, "no_speech_prob": 1.994651256609359e-06}, {"id": 612, "seek": 230278, "start": 2315.48, "end": 2321.84, "text": " You could have a single application wide message for your form messages and you just render", "tokens": [509, 727, 362, 257, 2167, 3861, 4874, 3636, 337, 428, 1254, 7897, 293, 291, 445, 15529], "temperature": 0.0, "avg_logprob": -0.23105894724527995, "compression_ratio": 1.8007380073800738, "no_speech_prob": 1.994651256609359e-06}, {"id": 613, "seek": 230278, "start": 2321.84, "end": 2326.52, "text": " your, so you could get that same magic that Elm pages sort of bakes in.", "tokens": [428, 11, 370, 291, 727, 483, 300, 912, 5585, 300, 2699, 76, 7183, 1333, 295, 272, 3419, 294, 13], "temperature": 0.0, "avg_logprob": -0.23105894724527995, "compression_ratio": 1.8007380073800738, "no_speech_prob": 1.994651256609359e-06}, {"id": 614, "seek": 230278, "start": 2326.52, "end": 2331.1600000000003, "text": " You could bake that into your own framework or there could be a Lambdaera specific framework", "tokens": [509, 727, 16562, 300, 666, 428, 1065, 8388, 420, 456, 727, 312, 257, 45691, 1663, 2685, 8388], "temperature": 0.0, "avg_logprob": -0.23105894724527995, "compression_ratio": 1.8007380073800738, "no_speech_prob": 1.994651256609359e-06}, {"id": 615, "seek": 230278, "start": 2331.1600000000003, "end": 2332.1600000000003, "text": " that bakes that in.", "tokens": [300, 272, 3419, 300, 294, 13], "temperature": 0.0, "avg_logprob": -0.23105894724527995, "compression_ratio": 1.8007380073800738, "no_speech_prob": 1.994651256609359e-06}, {"id": 616, "seek": 233216, "start": 2332.16, "end": 2337.72, "text": " So there's all sorts of opportunities for sort of getting that magic there, which is,", "tokens": [407, 456, 311, 439, 7527, 295, 4786, 337, 1333, 295, 1242, 300, 5585, 456, 11, 597, 307, 11], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 617, "seek": 233216, "start": 2337.72, "end": 2338.72, "text": " which is pretty cool.", "tokens": [597, 307, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 618, "seek": 233216, "start": 2338.72, "end": 2343.92, "text": " But yeah, I was pretty excited with how this, this pattern still works with Lambdaera.", "tokens": [583, 1338, 11, 286, 390, 1238, 2919, 365, 577, 341, 11, 341, 5102, 920, 1985, 365, 45691, 1663, 13], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 619, "seek": 233216, "start": 2343.92, "end": 2350.2999999999997, "text": " And to me, this, this takes this like Lambdaera philosophy of like removing glue and removes", "tokens": [400, 281, 385, 11, 341, 11, 341, 2516, 341, 411, 45691, 1663, 10675, 295, 411, 12720, 8998, 293, 30445], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 620, "seek": 233216, "start": 2350.2999999999997, "end": 2351.48, "text": " a big piece, right?", "tokens": [257, 955, 2522, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 621, "seek": 233216, "start": 2351.48, "end": 2355.08, "text": " Cause you're like, wow, I can build an app with no glue.", "tokens": [10865, 291, 434, 411, 11, 6076, 11, 286, 393, 1322, 364, 724, 365, 572, 8998, 13], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 622, "seek": 233216, "start": 2355.08, "end": 2356.08, "text": " That's amazing.", "tokens": [663, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 623, "seek": 233216, "start": 2356.08, "end": 2360.2799999999997, "text": " So you're saying I can like send data to the backend with no glue and receive it from the", "tokens": [407, 291, 434, 1566, 286, 393, 411, 2845, 1412, 281, 264, 38087, 365, 572, 8998, 293, 4774, 309, 490, 264], "temperature": 0.0, "avg_logprob": -0.22651731872558595, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.6796944919406087e-06}, {"id": 624, "seek": 236028, "start": 2360.28, "end": 2362.44, "text": " backend with no glue and it's all real time data.", "tokens": [38087, 365, 572, 8998, 293, 309, 311, 439, 957, 565, 1412, 13], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 625, "seek": 236028, "start": 2362.44, "end": 2363.44, "text": " That's amazing.", "tokens": [663, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 626, "seek": 236028, "start": 2363.44, "end": 2365.1600000000003, "text": " It's like, yeah, no, no glue.", "tokens": [467, 311, 411, 11, 1338, 11, 572, 11, 572, 8998, 13], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 627, "seek": 236028, "start": 2365.1600000000003, "end": 2366.1600000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 628, "seek": 236028, "start": 2366.1600000000003, "end": 2370.76, "text": " Well, what if I want to like send up some data, like input some data and send it up?", "tokens": [1042, 11, 437, 498, 286, 528, 281, 411, 2845, 493, 512, 1412, 11, 411, 4846, 512, 1412, 293, 2845, 309, 493, 30], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 629, "seek": 236028, "start": 2370.76, "end": 2374.96, "text": " It's like, okay, you write a message and you, it's like, oh, okay.", "tokens": [467, 311, 411, 11, 1392, 11, 291, 2464, 257, 3636, 293, 291, 11, 309, 311, 411, 11, 1954, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 630, "seek": 236028, "start": 2374.96, "end": 2378.4, "text": " That's, that sounds like a lot of, a lot of glue.", "tokens": [663, 311, 11, 300, 3263, 411, 257, 688, 295, 11, 257, 688, 295, 8998, 13], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 631, "seek": 236028, "start": 2378.4, "end": 2383.6800000000003, "text": " So I feel like, I feel that this removes a last remaining piece of Lambdaera glue if", "tokens": [407, 286, 841, 411, 11, 286, 841, 300, 341, 30445, 257, 1036, 8877, 2522, 295, 45691, 1663, 8998, 498], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 632, "seek": 236028, "start": 2383.6800000000003, "end": 2385.8, "text": " you, if you use this pattern in Lambdaera.", "tokens": [291, 11, 498, 291, 764, 341, 5102, 294, 45691, 1663, 13], "temperature": 0.0, "avg_logprob": -0.22988979311755103, "compression_ratio": 1.7883817427385893, "no_speech_prob": 5.626374672829115e-07}, {"id": 633, "seek": 238580, "start": 2385.8, "end": 2390.4, "text": " You're removing glue while keeping things working.", "tokens": [509, 434, 12720, 8998, 1339, 5145, 721, 1364, 13], "temperature": 0.0, "avg_logprob": -0.2808359482709099, "compression_ratio": 1.5648148148148149, "no_speech_prob": 2.94790288535296e-06}, {"id": 634, "seek": 238580, "start": 2390.4, "end": 2391.96, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2808359482709099, "compression_ratio": 1.5648148148148149, "no_speech_prob": 2.94790288535296e-06}, {"id": 635, "seek": 238580, "start": 2391.96, "end": 2393.5600000000004, "text": " In a way that is predictable.", "tokens": [682, 257, 636, 300, 307, 27737, 13], "temperature": 0.0, "avg_logprob": -0.2808359482709099, "compression_ratio": 1.5648148148148149, "no_speech_prob": 2.94790288535296e-06}, {"id": 636, "seek": 238580, "start": 2393.5600000000004, "end": 2394.5600000000004, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2808359482709099, "compression_ratio": 1.5648148148148149, "no_speech_prob": 2.94790288535296e-06}, {"id": 637, "seek": 238580, "start": 2394.5600000000004, "end": 2398.92, "text": " Now this gets to an interesting question in the case of Lambdaera.", "tokens": [823, 341, 2170, 281, 364, 1880, 1168, 294, 264, 1389, 295, 45691, 1663, 13], "temperature": 0.0, "avg_logprob": -0.2808359482709099, "compression_ratio": 1.5648148148148149, "no_speech_prob": 2.94790288535296e-06}, {"id": 638, "seek": 238580, "start": 2398.92, "end": 2404.7200000000003, "text": " So, you know, Lambdaera also has this really interesting guarantee that you can migrate", "tokens": [407, 11, 291, 458, 11, 45691, 1663, 611, 575, 341, 534, 1880, 10815, 300, 291, 393, 31821], "temperature": 0.0, "avg_logprob": -0.2808359482709099, "compression_ratio": 1.5648148148148149, "no_speech_prob": 2.94790288535296e-06}, {"id": 639, "seek": 238580, "start": 2404.7200000000003, "end": 2411.4, "text": " data and it's like a guaranteed safe migration because you have to account for all of your", "tokens": [1412, 293, 309, 311, 411, 257, 18031, 3273, 17011, 570, 291, 362, 281, 2696, 337, 439, 295, 428], "temperature": 0.0, "avg_logprob": -0.2808359482709099, "compression_ratio": 1.5648148148148149, "no_speech_prob": 2.94790288535296e-06}, {"id": 640, "seek": 241140, "start": 2411.4, "end": 2418.7200000000003, "text": " data and how you, even how you would deal with messages that are coming into the server", "tokens": [1412, 293, 577, 291, 11, 754, 577, 291, 576, 2028, 365, 7897, 300, 366, 1348, 666, 264, 7154], "temperature": 0.0, "avg_logprob": -0.26484092712402346, "compression_ratio": 1.5837837837837838, "no_speech_prob": 3.9896556813801e-07}, {"id": 641, "seek": 241140, "start": 2418.7200000000003, "end": 2420.4, "text": " from an old version.", "tokens": [490, 364, 1331, 3037, 13], "temperature": 0.0, "avg_logprob": -0.26484092712402346, "compression_ratio": 1.5837837837837838, "no_speech_prob": 3.9896556813801e-07}, {"id": 642, "seek": 241140, "start": 2420.4, "end": 2421.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.26484092712402346, "compression_ratio": 1.5837837837837838, "no_speech_prob": 3.9896556813801e-07}, {"id": 643, "seek": 241140, "start": 2421.4, "end": 2427.88, "text": " So when you say migration or migrating messages and models, it's when a old client talks to", "tokens": [407, 562, 291, 584, 17011, 420, 6186, 8754, 7897, 293, 5245, 11, 309, 311, 562, 257, 1331, 6423, 6686, 281], "temperature": 0.0, "avg_logprob": -0.26484092712402346, "compression_ratio": 1.5837837837837838, "no_speech_prob": 3.9896556813801e-07}, {"id": 644, "seek": 241140, "start": 2427.88, "end": 2435.7200000000003, "text": " a new backend or the other way around or in all those directions that are possible, it", "tokens": [257, 777, 38087, 420, 264, 661, 636, 926, 420, 294, 439, 729, 11095, 300, 366, 1944, 11, 309], "temperature": 0.0, "avg_logprob": -0.26484092712402346, "compression_ratio": 1.5837837837837838, "no_speech_prob": 3.9896556813801e-07}, {"id": 645, "seek": 243572, "start": 2435.72, "end": 2441.56, "text": " migrates things in a way that they can communicate safely.", "tokens": [6186, 12507, 721, 294, 257, 636, 300, 436, 393, 7890, 11750, 13], "temperature": 0.0, "avg_logprob": -0.2406605493454706, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.1363556445285212e-06}, {"id": 646, "seek": 243572, "start": 2441.56, "end": 2442.56, "text": " Right exactly.", "tokens": [1779, 2293, 13], "temperature": 0.0, "avg_logprob": -0.2406605493454706, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.1363556445285212e-06}, {"id": 647, "seek": 243572, "start": 2442.56, "end": 2447.52, "text": " And you can migrate your front end model and to new versions of the app and all this.", "tokens": [400, 291, 393, 31821, 428, 1868, 917, 2316, 293, 281, 777, 9606, 295, 264, 724, 293, 439, 341, 13], "temperature": 0.0, "avg_logprob": -0.2406605493454706, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.1363556445285212e-06}, {"id": 648, "seek": 243572, "start": 2447.52, "end": 2448.52, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2406605493454706, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.1363556445285212e-06}, {"id": 649, "seek": 243572, "start": 2448.52, "end": 2451.9199999999996, "text": " So that's one of the big value propositions of Lambdaera.", "tokens": [407, 300, 311, 472, 295, 264, 955, 2158, 7532, 2451, 295, 45691, 1663, 13], "temperature": 0.0, "avg_logprob": -0.2406605493454706, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.1363556445285212e-06}, {"id": 650, "seek": 243572, "start": 2451.9199999999996, "end": 2457.9599999999996, "text": " So then you take this form data, which maybe let's say before it was more structured form", "tokens": [407, 550, 291, 747, 341, 1254, 1412, 11, 597, 1310, 718, 311, 584, 949, 309, 390, 544, 18519, 1254], "temperature": 0.0, "avg_logprob": -0.2406605493454706, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.1363556445285212e-06}, {"id": 651, "seek": 243572, "start": 2457.9599999999996, "end": 2464.24, "text": " data, you had a check-in date was in your front end model and your checkout date and", "tokens": [1412, 11, 291, 632, 257, 1520, 12, 259, 4002, 390, 294, 428, 1868, 917, 2316, 293, 428, 37153, 4002, 293], "temperature": 0.0, "avg_logprob": -0.2406605493454706, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.1363556445285212e-06}, {"id": 652, "seek": 246424, "start": 2464.24, "end": 2466.2, "text": " your first name and last name.", "tokens": [428, 700, 1315, 293, 1036, 1315, 13], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 653, "seek": 246424, "start": 2466.2, "end": 2473.8799999999997, "text": " And now, and you had a send to backend, which took that and you had a specific message for", "tokens": [400, 586, 11, 293, 291, 632, 257, 2845, 281, 38087, 11, 597, 1890, 300, 293, 291, 632, 257, 2685, 3636, 337], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 654, "seek": 246424, "start": 2473.8799999999997, "end": 2474.8799999999997, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 655, "seek": 246424, "start": 2474.8799999999997, "end": 2480.04, "text": " And now you lose the migration of all of that type safe data that knew exactly what the", "tokens": [400, 586, 291, 3624, 264, 17011, 295, 439, 295, 300, 2010, 3273, 1412, 300, 2586, 2293, 437, 264], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 656, "seek": 246424, "start": 2480.04, "end": 2482.9599999999996, "text": " shape is and what you had in the front end and all of that.", "tokens": [3909, 307, 293, 437, 291, 632, 294, 264, 1868, 917, 293, 439, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 657, "seek": 246424, "start": 2482.9599999999996, "end": 2483.9599999999996, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 658, "seek": 246424, "start": 2483.9599999999996, "end": 2486.7599999999998, "text": " So this is something I've been thinking about a little bit.", "tokens": [407, 341, 307, 746, 286, 600, 668, 1953, 466, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 659, "seek": 246424, "start": 2486.7599999999998, "end": 2493.24, "text": " I think that let's say that you, let's say that you add a new field and it's a required", "tokens": [286, 519, 300, 718, 311, 584, 300, 291, 11, 718, 311, 584, 300, 291, 909, 257, 777, 2519, 293, 309, 311, 257, 4739], "temperature": 0.0, "avg_logprob": -0.24175403498801865, "compression_ratio": 1.7916666666666667, "no_speech_prob": 5.862608759343857e-06}, {"id": 660, "seek": 249324, "start": 2493.24, "end": 2494.24, "text": " field.", "tokens": [2519, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 661, "seek": 249324, "start": 2494.24, "end": 2500.16, "text": " Now you are going to, if the user's on that page, now suddenly there's a new field, but", "tokens": [823, 291, 366, 516, 281, 11, 498, 264, 4195, 311, 322, 300, 3028, 11, 586, 5800, 456, 311, 257, 777, 2519, 11, 457], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 662, "seek": 249324, "start": 2500.16, "end": 2505.3199999999997, "text": " there's also going to be a client side validation that says you're missing a required field.", "tokens": [456, 311, 611, 516, 281, 312, 257, 6423, 1252, 24071, 300, 1619, 291, 434, 5361, 257, 4739, 2519, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 663, "seek": 249324, "start": 2505.3199999999997, "end": 2511.6, "text": " Wait, just so we're clear, the client is on the old version or the new version with a", "tokens": [3802, 11, 445, 370, 321, 434, 1850, 11, 264, 6423, 307, 322, 264, 1331, 3037, 420, 264, 777, 3037, 365, 257], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 664, "seek": 249324, "start": 2511.6, "end": 2512.6, "text": " new field?", "tokens": [777, 2519, 30], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 665, "seek": 249324, "start": 2512.6, "end": 2515.2799999999997, "text": " Well, if the client migrates to the new version.", "tokens": [1042, 11, 498, 264, 6423, 6186, 12507, 281, 264, 777, 3037, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 666, "seek": 249324, "start": 2515.2799999999997, "end": 2516.2799999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 667, "seek": 249324, "start": 2516.2799999999997, "end": 2519.16, "text": " I hope I'm understanding how it works correctly.", "tokens": [286, 1454, 286, 478, 3701, 577, 309, 1985, 8944, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 668, "seek": 249324, "start": 2519.16, "end": 2520.64, "text": " Maybe it does it not migrate.", "tokens": [2704, 309, 775, 309, 406, 31821, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 669, "seek": 249324, "start": 2520.64, "end": 2522.08, "text": " I think it migrates.", "tokens": [286, 519, 309, 6186, 12507, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 670, "seek": 249324, "start": 2522.08, "end": 2523.08, "text": " I could be wrong on that.", "tokens": [286, 727, 312, 2085, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.24953788869521198, "compression_ratio": 1.768060836501901, "no_speech_prob": 2.4438800210191403e-06}, {"id": 671, "seek": 252308, "start": 2523.08, "end": 2531.0, "text": " I have to check, but whatever it does, the source of truth is the form parser and the", "tokens": [286, 362, 281, 1520, 11, 457, 2035, 309, 775, 11, 264, 4009, 295, 3494, 307, 264, 1254, 21156, 260, 293, 264], "temperature": 0.0, "avg_logprob": -0.24020438498639046, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.804971010547888e-07}, {"id": 672, "seek": 252308, "start": 2531.0, "end": 2537.2799999999997, "text": " backend checks that source of truth and the backend is able to give validation errors", "tokens": [38087, 13834, 300, 4009, 295, 3494, 293, 264, 38087, 307, 1075, 281, 976, 24071, 13603], "temperature": 0.0, "avg_logprob": -0.24020438498639046, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.804971010547888e-07}, {"id": 673, "seek": 252308, "start": 2537.2799999999997, "end": 2538.2799999999997, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.24020438498639046, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.804971010547888e-07}, {"id": 674, "seek": 252308, "start": 2538.2799999999997, "end": 2541.0, "text": " So, I think it lines up.", "tokens": [407, 11, 286, 519, 309, 3876, 493, 13], "temperature": 0.0, "avg_logprob": -0.24020438498639046, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.804971010547888e-07}, {"id": 675, "seek": 252308, "start": 2541.0, "end": 2547.4, "text": " I might be missing a detail about how this works, but I believe it's the paradigm still", "tokens": [286, 1062, 312, 5361, 257, 2607, 466, 577, 341, 1985, 11, 457, 286, 1697, 309, 311, 264, 24709, 920], "temperature": 0.0, "avg_logprob": -0.24020438498639046, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.804971010547888e-07}, {"id": 676, "seek": 252308, "start": 2547.4, "end": 2552.4, "text": " works out for at least for cases where you're adding a field.", "tokens": [1985, 484, 337, 412, 1935, 337, 3331, 689, 291, 434, 5127, 257, 2519, 13], "temperature": 0.0, "avg_logprob": -0.24020438498639046, "compression_ratio": 1.6745283018867925, "no_speech_prob": 5.804971010547888e-07}, {"id": 677, "seek": 255240, "start": 2552.4, "end": 2556.76, "text": " If you're removing a field, of course, it's all kind of messy.", "tokens": [759, 291, 434, 12720, 257, 2519, 11, 295, 1164, 11, 309, 311, 439, 733, 295, 16191, 13], "temperature": 0.0, "avg_logprob": -0.3124494106970101, "compression_ratio": 1.6869565217391305, "no_speech_prob": 5.714994699701492e-07}, {"id": 678, "seek": 255240, "start": 2556.76, "end": 2564.12, "text": " There's no simple, neat way to migrate a form as somebody is entering data into a form.", "tokens": [821, 311, 572, 2199, 11, 10654, 636, 281, 31821, 257, 1254, 382, 2618, 307, 11104, 1412, 666, 257, 1254, 13], "temperature": 0.0, "avg_logprob": -0.3124494106970101, "compression_ratio": 1.6869565217391305, "no_speech_prob": 5.714994699701492e-07}, {"id": 679, "seek": 255240, "start": 2564.12, "end": 2570.36, "text": " I think with the Lemdara, if you have a frontend in V1 and it sends a message to your backend", "tokens": [286, 519, 365, 264, 16905, 67, 2419, 11, 498, 291, 362, 257, 1868, 521, 294, 691, 16, 293, 309, 14790, 257, 3636, 281, 428, 38087], "temperature": 0.0, "avg_logprob": -0.3124494106970101, "compression_ratio": 1.6869565217391305, "no_speech_prob": 5.714994699701492e-07}, {"id": 680, "seek": 255240, "start": 2570.36, "end": 2576.88, "text": " in V2, then that message will be, that sent to backend message will be migrated.", "tokens": [294, 691, 17, 11, 550, 300, 3636, 486, 312, 11, 300, 2279, 281, 38087, 3636, 486, 312, 48329, 13], "temperature": 0.0, "avg_logprob": -0.3124494106970101, "compression_ratio": 1.6869565217391305, "no_speech_prob": 5.714994699701492e-07}, {"id": 681, "seek": 255240, "start": 2576.88, "end": 2581.04, "text": " Because your data is low level, you will not have a migration.", "tokens": [1436, 428, 1412, 307, 2295, 1496, 11, 291, 486, 406, 362, 257, 17011, 13], "temperature": 0.0, "avg_logprob": -0.3124494106970101, "compression_ratio": 1.6869565217391305, "no_speech_prob": 5.714994699701492e-07}, {"id": 682, "seek": 258104, "start": 2581.04, "end": 2586.7599999999998, "text": " Although I'm guessing you could write a migration, even when the types don't change.", "tokens": [5780, 286, 478, 17939, 291, 727, 2464, 257, 17011, 11, 754, 562, 264, 3467, 500, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 683, "seek": 258104, "start": 2586.7599999999998, "end": 2592.48, "text": " If you wanted to create an introduce a new variance name, just to be explicit about that.", "tokens": [759, 291, 1415, 281, 1884, 364, 5366, 257, 777, 1374, 952, 384, 1315, 11, 445, 281, 312, 13691, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 684, "seek": 258104, "start": 2592.48, "end": 2593.92, "text": " Yeah, you absolutely could.", "tokens": [865, 11, 291, 3122, 727, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 685, "seek": 258104, "start": 2593.92, "end": 2594.92, "text": " That's a good point.", "tokens": [663, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 686, "seek": 258104, "start": 2594.92, "end": 2596.52, "text": " If you wanted to, you still could.", "tokens": [759, 291, 1415, 281, 11, 291, 920, 727, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 687, "seek": 258104, "start": 2596.52, "end": 2599.92, "text": " But in this case, you would probably say, hey, you're missing this required field.", "tokens": [583, 294, 341, 1389, 11, 291, 576, 1391, 584, 11, 4177, 11, 291, 434, 5361, 341, 4739, 2519, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 688, "seek": 258104, "start": 2599.92, "end": 2604.02, "text": " Just refresh the page because you can't migrate.", "tokens": [1449, 15134, 264, 3028, 570, 291, 393, 380, 31821, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 689, "seek": 258104, "start": 2604.02, "end": 2607.04, "text": " You can't add missing data to form, right?", "tokens": [509, 393, 380, 909, 5361, 1412, 281, 1254, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 690, "seek": 258104, "start": 2607.04, "end": 2608.04, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 691, "seek": 258104, "start": 2608.04, "end": 2609.04, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 692, "seek": 258104, "start": 2609.04, "end": 2610.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28736331646259017, "compression_ratio": 1.6366906474820144, "no_speech_prob": 3.205811651696422e-07}, {"id": 693, "seek": 261004, "start": 2610.04, "end": 2614.7599999999998, "text": " And I believe it, looking at the Lemdara docs, I think it does migrate the frontend model", "tokens": [400, 286, 1697, 309, 11, 1237, 412, 264, 16905, 67, 2419, 45623, 11, 286, 519, 309, 775, 31821, 264, 1868, 521, 2316], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 694, "seek": 261004, "start": 2614.7599999999998, "end": 2615.7599999999998, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 695, "seek": 261004, "start": 2615.7599999999998, "end": 2622.36, "text": " So you're going to get an update where it's showing you the new form fields.", "tokens": [407, 291, 434, 516, 281, 483, 364, 5623, 689, 309, 311, 4099, 291, 264, 777, 1254, 7909, 13], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 696, "seek": 261004, "start": 2622.36, "end": 2623.36, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 697, "seek": 261004, "start": 2623.36, "end": 2628.68, "text": " Then it's mostly an issue about when do you get the update and when is the, how does the", "tokens": [1396, 309, 311, 5240, 364, 2734, 466, 562, 360, 291, 483, 264, 5623, 293, 562, 307, 264, 11, 577, 775, 264], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 698, "seek": 261004, "start": 2628.68, "end": 2630.0, "text": " deployment work?", "tokens": [19317, 589, 30], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 699, "seek": 261004, "start": 2630.0, "end": 2631.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 700, "seek": 261004, "start": 2631.0, "end": 2638.56, "text": " But at the end of the day, I think it's still like, so my bottom line is I feel like you", "tokens": [583, 412, 264, 917, 295, 264, 786, 11, 286, 519, 309, 311, 920, 411, 11, 370, 452, 2767, 1622, 307, 286, 841, 411, 291], "temperature": 0.0, "avg_logprob": -0.30369686025433834, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.059300284912752e-07}, {"id": 701, "seek": 263856, "start": 2638.56, "end": 2644.2, "text": " don't lose a whole lot by using the lower level form data to send it to the backend", "tokens": [500, 380, 3624, 257, 1379, 688, 538, 1228, 264, 3126, 1496, 1254, 1412, 281, 2845, 309, 281, 264, 38087], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 702, "seek": 263856, "start": 2644.2, "end": 2651.0, "text": " because at the end of the day, you're going to need to handle the possibility of an invalid", "tokens": [570, 412, 264, 917, 295, 264, 786, 11, 291, 434, 516, 281, 643, 281, 4813, 264, 7959, 295, 364, 34702], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 703, "seek": 263856, "start": 2651.0, "end": 2654.2, "text": " form being sent to the backend.", "tokens": [1254, 885, 2279, 281, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 704, "seek": 263856, "start": 2654.2, "end": 2657.7599999999998, "text": " There's no avoiding that because clients can send anything.", "tokens": [821, 311, 572, 20220, 300, 570, 6982, 393, 2845, 1340, 13], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 705, "seek": 263856, "start": 2657.7599999999998, "end": 2662.44, "text": " Even if it's only supposed to send through Lemdara, you can still hack it and send bytes", "tokens": [2754, 498, 309, 311, 787, 3442, 281, 2845, 807, 16905, 67, 2419, 11, 291, 393, 920, 10339, 309, 293, 2845, 36088], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 706, "seek": 263856, "start": 2662.44, "end": 2663.44, "text": " over the wire.", "tokens": [670, 264, 6234, 13], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 707, "seek": 263856, "start": 2663.44, "end": 2665.44, "text": " It's like, that's just how servers work.", "tokens": [467, 311, 411, 11, 300, 311, 445, 577, 15909, 589, 13], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 708, "seek": 263856, "start": 2665.44, "end": 2668.04, "text": " You can send data to them.", "tokens": [509, 393, 2845, 1412, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.20332202264818094, "compression_ratio": 1.6884615384615385, "no_speech_prob": 5.53913423573249e-07}, {"id": 709, "seek": 266804, "start": 2668.04, "end": 2674.2, "text": " And so I think that this approach works really well where you're just saying the source of", "tokens": [400, 370, 286, 519, 300, 341, 3109, 1985, 534, 731, 689, 291, 434, 445, 1566, 264, 4009, 295], "temperature": 0.0, "avg_logprob": -0.24673866342615197, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.4593658761441475e-06}, {"id": 710, "seek": 266804, "start": 2674.2, "end": 2680.32, "text": " truth, the gatekeeper is the form parser and you can code share that to show the client", "tokens": [3494, 11, 264, 8539, 23083, 307, 264, 1254, 21156, 260, 293, 291, 393, 3089, 2073, 300, 281, 855, 264, 6423], "temperature": 0.0, "avg_logprob": -0.24673866342615197, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.4593658761441475e-06}, {"id": 711, "seek": 266804, "start": 2680.32, "end": 2686.72, "text": " side validations and turn it into structured data on the backend or get validations, which", "tokens": [1252, 7363, 763, 293, 1261, 309, 666, 18519, 1412, 322, 264, 38087, 420, 483, 7363, 763, 11, 597], "temperature": 0.0, "avg_logprob": -0.24673866342615197, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.4593658761441475e-06}, {"id": 712, "seek": 266804, "start": 2686.72, "end": 2690.8, "text": " we can send back to the client and say, Hey, you must have bypassed something, but you", "tokens": [321, 393, 2845, 646, 281, 264, 6423, 293, 584, 11, 1911, 11, 291, 1633, 362, 24996, 292, 746, 11, 457, 291], "temperature": 0.0, "avg_logprob": -0.24673866342615197, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.4593658761441475e-06}, {"id": 713, "seek": 266804, "start": 2690.8, "end": 2692.8, "text": " gave me something invalid, right?", "tokens": [2729, 385, 746, 34702, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24673866342615197, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.4593658761441475e-06}, {"id": 714, "seek": 266804, "start": 2692.8, "end": 2696.64, "text": " Or you can also do backend specific validations.", "tokens": [1610, 291, 393, 611, 360, 38087, 2685, 7363, 763, 13], "temperature": 0.0, "avg_logprob": -0.24673866342615197, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.4593658761441475e-06}, {"id": 715, "seek": 269664, "start": 2696.64, "end": 2699.52, "text": " That's also possible with this design.", "tokens": [663, 311, 611, 1944, 365, 341, 1715, 13], "temperature": 0.0, "avg_logprob": -0.19161395436709688, "compression_ratio": 1.691304347826087, "no_speech_prob": 4.6644427698083746e-07}, {"id": 716, "seek": 269664, "start": 2699.52, "end": 2704.44, "text": " But I think it's a sound approach because at the end of the day, you still need to handle", "tokens": [583, 286, 519, 309, 311, 257, 1626, 3109, 570, 412, 264, 917, 295, 264, 786, 11, 291, 920, 643, 281, 4813], "temperature": 0.0, "avg_logprob": -0.19161395436709688, "compression_ratio": 1.691304347826087, "no_speech_prob": 4.6644427698083746e-07}, {"id": 717, "seek": 269664, "start": 2704.44, "end": 2708.2799999999997, "text": " the possibility of receiving invalid data on the backend.", "tokens": [264, 7959, 295, 10040, 34702, 1412, 322, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.19161395436709688, "compression_ratio": 1.691304347826087, "no_speech_prob": 4.6644427698083746e-07}, {"id": 718, "seek": 269664, "start": 2708.2799999999997, "end": 2714.2799999999997, "text": " And if there's a migration and suddenly the front end just migrates to something where", "tokens": [400, 498, 456, 311, 257, 17011, 293, 5800, 264, 1868, 917, 445, 6186, 12507, 281, 746, 689], "temperature": 0.0, "avg_logprob": -0.19161395436709688, "compression_ratio": 1.691304347826087, "no_speech_prob": 4.6644427698083746e-07}, {"id": 719, "seek": 269664, "start": 2714.2799999999997, "end": 2717.7599999999998, "text": " now there's a missing required field, you send it to the backend.", "tokens": [586, 456, 311, 257, 5361, 4739, 2519, 11, 291, 2845, 309, 281, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.19161395436709688, "compression_ratio": 1.691304347826087, "no_speech_prob": 4.6644427698083746e-07}, {"id": 720, "seek": 269664, "start": 2717.7599999999998, "end": 2719.96, "text": " It doesn't matter that you didn't do a migration.", "tokens": [467, 1177, 380, 1871, 300, 291, 994, 380, 360, 257, 17011, 13], "temperature": 0.0, "avg_logprob": -0.19161395436709688, "compression_ratio": 1.691304347826087, "no_speech_prob": 4.6644427698083746e-07}, {"id": 721, "seek": 271996, "start": 2719.96, "end": 2727.56, "text": " The new version of the code will handle that validation and then you'll also see that validation", "tokens": [440, 777, 3037, 295, 264, 3089, 486, 4813, 300, 24071, 293, 550, 291, 603, 611, 536, 300, 24071], "temperature": 0.0, "avg_logprob": -0.22573336330028848, "compression_ratio": 1.7242798353909465, "no_speech_prob": 9.721453579913941e-07}, {"id": 722, "seek": 271996, "start": 2727.56, "end": 2728.56, "text": " error on the front end.", "tokens": [6713, 322, 264, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.22573336330028848, "compression_ratio": 1.7242798353909465, "no_speech_prob": 9.721453579913941e-07}, {"id": 723, "seek": 271996, "start": 2728.56, "end": 2732.68, "text": " So I think it's a sound approach.", "tokens": [407, 286, 519, 309, 311, 257, 1626, 3109, 13], "temperature": 0.0, "avg_logprob": -0.22573336330028848, "compression_ratio": 1.7242798353909465, "no_speech_prob": 9.721453579913941e-07}, {"id": 724, "seek": 271996, "start": 2732.68, "end": 2737.16, "text": " Maybe some Lambdaero nerd will give me a corner case.", "tokens": [2704, 512, 45691, 2032, 23229, 486, 976, 385, 257, 4538, 1389, 13], "temperature": 0.0, "avg_logprob": -0.22573336330028848, "compression_ratio": 1.7242798353909465, "no_speech_prob": 9.721453579913941e-07}, {"id": 725, "seek": 271996, "start": 2737.16, "end": 2742.68, "text": " I would be very interested to hear if there's a corner case where it really isn't as safe,", "tokens": [286, 576, 312, 588, 3102, 281, 1568, 498, 456, 311, 257, 4538, 1389, 689, 309, 534, 1943, 380, 382, 3273, 11], "temperature": 0.0, "avg_logprob": -0.22573336330028848, "compression_ratio": 1.7242798353909465, "no_speech_prob": 9.721453579913941e-07}, {"id": 726, "seek": 271996, "start": 2742.68, "end": 2744.36, "text": " but I think it works pretty well.", "tokens": [457, 286, 519, 309, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.22573336330028848, "compression_ratio": 1.7242798353909465, "no_speech_prob": 9.721453579913941e-07}, {"id": 727, "seek": 271996, "start": 2744.36, "end": 2749.62, "text": " So as far as I can tell, this works pretty well when the backend expects the same low", "tokens": [407, 382, 1400, 382, 286, 393, 980, 11, 341, 1985, 1238, 731, 562, 264, 38087, 33280, 264, 912, 2295], "temperature": 0.0, "avg_logprob": -0.22573336330028848, "compression_ratio": 1.7242798353909465, "no_speech_prob": 9.721453579913941e-07}, {"id": 728, "seek": 274962, "start": 2749.62, "end": 2753.24, "text": " level data that you're using in the front end.", "tokens": [1496, 1412, 300, 291, 434, 1228, 294, 264, 1868, 917, 13], "temperature": 0.0, "avg_logprob": -0.2899797959761186, "compression_ratio": 1.6487603305785123, "no_speech_prob": 3.187539732607547e-06}, {"id": 729, "seek": 274962, "start": 2753.24, "end": 2755.0, "text": " So that works very well on pages.", "tokens": [407, 300, 1985, 588, 731, 322, 7183, 13], "temperature": 0.0, "avg_logprob": -0.2899797959761186, "compression_ratio": 1.6487603305785123, "no_speech_prob": 3.187539732607547e-06}, {"id": 730, "seek": 274962, "start": 2755.0, "end": 2757.2799999999997, "text": " I can work very well in Lambdaero as well.", "tokens": [286, 393, 589, 588, 731, 294, 45691, 2032, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2899797959761186, "compression_ratio": 1.6487603305785123, "no_speech_prob": 3.187539732607547e-06}, {"id": 731, "seek": 274962, "start": 2757.2799999999997, "end": 2762.7599999999998, "text": " How does that work with REST APIs or GraphQL?", "tokens": [1012, 775, 300, 589, 365, 497, 14497, 21445, 420, 21884, 13695, 30], "temperature": 0.0, "avg_logprob": -0.2899797959761186, "compression_ratio": 1.6487603305785123, "no_speech_prob": 3.187539732607547e-06}, {"id": 732, "seek": 274962, "start": 2762.7599999999998, "end": 2767.56, "text": " Do you just keep that low level data in the front end?", "tokens": [1144, 291, 445, 1066, 300, 2295, 1496, 1412, 294, 264, 1868, 917, 30], "temperature": 0.0, "avg_logprob": -0.2899797959761186, "compression_ratio": 1.6487603305785123, "no_speech_prob": 3.187539732607547e-06}, {"id": 733, "seek": 274962, "start": 2767.56, "end": 2773.3599999999997, "text": " And then when you try to submit, you need to parse that and make the REST or GraphQL", "tokens": [400, 550, 562, 291, 853, 281, 10315, 11, 291, 643, 281, 48377, 300, 293, 652, 264, 497, 14497, 420, 21884, 13695], "temperature": 0.0, "avg_logprob": -0.2899797959761186, "compression_ratio": 1.6487603305785123, "no_speech_prob": 3.187539732607547e-06}, {"id": 734, "seek": 274962, "start": 2773.3599999999997, "end": 2778.96, "text": " request, but you don't have that same technique that you use on the backend about getting", "tokens": [5308, 11, 457, 291, 500, 380, 362, 300, 912, 6532, 300, 291, 764, 322, 264, 38087, 466, 1242], "temperature": 0.0, "avg_logprob": -0.2899797959761186, "compression_ratio": 1.6487603305785123, "no_speech_prob": 3.187539732607547e-06}, {"id": 735, "seek": 277896, "start": 2778.96, "end": 2779.96, "text": " low level data.", "tokens": [2295, 1496, 1412, 13], "temperature": 0.0, "avg_logprob": -0.29924577713012696, "compression_ratio": 1.4852320675105486, "no_speech_prob": 4.936903678753879e-06}, {"id": 736, "seek": 277896, "start": 2779.96, "end": 2780.96, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.29924577713012696, "compression_ratio": 1.4852320675105486, "no_speech_prob": 4.936903678753879e-06}, {"id": 737, "seek": 277896, "start": 2780.96, "end": 2782.7200000000003, "text": " Yeah, that's a great question.", "tokens": [865, 11, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.29924577713012696, "compression_ratio": 1.4852320675105486, "no_speech_prob": 4.936903678753879e-06}, {"id": 738, "seek": 277896, "start": 2782.7200000000003, "end": 2788.36, "text": " So and at first when I was extracting this to a standalone API, I wasn't sure if that", "tokens": [407, 293, 412, 700, 562, 286, 390, 49844, 341, 281, 257, 37454, 9362, 11, 286, 2067, 380, 988, 498, 300], "temperature": 0.0, "avg_logprob": -0.29924577713012696, "compression_ratio": 1.4852320675105486, "no_speech_prob": 4.936903678753879e-06}, {"id": 739, "seek": 277896, "start": 2788.36, "end": 2794.84, "text": " would pan out and if the pattern would apply well to a use case where it's front end only", "tokens": [576, 2462, 484, 293, 498, 264, 5102, 576, 3079, 731, 281, 257, 764, 1389, 689, 309, 311, 1868, 917, 787], "temperature": 0.0, "avg_logprob": -0.29924577713012696, "compression_ratio": 1.4852320675105486, "no_speech_prob": 4.936903678753879e-06}, {"id": 740, "seek": 277896, "start": 2794.84, "end": 2799.84, "text": " Elm because we're talking about full stack on pages v3 and Lambdaero Elm.", "tokens": [2699, 76, 570, 321, 434, 1417, 466, 1577, 8630, 322, 7183, 371, 18, 293, 45691, 2032, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.29924577713012696, "compression_ratio": 1.4852320675105486, "no_speech_prob": 4.936903678753879e-06}, {"id": 741, "seek": 277896, "start": 2799.84, "end": 2802.52, "text": " But actually, I think it does work quite nicely.", "tokens": [583, 767, 11, 286, 519, 309, 775, 589, 1596, 9594, 13], "temperature": 0.0, "avg_logprob": -0.29924577713012696, "compression_ratio": 1.4852320675105486, "no_speech_prob": 4.936903678753879e-06}, {"id": 742, "seek": 280252, "start": 2802.52, "end": 2811.2, "text": " And one of the patterns I hinted at earlier is that so in your form definition, you take", "tokens": [400, 472, 295, 264, 8294, 286, 12075, 292, 412, 3071, 307, 300, 370, 294, 428, 1254, 7123, 11, 291, 747], "temperature": 0.0, "avg_logprob": -0.23390934684059836, "compression_ratio": 1.6877828054298643, "no_speech_prob": 7.934324912639568e-07}, {"id": 743, "seek": 280252, "start": 2811.2, "end": 2817.02, "text": " all of your form fields and combine them and you can add additional validations and dependent", "tokens": [439, 295, 428, 1254, 7909, 293, 10432, 552, 293, 291, 393, 909, 4497, 7363, 763, 293, 12334], "temperature": 0.0, "avg_logprob": -0.23390934684059836, "compression_ratio": 1.6877828054298643, "no_speech_prob": 7.934324912639568e-07}, {"id": 744, "seek": 280252, "start": 2817.02, "end": 2819.4, "text": " validations between the form fields.", "tokens": [7363, 763, 1296, 264, 1254, 7909, 13], "temperature": 0.0, "avg_logprob": -0.23390934684059836, "compression_ratio": 1.6877828054298643, "no_speech_prob": 7.934324912639568e-07}, {"id": 745, "seek": 280252, "start": 2819.4, "end": 2826.68, "text": " And then you tell it how to parse that successfully or unsuccessfully into structured data.", "tokens": [400, 550, 291, 980, 309, 577, 281, 48377, 300, 10727, 420, 40501, 2277, 666, 18519, 1412, 13], "temperature": 0.0, "avg_logprob": -0.23390934684059836, "compression_ratio": 1.6877828054298643, "no_speech_prob": 7.934324912639568e-07}, {"id": 746, "seek": 280252, "start": 2826.68, "end": 2831.04, "text": " So you could parse it into a record just like a JSON decoder.", "tokens": [407, 291, 727, 48377, 309, 666, 257, 2136, 445, 411, 257, 31828, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.23390934684059836, "compression_ratio": 1.6877828054298643, "no_speech_prob": 7.934324912639568e-07}, {"id": 747, "seek": 283104, "start": 2831.04, "end": 2835.04, "text": " You can parse into a record, but you could parse it into any data.", "tokens": [509, 393, 48377, 666, 257, 2136, 11, 457, 291, 727, 48377, 309, 666, 604, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21475690439206743, "compression_ratio": 1.8193832599118942, "no_speech_prob": 3.089473466388881e-06}, {"id": 748, "seek": 283104, "start": 2835.04, "end": 2840.64, "text": " And as you're parsing it, you're basically telling it, you know, so the most common pattern", "tokens": [400, 382, 291, 434, 21156, 278, 309, 11, 291, 434, 1936, 3585, 309, 11, 291, 458, 11, 370, 264, 881, 2689, 5102], "temperature": 0.0, "avg_logprob": -0.21475690439206743, "compression_ratio": 1.8193832599118942, "no_speech_prob": 3.089473466388881e-06}, {"id": 749, "seek": 283104, "start": 2840.64, "end": 2846.72, "text": " will have will be to have like a type alias for a record and to use that record constructor,", "tokens": [486, 362, 486, 312, 281, 362, 411, 257, 2010, 419, 4609, 337, 257, 2136, 293, 281, 764, 300, 2136, 47479, 11], "temperature": 0.0, "avg_logprob": -0.21475690439206743, "compression_ratio": 1.8193832599118942, "no_speech_prob": 3.089473466388881e-06}, {"id": 750, "seek": 283104, "start": 2846.72, "end": 2852.68, "text": " you know, just like in a decoder, you say decode dot succeed user, and then you just", "tokens": [291, 458, 11, 445, 411, 294, 257, 979, 19866, 11, 291, 584, 979, 1429, 5893, 7754, 4195, 11, 293, 550, 291, 445], "temperature": 0.0, "avg_logprob": -0.21475690439206743, "compression_ratio": 1.8193832599118942, "no_speech_prob": 3.089473466388881e-06}, {"id": 751, "seek": 283104, "start": 2852.68, "end": 2859.44, "text": " keep adding on these decoders to applicatively add data to that constructor.", "tokens": [1066, 5127, 322, 613, 979, 378, 433, 281, 2580, 19020, 909, 1412, 281, 300, 47479, 13], "temperature": 0.0, "avg_logprob": -0.21475690439206743, "compression_ratio": 1.8193832599118942, "no_speech_prob": 3.089473466388881e-06}, {"id": 752, "seek": 285944, "start": 2859.44, "end": 2863.08, "text": " But that record constructor is just a function.", "tokens": [583, 300, 2136, 47479, 307, 445, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.23461713495942735, "compression_ratio": 1.8113207547169812, "no_speech_prob": 5.896413313166704e-07}, {"id": 753, "seek": 285944, "start": 2863.08, "end": 2869.28, "text": " So you could just as well say a function that takes this value and this value and turns", "tokens": [407, 291, 727, 445, 382, 731, 584, 257, 2445, 300, 2516, 341, 2158, 293, 341, 2158, 293, 4523], "temperature": 0.0, "avg_logprob": -0.23461713495942735, "compression_ratio": 1.8113207547169812, "no_speech_prob": 5.896413313166704e-07}, {"id": 754, "seek": 285944, "start": 2869.28, "end": 2870.44, "text": " it into these values.", "tokens": [309, 666, 613, 4190, 13], "temperature": 0.0, "avg_logprob": -0.23461713495942735, "compression_ratio": 1.8113207547169812, "no_speech_prob": 5.896413313166704e-07}, {"id": 755, "seek": 285944, "start": 2870.44, "end": 2872.92, "text": " And you can put it into a record if you want.", "tokens": [400, 291, 393, 829, 309, 666, 257, 2136, 498, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.23461713495942735, "compression_ratio": 1.8113207547169812, "no_speech_prob": 5.896413313166704e-07}, {"id": 756, "seek": 285944, "start": 2872.92, "end": 2881.2000000000003, "text": " But what if instead of putting it into a record, you created a tuple with a JSON encode object", "tokens": [583, 437, 498, 2602, 295, 3372, 309, 666, 257, 2136, 11, 291, 2942, 257, 2604, 781, 365, 257, 31828, 2058, 1429, 2657], "temperature": 0.0, "avg_logprob": -0.23461713495942735, "compression_ratio": 1.8113207547169812, "no_speech_prob": 5.896413313166704e-07}, {"id": 757, "seek": 285944, "start": 2881.2000000000003, "end": 2888.12, "text": " and the structured data if you wanted some structured data to show in a pending UI to", "tokens": [293, 264, 18519, 1412, 498, 291, 1415, 512, 18519, 1412, 281, 855, 294, 257, 32110, 15682, 281], "temperature": 0.0, "avg_logprob": -0.23461713495942735, "compression_ratio": 1.8113207547169812, "no_speech_prob": 5.896413313166704e-07}, {"id": 758, "seek": 288812, "start": 2888.12, "end": 2891.48, "text": " show this is the item that we're creating right now.", "tokens": [855, 341, 307, 264, 3174, 300, 321, 434, 4084, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.2112757570000105, "compression_ratio": 1.587719298245614, "no_speech_prob": 3.0415837954933522e-06}, {"id": 759, "seek": 288812, "start": 2891.48, "end": 2897.0, "text": " But you can also construct, you know, you could even construct like an API request payload", "tokens": [583, 291, 393, 611, 7690, 11, 291, 458, 11, 291, 727, 754, 7690, 411, 364, 9362, 5308, 30918], "temperature": 0.0, "avg_logprob": -0.2112757570000105, "compression_ratio": 1.587719298245614, "no_speech_prob": 3.0415837954933522e-06}, {"id": 760, "seek": 288812, "start": 2897.0, "end": 2902.96, "text": " thing that maybe there's a custom type for which endpoint you're sending to and maybe", "tokens": [551, 300, 1310, 456, 311, 257, 2375, 2010, 337, 597, 35795, 291, 434, 7750, 281, 293, 1310], "temperature": 0.0, "avg_logprob": -0.2112757570000105, "compression_ratio": 1.587719298245614, "no_speech_prob": 3.0415837954933522e-06}, {"id": 761, "seek": 288812, "start": 2902.96, "end": 2908.3199999999997, "text": " there's a JSON encode value for the payload to send.", "tokens": [456, 311, 257, 31828, 2058, 1429, 2158, 337, 264, 30918, 281, 2845, 13], "temperature": 0.0, "avg_logprob": -0.2112757570000105, "compression_ratio": 1.587719298245614, "no_speech_prob": 3.0415837954933522e-06}, {"id": 762, "seek": 288812, "start": 2908.3199999999997, "end": 2913.48, "text": " Or you could build up, you know, an Elm GraphQL input object or whatever it is.", "tokens": [1610, 291, 727, 1322, 493, 11, 291, 458, 11, 364, 2699, 76, 21884, 13695, 4846, 2657, 420, 2035, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.2112757570000105, "compression_ratio": 1.587719298245614, "no_speech_prob": 3.0415837954933522e-06}, {"id": 763, "seek": 291348, "start": 2913.48, "end": 2920.96, "text": " So like, because you have the data right there, you have all of the fields directly to combine", "tokens": [407, 411, 11, 570, 291, 362, 264, 1412, 558, 456, 11, 291, 362, 439, 295, 264, 7909, 3838, 281, 10432], "temperature": 0.0, "avg_logprob": -0.22993033272879465, "compression_ratio": 1.7358490566037736, "no_speech_prob": 3.237752707718755e-06}, {"id": 764, "seek": 291348, "start": 2920.96, "end": 2923.32, "text": " into whatever data type you want.", "tokens": [666, 2035, 1412, 2010, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.22993033272879465, "compression_ratio": 1.7358490566037736, "no_speech_prob": 3.237752707718755e-06}, {"id": 765, "seek": 291348, "start": 2923.32, "end": 2932.42, "text": " So, so when you render the form, you can say, with on submit, and, and then you can either", "tokens": [407, 11, 370, 562, 291, 15529, 264, 1254, 11, 291, 393, 584, 11, 365, 322, 10315, 11, 293, 11, 293, 550, 291, 393, 2139], "temperature": 0.0, "avg_logprob": -0.22993033272879465, "compression_ratio": 1.7358490566037736, "no_speech_prob": 3.237752707718755e-06}, {"id": 766, "seek": 291348, "start": 2932.42, "end": 2935.16, "text": " get the valid or invalid data.", "tokens": [483, 264, 7363, 420, 34702, 1412, 13], "temperature": 0.0, "avg_logprob": -0.22993033272879465, "compression_ratio": 1.7358490566037736, "no_speech_prob": 3.237752707718755e-06}, {"id": 767, "seek": 291348, "start": 2935.16, "end": 2939.16, "text": " And with the valid data, you have the successfully parsed data type.", "tokens": [400, 365, 264, 7363, 1412, 11, 291, 362, 264, 10727, 21156, 292, 1412, 2010, 13], "temperature": 0.0, "avg_logprob": -0.22993033272879465, "compression_ratio": 1.7358490566037736, "no_speech_prob": 3.237752707718755e-06}, {"id": 768, "seek": 291348, "start": 2939.16, "end": 2942.96, "text": " So we've talked about low level data with forms.", "tokens": [407, 321, 600, 2825, 466, 2295, 1496, 1412, 365, 6422, 13], "temperature": 0.0, "avg_logprob": -0.22993033272879465, "compression_ratio": 1.7358490566037736, "no_speech_prob": 3.237752707718755e-06}, {"id": 769, "seek": 294296, "start": 2942.96, "end": 2946.84, "text": " Do you see any other places where we could use this technique?", "tokens": [1144, 291, 536, 604, 661, 3190, 689, 321, 727, 764, 341, 6532, 30], "temperature": 0.0, "avg_logprob": -0.2953960377237071, "compression_ratio": 1.4537444933920705, "no_speech_prob": 7.811436830706953e-07}, {"id": 770, "seek": 294296, "start": 2946.84, "end": 2954.68, "text": " I'm thinking maybe HTML bytes or I mean, we use it for JSON to some extent, maybe?", "tokens": [286, 478, 1953, 1310, 17995, 36088, 420, 286, 914, 11, 321, 764, 309, 337, 31828, 281, 512, 8396, 11, 1310, 30], "temperature": 0.0, "avg_logprob": -0.2953960377237071, "compression_ratio": 1.4537444933920705, "no_speech_prob": 7.811436830706953e-07}, {"id": 771, "seek": 294296, "start": 2954.68, "end": 2958.12, "text": " No, it's not the same thing, right?", "tokens": [883, 11, 309, 311, 406, 264, 912, 551, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2953960377237071, "compression_ratio": 1.4537444933920705, "no_speech_prob": 7.811436830706953e-07}, {"id": 772, "seek": 294296, "start": 2958.12, "end": 2960.04, "text": " This is a very good question.", "tokens": [639, 307, 257, 588, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2953960377237071, "compression_ratio": 1.4537444933920705, "no_speech_prob": 7.811436830706953e-07}, {"id": 773, "seek": 294296, "start": 2960.04, "end": 2961.52, "text": " I'm trying to think now.", "tokens": [286, 478, 1382, 281, 519, 586, 13], "temperature": 0.0, "avg_logprob": -0.2953960377237071, "compression_ratio": 1.4537444933920705, "no_speech_prob": 7.811436830706953e-07}, {"id": 774, "seek": 294296, "start": 2961.52, "end": 2970.0, "text": " I have noticed as a general principle, with API design, sometimes when I have a type variable", "tokens": [286, 362, 5694, 382, 257, 2674, 8665, 11, 365, 9362, 1715, 11, 2171, 562, 286, 362, 257, 2010, 7006], "temperature": 0.0, "avg_logprob": -0.2953960377237071, "compression_ratio": 1.4537444933920705, "no_speech_prob": 7.811436830706953e-07}, {"id": 775, "seek": 297000, "start": 2970.0, "end": 2976.2, "text": " in an API I'm designing, I'll try to sort of squeeze out the type variable by distilling", "tokens": [294, 364, 9362, 286, 478, 14685, 11, 286, 603, 853, 281, 1333, 295, 13578, 484, 264, 2010, 7006, 538, 1483, 7345], "temperature": 0.0, "avg_logprob": -0.2348907636559528, "compression_ratio": 1.5888888888888888, "no_speech_prob": 9.276307650907256e-07}, {"id": 776, "seek": 297000, "start": 2976.2, "end": 2978.68, "text": " things down to lower level data.", "tokens": [721, 760, 281, 3126, 1496, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2348907636559528, "compression_ratio": 1.5888888888888888, "no_speech_prob": 9.276307650907256e-07}, {"id": 777, "seek": 297000, "start": 2978.68, "end": 2982.32, "text": " So that's sort of like something I look for when I'm designing APIs.", "tokens": [407, 300, 311, 1333, 295, 411, 746, 286, 574, 337, 562, 286, 478, 14685, 21445, 13], "temperature": 0.0, "avg_logprob": -0.2348907636559528, "compression_ratio": 1.5888888888888888, "no_speech_prob": 9.276307650907256e-07}, {"id": 778, "seek": 297000, "start": 2982.32, "end": 2984.8, "text": " But but yeah, that's a good question.", "tokens": [583, 457, 1338, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2348907636559528, "compression_ratio": 1.5888888888888888, "no_speech_prob": 9.276307650907256e-07}, {"id": 779, "seek": 297000, "start": 2984.8, "end": 2989.2, "text": " Like what are the pain points from because I really feel like, you know, I might sound", "tokens": [1743, 437, 366, 264, 1822, 2793, 490, 570, 286, 534, 841, 411, 11, 291, 458, 11, 286, 1062, 1626], "temperature": 0.0, "avg_logprob": -0.2348907636559528, "compression_ratio": 1.5888888888888888, "no_speech_prob": 9.276307650907256e-07}, {"id": 780, "seek": 297000, "start": 2989.2, "end": 2995.32, "text": " like a broken record now, but it just feels like the way we were working with forms in", "tokens": [411, 257, 5463, 2136, 586, 11, 457, 309, 445, 3417, 411, 264, 636, 321, 645, 1364, 365, 6422, 294], "temperature": 0.0, "avg_logprob": -0.2348907636559528, "compression_ratio": 1.5888888888888888, "no_speech_prob": 9.276307650907256e-07}, {"id": 781, "seek": 297000, "start": 2995.32, "end": 2999.68, "text": " Elm before was so tedious.", "tokens": [2699, 76, 949, 390, 370, 38284, 13], "temperature": 0.0, "avg_logprob": -0.2348907636559528, "compression_ratio": 1.5888888888888888, "no_speech_prob": 9.276307650907256e-07}, {"id": 782, "seek": 299968, "start": 2999.68, "end": 3003.68, "text": " And we worked with forms that way for so long.", "tokens": [400, 321, 2732, 365, 6422, 300, 636, 337, 370, 938, 13], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 783, "seek": 299968, "start": 3003.68, "end": 3007.3599999999997, "text": " And it like it wasn't it wasn't good.", "tokens": [400, 309, 411, 309, 2067, 380, 309, 2067, 380, 665, 13], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 784, "seek": 299968, "start": 3007.3599999999997, "end": 3009.3599999999997, "text": " I don't know.", "tokens": [286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 785, "seek": 299968, "start": 3009.3599999999997, "end": 3016.3999999999996, "text": " I mean, we were clueless and now we are glueless.", "tokens": [286, 914, 11, 321, 645, 596, 3483, 442, 293, 586, 321, 366, 1563, 3483, 442, 13], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 786, "seek": 299968, "start": 3016.3999999999996, "end": 3018.8399999999997, "text": " Let's just say it's been a formative experience.", "tokens": [961, 311, 445, 584, 309, 311, 668, 257, 1254, 1166, 1752, 13], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 787, "seek": 299968, "start": 3018.8399999999997, "end": 3020.52, "text": " We needed a reform.", "tokens": [492, 2978, 257, 8290, 13], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 788, "seek": 299968, "start": 3020.52, "end": 3021.52, "text": " We can all agree.", "tokens": [492, 393, 439, 3986, 13], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 789, "seek": 299968, "start": 3021.52, "end": 3028.64, "text": " I almost feel like I have just made one pun and you're like, okay, now I can throw them", "tokens": [286, 1920, 841, 411, 286, 362, 445, 1027, 472, 4468, 293, 291, 434, 411, 11, 1392, 11, 586, 286, 393, 3507, 552], "temperature": 0.0, "avg_logprob": -0.2886300223214286, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.332052190467948e-06}, {"id": 790, "seek": 302864, "start": 3028.64, "end": 3031.7999999999997, "text": " all.", "tokens": [439, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 791, "seek": 302864, "start": 3031.7999999999997, "end": 3037.8799999999997, "text": " Never never look a pun artist directly in the eye.", "tokens": [7344, 1128, 574, 257, 4468, 5748, 3838, 294, 264, 3313, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 792, "seek": 302864, "start": 3037.8799999999997, "end": 3040.68, "text": " Your kids are never going to look you in the eye.", "tokens": [2260, 2301, 366, 1128, 516, 281, 574, 291, 294, 264, 3313, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 793, "seek": 302864, "start": 3040.68, "end": 3043.8799999999997, "text": " It's too dangerous.", "tokens": [467, 311, 886, 5795, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 794, "seek": 302864, "start": 3043.8799999999997, "end": 3044.8799999999997, "text": " It's a great question.", "tokens": [467, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 795, "seek": 302864, "start": 3044.8799999999997, "end": 3048.48, "text": " Can you think of anything that might benefit from low level data?", "tokens": [1664, 291, 519, 295, 1340, 300, 1062, 5121, 490, 2295, 1496, 1412, 30], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 796, "seek": 302864, "start": 3048.48, "end": 3050.7599999999998, "text": " I don't have anything in mind.", "tokens": [286, 500, 380, 362, 1340, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 797, "seek": 302864, "start": 3050.7599999999998, "end": 3051.7599999999998, "text": " No, no.", "tokens": [883, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 798, "seek": 302864, "start": 3051.7599999999998, "end": 3056.3599999999997, "text": " But I'm sure the listener will tell us if they come up with something.", "tokens": [583, 286, 478, 988, 264, 31569, 486, 980, 505, 498, 436, 808, 493, 365, 746, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 799, "seek": 302864, "start": 3056.3599999999997, "end": 3057.3599999999997, "text": " Please do.", "tokens": [2555, 360, 13], "temperature": 0.0, "avg_logprob": -0.3590114817899816, "compression_ratio": 1.543778801843318, "no_speech_prob": 1.52939412600972e-06}, {"id": 800, "seek": 305736, "start": 3057.36, "end": 3060.0, "text": " Actually, there's one use case.", "tokens": [5135, 11, 456, 311, 472, 764, 1389, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 801, "seek": 305736, "start": 3060.0, "end": 3064.04, "text": " Simon Liddell, he made a Elm URL package.", "tokens": [13193, 441, 14273, 898, 11, 415, 1027, 257, 2699, 76, 12905, 7372, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 802, "seek": 305736, "start": 3064.04, "end": 3066.76, "text": " I don't remember the exact name.", "tokens": [286, 500, 380, 1604, 264, 1900, 1315, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 803, "seek": 305736, "start": 3066.76, "end": 3067.76, "text": " Elm app URL.", "tokens": [2699, 76, 724, 12905, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 804, "seek": 305736, "start": 3067.76, "end": 3068.88, "text": " Elm app URL.", "tokens": [2699, 76, 724, 12905, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 805, "seek": 305736, "start": 3068.88, "end": 3072.2400000000002, "text": " And that is basically the URL as a string.", "tokens": [400, 300, 307, 1936, 264, 12905, 382, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 806, "seek": 305736, "start": 3072.2400000000002, "end": 3073.6, "text": " Let's keep it as a string.", "tokens": [961, 311, 1066, 309, 382, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 807, "seek": 305736, "start": 3073.6, "end": 3076.6, "text": " Let's pattern match on it as a string.", "tokens": [961, 311, 5102, 2995, 322, 309, 382, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 808, "seek": 305736, "start": 3076.6, "end": 3079.8, "text": " Or as a list of strings, but still pretty low level.", "tokens": [1610, 382, 257, 1329, 295, 13985, 11, 457, 920, 1238, 2295, 1496, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 809, "seek": 305736, "start": 3079.8, "end": 3086.56, "text": " So yeah, I think that works instead of having a parser with a type and yeah, URL.", "tokens": [407, 1338, 11, 286, 519, 300, 1985, 2602, 295, 1419, 257, 21156, 260, 365, 257, 2010, 293, 1338, 11, 12905, 13], "temperature": 0.0, "avg_logprob": -0.3332036026125032, "compression_ratio": 1.6419213973799127, "no_speech_prob": 3.806974859799084e-07}, {"id": 810, "seek": 308656, "start": 3086.56, "end": 3087.56, "text": " Good point.", "tokens": [2205, 935, 13], "temperature": 0.0, "avg_logprob": -0.2582450503394717, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.2482616966735804e-06}, {"id": 811, "seek": 308656, "start": 3087.56, "end": 3093.6, "text": " Yeah, actually, you know, this technique of defunctionalization too, in a way is a form", "tokens": [865, 11, 767, 11, 291, 458, 11, 341, 6532, 295, 1060, 32627, 304, 2144, 886, 11, 294, 257, 636, 307, 257, 1254], "temperature": 0.0, "avg_logprob": -0.2582450503394717, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.2482616966735804e-06}, {"id": 812, "seek": 308656, "start": 3093.6, "end": 3094.6, "text": " of this.", "tokens": [295, 341, 13], "temperature": 0.0, "avg_logprob": -0.2582450503394717, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.2482616966735804e-06}, {"id": 813, "seek": 308656, "start": 3094.6, "end": 3101.84, "text": " So defunctionalization being instead of having like closures of things to execute, you turn", "tokens": [407, 1060, 32627, 304, 2144, 885, 2602, 295, 1419, 411, 2611, 1303, 295, 721, 281, 14483, 11, 291, 1261], "temperature": 0.0, "avg_logprob": -0.2582450503394717, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.2482616966735804e-06}, {"id": 814, "seek": 308656, "start": 3101.84, "end": 3105.44, "text": " it into data, just like a message is defunctionalization.", "tokens": [309, 666, 1412, 11, 445, 411, 257, 3636, 307, 1060, 32627, 304, 2144, 13], "temperature": 0.0, "avg_logprob": -0.2582450503394717, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.2482616966735804e-06}, {"id": 815, "seek": 308656, "start": 3105.44, "end": 3110.56, "text": " Instead of having like an on click function, you have a message.", "tokens": [7156, 295, 1419, 411, 364, 322, 2052, 2445, 11, 291, 362, 257, 3636, 13], "temperature": 0.0, "avg_logprob": -0.2582450503394717, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.2482616966735804e-06}, {"id": 816, "seek": 308656, "start": 3110.56, "end": 3116.44, "text": " And each and the handling for each message will do a different thing.", "tokens": [400, 1184, 293, 264, 13175, 337, 1184, 3636, 486, 360, 257, 819, 551, 13], "temperature": 0.0, "avg_logprob": -0.2582450503394717, "compression_ratio": 1.8110599078341014, "no_speech_prob": 1.2482616966735804e-06}, {"id": 817, "seek": 311644, "start": 3116.44, "end": 3117.44, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.26127028465270996, "compression_ratio": 1.4619047619047618, "no_speech_prob": 1.1726295952030341e-06}, {"id": 818, "seek": 311644, "start": 3117.44, "end": 3122.88, "text": " And so I think there are certain ways to kind of combine this idea of defunctionalization", "tokens": [400, 370, 286, 519, 456, 366, 1629, 2098, 281, 733, 295, 10432, 341, 1558, 295, 1060, 32627, 304, 2144], "temperature": 0.0, "avg_logprob": -0.26127028465270996, "compression_ratio": 1.4619047619047618, "no_speech_prob": 1.1726295952030341e-06}, {"id": 819, "seek": 311644, "start": 3122.88, "end": 3125.76, "text": " and decoupling to lower level data.", "tokens": [293, 979, 263, 11970, 281, 3126, 1496, 1412, 13], "temperature": 0.0, "avg_logprob": -0.26127028465270996, "compression_ratio": 1.4619047619047618, "no_speech_prob": 1.1726295952030341e-06}, {"id": 820, "seek": 311644, "start": 3125.76, "end": 3129.4, "text": " For example, an HTTP call.", "tokens": [1171, 1365, 11, 364, 33283, 818, 13], "temperature": 0.0, "avg_logprob": -0.26127028465270996, "compression_ratio": 1.4619047619047618, "no_speech_prob": 1.1726295952030341e-06}, {"id": 821, "seek": 311644, "start": 3129.4, "end": 3137.16, "text": " You could so like Mario and I were discussing with the Elm pages back end task API, it doesn't", "tokens": [509, 727, 370, 411, 9343, 293, 286, 645, 10850, 365, 264, 2699, 76, 7183, 646, 917, 5633, 9362, 11, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.26127028465270996, "compression_ratio": 1.4619047619047618, "no_speech_prob": 1.1726295952030341e-06}, {"id": 822, "seek": 311644, "start": 3137.16, "end": 3141.94, "text": " use Elm HTTP, it has its own back end task HTTP API.", "tokens": [764, 2699, 76, 33283, 11, 309, 575, 1080, 1065, 646, 917, 5633, 33283, 9362, 13], "temperature": 0.0, "avg_logprob": -0.26127028465270996, "compression_ratio": 1.4619047619047618, "no_speech_prob": 1.1726295952030341e-06}, {"id": 823, "seek": 314194, "start": 3141.94, "end": 3148.08, "text": " So how could you take a request, but the request is this sort of opaque thing that you can't", "tokens": [407, 577, 727, 291, 747, 257, 5308, 11, 457, 264, 5308, 307, 341, 1333, 295, 42687, 551, 300, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.27634313994762943, "compression_ratio": 1.691304347826087, "no_speech_prob": 5.896412176298327e-07}, {"id": 824, "seek": 314194, "start": 3148.08, "end": 3149.08, "text": " do anything with.", "tokens": [360, 1340, 365, 13], "temperature": 0.0, "avg_logprob": -0.27634313994762943, "compression_ratio": 1.691304347826087, "no_speech_prob": 5.896412176298327e-07}, {"id": 825, "seek": 314194, "start": 3149.08, "end": 3156.88, "text": " So if you extracted that out to its parts and had the request object be just a data", "tokens": [407, 498, 291, 34086, 300, 484, 281, 1080, 3166, 293, 632, 264, 5308, 2657, 312, 445, 257, 1412], "temperature": 0.0, "avg_logprob": -0.27634313994762943, "compression_ratio": 1.691304347826087, "no_speech_prob": 5.896412176298327e-07}, {"id": 826, "seek": 314194, "start": 3156.88, "end": 3160.48, "text": " type, maybe that would make it more shareable.", "tokens": [2010, 11, 1310, 300, 576, 652, 309, 544, 2073, 712, 13], "temperature": 0.0, "avg_logprob": -0.27634313994762943, "compression_ratio": 1.691304347826087, "no_speech_prob": 5.896412176298327e-07}, {"id": 827, "seek": 314194, "start": 3160.48, "end": 3163.2400000000002, "text": " Yeah, because that would be a common denominator.", "tokens": [865, 11, 570, 300, 576, 312, 257, 2689, 20687, 13], "temperature": 0.0, "avg_logprob": -0.27634313994762943, "compression_ratio": 1.691304347826087, "no_speech_prob": 5.896412176298327e-07}, {"id": 828, "seek": 314194, "start": 3163.2400000000002, "end": 3170.0, "text": " Yeah, I'm not sure it's exactly the same principle, though, because like, there's still, it still", "tokens": [865, 11, 286, 478, 406, 988, 309, 311, 2293, 264, 912, 8665, 11, 1673, 11, 570, 411, 11, 456, 311, 920, 11, 309, 920], "temperature": 0.0, "avg_logprob": -0.27634313994762943, "compression_ratio": 1.691304347826087, "no_speech_prob": 5.896412176298327e-07}, {"id": 829, "seek": 317000, "start": 3170.0, "end": 3176.18, "text": " knows about some of the types in your specific use case, whereas we're talking about not", "tokens": [3255, 466, 512, 295, 264, 3467, 294, 428, 2685, 764, 1389, 11, 9735, 321, 434, 1417, 466, 406], "temperature": 0.0, "avg_logprob": -0.28493047834516644, "compression_ratio": 1.65, "no_speech_prob": 3.9896994508126227e-07}, {"id": 830, "seek": 317000, "start": 3176.18, "end": 3179.12, "text": " knowing about anything like form data doesn't know about anything.", "tokens": [5276, 466, 1340, 411, 1254, 1412, 1177, 380, 458, 466, 1340, 13], "temperature": 0.0, "avg_logprob": -0.28493047834516644, "compression_ratio": 1.65, "no_speech_prob": 3.9896994508126227e-07}, {"id": 831, "seek": 317000, "start": 3179.12, "end": 3181.84, "text": " Yeah, I'm gonna have to think on this a little bit.", "tokens": [865, 11, 286, 478, 799, 362, 281, 519, 322, 341, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.28493047834516644, "compression_ratio": 1.65, "no_speech_prob": 3.9896994508126227e-07}, {"id": 832, "seek": 317000, "start": 3181.84, "end": 3187.12, "text": " But but yeah, listeners, please do tweet at us or message us in the Elm Radio Slack channel", "tokens": [583, 457, 1338, 11, 23274, 11, 1767, 360, 15258, 412, 505, 420, 3636, 505, 294, 264, 2699, 76, 17296, 37211, 2269], "temperature": 0.0, "avg_logprob": -0.28493047834516644, "compression_ratio": 1.65, "no_speech_prob": 3.9896994508126227e-07}, {"id": 833, "seek": 317000, "start": 3187.12, "end": 3188.12, "text": " if you think of anything.", "tokens": [498, 291, 519, 295, 1340, 13], "temperature": 0.0, "avg_logprob": -0.28493047834516644, "compression_ratio": 1.65, "no_speech_prob": 3.9896994508126227e-07}, {"id": 834, "seek": 317000, "start": 3188.12, "end": 3193.28, "text": " Don't talk about it too hard, because I don't want to delay the publishing of this package.", "tokens": [1468, 380, 751, 466, 309, 886, 1152, 11, 570, 286, 500, 380, 528, 281, 8577, 264, 17832, 295, 341, 7372, 13], "temperature": 0.0, "avg_logprob": -0.28493047834516644, "compression_ratio": 1.65, "no_speech_prob": 3.9896994508126227e-07}, {"id": 835, "seek": 317000, "start": 3193.28, "end": 3196.56, "text": " Good point.", "tokens": [2205, 935, 13], "temperature": 0.0, "avg_logprob": -0.28493047834516644, "compression_ratio": 1.65, "no_speech_prob": 3.9896994508126227e-07}, {"id": 836, "seek": 319656, "start": 3196.56, "end": 3204.58, "text": " So there is one thing I feel I should mention, which this package works with Elm CSS.", "tokens": [407, 456, 307, 472, 551, 286, 841, 286, 820, 2152, 11, 597, 341, 7372, 1985, 365, 2699, 76, 24387, 13], "temperature": 0.0, "avg_logprob": -0.22868345279504756, "compression_ratio": 1.6535087719298245, "no_speech_prob": 8.62834284021119e-08}, {"id": 837, "seek": 319656, "start": 3204.58, "end": 3207.14, "text": " And it works with Elm HTML.", "tokens": [400, 309, 1985, 365, 2699, 76, 17995, 13], "temperature": 0.0, "avg_logprob": -0.22868345279504756, "compression_ratio": 1.6535087719298245, "no_speech_prob": 8.62834284021119e-08}, {"id": 838, "seek": 319656, "start": 3207.14, "end": 3209.72, "text": " It does not work with Elm UI, unfortunately.", "tokens": [467, 775, 406, 589, 365, 2699, 76, 15682, 11, 7015, 13], "temperature": 0.0, "avg_logprob": -0.22868345279504756, "compression_ratio": 1.6535087719298245, "no_speech_prob": 8.62834284021119e-08}, {"id": 839, "seek": 319656, "start": 3209.72, "end": 3215.88, "text": " The reason is because this package has strong opinions about form fields.", "tokens": [440, 1778, 307, 570, 341, 7372, 575, 2068, 11819, 466, 1254, 7909, 13], "temperature": 0.0, "avg_logprob": -0.22868345279504756, "compression_ratio": 1.6535087719298245, "no_speech_prob": 8.62834284021119e-08}, {"id": 840, "seek": 319656, "start": 3215.88, "end": 3221.64, "text": " And Elm UI doesn't expose a way to natively render form fields, except by dropping into", "tokens": [400, 2699, 76, 15682, 1177, 380, 19219, 257, 636, 281, 8470, 356, 15529, 1254, 7909, 11, 3993, 538, 13601, 666], "temperature": 0.0, "avg_logprob": -0.22868345279504756, "compression_ratio": 1.6535087719298245, "no_speech_prob": 8.62834284021119e-08}, {"id": 841, "seek": 319656, "start": 3221.64, "end": 3223.04, "text": " HTML.", "tokens": [17995, 13], "temperature": 0.0, "avg_logprob": -0.22868345279504756, "compression_ratio": 1.6535087719298245, "no_speech_prob": 8.62834284021119e-08}, {"id": 842, "seek": 319656, "start": 3223.04, "end": 3225.6, "text": " But there's no way to render semantic form fields.", "tokens": [583, 456, 311, 572, 636, 281, 15529, 47982, 1254, 7909, 13], "temperature": 0.0, "avg_logprob": -0.22868345279504756, "compression_ratio": 1.6535087719298245, "no_speech_prob": 8.62834284021119e-08}, {"id": 843, "seek": 322560, "start": 3225.6, "end": 3233.48, "text": " So like, this library, this package assumes form field events, like native HTML form field", "tokens": [407, 411, 11, 341, 6405, 11, 341, 7372, 37808, 1254, 2519, 3931, 11, 411, 8470, 17995, 1254, 2519], "temperature": 0.0, "avg_logprob": -0.23208645136669429, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.934437462608912e-07}, {"id": 844, "seek": 322560, "start": 3233.48, "end": 3234.48, "text": " events.", "tokens": [3931, 13], "temperature": 0.0, "avg_logprob": -0.23208645136669429, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.934437462608912e-07}, {"id": 845, "seek": 322560, "start": 3234.48, "end": 3239.98, "text": " And it also just doubles down on trying to use forms for accessibility and wrap things", "tokens": [400, 309, 611, 445, 31634, 760, 322, 1382, 281, 764, 6422, 337, 15002, 293, 7019, 721], "temperature": 0.0, "avg_logprob": -0.23208645136669429, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.934437462608912e-07}, {"id": 846, "seek": 322560, "start": 3239.98, "end": 3241.72, "text": " in form elements and that sort of thing.", "tokens": [294, 1254, 4959, 293, 300, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.23208645136669429, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.934437462608912e-07}, {"id": 847, "seek": 322560, "start": 3241.72, "end": 3244.24, "text": " So that's just sort of a baked in opinion.", "tokens": [407, 300, 311, 445, 1333, 295, 257, 19453, 294, 4800, 13], "temperature": 0.0, "avg_logprob": -0.23208645136669429, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.934437462608912e-07}, {"id": 848, "seek": 322560, "start": 3244.24, "end": 3251.94, "text": " I would love to see Elm UI expose some way to render form components, but at the moment,", "tokens": [286, 576, 959, 281, 536, 2699, 76, 15682, 19219, 512, 636, 281, 15529, 1254, 6677, 11, 457, 412, 264, 1623, 11], "temperature": 0.0, "avg_logprob": -0.23208645136669429, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.934437462608912e-07}, {"id": 849, "seek": 322560, "start": 3251.94, "end": 3253.24, "text": " it doesn't expose that.", "tokens": [309, 1177, 380, 19219, 300, 13], "temperature": 0.0, "avg_logprob": -0.23208645136669429, "compression_ratio": 1.6050420168067228, "no_speech_prob": 7.934437462608912e-07}, {"id": 850, "seek": 325324, "start": 3253.24, "end": 3257.04, "text": " So it was quite an interesting experience, sort of.", "tokens": [407, 309, 390, 1596, 364, 1880, 1752, 11, 1333, 295, 13], "temperature": 0.0, "avg_logprob": -0.2592029571533203, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.3320353648159653e-06}, {"id": 851, "seek": 325324, "start": 3257.04, "end": 3263.4399999999996, "text": " When I was building this, I first started with an experiment to extract it to a standalone", "tokens": [1133, 286, 390, 2390, 341, 11, 286, 700, 1409, 365, 364, 5120, 281, 8947, 309, 281, 257, 37454], "temperature": 0.0, "avg_logprob": -0.2592029571533203, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.3320353648159653e-06}, {"id": 852, "seek": 325324, "start": 3263.4399999999996, "end": 3265.4399999999996, "text": " package.", "tokens": [7372, 13], "temperature": 0.0, "avg_logprob": -0.2592029571533203, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.3320353648159653e-06}, {"id": 853, "seek": 325324, "start": 3265.4399999999996, "end": 3273.16, "text": " I just took this Elm package starter repo that I have, and I just copied the relevant", "tokens": [286, 445, 1890, 341, 2699, 76, 7372, 22465, 49040, 300, 286, 362, 11, 293, 286, 445, 25365, 264, 7340], "temperature": 0.0, "avg_logprob": -0.2592029571533203, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.3320353648159653e-06}, {"id": 854, "seek": 325324, "start": 3273.16, "end": 3280.68, "text": " code in there, and then without having the specific Elm pages dependencies, and then", "tokens": [3089, 294, 456, 11, 293, 550, 1553, 1419, 264, 2685, 2699, 76, 7183, 36606, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.2592029571533203, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.3320353648159653e-06}, {"id": 855, "seek": 325324, "start": 3280.68, "end": 3282.9599999999996, "text": " I just saw what was read.", "tokens": [286, 445, 1866, 437, 390, 1401, 13], "temperature": 0.0, "avg_logprob": -0.2592029571533203, "compression_ratio": 1.6650717703349283, "no_speech_prob": 2.3320353648159653e-06}, {"id": 856, "seek": 328296, "start": 3282.96, "end": 3285.2400000000002, "text": " It was really, really interesting.", "tokens": [467, 390, 534, 11, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.26037538666086096, "compression_ratio": 1.652542372881356, "no_speech_prob": 5.093580966786249e-06}, {"id": 857, "seek": 328296, "start": 3285.2400000000002, "end": 3292.44, "text": " Like a few of those things, like those were the points that I needed to abstract away.", "tokens": [1743, 257, 1326, 295, 729, 721, 11, 411, 729, 645, 264, 2793, 300, 286, 2978, 281, 12649, 1314, 13], "temperature": 0.0, "avg_logprob": -0.26037538666086096, "compression_ratio": 1.652542372881356, "no_speech_prob": 5.093580966786249e-06}, {"id": 858, "seek": 328296, "start": 3292.44, "end": 3296.2400000000002, "text": " And what I ended up doing was really like doing an inversion of control in a lot of", "tokens": [400, 437, 286, 4590, 493, 884, 390, 534, 411, 884, 364, 43576, 295, 1969, 294, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.26037538666086096, "compression_ratio": 1.652542372881356, "no_speech_prob": 5.093580966786249e-06}, {"id": 859, "seek": 328296, "start": 3296.2400000000002, "end": 3303.16, "text": " those places, which turned out to be, I think, a really just a nice way to design it overall.", "tokens": [729, 3190, 11, 597, 3574, 484, 281, 312, 11, 286, 519, 11, 257, 534, 445, 257, 1481, 636, 281, 1715, 309, 4787, 13], "temperature": 0.0, "avg_logprob": -0.26037538666086096, "compression_ratio": 1.652542372881356, "no_speech_prob": 5.093580966786249e-06}, {"id": 860, "seek": 328296, "start": 3303.16, "end": 3310.44, "text": " For example, like the original API just sort of reached in and grabbed state from the sort", "tokens": [1171, 1365, 11, 411, 264, 3380, 9362, 445, 1333, 295, 6488, 294, 293, 18607, 1785, 490, 264, 1333], "temperature": 0.0, "avg_logprob": -0.26037538666086096, "compression_ratio": 1.652542372881356, "no_speech_prob": 5.093580966786249e-06}, {"id": 861, "seek": 331044, "start": 3310.44, "end": 3313.8, "text": " of Elm application state, which it knew about.", "tokens": [295, 2699, 76, 3861, 1785, 11, 597, 309, 2586, 466, 13], "temperature": 0.0, "avg_logprob": -0.2316087032186574, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.453702437487664e-07}, {"id": 862, "seek": 331044, "start": 3313.8, "end": 3319.48, "text": " But then when you decouple it and it doesn't know what Elm application state is, now you", "tokens": [583, 550, 562, 291, 979, 263, 781, 309, 293, 309, 1177, 380, 458, 437, 2699, 76, 3861, 1785, 307, 11, 586, 291], "temperature": 0.0, "avg_logprob": -0.2316087032186574, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.453702437487664e-07}, {"id": 863, "seek": 331044, "start": 3319.48, "end": 3320.7200000000003, "text": " have to pass that in.", "tokens": [362, 281, 1320, 300, 294, 13], "temperature": 0.0, "avg_logprob": -0.2316087032186574, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.453702437487664e-07}, {"id": 864, "seek": 331044, "start": 3320.7200000000003, "end": 3324.64, "text": " So you say, is it submitting this form?", "tokens": [407, 291, 584, 11, 307, 309, 31836, 341, 1254, 30], "temperature": 0.0, "avg_logprob": -0.2316087032186574, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.453702437487664e-07}, {"id": 865, "seek": 331044, "start": 3324.64, "end": 3330.48, "text": " And then what I ended up with is I ended up creating like a little adapter module.", "tokens": [400, 550, 437, 286, 4590, 493, 365, 307, 286, 4590, 493, 4084, 411, 257, 707, 22860, 10088, 13], "temperature": 0.0, "avg_logprob": -0.2316087032186574, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.453702437487664e-07}, {"id": 866, "seek": 331044, "start": 3330.48, "end": 3332.6, "text": " It's just like using the adapter pattern.", "tokens": [467, 311, 445, 411, 1228, 264, 22860, 5102, 13], "temperature": 0.0, "avg_logprob": -0.2316087032186574, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.453702437487664e-07}, {"id": 867, "seek": 331044, "start": 3332.6, "end": 3339.32, "text": " So it's a very thin layer that, so instead of calling form.renderHTML in an Elm pages", "tokens": [407, 309, 311, 257, 588, 5862, 4583, 300, 11, 370, 2602, 295, 5141, 1254, 13, 13292, 39, 51, 12683, 294, 364, 2699, 76, 7183], "temperature": 0.0, "avg_logprob": -0.2316087032186574, "compression_ratio": 1.6451612903225807, "no_speech_prob": 7.453702437487664e-07}, {"id": 868, "seek": 333932, "start": 3339.32, "end": 3343.76, "text": " app, you can call pages.form.renderHTML.", "tokens": [724, 11, 291, 393, 818, 7183, 13, 837, 13, 13292, 39, 51, 12683, 13], "temperature": 0.0, "avg_logprob": -0.21157333877060439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.2482631746024708e-06}, {"id": 869, "seek": 333932, "start": 3343.76, "end": 3350.6000000000004, "text": " And whereas the standalone package takes a record where you give it submitting equals", "tokens": [400, 9735, 264, 37454, 7372, 2516, 257, 2136, 689, 291, 976, 309, 31836, 6915], "temperature": 0.0, "avg_logprob": -0.21157333877060439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.2482631746024708e-06}, {"id": 870, "seek": 333932, "start": 3350.6000000000004, "end": 3356.0800000000004, "text": " and then however you want to manage that, you can say model.submitting in Elm pages,", "tokens": [293, 550, 4461, 291, 528, 281, 3067, 300, 11, 291, 393, 584, 2316, 13, 30131, 76, 2414, 294, 2699, 76, 7183, 11], "temperature": 0.0, "avg_logprob": -0.21157333877060439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.2482631746024708e-06}, {"id": 871, "seek": 333932, "start": 3356.0800000000004, "end": 3361.36, "text": " it knows whether it's submitting because it handles submitting for you.", "tokens": [309, 3255, 1968, 309, 311, 31836, 570, 309, 18722, 31836, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.21157333877060439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.2482631746024708e-06}, {"id": 872, "seek": 333932, "start": 3361.36, "end": 3366.4, "text": " So when you say submit, Elm pages takes the low level form data.", "tokens": [407, 562, 291, 584, 10315, 11, 2699, 76, 7183, 2516, 264, 2295, 1496, 1254, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21157333877060439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 1.2482631746024708e-06}, {"id": 873, "seek": 336640, "start": 3366.4, "end": 3373.88, "text": " It has, it keeps track of the state in its own framework level model of the in progress", "tokens": [467, 575, 11, 309, 5965, 2837, 295, 264, 1785, 294, 1080, 1065, 8388, 1496, 2316, 295, 264, 294, 4205], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 874, "seek": 336640, "start": 3373.88, "end": 3375.6, "text": " form submissions.", "tokens": [1254, 40429, 13], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 875, "seek": 336640, "start": 3375.6, "end": 3382.7200000000003, "text": " And it knows how, given its model, how to say whether a field is being submitted.", "tokens": [400, 309, 3255, 577, 11, 2212, 1080, 2316, 11, 577, 281, 584, 1968, 257, 2519, 307, 885, 14405, 13], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 876, "seek": 336640, "start": 3382.7200000000003, "end": 3384.08, "text": " That was, so it was pretty cool.", "tokens": [663, 390, 11, 370, 309, 390, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 877, "seek": 336640, "start": 3384.08, "end": 3390.6800000000003, "text": " Like I basically just said like, oh, all these things that depend on the Elm pages application", "tokens": [1743, 286, 1936, 445, 848, 411, 11, 1954, 11, 439, 613, 721, 300, 5672, 322, 264, 2699, 76, 7183, 3861], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 878, "seek": 336640, "start": 3390.6800000000003, "end": 3391.6800000000003, "text": " state.", "tokens": [1785, 13], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 879, "seek": 336640, "start": 3391.6800000000003, "end": 3392.6800000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 880, "seek": 336640, "start": 3392.6800000000003, "end": 3393.96, "text": " I'll invert that.", "tokens": [286, 603, 33966, 300, 13], "temperature": 0.0, "avg_logprob": -0.2620893262096287, "compression_ratio": 1.5109170305676856, "no_speech_prob": 4.7378861722791044e-07}, {"id": 881, "seek": 339396, "start": 3393.96, "end": 3396.4, "text": " Submitting equals some Boolean.", "tokens": [8511, 76, 2414, 6915, 512, 23351, 28499, 13], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 882, "seek": 339396, "start": 3396.4, "end": 3401.84, "text": " And then in the adapter, I took all that code that had the logic that knew about the Elm", "tokens": [400, 550, 294, 264, 22860, 11, 286, 1890, 439, 300, 3089, 300, 632, 264, 9952, 300, 2586, 466, 264, 2699, 76], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 883, "seek": 339396, "start": 3401.84, "end": 3405.78, "text": " pages application state to deduce whether it was submitting or not.", "tokens": [7183, 3861, 1785, 281, 4172, 4176, 1968, 309, 390, 31836, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 884, "seek": 339396, "start": 3405.78, "end": 3410.84, "text": " And just gave that as the value for submitting equals all that logic I extracted that knew", "tokens": [400, 445, 2729, 300, 382, 264, 2158, 337, 31836, 6915, 439, 300, 9952, 286, 34086, 300, 2586], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 885, "seek": 339396, "start": 3410.84, "end": 3411.84, "text": " about Elm pages.", "tokens": [466, 2699, 76, 7183, 13], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 886, "seek": 339396, "start": 3411.84, "end": 3413.36, "text": " So it's pretty cool.", "tokens": [407, 309, 311, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 887, "seek": 339396, "start": 3413.36, "end": 3415.64, "text": " It was a lot cleaner than I expected.", "tokens": [467, 390, 257, 688, 16532, 813, 286, 5176, 13], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 888, "seek": 339396, "start": 3415.64, "end": 3418.96, "text": " When you started, did you think it would not be possible?", "tokens": [1133, 291, 1409, 11, 630, 291, 519, 309, 576, 406, 312, 1944, 30], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 889, "seek": 339396, "start": 3418.96, "end": 3420.08, "text": " I kind of did.", "tokens": [286, 733, 295, 630, 13], "temperature": 0.0, "avg_logprob": -0.2566990159515642, "compression_ratio": 1.705179282868526, "no_speech_prob": 2.2732619697762857e-07}, {"id": 890, "seek": 342008, "start": 3420.08, "end": 3425.44, "text": " I also like had a few places where it was coupled to the backend task API, because you", "tokens": [286, 611, 411, 632, 257, 1326, 3190, 689, 309, 390, 29482, 281, 264, 38087, 5633, 9362, 11, 570, 291], "temperature": 0.0, "avg_logprob": -0.22224393757906827, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.6577063749755325e-07}, {"id": 891, "seek": 342008, "start": 3425.44, "end": 3428.92, "text": " could do backend specific validations.", "tokens": [727, 360, 38087, 2685, 7363, 763, 13], "temperature": 0.0, "avg_logprob": -0.22224393757906827, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.6577063749755325e-07}, {"id": 892, "seek": 342008, "start": 3428.92, "end": 3433.88, "text": " If you wanted to, for example, check that a username is unique or try sending an email", "tokens": [759, 291, 1415, 281, 11, 337, 1365, 11, 1520, 300, 257, 30351, 307, 3845, 420, 853, 7750, 364, 3796], "temperature": 0.0, "avg_logprob": -0.22224393757906827, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.6577063749755325e-07}, {"id": 893, "seek": 342008, "start": 3433.88, "end": 3437.62, "text": " to check if an email is valid or whatever on the backend.", "tokens": [281, 1520, 498, 364, 3796, 307, 7363, 420, 2035, 322, 264, 38087, 13], "temperature": 0.0, "avg_logprob": -0.22224393757906827, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.6577063749755325e-07}, {"id": 894, "seek": 342008, "start": 3437.62, "end": 3445.2, "text": " But it turned out I was able to extract that out into this API where you combine together", "tokens": [583, 309, 3574, 484, 286, 390, 1075, 281, 8947, 300, 484, 666, 341, 9362, 689, 291, 10432, 1214], "temperature": 0.0, "avg_logprob": -0.22224393757906827, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.6577063749755325e-07}, {"id": 895, "seek": 344520, "start": 3445.2, "end": 3451.7999999999997, "text": " a set of form definitions, and it will try running the parser on any of those.", "tokens": [257, 992, 295, 1254, 21988, 11, 293, 309, 486, 853, 2614, 264, 21156, 260, 322, 604, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.23968415849664237, "compression_ratio": 1.6938775510204083, "no_speech_prob": 4.888244120593299e-07}, {"id": 896, "seek": 344520, "start": 3451.7999999999997, "end": 3459.8399999999997, "text": " And it turned out it was a very light wrapper to be able to parse into a form.", "tokens": [400, 309, 3574, 484, 309, 390, 257, 588, 1442, 46906, 281, 312, 1075, 281, 48377, 666, 257, 1254, 13], "temperature": 0.0, "avg_logprob": -0.23968415849664237, "compression_ratio": 1.6938775510204083, "no_speech_prob": 4.888244120593299e-07}, {"id": 897, "seek": 344520, "start": 3459.8399999999997, "end": 3468.96, "text": " Basically what it turned into is, so instead of the form parsing into a user record, or", "tokens": [8537, 437, 309, 3574, 666, 307, 11, 370, 2602, 295, 264, 1254, 21156, 278, 666, 257, 4195, 2136, 11, 420], "temperature": 0.0, "avg_logprob": -0.23968415849664237, "compression_ratio": 1.6938775510204083, "no_speech_prob": 4.888244120593299e-07}, {"id": 898, "seek": 344520, "start": 3468.96, "end": 3474.8999999999996, "text": " the form parsing into a JSON encode value, what it ended up being is just the form can", "tokens": [264, 1254, 21156, 278, 666, 257, 31828, 2058, 1429, 2158, 11, 437, 309, 4590, 493, 885, 307, 445, 264, 1254, 393], "temperature": 0.0, "avg_logprob": -0.23968415849664237, "compression_ratio": 1.6938775510204083, "no_speech_prob": 4.888244120593299e-07}, {"id": 899, "seek": 347490, "start": 3474.9, "end": 3478.88, "text": " parse into a backend task.", "tokens": [48377, 666, 257, 38087, 5633, 13], "temperature": 0.0, "avg_logprob": -0.22603790576641375, "compression_ratio": 1.7906976744186047, "no_speech_prob": 8.801044714346062e-06}, {"id": 900, "seek": 347490, "start": 3478.88, "end": 3483.36, "text": " If it fails to parse into a backend task, that means that the client-side validation", "tokens": [759, 309, 18199, 281, 48377, 666, 257, 38087, 5633, 11, 300, 1355, 300, 264, 6423, 12, 1812, 24071], "temperature": 0.0, "avg_logprob": -0.22603790576641375, "compression_ratio": 1.7906976744186047, "no_speech_prob": 8.801044714346062e-06}, {"id": 901, "seek": 347490, "start": 3483.36, "end": 3484.36, "text": " has failed.", "tokens": [575, 7612, 13], "temperature": 0.0, "avg_logprob": -0.22603790576641375, "compression_ratio": 1.7906976744186047, "no_speech_prob": 8.801044714346062e-06}, {"id": 902, "seek": 347490, "start": 3484.36, "end": 3491.76, "text": " If it succeeds in parsing into a backend task, now you can continue doing backend validations.", "tokens": [759, 309, 49263, 294, 21156, 278, 666, 257, 38087, 5633, 11, 586, 291, 393, 2354, 884, 38087, 7363, 763, 13], "temperature": 0.0, "avg_logprob": -0.22603790576641375, "compression_ratio": 1.7906976744186047, "no_speech_prob": 8.801044714346062e-06}, {"id": 903, "seek": 347490, "start": 3491.76, "end": 3499.46, "text": " And I ended up splitting off this thing that you can render to kind of give server state.", "tokens": [400, 286, 4590, 493, 30348, 766, 341, 551, 300, 291, 393, 15529, 281, 733, 295, 976, 7154, 1785, 13], "temperature": 0.0, "avg_logprob": -0.22603790576641375, "compression_ratio": 1.7906976744186047, "no_speech_prob": 8.801044714346062e-06}, {"id": 904, "seek": 349946, "start": 3499.46, "end": 3505.12, "text": " So you can give error messages from the backend if you want to.", "tokens": [407, 291, 393, 976, 6713, 7897, 490, 264, 38087, 498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.2676135375436428, "compression_ratio": 1.632, "no_speech_prob": 3.3404951409465866e-06}, {"id": 905, "seek": 349946, "start": 3505.12, "end": 3508.28, "text": " I was actually not expecting it to work out.", "tokens": [286, 390, 767, 406, 9650, 309, 281, 589, 484, 13], "temperature": 0.0, "avg_logprob": -0.2676135375436428, "compression_ratio": 1.632, "no_speech_prob": 3.3404951409465866e-06}, {"id": 906, "seek": 349946, "start": 3508.28, "end": 3509.76, "text": " Maybe not even at all.", "tokens": [2704, 406, 754, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.2676135375436428, "compression_ratio": 1.632, "no_speech_prob": 3.3404951409465866e-06}, {"id": 907, "seek": 349946, "start": 3509.76, "end": 3511.88, "text": " So it was a pleasant surprise.", "tokens": [407, 309, 390, 257, 16232, 6365, 13], "temperature": 0.0, "avg_logprob": -0.2676135375436428, "compression_ratio": 1.632, "no_speech_prob": 3.3404951409465866e-06}, {"id": 908, "seek": 349946, "start": 3511.88, "end": 3516.32, "text": " But now that I did it that way, I think it's really nice also to be able to just look at", "tokens": [583, 586, 300, 286, 630, 309, 300, 636, 11, 286, 519, 309, 311, 534, 1481, 611, 281, 312, 1075, 281, 445, 574, 412], "temperature": 0.0, "avg_logprob": -0.2676135375436428, "compression_ratio": 1.632, "no_speech_prob": 3.3404951409465866e-06}, {"id": 909, "seek": 349946, "start": 3516.32, "end": 3522.68, "text": " the docs separately, to learn this tool separately, for people to be able to talk about this package,", "tokens": [264, 45623, 14759, 11, 281, 1466, 341, 2290, 14759, 11, 337, 561, 281, 312, 1075, 281, 751, 466, 341, 7372, 11], "temperature": 0.0, "avg_logprob": -0.2676135375436428, "compression_ratio": 1.632, "no_speech_prob": 3.3404951409465866e-06}, {"id": 910, "seek": 349946, "start": 3522.68, "end": 3525.32, "text": " not just like, oh, it's this Elm Pages specific thing.", "tokens": [406, 445, 411, 11, 1954, 11, 309, 311, 341, 2699, 76, 430, 1660, 2685, 551, 13], "temperature": 0.0, "avg_logprob": -0.2676135375436428, "compression_ratio": 1.632, "no_speech_prob": 3.3404951409465866e-06}, {"id": 911, "seek": 352532, "start": 3525.32, "end": 3530.1200000000003, "text": " It's the same API, if somebody's using Elm Pages or somebody's using Lambda or somebody's", "tokens": [467, 311, 264, 912, 9362, 11, 498, 2618, 311, 1228, 2699, 76, 430, 1660, 420, 2618, 311, 1228, 45691, 420, 2618, 311], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 912, "seek": 352532, "start": 3530.1200000000003, "end": 3533.6000000000004, "text": " using Elm SBA, they can all talk about the same package.", "tokens": [1228, 2699, 76, 318, 9295, 11, 436, 393, 439, 751, 466, 264, 912, 7372, 13], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 913, "seek": 352532, "start": 3533.6000000000004, "end": 3534.6000000000004, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 914, "seek": 352532, "start": 3534.6000000000004, "end": 3539.96, "text": " I think it might add a bit of confusion if you're used to neither, because then, oh,", "tokens": [286, 519, 309, 1062, 909, 257, 857, 295, 15075, 498, 291, 434, 1143, 281, 9662, 11, 570, 550, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 915, "seek": 352532, "start": 3539.96, "end": 3540.96, "text": " what is this form?", "tokens": [437, 307, 341, 1254, 30], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 916, "seek": 352532, "start": 3540.96, "end": 3542.32, "text": " Oh, well, this is in that other package.", "tokens": [876, 11, 731, 11, 341, 307, 294, 300, 661, 7372, 13], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 917, "seek": 352532, "start": 3542.32, "end": 3544.6400000000003, "text": " Oh, I didn't see that one.", "tokens": [876, 11, 286, 994, 380, 536, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 918, "seek": 352532, "start": 3544.6400000000003, "end": 3548.2000000000003, "text": " But apart from that, sounds like a clear win to me.", "tokens": [583, 4936, 490, 300, 11, 3263, 411, 257, 1850, 1942, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 919, "seek": 352532, "start": 3548.2000000000003, "end": 3551.7200000000003, "text": " Especially if people start using it in other places, and then they start using Elm Pages,", "tokens": [8545, 498, 561, 722, 1228, 309, 294, 661, 3190, 11, 293, 550, 436, 722, 1228, 2699, 76, 430, 1660, 11], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 920, "seek": 352532, "start": 3551.7200000000003, "end": 3554.1600000000003, "text": " then the learning curve is a lot smaller.", "tokens": [550, 264, 2539, 7605, 307, 257, 688, 4356, 13], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 921, "seek": 352532, "start": 3554.1600000000003, "end": 3555.1600000000003, "text": " So that's nice.", "tokens": [407, 300, 311, 1481, 13], "temperature": 0.0, "avg_logprob": -0.2948147700383113, "compression_ratio": 1.7525083612040133, "no_speech_prob": 7.646438461961225e-06}, {"id": 922, "seek": 355516, "start": 3555.16, "end": 3556.16, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 923, "seek": 355516, "start": 3556.16, "end": 3557.16, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 924, "seek": 355516, "start": 3557.16, "end": 3558.16, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 925, "seek": 355516, "start": 3558.16, "end": 3565.8799999999997, "text": " One of the tricky things, design decisions that I dealt with here was, there's this config.", "tokens": [1485, 295, 264, 12414, 721, 11, 1715, 5327, 300, 286, 15991, 365, 510, 390, 11, 456, 311, 341, 6662, 13], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 926, "seek": 355516, "start": 3565.8799999999997, "end": 3571.24, "text": " So I have this options type, which when you render a form, you give it options, which", "tokens": [407, 286, 362, 341, 3956, 2010, 11, 597, 562, 291, 15529, 257, 1254, 11, 291, 976, 309, 3956, 11, 597], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 927, "seek": 355516, "start": 3571.24, "end": 3576.12, "text": " has like, it must have the form ID, the unique ID for the form.", "tokens": [575, 411, 11, 309, 1633, 362, 264, 1254, 7348, 11, 264, 3845, 7348, 337, 264, 1254, 13], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 928, "seek": 355516, "start": 3576.12, "end": 3579.3199999999997, "text": " But it can also have a form method.", "tokens": [583, 309, 393, 611, 362, 257, 1254, 3170, 13], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 929, "seek": 355516, "start": 3579.3199999999997, "end": 3581.7999999999997, "text": " So a form can be get or post.", "tokens": [407, 257, 1254, 393, 312, 483, 420, 2183, 13], "temperature": 0.0, "avg_logprob": -0.20786963568793404, "compression_ratio": 1.6218905472636815, "no_speech_prob": 4.88819466681889e-07}, {"id": 930, "seek": 358180, "start": 3581.8, "end": 3590.52, "text": " This is like, most front-end only applications aren't going to care about this, because they're", "tokens": [639, 307, 411, 11, 881, 1868, 12, 521, 787, 5821, 3212, 380, 516, 281, 1127, 466, 341, 11, 570, 436, 434], "temperature": 0.0, "avg_logprob": -0.25110903653231537, "compression_ratio": 1.5964125560538116, "no_speech_prob": 7.690358074796677e-07}, {"id": 931, "seek": 358180, "start": 3590.52, "end": 3595.4, "text": " not progressively enhancing a form, whereas Elm Pages uses progressive enhancement to", "tokens": [406, 46667, 36579, 257, 1254, 11, 9735, 2699, 76, 430, 1660, 4960, 16131, 40776, 281], "temperature": 0.0, "avg_logprob": -0.25110903653231537, "compression_ratio": 1.5964125560538116, "no_speech_prob": 7.690358074796677e-07}, {"id": 932, "seek": 358180, "start": 3595.4, "end": 3596.4, "text": " say...", "tokens": [584, 485], "temperature": 0.0, "avg_logprob": -0.25110903653231537, "compression_ratio": 1.5964125560538116, "no_speech_prob": 7.690358074796677e-07}, {"id": 933, "seek": 358180, "start": 3596.4, "end": 3604.6800000000003, "text": " So I have Elm Pages examples where I use get form submissions, because when you do a get", "tokens": [407, 286, 362, 2699, 76, 430, 1660, 5110, 689, 286, 764, 483, 1254, 40429, 11, 570, 562, 291, 360, 257, 483], "temperature": 0.0, "avg_logprob": -0.25110903653231537, "compression_ratio": 1.5964125560538116, "no_speech_prob": 7.690358074796677e-07}, {"id": 934, "seek": 358180, "start": 3604.6800000000003, "end": 3611.6000000000004, "text": " form submission, it appends query parameters to your URL and reloads the page.", "tokens": [1254, 23689, 11, 309, 724, 2581, 14581, 9834, 281, 428, 12905, 293, 25628, 82, 264, 3028, 13], "temperature": 0.0, "avg_logprob": -0.25110903653231537, "compression_ratio": 1.5964125560538116, "no_speech_prob": 7.690358074796677e-07}, {"id": 935, "seek": 361160, "start": 3611.6, "end": 3617.2, "text": " So for example, if you're doing a search query, that can be a really nice way to do that.", "tokens": [407, 337, 1365, 11, 498, 291, 434, 884, 257, 3164, 14581, 11, 300, 393, 312, 257, 534, 1481, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.22788470500224345, "compression_ratio": 1.5866141732283465, "no_speech_prob": 8.57915836149914e-07}, {"id": 936, "seek": 361160, "start": 3617.2, "end": 3618.7999999999997, "text": " And you can progressively enhance that.", "tokens": [400, 291, 393, 46667, 11985, 300, 13], "temperature": 0.0, "avg_logprob": -0.22788470500224345, "compression_ratio": 1.5866141732283465, "no_speech_prob": 8.57915836149914e-07}, {"id": 937, "seek": 361160, "start": 3618.7999999999997, "end": 3625.44, "text": " So instead of doing a hard page load, you can do an XHR request.", "tokens": [407, 2602, 295, 884, 257, 1152, 3028, 3677, 11, 291, 393, 360, 364, 1783, 39, 49, 5308, 13], "temperature": 0.0, "avg_logprob": -0.22788470500224345, "compression_ratio": 1.5866141732283465, "no_speech_prob": 8.57915836149914e-07}, {"id": 938, "seek": 361160, "start": 3625.44, "end": 3626.44, "text": " That's great.", "tokens": [663, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.22788470500224345, "compression_ratio": 1.5866141732283465, "no_speech_prob": 8.57915836149914e-07}, {"id": 939, "seek": 361160, "start": 3626.44, "end": 3633.52, "text": " But so anyway, the Elm Form API has this baked in opinion that a form method is a thing that", "tokens": [583, 370, 4033, 11, 264, 2699, 76, 10126, 9362, 575, 341, 19453, 294, 4800, 300, 257, 1254, 3170, 307, 257, 551, 300], "temperature": 0.0, "avg_logprob": -0.22788470500224345, "compression_ratio": 1.5866141732283465, "no_speech_prob": 8.57915836149914e-07}, {"id": 940, "seek": 361160, "start": 3633.52, "end": 3634.64, "text": " exists.", "tokens": [8198, 13], "temperature": 0.0, "avg_logprob": -0.22788470500224345, "compression_ratio": 1.5866141732283465, "no_speech_prob": 8.57915836149914e-07}, {"id": 941, "seek": 361160, "start": 3634.64, "end": 3640.24, "text": " And most people will probably ignore it, but it does have that baked in, even though probably", "tokens": [400, 881, 561, 486, 1391, 11200, 309, 11, 457, 309, 775, 362, 300, 19453, 294, 11, 754, 1673, 1391], "temperature": 0.0, "avg_logprob": -0.22788470500224345, "compression_ratio": 1.5866141732283465, "no_speech_prob": 8.57915836149914e-07}, {"id": 942, "seek": 364024, "start": 3640.24, "end": 3642.4799999999996, "text": " Elm Pages apps will be the only ones to use it.", "tokens": [2699, 76, 430, 1660, 7733, 486, 312, 264, 787, 2306, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.2515782425084065, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.653684535085631e-06}, {"id": 943, "seek": 364024, "start": 3642.4799999999996, "end": 3647.7599999999998, "text": " But a Lambdaera app could use it too, if it wanted to, or a front-end only app.", "tokens": [583, 257, 45691, 1663, 724, 727, 764, 309, 886, 11, 498, 309, 1415, 281, 11, 420, 257, 1868, 12, 521, 787, 724, 13], "temperature": 0.0, "avg_logprob": -0.2515782425084065, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.653684535085631e-06}, {"id": 944, "seek": 364024, "start": 3647.7599999999998, "end": 3653.7599999999998, "text": " So I left it in there because it seemed like a reasonable opinion that that exists, since", "tokens": [407, 286, 1411, 309, 294, 456, 570, 309, 6576, 411, 257, 10585, 4800, 300, 300, 8198, 11, 1670], "temperature": 0.0, "avg_logprob": -0.2515782425084065, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.653684535085631e-06}, {"id": 945, "seek": 364024, "start": 3653.7599999999998, "end": 3655.3199999999997, "text": " it does exist in the browser.", "tokens": [309, 775, 2514, 294, 264, 11185, 13], "temperature": 0.0, "avg_logprob": -0.2515782425084065, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.653684535085631e-06}, {"id": 946, "seek": 364024, "start": 3655.3199999999997, "end": 3662.9599999999996, "text": " But yeah, so I created this options record and use a little builder pattern to build", "tokens": [583, 1338, 11, 370, 286, 2942, 341, 3956, 2136, 293, 764, 257, 707, 27377, 5102, 281, 1322], "temperature": 0.0, "avg_logprob": -0.2515782425084065, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.653684535085631e-06}, {"id": 947, "seek": 364024, "start": 3662.9599999999996, "end": 3664.52, "text": " up these options.", "tokens": [493, 613, 3956, 13], "temperature": 0.0, "avg_logprob": -0.2515782425084065, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.653684535085631e-06}, {"id": 948, "seek": 366452, "start": 3664.52, "end": 3671.72, "text": " And I share that options record between the Elm Pages package and the standalone package.", "tokens": [400, 286, 2073, 300, 3956, 2136, 1296, 264, 2699, 76, 430, 1660, 7372, 293, 264, 37454, 7372, 13], "temperature": 0.0, "avg_logprob": -0.24405952210121967, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0676945976229035e-06}, {"id": 949, "seek": 366452, "start": 3671.72, "end": 3680.52, "text": " So I'm guessing that means that whenever you need to do a change in the form package, you", "tokens": [407, 286, 478, 17939, 300, 1355, 300, 5699, 291, 643, 281, 360, 257, 1319, 294, 264, 1254, 7372, 11, 291], "temperature": 0.0, "avg_logprob": -0.24405952210121967, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0676945976229035e-06}, {"id": 950, "seek": 366452, "start": 3680.52, "end": 3684.52, "text": " will also need to update that in Elm Pages.", "tokens": [486, 611, 643, 281, 5623, 300, 294, 2699, 76, 430, 1660, 13], "temperature": 0.0, "avg_logprob": -0.24405952210121967, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0676945976229035e-06}, {"id": 951, "seek": 366452, "start": 3684.52, "end": 3688.6, "text": " So you will need to bump them together to keep them in sync.", "tokens": [407, 291, 486, 643, 281, 9961, 552, 1214, 281, 1066, 552, 294, 20271, 13], "temperature": 0.0, "avg_logprob": -0.24405952210121967, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0676945976229035e-06}, {"id": 952, "seek": 366452, "start": 3688.6, "end": 3693.88, "text": " What I ended up doing is I just basically in that little facade module that I have in", "tokens": [708, 286, 4590, 493, 884, 307, 286, 445, 1936, 294, 300, 707, 46261, 10088, 300, 286, 362, 294], "temperature": 0.0, "avg_logprob": -0.24405952210121967, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0676945976229035e-06}, {"id": 953, "seek": 369388, "start": 3693.88, "end": 3702.0, "text": " the Elm Pages API, pages.form, what I did is one of the arguments is the standalone", "tokens": [264, 2699, 76, 430, 1660, 9362, 11, 7183, 13, 837, 11, 437, 286, 630, 307, 472, 295, 264, 12869, 307, 264, 37454], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 954, "seek": 369388, "start": 3702.0, "end": 3703.0, "text": " packages options.", "tokens": [17401, 3956, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 955, "seek": 369388, "start": 3703.0, "end": 3704.0, "text": " Got you.", "tokens": [5803, 291, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 956, "seek": 369388, "start": 3704.0, "end": 3705.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 957, "seek": 369388, "start": 3705.0, "end": 3709.08, "text": " And you transform that to the options of Elm Pages.", "tokens": [400, 291, 4088, 300, 281, 264, 3956, 295, 2699, 76, 430, 1660, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 958, "seek": 369388, "start": 3709.08, "end": 3710.48, "text": " Right, exactly.", "tokens": [1779, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 959, "seek": 369388, "start": 3710.48, "end": 3711.96, "text": " So I supplemented.", "tokens": [407, 286, 15436, 292, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 960, "seek": 369388, "start": 3711.96, "end": 3715.76, "text": " So the Elm Pages options are a superset of that.", "tokens": [407, 264, 2699, 76, 430, 1660, 3956, 366, 257, 37906, 302, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 961, "seek": 369388, "start": 3715.76, "end": 3720.36, "text": " So I said, you know what, I'm just going to, so that they can share as much as possible,", "tokens": [407, 286, 848, 11, 291, 458, 437, 11, 286, 478, 445, 516, 281, 11, 370, 300, 436, 393, 2073, 382, 709, 382, 1944, 11], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 962, "seek": 369388, "start": 3720.36, "end": 3722.56, "text": " I'm going to share this options type between the two.", "tokens": [286, 478, 516, 281, 2073, 341, 3956, 2010, 1296, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.2867718665830551, "compression_ratio": 1.688034188034188, "no_speech_prob": 3.1875056265562307e-06}, {"id": 963, "seek": 372256, "start": 3722.56, "end": 3726.44, "text": " Even though there are a few additional options that you can pass into the Elm Pages one.", "tokens": [2754, 1673, 456, 366, 257, 1326, 4497, 3956, 300, 291, 393, 1320, 666, 264, 2699, 76, 430, 1660, 472, 13], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 964, "seek": 372256, "start": 3726.44, "end": 3731.48, "text": " So I think that worked out pretty nicely, but that was definitely a subtle design decision", "tokens": [407, 286, 519, 300, 2732, 484, 1238, 9594, 11, 457, 300, 390, 2138, 257, 13743, 1715, 3537], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 965, "seek": 372256, "start": 3731.48, "end": 3732.48, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 966, "seek": 372256, "start": 3732.48, "end": 3733.48, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 967, "seek": 372256, "start": 3733.48, "end": 3739.68, "text": " So if people want to know more about Elm Form and Elm Pages, or your form package, did you", "tokens": [407, 498, 561, 528, 281, 458, 544, 466, 2699, 76, 10126, 293, 2699, 76, 430, 1660, 11, 420, 428, 1254, 7372, 11, 630, 291], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 968, "seek": 372256, "start": 3739.68, "end": 3740.88, "text": " even call it Elm Form?", "tokens": [754, 818, 309, 2699, 76, 10126, 30], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 969, "seek": 372256, "start": 3740.88, "end": 3742.08, "text": " What is the name?", "tokens": [708, 307, 264, 1315, 30], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 970, "seek": 372256, "start": 3742.08, "end": 3745.32, "text": " Did you announce it during this whole episode?", "tokens": [2589, 291, 7478, 309, 1830, 341, 1379, 3500, 30], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 971, "seek": 372256, "start": 3745.32, "end": 3752.32, "text": " No, I somewhat was leaving room for the possibility that I changed my mind about what to call", "tokens": [883, 11, 286, 8344, 390, 5012, 1808, 337, 264, 7959, 300, 286, 3105, 452, 1575, 466, 437, 281, 818], "temperature": 0.0, "avg_logprob": -0.25856871831984746, "compression_ratio": 1.6319444444444444, "no_speech_prob": 8.315162176586455e-07}, {"id": 972, "seek": 375232, "start": 3752.32, "end": 3753.32, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 973, "seek": 375232, "start": 3753.32, "end": 3755.04, "text": " But yeah, I think for now I'm calling it Elm Form.", "tokens": [583, 1338, 11, 286, 519, 337, 586, 286, 478, 5141, 309, 2699, 76, 10126, 13], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 974, "seek": 375232, "start": 3755.04, "end": 3759.44, "text": " I think Elm Form, I don't think I need to get too clever about it.", "tokens": [286, 519, 2699, 76, 10126, 11, 286, 500, 380, 519, 286, 643, 281, 483, 886, 13494, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 975, "seek": 375232, "start": 3759.44, "end": 3765.32, "text": " It does kind of like represent a, I think, pretty different philosophy around how to", "tokens": [467, 775, 733, 295, 411, 2906, 257, 11, 286, 519, 11, 1238, 819, 10675, 926, 577, 281], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 976, "seek": 375232, "start": 3765.32, "end": 3767.2400000000002, "text": " deal with forms in Elm.", "tokens": [2028, 365, 6422, 294, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 977, "seek": 375232, "start": 3767.2400000000002, "end": 3770.92, "text": " So I definitely have considered like, does that deserve a different name?", "tokens": [407, 286, 2138, 362, 4888, 411, 11, 775, 300, 9948, 257, 819, 1315, 30], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 978, "seek": 375232, "start": 3770.92, "end": 3773.4, "text": " But yeah, I'm just calling it Elm Form.", "tokens": [583, 1338, 11, 286, 478, 445, 5141, 309, 2699, 76, 10126, 13], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 979, "seek": 375232, "start": 3773.4, "end": 3778.0, "text": " Not Elm Awesome Form Meatballs or something.", "tokens": [1726, 2699, 76, 10391, 10126, 30502, 19194, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 980, "seek": 375232, "start": 3778.0, "end": 3779.48, "text": " That sounds delicious.", "tokens": [663, 3263, 4809, 13], "temperature": 0.0, "avg_logprob": -0.23211720784505208, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.414458438375732e-06}, {"id": 981, "seek": 377948, "start": 3779.48, "end": 3783.4, "text": " It does sound delicious.", "tokens": [467, 775, 1626, 4809, 13], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 982, "seek": 377948, "start": 3783.4, "end": 3784.4, "text": " Yeah so Elm Form.", "tokens": [865, 370, 2699, 76, 10126, 13], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 983, "seek": 377948, "start": 3784.4, "end": 3788.0, "text": " So you know, we'll link to the package docs.", "tokens": [407, 291, 458, 11, 321, 603, 2113, 281, 264, 7372, 45623, 13], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 984, "seek": 377948, "start": 3788.0, "end": 3794.2, "text": " Hopefully by the time this is live, we'll link to live package docs, not the preview", "tokens": [10429, 538, 264, 565, 341, 307, 1621, 11, 321, 603, 2113, 281, 1621, 7372, 45623, 11, 406, 264, 14281], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 985, "seek": 377948, "start": 3794.2, "end": 3796.04, "text": " docs.", "tokens": [45623, 13], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 986, "seek": 377948, "start": 3796.04, "end": 3799.88, "text": " And I'll try to get some nice ELE examples there, because that's one of the cool things", "tokens": [400, 286, 603, 853, 281, 483, 512, 1481, 462, 2634, 5110, 456, 11, 570, 300, 311, 472, 295, 264, 1627, 721], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 987, "seek": 377948, "start": 3799.88, "end": 3801.08, "text": " we can get ELE examples.", "tokens": [321, 393, 483, 462, 2634, 5110, 13], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 988, "seek": 377948, "start": 3801.08, "end": 3806.28, "text": " But there is a nice examples folder in the repo, which I'll link to.", "tokens": [583, 456, 307, 257, 1481, 5110, 10820, 294, 264, 49040, 11, 597, 286, 603, 2113, 281, 13], "temperature": 0.0, "avg_logprob": -0.28879884024646796, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.224335831779172e-07}, {"id": 989, "seek": 380628, "start": 3806.28, "end": 3813.6800000000003, "text": " And yeah, I'll also link to a few examples of it in action in Lambdaera and in Elm Pages.", "tokens": [400, 1338, 11, 286, 603, 611, 2113, 281, 257, 1326, 5110, 295, 309, 294, 3069, 294, 45691, 1663, 293, 294, 2699, 76, 430, 1660, 13], "temperature": 0.0, "avg_logprob": -0.26898110129616476, "compression_ratio": 1.5408560311284047, "no_speech_prob": 3.966302756452933e-06}, {"id": 990, "seek": 380628, "start": 3813.6800000000003, "end": 3816.96, "text": " So you can sort of compare the Elm Pages one.", "tokens": [407, 291, 393, 1333, 295, 6794, 264, 2699, 76, 430, 1660, 472, 13], "temperature": 0.0, "avg_logprob": -0.26898110129616476, "compression_ratio": 1.5408560311284047, "no_speech_prob": 3.966302756452933e-06}, {"id": 991, "seek": 380628, "start": 3816.96, "end": 3822.5600000000004, "text": " It can get pretty sophisticated because you can do all these use cases like dealing with", "tokens": [467, 393, 483, 1238, 16950, 570, 291, 393, 360, 439, 613, 764, 3331, 411, 6260, 365], "temperature": 0.0, "avg_logprob": -0.26898110129616476, "compression_ratio": 1.5408560311284047, "no_speech_prob": 3.966302756452933e-06}, {"id": 992, "seek": 380628, "start": 3822.5600000000004, "end": 3827.7200000000003, "text": " in-flight submissions and parsing that data to get optimistic UI and things like that.", "tokens": [294, 12, 43636, 40429, 293, 21156, 278, 300, 1412, 281, 483, 19397, 15682, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.26898110129616476, "compression_ratio": 1.5408560311284047, "no_speech_prob": 3.966302756452933e-06}, {"id": 993, "seek": 380628, "start": 3827.7200000000003, "end": 3830.52, "text": " But yeah, so I would start there.", "tokens": [583, 1338, 11, 370, 286, 576, 722, 456, 13], "temperature": 0.0, "avg_logprob": -0.26898110129616476, "compression_ratio": 1.5408560311284047, "no_speech_prob": 3.966302756452933e-06}, {"id": 994, "seek": 380628, "start": 3830.52, "end": 3834.1200000000003, "text": " And if you have any feedback, let Dillon know.", "tokens": [400, 498, 291, 362, 604, 5824, 11, 718, 28160, 458, 13], "temperature": 0.0, "avg_logprob": -0.26898110129616476, "compression_ratio": 1.5408560311284047, "no_speech_prob": 3.966302756452933e-06}, {"id": 995, "seek": 380628, "start": 3834.1200000000003, "end": 3835.1200000000003, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.26898110129616476, "compression_ratio": 1.5408560311284047, "no_speech_prob": 3.966302756452933e-06}, {"id": 996, "seek": 383512, "start": 3835.12, "end": 3837.6, "text": " And Jeroen, until next time.", "tokens": [400, 508, 2032, 268, 11, 1826, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.35012639893425834, "compression_ratio": 1.125, "no_speech_prob": 2.1685778847313486e-05}, {"id": 997, "seek": 383760, "start": 3837.6, "end": 3867.36, "text": " Until next time.", "tokens": [50364, 9088, 958, 565, 13, 51852], "temperature": 0.0, "avg_logprob": -0.6747794832502093, "compression_ratio": 0.6666666666666666, "no_speech_prob": 2.9077138606226072e-05}], "language": "en"}