{"text": " Hello Jeroen. Hello Dylan. So right before recording you just admitted that you didn't have a pun. That's true. I think that's going to disappoint a lot of listeners. So what we can do is we can both concurrently try to come up with a good pun. That's going to be our task. That's the task you've set for us. I see what you did there. I see what you did there. Did I succeed? The task may have failed, but we might have a little bit of failure recovery, some error handling on this. Let's try to end then and move on from this sequentially. I highly approve of these puns. Andrew, please save us from these puns. We've got Andrew McMurray joining us. Thanks so much for coming on the show, Andrew. Hello. Thanks for having me. Pleasure to have you. And today we are talking about your LMS. Elm Concurrent Task Package. So, well, to start us off, what is Elm Concurrent Task in a nutshell? Like, what even is it and what does it do? Well, it is, I guess you could call it, it's a task-like API. So if you've used Elm Cause Task, it's designed to look very similar. The main difference, I'd say, is that Elm Cause Task runs, it's map to map. Three and friends, it runs each of those one after the other. Whereas the name implies an Elm Concurrent Task, it runs them at the same time. So you can run like a tree of these concurrently. The other thing it does is it also provides a convenient JavaScript FFI. So you can call, the idea is that you could call a sequence of JavaScript functions and chain the results together. This has also been called task ports in the past. And if you've. If you've used Elm Pages v3 and you're familiar with backend task, it is basically a standalone or more or less a standalone implementation of that. So, yeah. Right. And one that can be run on the front end too, which backend tasks as the name implies, cannot be. True. Yes. But I will add, it was fully inspired by backend task. You gave me the idea, Dylan, and I just took him around with it. So. Fantastic. I love that. Yeah. And I, on the point of running concurrently rather than sequentially, I just want to maybe give people an example to, to motivate that a little bit because, you know, it sounds, it sounds pretty in the weeds, but if you think about it, if you say, if you have two tasks defined for making HTTP requests, you know, to get the user users profile information and get the current weather. Yeah. Yeah. Right. There's some latency for that. And, you know, maybe it takes some time to process the request on, on these different respective backends. If you do test on map two, it's going to run the first one. And then once that's done, it's going to go and run the second one, which is not great. So it's a serious problem. Why isn't that great in this case? Well, because you, you stack the latency of the one on top of the other. You, you turn it into a, you turn something that doesn't need to be a waterfall, i.e. stacked one in front of the other finishing before the next one starts. You, you could just fire them off both at the same time. And that's sort of like the basic structure of, of web browsers is designed and, and of JavaScript of the JavaScript runtime. It's designed to be really good at doing things asynchronously. So it's at its best when you can sort of. Fire things off and let them asynchronously come back and continue on. It's not good when you hold up the event loop or, or waterfall things too much. Yeah. I actually recently learned that all the map functions are sequential. Like I thought they were concurrent, but no. It's surprising, right? Yeah. Yeah. So in the example of the. You make two requests, one to get the time or the weather and one another to get some other information. Doesn't matter too much. It's just slower, which has some impact obviously. But if you like, you make a get request and another request to a post request or put a request to actively do something and you expect those two to, to be done, even if the first one fails, then that changes the behavior. Right? True. Although. You could always, um, Hmm. You know, I'm not even sure what task dot map two and in the Elm core task definitions would do, but yeah, maybe it would not fire the second one. You could always do. It doesn't, it doesn't like it, it actually does like a task that, and then on the first one, right. And then it runs the second one. So nothing gets sent out or triggered before you, before the first one completes. Um, we. We just had a new simplification in the arm review, simplify that if you have like task dot map three and the second element, uh, is clearly something that will fail. Then the map three gets changed to a map two and we remove the third or the fourth argument. Like if it's a task dot fail or whatever. Yeah. We, which is not going to happen very often, but it's now you will have like something that tells you like, Hey, this is simple. For. Um, simplifiable or here's maybe something you didn't expect. That is interesting. Yeah. Cause generally the Elm standard libraries have very intuitive semantics, but this is definitely one that's surprising. And it is interesting because it is technically possible to define, and there are packages out there that define a way to use the Elm task package to to do parallel tasks. Um, so that is technically possible, but but yeah, so, so. So the FFI part of Elm concurrent task, I think is also really crucial. So, and, and now the idea here you talk about in the read me is, is a hack free of way of doing FFI. I think sometimes in the Elm community, FFI is sort of looked at as a dirty word, but I mean, we need to interact with things outside of the sandbox of Elm in some way. So it's a question of how do we do that? So, so what does that. Look like an Elm concurrent task? What does that look like? So very similar to backend task, funnily enough, the idea is you would define a function name in Elm. So like a string, like the, the string function name, you give it, uh, a decoder effectively, like what you expect to come back from the function, some way of interpreting the errors too, and then an encoded set of arguments. And. On the other side, the JavaScript side, there's a, there's a JavaScript package that goes alongside the Elm package that just looks up the stringified name in effectively like an object of functions that you provide on the JavaScript side. So it's when I saw it in backend test, cause I was like, that's such a simple, elegant idea. And it actually has quite a nice, very nice kind of back and forth between the two packages. And I think the main, the main safety. That you provide is like library authors is telling Elm when things like the function hasn't been defined or like if it throws an exception or doesn't give you back what you expect to give back. You're sort of baking all of that bits where if you did it in an application without some of the wiring, you could get, you know, unpleasant errors, things that go wrong. They use your stuff when you're interacting with ports. Right. Right. And if you contrast that with defining a. Port in, in Elm and running that as a command or a subscription, if you, let's say define an outgoing port to set an item in local storage, I believe if you, if you throw an exception in a port in Elm, it's just throwing that exception in, in JavaScript. Right. So it's not, there are no guardrails that are going to prevent that unless you add them in yourself. Exactly. So what would be the impact here? Would it cancel all the other? Tasks that are remaining? There's some, so it would, there's, there's actually two kinds of errors that you can get back. You'd, you'd receive an error back through via concurrent task, and you can define certain tasks where you expect an exception to be thrown. You can say like, catch the exception and then lift it into an error type. Or you can, there's, there's actually kind of a lower level, which are called unexpected errors where like. If you're, if you're not expecting this, this JavaScript function to throw an exception and then something happens, it's kind of like it will cancel everything. And then you get a descriptive error message on the other side. So yeah, that's, that's I've been, I've, I've tried it out in a couple of, a couple of applications. That's pattern seems to have worked quite nicely. So I'm a little bit confused about something. What happens if in one of the ports through concurrent task or just plain. Regular ports on JavaScript side, what happens if there's an exception that is being thrown synchronously? Does that then gets does that then cancel all the other tasks that are related to, or in the same batch of commands being sent? Are those canceled? Do you know that by any chance? So with the, if you've got a batch, like a batch that's gone out and one of them throws. Yeah. It's not clever enough to like stop that. The, the ones that are in flight at that moment, they've already gone through the port, but any, any, any followup ones from that will be canceled. So those that are like, and then, or map on that one will be canceled, but the other batch commands will just go through. So yeah. Which I think makes sense. Like that. There's no reason that maybe they're disconnected. They I'm doing. One thing and another totally unrelated thing. And if the first one fails, then I don't want that to be affected. Yeah. It depends on the case, but I guess having the possibility to say, well, these are connected and these are not connected. Let's have them be separate just in case. Who knows? Okay. So that's, that's interesting. If, if it's something that folks are actually interested in this definitely using like timeouts and HTTP, because one of the things that to do to make it compatible. With well fill in the functionality of existing Elm core tasks, because that's, that's one thing that they're not actually compatible with Elm cores tasks, because they're from many different types, but re-implementing HTTP timeouts, there's something called an abort signal in the fetch API. So that may be, that may be a way of like, if people want cancelable, say like you've got a batch of like a hundred tests that have gone out. One of them fails. And you actually don't want any of the other ones to carry on. You could, then maybe there's a way of sending them an abort signal to say, just stop. Like, so you kind of reclaim memory or they may be they're expensive HTTP requests that you want to stop. But I, I totally agree with you saying like, it's probably a case by case basis, like necessarily want that on every single task you define. Yeah. So you said something interesting there. You had to re-implement HTTP task. What is that? What is that all about? So they, the concurrent task, like task type is not, uh, unfortunately it's like, it's not the same as the Elm core task type. So if you wanted to use like time dot now or process dot sleep from Elm cores library, they wouldn't, they wouldn't fit in. So the it's actually, I mean, I took the idea from Dylan as well, who did the same with backend task and that you, you write. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. So at the concurrent task HTTP actually calls the fetch API underneath under the hood, which is actually, which is actually nice because, you know, the built in Elm HTTP package uses, uses XHR requests, which is kind of an outdated standard. And there are some performance improvements and modernizations. They're subtle, but there are some minor performance improvements and general improvements in fetch. So that's kind of a nice feature. Okay, so that means that you had to implement some of the things in Elm and some things in JavaScript. And does that mean that your Elm package does not work without the JavaScript package? Like, for instance, I thought that the JavaScript package was if you wanted to enable task ports, but it just doesn't work without setting up that JavaScript library. That's right, yeah, you do need the JavaScript library too, because the JavaScript library will wire in the ports that you provide from Elm. It's actually the thing that runs. It runs the tasks themselves. Right, so even if you wanted to run two HTTP requests or two time.now functions that call out to the core one, you wouldn't be able to run them concurrently because you need a new primitive to run them concurrently. Gotcha. Yeah, so it's basically using the exact same mechanism for these internal primitives. Primitives of, you know, concurrent task.now and HTTP as a user would use to define their own custom task definitions in JavaScript. Basically, the only difference is that when you call the code in JavaScript to set up your additional definitions for concurrent tasks, it already comes with some sort of pre-installed sort of core concurrent task definition. That's the only difference, I think. So how do you actually make those tasks concurrent in JavaScript? Do you wrap everything in a set immediate callback or I don't even remember the name of the function? So it's basically you're sending out, I call them internally, I call them task definitions. So it's like that record of the function name, the arguments that you're sending. You send those all out in a command.batch. Okay. So it's like everything goes out the port whenever it's ready to be executed. And then the JavaScript runner will just run them. And then once it's got the results, it calls an incoming port with an ID. The main challenge of that package and the core bit of it is actually just attaching IDs to all of the tasks and then sending them back through and being able to reassociate them. So because Elm, as it stands, can run lots of things concurrently, like command.batch will let you run as many things as you can imagine or that the program will take. But the problem is actually being able to associate them with the previous call. So that's all the wiring that this package is doing to make it appear like they're just happening one after the other. Right. And as far as set timeout and things like that, the way that JavaScript works, things are asynchronous in the event loop by default. So when you do fetch, you don't have to do anything for that to be an asynchronous task. You just fire it off and it's going to be running and then continue the event loop. But the set timeout thing is more of a hack or the run immediate or whatever. Those hacks are more for getting the event loop, to tick, to try to hack the processing of things in the event loop. Yeah. So the example that I had in mind where things are not async by default is the time.now command because that just calls date.now in JavaScript and sends it back to the port or to the message in Elm. So that one is not asynchronous by default. So do you wrap that one back into a set timeout? Or is it fine if it's just synchronous? It's not currently wrapped in a set timeout. I think there's an inevitable very small amount of lag probably in that it has to be sent. It probably does this just by default of it having to go out of the port and then back in. There'll be a very small... I don't know actually how I'd measure the lag that it's got on it. I mean, it's also like, we're not... I mean, it's not like we're interested in the moment of when the update function returns, right? It's when you get a message back. Like you will have some time between the two, between the return of the update and the start of the update function again. And thankfully, we're not that... We don't need to be that precise in browsers usually, hopefully. Yeah. Not often at least. True. Yeah. I think there are like... There's asynchronous for like things performing in parallel. And then there are like the semantics of the sort of chain of concurrent tasks. And if, you know, assuming that it's similar under the hood to Elm Pages, it's basically like able to just resolve to check, is this task complete and continue the chain of remaining tasks based on which ones needed to be completed before it continues. So it's not... That part of it is not using this sort of JavaScript event loop. It's just checking whenever it can, if it's ready for the next task. So for the... Another comparison between commands and tasks, I always find that that's one of these kind of almost rough edges in Elm is like there are these two similar but different ways of doing things. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. There are different ways of executing things. And the semantics are a little bit different. Like a, you know, task has an error type and they're chainable and they're sequential, but they can technically be done in parallel. And like... And you might get a response, which might not... Which is not the case for... Well, you will always get a response, which is not the case for commands. Right. That's an interesting one. I think that's like the main reason why you don't do that. have what commands and tasks are different and then the other strange thing is like there are only a handful of things in elm that you can use to create like a native task like http and some of the you know dom operations give you tasks but then a lot of things there just is no task version of you just do a command for example ports like you define a port and it gives you you know a task for an outgoing port but i think a lot of people say well but i want to be able to have a perform an http request and then take some decoded data from that http request and write that to local storage and then i want to go and perform an http post request and then i want to do this so like but you can't chain an http request with a port because It's a command. So I've always felt like sometimes when people are complaining about Elm being very limited in the way you can do FFI, that there's no FFI in Elm. You can only do ports or web components, right? Those are like sort of the two standard ways to do communication with JavaScript and Elm. Well, I feel like one of the biggest pain points of that, at least for me personally, is just that it's very awkward to do that with a command. I want a sort of chainable task style way of doing that. I want to be able to include that in a chain with HTTP requests and other types of tasks. So I think it's very cool that you've sort of built this abstraction that with minimal hacks gives you a way to kind of do these different things all in that same chainable paradigm. With minimal JavaScripts. Set up. Right. Would you say it's a hack, Andrew? The only reason it's not a hack is comparing it to the other ways that you have been done in the past to make this kind of thing work. Those are definitely hacks. Why don't you explain a couple of those approaches that other tools have used? Sure. There was one that I used in the past on an application that did inspire some of the work on this package. There's a package called Elm Taskport. And it's a very clever hack. And the idea is that you monkey patch XML HTTP requests. So you add, like, you modify that global object to whenever you're sending off a HTTP request with, say, you've got like a special URL scheme. You might be like Elm custom function. Your monkey packs version will. Check for that. That URL. And then you can call custom JavaScript from inside there. And it works really well. It's clever. But you're modifying, like, global objects, which is a little bit dodgy. So I think that's, like, that's without a doubt a hack. Yeah. Don't let your mother know about this. Exactly. But what your mother don't know won't hurt her. Well. I mean, if you whack your mother in the back of her. Of her head. She won't know. Yes, it will hurt her. What is that idiom? Like, that makes no sense. Don't modify global objects. If you can help it. Just don't hurt your mother, Dylan. But that is the interesting thing about this approach is that. I mean, your read me. Andrew talks about this as FFI. And conceptually it is. But it's actually just doing this, like, accepted thing in Elm, which is like passing in some flags. And, you know, it's not using any frowned upon techniques to do these things. It's just an abstraction that makes it feel a little bit more natural to interrupt with JavaScript. Exactly. I think the main thing that really I found very interesting with that approach was you can use it. You can use it in Node or in the browser. Whereas if you've got if you're relying on the hacks like XMLHttpQuest, there's another hack with service workers that you can do. Like they work great on the browser. But if you wanted to run it in like, say, you wanted to run it in Dino or Node, like you have to add polyfills in and you're getting into a real fun situation at that point. And now let's imagine that's Elm got a new release of ElmHttp, which now uses fetch. Yeah. Yeah. Well, now you can't use that hack anymore. So aren't you glad that you can that it uses XMLHttpQuest? Exactly. True. And then actually, if you want to use an Elm platform worker with Node.js, you also have to use a sort of polyfill to make XHR requests work in Node.js with Elm. So this one in a way. Yeah. Otherwise, HTTP requests will fail. Oh. I was like, I mean, if that's the case, and I would have known about this, but because Elm reviews is using Node.js, I'm not making any HTTP requests from the Elm app. So there you go. That's why I didn't know where I forgot about it. Yeah. So if we go back to that weirdness between commands and tasks. So the way that I always understood it is with a tasker, you always have the you always have. A response guaranteed or someone guaranteed. So if you call time that now, then it will respond right away with the date. If you do HTTP request, then it will respond when the response comes back or after a time out. So one way or another, it will always come back unless the user exits the page before end. But the problem with commands is especially. With ports, you don't you don't have the guarantee. So you can't make a request to a port and expect a response to come back. I don't know if it practice it matters much because I don't know if it's a problem if there is a task hanging waiting for a response which will never come back. Yeah. As soon as some things are implemented by the user, you don't have the guarantee that it will come back. But what I can imagine is that. If you do like if we if we know that everyone uses concurrent task and that scheme and everything is set up as it should, then the line between what tasks and commands are useful for slims down a lot. And I don't look don't know if there's actually any reason not to have the same API for both. Well, one thing that I'm going to test doesn't help is sending a request. And not expecting a response. That is the only command is the only one that can do that. Not not with many things, though. But yeah, I think the what you could do in our current task seems like the pattern. If you say you do something like log to the console, like you just return unit an empty tuple. It is kind of like it's sort of a response. But yeah, that's what you do in a lot of things with the Elms tasks API as well. But I don't know if there's a real reason for all of them responding. Right. Maybe it's just like, well, if we have a way to send a task request and not get a response and some of the APIs get weird, maybe that's it's like, oh, well, we just did this entire HTTP chain function. And then at the end, because you use this function now, we won't get a response. And that's unexpected. Or. That's just like, oh, that's a bit weird. Like it's a foot gun is what I mean. Yeah, you're right. It does give you the option to before like there's always the guarantee that tasks will complete. Whereas you're not if you if you can write them yourself, you can break those guarantees. Yeah. Although there's there's always sort of an illusion of guarantees any time you're working with JavaScript, right? Like JavaScript is inherently something that you can't explain. You can't expect to be well behaved. And as anybody who has used it knows, I think. And so, for example, it can throw exceptions like that's just a thing that JavaScript code can do. And so you have to handle that for that. And it may also time out. That's like another class of poorly behaved behavior it could fall into. Right. But you could have a framework like. So you could have a framework like Elm concurrent task, wrap the thing into a try catch. Exactly. And have a timeout. That's right. Yeah, exactly. And then you're sure to have a response. Yes. Will the timeout be long enough? That's a different question. But. Right. And like, do you also want a response that is a timeout? Plenty of cases like I think you would prefer not have the response. But you would prefer to have a timeout or at least it could be possible. If I know. So one thing you mentioned as well is when you're getting back the data from JavaScript, you have a decoder. How does that work? And what happens if the decoding fails or do you need to do? Do you just get a JSON encode value and you need to decode it manually? Exactly. So you get back a JSON encode value. There. Right under the hood, it will be a JSON encode value. The JSON decoder gets run on it. If it succeeds, you get the value back through your. I've called it in the readme like the task flow, which is like your kind of regular error success flow that you do when you're chaining tasks together. If there's some error handlers, there's some functions that you can use to say, like, if you get a decode error. Do something with it. You can either, like, fail with a custom error or you could, like, recover it and lift it into your, like, success type. But if you don't add that in, there's what's called, like, an unexpected error gets returned out through the on response callback, like a message that you provide. And that stops everything. You mean it gets sent back to JavaScript again? No. So this is all Elm side. Like, it's not. Oh, yeah. On the Elm side. Yeah. On the JavaScript side, it's all, like, it doesn't really know anything about it. And the ports are just, like, just send me, like, JSON encode values back through. So it's very those ports are very safe from, like, I think you'd have a very hard time to make them blow up. But then on the Elm side, it's all handling the, like, what do you expect to come back from the task? And if it does something weird, it's, like, stop. Abort. Like, abort the task chain. Like, don't send any more values back out through JavaScript at that point. Mm-hmm. So if somebody wants to wire this into an app, what does that involve kind of getting it set up with the boilerplate? What do you mean? Sorry. Basically, what do you need in your model? What do you need on the JavaScript side to wire it in? What other messages and all that sort of thing do you need? So you need a couple of things. There's in your model, there's what's effectively the task, the tasks model. I've called it a task pool. The name could change. But it's an idea that you can have multiple. Yeah, you could have, like, multiple tasks running at the same time. So it's, like, this is something that's managing all of the state internally of the in-flight tasks. And you might want to swim in it, a task pool. Exactly. Yeah, exactly. Every time. Yeah. So you need the task pool. There's two messages. So there's one that I've called, like, on progress. So that's kind of, like, the internal wiring that's, like, every time you get, like, a response through the port, it's, like, kind of funneling that back through. It's just, like, updating the progress of the task and then giving the next command to be performed. Yeah. And then there's one message which is, like, when everything's done, like, you've got, like, a final result from the task, which is either, like, an unexpected error or the success. Okay. And then there's just some subscriptions to wire all of that up together. So it is a little bit of boilerplate. The advantage of the core task or command is all that, like, wiring is hidden away from you. So you can just fire the task and then give it, like, a callback, like a success callback. Then in, like, a command's case, it's just, like, just fire it and then stuff will happen. Yeah. So, like, you abstract away a few of the details of the JavaScript wiring. So, like, you initialize your Elm application and then you can, you pass in a JavaScript object with all of your port definitions. And your port definitions are, you know, instead of, like, going through that dance of the incoming and outgoing ports and wiring those in and adding subscriptions. And then if you have a subscriptions port that you stop using and then the Elm compiler or the JavaScript code crashes because it's no longer defining this port because it's unused code in your Elm code. And all these, like, rough edges kind of go away. And instead you just are defining these ports, these tasks for Elm concurrent task in a set of key value pairs. You give it a name. And you give it an async function or a synchronous function. And then you can just go back and do it. Okay. So, you can do a synchronous function if you want in JavaScript. And return some data and give it a decoder. So, like, that feels like the right mental model for my brain to interact with that. So, it's a little bit of wiring, but it cleans up a lot of things as well. That's good to hear. It's good to hear the mental model fits. I've tried to abstract as many of those wiring details away. But there is inevitably a bit of. Spoiler plate there. You just can't escape. Yeah. Definitely. In this community, we have long accepted boilerplate as being okay. Like, sure, if we could use less, we would not say no. But, I mean, we're okay with it, right? Yeah. I think for me, it's like as long as I don't have to think too much about the boilerplate, then I'm happy with it. Like, I don't want to be figuring out complicated logic. It's just like. Give me a list of things I need to provide and I'll do it. Actually, like, there's one thing that I'm wondering about. So, in the JavaScript, you set a, you call a function called concurrent task dot register. And you give it your ports, your input ports and your output port. And a list of tasks. So, those are the, that is the object which contains names of the functions and then what to do with the arguments. That were sent. And every time you need to ask to add a new task, you need to both do it in the JavaScript side and do it in the Elm side. And you have to make sure that those are always in sync, right? Have you had any problems so far with them? Like, oh, I forgot to set up one up or I had a typo in one of them. I'm guessing not, but. Well, it. I mean, that inevitably happened. I spent a bit of time wiring it into, like, an existing, like, running application that I've got. And you spell the names wrong. You're like, oh, damn, I missed that one. It gives you, so it will give you back an unexpected error at that point, which is, like, it says, like, I couldn't find, I couldn't find a registered task with this name. So, it's not perfect. You can, like, you can mismatch the names. But it does give you some hints to it. You can say, like, you might have a typo or maybe you forgot to add it into the JavaScript object of, like, your list of tasks. Yes. So, this is, like, one of the things that I would like to do with Elm review. And I put it on pause, but it's something that I'm pretty close to having done, which is having Elm review look at both Elm code and other files. So, you could have, in theory, or it would work with this new feature that I'm working on. That I have somewhere stashed up in a Git branch. You could have a rule that says, I want to look at all the Elm code, but I also want to look at this specific JavaScript file or just all JavaScript files. And it could look at the list of tasks that you define in your call to concurrent register. And then it can compare that with what you have in Elm on the Elm side. And it could figure out, like, oh, I have this. It could figure out, like, oh, well, the one you call on the Elm side does not exist in JavaScript. Or even, like, oh, you have a task on the JavaScript side that is defined, but that is never used in Elm land. So, that's something that I would like to see happen at some point. Just, like, an extra layer of guarantees. Like, you're sure that you have boilerplate, but it's done correctly for sure. I would like to see that. Yeah. Because that case where you, that's an interesting one that you mentioned where you can't know whether it's unused. Even if they do match up, you're not sure if there's no way of figuring out unless you've got something like Elm review to tell you, like, you can get rid of these two. Like, they're never called. The less JavaScript, the better. As long as you put as many rules as possible, then it's fine. Yeah. Just exchange, like, 10 lines of JavaScript for 10 lines of JavaScript. Yeah. And then you can just write it for 10 Elm review rules and, yeah, it will be worth it, right? Exactly. It's great. I'm obviously biased. Something doesn't sound right here. So, this might be less of a 1.0 thing, Andrew, and more of a future thing. But I'm curious, especially as a maintainer of a similar mechanism who's been thinking about these things as well. What are your thoughts on unit testing something like this? I think there are ways to make it doable in a pretty nice way, but it takes a little bit of introducing some abstractions to make that possible. Is that something you've thought about? Very interesting you mentioned that. So, I had the same thought. I was like, I really wish I had a way to try these in isolation and, like, what are they doing in the real world? And I wrote for that runs on CI. And I wrote for that runs on CI. And I wrote for that runs on CI. And then they changed what I'm doing. But I've got, like, a little integration test runner that's, like, a custom. It's kind of a custom program for it. And it's not fancy, but it, like, it does the trick. Like, it's kind of a simple way of saying, like, I expect this value to be a success and you can match on this. You can assert on, like, how long the tasks take. So, I've got one that it was actually very hard to test the batch. It was very hard. It was very hard. It was very hard. So, this batch implementation of, like, running a very large, very large list of tasks, running that in Elm test for some reason, which I haven't figured out, it does not enjoy it. It really, really struggles. But if you're actually running it in a real application, it goes very quickly and doesn't have any problems. If anybody knows why, that would be, I'm all open to suggestions. But having that confidence, being able to actually run the tasks in, like, a test suite was really, really, really hard. I mean, I think that the fact that I had to run all of my tasks in, like, a test suite was really, really valuable to make sure that I didn't mess any of the wiring up or stuff was coming back strange. So, that is a JavaScript test suite, right, which runs the Elm or partial Elm application maybe? So, it's actually just an Elm application. And I've written, it's like a single, like, a platform worker application. And you define, like, a list of tasks in your, like, in your test file. And then you're just passing those to the runner and it runs them. And then you can, there's some Elm functions to match on the results that come back. It's not, it's not packaged up at the moment. But if it's something that folks are interested in, like, if there's a need for it, I could definitely think about extracting it out into something that's a bit more usable in other projects. My initial thought would be to use Elm program test for this. Yeah. And then have a library that makes Elm concur task work for Elm program test. And where you would mock the JavaScript responses or the HTTP responses and so on. And I think that would work quite well. And it would probably work in Elm test. Maybe you should open an issue for that problem you've had if you haven't already. But, yeah, you're right that you're mocking the JavaScript side. So it wouldn't be as reliable as your integration tests. I mean, integration tests are usually more reliable, just not as precise as unit tests. Yeah. That's a very interesting idea. Yeah. It makes me think of, like, Elm program test, obviously. And then Martin Stewart's Elm program test for Landerer where you get responses from the backend, interactions with the backend, with the frontend, and all those kinds of things that you could simulate. And that's, since it's all just Elm code, it's fine. I haven't seen that before, the Elm program test for Landerer. It's worth checking out. There's a great talk where Martin walks through using the tool and shows the visual runner with all the connected clients. It's very cool. And, yeah, it gets really interesting, too, if you had some way. I really like the Elm program test idea as well. And, like, if you had some way to even have a few sort of clients. Yeah. I mean, I think it's a really good idea. And I think it's a really good idea to have a few sort of core implementations for things like local storage where you could, you know, you could imagine if you wanted to get really fancy with this. Oh. You could say, given this initial value in local storage and then let it actually simulate for the setting and getting in local storage. Let it actually simulate that. And now you could have Elm program test. You could even have a way to expand some of these definitions. Where there's, like, a certain set of core web platform primitives that you're able to simulate in a fairly realistic way. So, it's really interesting, man. I think you could go one or two or ten steps further. Like, if you try to simulate HTTP, you just simulate a browser and DNS systems and everything. And, I mean, how hard can it just be to simulate Google.com? Any Elm package? Like, come on. You can do it, Andrew. Exactly. I will say it too after this call. Assuming that there's no free will and that the universe is deterministic and not probabilistic, in theory you could simulate the entire world and all its behaviors in a pure way. Just as a pure function. So, that's if you want to go a few steps further than that. I think that's a good idea. I think that's a good idea. Yeah. I think that's a good idea. I think that's a good idea. Yeah. I think that's a good idea. Yeah. I think that's a good idea. Yeah. I think that's a good idea. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. I feel like you went from giving him, like, oh, this is not for V1 but this could be for V2. And now we're, like, can you just reimplement all of computer science please? PR is welcome. Yeah. So, one thing we haven't asked. you about or maybe not enough like what kind of use cases did you actually use this uh package for like when did you feel the need for something like this yeah that's a good question i so my my initial need for it is about a year and a bit ago i was working on a small medical app and i wrote the back end in elm as an experiment and thought this is great it's lovely just just such a nice pleasant experience and i'd set it up where each hdp endpoint was effectively a task and i was using elm task port then and everything it was working great i was really really happy with the with the the readability and like how easy it was to change stuff but the only problem i found was that these tasks were performed one after the other so all of the subtasks and there were a couple of things that i had to do to get it up and there were a couple of i had it in a cli application as well that was kind of doing some background stuff on like there was some sort of like work processes that was running and it just made me think this would be so much nicer if i could run a lot if i could batch up a lot of these tasks at the same time so and then i did that and now that's all really fast and works exactly the same so well almost exactly the same it's yeah so that's i from my perspective i'd say like these kind of bucket like if you're using kind of a back end as like hdp endpoints this like clear sequence of tasks i've found and like a cli application if you got a cli command like very much like elm pages scripts it's the same kind of idea like those i found those use cases really they fit really nicely there yeah because you want your server to be as stateless as possible ideally or each endpoint but then because you have to chain tasks that go through transform to command now you need to store the state of ongoing requests in the model and yeah that doesn't feel very good you can imagine i definitely tried that to start with and quickly got oh my goodness this is it's just not a nice it's definitely not a nice way of writing for exactly as you say like a stateless a stateless process so it's it's basically the the pattern of a functional core imperative shell which i think is a really nice model and it's like so elm is a really good sort of central processing unit brain for like the the hub of everything and then the messy details as needed you can delegate out to javascript code and it can do whatever you need it to do and you can define an asynchronous code that's you know just going and doing what it does best like firing something off and coming back on the event loop when it's done and not holding up the event loop from continuing while it processes those things so yeah actually like elm is pretty nice for for using for these sorts of uh functional core imperative shell ways of scripting doing some sort of back-end work or or running in your front end So why did you call it Elm concurrent task and not Elm script? Like, just like JavaScript. Elm script. Elm script, yeah. I actually wouldn't be surprised if there was already a project called that. Yeah, probably, actually. Sorry if I don't remember you, author of Elm script. I am curious about the word concurrent. So it's the same thing as parallel. The exact same thing. Yes. Let's open up that can of worms, Andrew, if you'd be so kind. To be honest with you, my initial calling of concurrent task is there's already Elm task parallel as a package out there. One that would be very confusing. Like, which one do I use? I think the other. The reason I add on the side of calling it concurrent was that technically JavaScript is single threaded. So it's a, it's a, it's a, it's a technicality that, um, yeah, is that technically things in that environment cannot run in parallel. Like true parallelism is like on different cores. Let's say, but they're not just parallel, but yeah, I thought the parallel. That educational means doing the same task on different data at the same time and concurrent just like two things are running at the same time, but they might be doing it different things. I feel like. I'm right, but I also feel like people are just gonna shut up. Me? Sorry, listener. is, there's a thread. There's a long thread on own Slack. If it's still there, I could, . Yeah. I feel like it made the same point. And then people corrected. Me there. There's a thread, just one thread, or are there multiple threads on it? I would have to reread it and process all that. It's much of a muchness, I would say. You could call it either. You could call it parallel, concurrent. To be honest with you, I think my main motivator is I didn't want it to clash with the other package that's already out there. Just because it's like if you're reaching for that same name, it's a small thing, but it's likely to cause a bit of confusion. But I think your name is technically correct anyway. Technically. I think it is. So technically correct is the best kind of correct. Yeah, so I've definitely thought about, Andrew and I have discussed whether backend task would be a nice fit for sort of using the concurrent task API under the hood and maybe even making it interoperable. But it's definitely a similar paradigm. And it makes me think also like as a framework, Elm pages would have the ability to give you a little bit of wiring, like all of the boilerplate for adding the right thing to your model and defining. And then also, you know, you could be adding an incoming and outgoing port and. And making sure that there's no typo in the names of. Right. Yeah, exactly. All these little details and right. Adding the right message. They're just like a handful of little things. But Elm pages could build in something like this for you. Because in my opinion, like this is kind of like the ideal paradigm for using these things. So that could be an interesting thing to explore as well. Definitely. It's just I know it's a lot of work actually. Yeah. I think it's really important to be able to actually rewiring all of it in because it's yeah, you've like Elm pages is an expansive API. It's very impressive what it's doing. But I yeah, it would be a lot of work actually wiring it in. So it's only like 50 modules. Yeah, right. I know. And it only took him like a month to release or something. Something like that. Yeah. I think it's a bunch of credits. It all goes to his head. I wonder if there is a way we could do it incrementally like to get some useful feedback on like is it providing the functionality that you need or are there some like weird edge cases? We don't have to like reimplement everything under the hood with that. I'm not sure how to do it. That's the tricky thing when it's like such a backbone of it and there are all these like things that are. Deeply integrated but but it would be so nice if like the community could sort of coalesce on like one nice way to do this and yeah, I definitely think you've hit up hit upon a really elegant formulation of this sort of thing that as a community we've been iterating on for a while in some way shape or form and starting to feel like it's really coming together as a as a paradigm. Andrew. Have you by any chance heard about the Elm package JS? Approach from Lambda. I haven't no. Okay, Dylan. Do you remember much about that? Because I have yeah, I don't but I feel like there's some kind of overlap here where basically Lambda is a full Elm JavaScript. Sorry full Elm framework. So back ends and front end isn't in Elm and there are you can't use ports or at least at some point you can use ports. So there's no way to do that. But I mean, I think there is a whole range of possibilities. Yeah. point you couldn't use sports but in some cases like it's necessary even just to use more recent browser functionalities so mario mario rojic suggested an approach called elm package yes pkg to make that work and i don't remember how it works i wonder whether it works somewhat similarly like there are names that you can call and they are available as elm values or ports or i don't remember maybe something to look at yeah and there was also a similar like a goal i think of giving a standard way of you know if if somebody implements uh something for local storage or copying to clipboard or these basic web platform adapters that you can sort of plug and play with different adapters that the community is maintaining yeah and i mean actually like so elm packages would be a solution for that i guess but i would have to look at it again but you could also potentially do that with elm taskport like you already have a javascript implementation that just adds arbitrary primitives and you make them available in elmland you you have the power to add new primitives like that that is what this framework enables right yeah i've been working on my own set of uh primitives that are useful for something like node i had it was interesting i had um there's an example in the examples repo where oh sorry in the example in the examples in the elm concurrent task repo there's like it's a little kind of pipeline worker so it's like it's like a platform platform worker that will like list all the different tools in the elm taskport listen it's listening out for like sqs messages and then orchestrates uh sorry like a amazon sqs like a queue so it's like this yeah it's this thing like on a queue and then does like a bunch of a bunch of things with various aws services and i had like i was quite surprised at how pleasant it was to write so there was some very minimal bindings to the aws sdk so i mean that's a much bigger example you don't want to go like full hog on that but actually having a set of like simple primitives like you could have for the web platform or you could have with things on node like this seems to work quite nicely and then you just use as dylan you were saying before you just use elm as the brain effectively for orchestrating all of these together so it's interesting having a standard i like the idea of having a some kind of standard way of distributing those two two things alongside like the javascript and the elm module yeah i mean like to me it's it's one of the things that's so at the heart of elm and its design is this this separation of the clean pure elm sandbox from the rest of the world but then to me the the current design of ports as a command subscription sort of thing i know there is this concept of like the actor actor actor actor actor model and and kind of sending something out into the world and not necessarily tying that together to something that comes back in from the outside world and that that is an idea that i think evan did design intentionally to like have it be maybe more fault tolerant like you know like elixir or um erlang's sort of actor model for for how the erlang vm works but i to me the thing that makes that's essential to elm is the purity of the elm sandbox and all bets are off for anything outside of that in the outside world and this preserves that but it makes it much more manageable to work with that and then similarly like if you can install elm packages and they violate those guarantees and expectations that would go against the that core design to me um so if you could install a package and it has kernel code or it has ports defined but at the same time like we've been trying i think in the elm community for a long time to find a nice way to share a definition for how you define something for local storage and these basic things you know how you use the internationalization apis or apis that need to be built into the web platform they're important we need to use them for a lot of different things but i think it's a good idea to as web developers um but how do you share them and the type definitions for them and stuff like that and we don't want ffi bindings that we can just trust but i feel like there is something like elm elm pkg js elm package js mario's specification is trying to address this problem in some way where it's like a shareable definition that has an elm type associated with it and elm concurrent task does seem like it would pair really well with the elm package js but i think it's a good idea to use this as a way to nicely with that, like letting you define these tasks. And if you could like install a task, that could certainly be very interesting. And again, other people may have different opinions on this, but to me, that preserves what's essential about the guarantees of Elm, which is you can't trust the outside world. You're still not trusting it, but you're making it a little more convenient to use it in a way where you're skeptical of it being safe. The only problem is then everyone would have to install Elm concurrent task. It would be a quite critical dependency. True. But it is interesting. Like, yeah, you don't really want to be re-implementing a lot of the very low level stuff over and over again. Right. Exactly. Well, Andrew, if somebody wants to get started playing around with Elm concurrent task and learning more about it, what's a good place to do that? Because if you search on the Elm package website, Elm concurrent task, that should point you in the right direction. The readme has some instructions on installing the Elm package and the NPM package. And yeah, if you've got, please get in touch if anything is unclear, like I'm available on Slack or GitHub. Wonderful. Yeah. And I definitely recommend there are some really nice examples there as well that are worth checking out. So yeah. Wonderful. Well, thanks again for coming on the show, Andrew. It was a pleasure having you. Thanks for having me. And Jeroen. Until next time. Until next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.18, "text": " Hello Jeroen.", "tokens": [50365, 2425, 508, 2032, 268, 13, 50474], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 1, "seek": 0, "start": 2.7800000000000002, "end": 3.3200000000000003, "text": " Hello Dylan.", "tokens": [50504, 2425, 28160, 13, 50531], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 2, "seek": 0, "start": 3.98, "end": 7.16, "text": " So right before recording you just admitted that you didn't have a pun.", "tokens": [50564, 407, 558, 949, 6613, 291, 445, 14920, 300, 291, 994, 380, 362, 257, 4468, 13, 50723], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 3, "seek": 0, "start": 7.76, "end": 8.36, "text": " That's true.", "tokens": [50753, 663, 311, 2074, 13, 50783], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 4, "seek": 0, "start": 8.620000000000001, "end": 10.74, "text": " I think that's going to disappoint a lot of listeners.", "tokens": [50796, 286, 519, 300, 311, 516, 281, 8505, 257, 688, 295, 23274, 13, 50902], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 5, "seek": 0, "start": 11.700000000000001, "end": 16.34, "text": " So what we can do is we can both concurrently try to come up with a good pun.", "tokens": [50950, 407, 437, 321, 393, 360, 307, 321, 393, 1293, 37702, 356, 853, 281, 808, 493, 365, 257, 665, 4468, 13, 51182], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 6, "seek": 0, "start": 17.86, "end": 19.12, "text": " That's going to be our task.", "tokens": [51258, 663, 311, 516, 281, 312, 527, 5633, 13, 51321], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 7, "seek": 0, "start": 19.54, "end": 20.36, "text": " That's the task you've set for us.", "tokens": [51342, 663, 311, 264, 5633, 291, 600, 992, 337, 505, 13, 51383], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 8, "seek": 0, "start": 20.46, "end": 21.76, "text": " I see what you did there.", "tokens": [51388, 286, 536, 437, 291, 630, 456, 13, 51453], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 9, "seek": 0, "start": 21.86, "end": 22.94, "text": " I see what you did there.", "tokens": [51458, 286, 536, 437, 291, 630, 456, 13, 51512], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 10, "seek": 0, "start": 23.62, "end": 24.3, "text": " Did I succeed?", "tokens": [51546, 2589, 286, 7754, 30, 51580], "temperature": 0.0, "avg_logprob": -0.24749090691574482, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.049556005746126175}, {"id": 11, "seek": 2430, "start": 24.3, "end": 30.32, "text": " The task may have failed, but we might have a little bit of failure recovery,", "tokens": [50365, 440, 5633, 815, 362, 7612, 11, 457, 321, 1062, 362, 257, 707, 857, 295, 7763, 8597, 11, 50666], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 12, "seek": 2430, "start": 30.52, "end": 31.94, "text": " some error handling on this.", "tokens": [50676, 512, 6713, 13175, 322, 341, 13, 50747], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 13, "seek": 2430, "start": 32.22, "end": 36.2, "text": " Let's try to end then and move on from this sequentially.", "tokens": [50761, 961, 311, 853, 281, 917, 550, 293, 1286, 322, 490, 341, 5123, 3137, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 14, "seek": 2430, "start": 37.34, "end": 39.620000000000005, "text": " I highly approve of these puns.", "tokens": [51017, 286, 5405, 18827, 295, 613, 4468, 82, 13, 51131], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 15, "seek": 2430, "start": 41.040000000000006, "end": 43.7, "text": " Andrew, please save us from these puns.", "tokens": [51202, 10110, 11, 1767, 3155, 505, 490, 613, 4468, 82, 13, 51335], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 16, "seek": 2430, "start": 44.06, "end": 45.92, "text": " We've got Andrew McMurray joining us.", "tokens": [51353, 492, 600, 658, 10110, 25549, 374, 3458, 5549, 505, 13, 51446], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 17, "seek": 2430, "start": 46.0, "end": 47.36, "text": " Thanks so much for coming on the show, Andrew.", "tokens": [51450, 2561, 370, 709, 337, 1348, 322, 264, 855, 11, 10110, 13, 51518], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 18, "seek": 2430, "start": 47.94, "end": 48.260000000000005, "text": " Hello.", "tokens": [51547, 2425, 13, 51563], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 19, "seek": 2430, "start": 48.6, "end": 49.400000000000006, "text": " Thanks for having me.", "tokens": [51580, 2561, 337, 1419, 385, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 20, "seek": 2430, "start": 49.760000000000005, "end": 50.620000000000005, "text": " Pleasure to have you.", "tokens": [51638, 25658, 2508, 281, 362, 291, 13, 51681], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 21, "seek": 2430, "start": 50.620000000000005, "end": 54.120000000000005, "text": " And today we are talking about your LMS.", "tokens": [51681, 400, 965, 321, 366, 1417, 466, 428, 441, 10288, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1816191126088627, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0005233532865531743}, {"id": 22, "seek": 5430, "start": 54.3, "end": 56.0, "text": " Elm Concurrent Task Package.", "tokens": [50365, 2699, 76, 2656, 49827, 30428, 18466, 609, 13, 50450], "temperature": 0.0, "avg_logprob": -0.1576665629511294, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0005792832234874368}, {"id": 23, "seek": 5430, "start": 56.4, "end": 61.8, "text": " So, well, to start us off, what is Elm Concurrent Task in a nutshell?", "tokens": [50470, 407, 11, 731, 11, 281, 722, 505, 766, 11, 437, 307, 2699, 76, 2656, 49827, 30428, 294, 257, 37711, 30, 50740], "temperature": 0.0, "avg_logprob": -0.1576665629511294, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0005792832234874368}, {"id": 24, "seek": 5430, "start": 62.04, "end": 66.38, "text": " Like, what even is it and what does it do?", "tokens": [50752, 1743, 11, 437, 754, 307, 309, 293, 437, 775, 309, 360, 30, 50969], "temperature": 0.0, "avg_logprob": -0.1576665629511294, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0005792832234874368}, {"id": 25, "seek": 5430, "start": 67.74, "end": 72.56, "text": " Well, it is, I guess you could call it, it's a task-like API.", "tokens": [51037, 1042, 11, 309, 307, 11, 286, 2041, 291, 727, 818, 309, 11, 309, 311, 257, 5633, 12, 4092, 9362, 13, 51278], "temperature": 0.0, "avg_logprob": -0.1576665629511294, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0005792832234874368}, {"id": 26, "seek": 5430, "start": 72.82, "end": 76.82, "text": " So if you've used Elm Cause Task, it's designed to look very similar.", "tokens": [51291, 407, 498, 291, 600, 1143, 2699, 76, 10865, 30428, 11, 309, 311, 4761, 281, 574, 588, 2531, 13, 51491], "temperature": 0.0, "avg_logprob": -0.1576665629511294, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0005792832234874368}, {"id": 27, "seek": 5430, "start": 77.14, "end": 82.72, "text": " The main difference, I'd say, is that Elm Cause Task runs,", "tokens": [51507, 440, 2135, 2649, 11, 286, 1116, 584, 11, 307, 300, 2699, 76, 10865, 30428, 6676, 11, 51786], "temperature": 0.0, "avg_logprob": -0.1576665629511294, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0005792832234874368}, {"id": 28, "seek": 5430, "start": 83.0, "end": 84.28, "text": " it's map to map.", "tokens": [51800, 309, 311, 4471, 281, 4471, 13, 51864], "temperature": 0.0, "avg_logprob": -0.1576665629511294, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0005792832234874368}, {"id": 29, "seek": 8430, "start": 84.36, "end": 88.24, "text": " Three and friends, it runs each of those one after the other.", "tokens": [50368, 6244, 293, 1855, 11, 309, 6676, 1184, 295, 729, 472, 934, 264, 661, 13, 50562], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 30, "seek": 8430, "start": 88.67999999999999, "end": 92.39999999999999, "text": " Whereas the name implies an Elm Concurrent Task, it runs them at the same time.", "tokens": [50584, 13813, 264, 1315, 18779, 364, 2699, 76, 2656, 49827, 30428, 11, 309, 6676, 552, 412, 264, 912, 565, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 31, "seek": 8430, "start": 92.86, "end": 96.0, "text": " So you can run like a tree of these concurrently.", "tokens": [50793, 407, 291, 393, 1190, 411, 257, 4230, 295, 613, 37702, 356, 13, 50950], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 32, "seek": 8430, "start": 96.36, "end": 102.66, "text": " The other thing it does is it also provides a convenient JavaScript FFI.", "tokens": [50968, 440, 661, 551, 309, 775, 307, 309, 611, 6417, 257, 10851, 15778, 479, 38568, 13, 51283], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 33, "seek": 8430, "start": 102.74, "end": 106.78, "text": " So you can call, the idea is that you could call a sequence of JavaScript", "tokens": [51287, 407, 291, 393, 818, 11, 264, 1558, 307, 300, 291, 727, 818, 257, 8310, 295, 15778, 51489], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 34, "seek": 8430, "start": 106.78, "end": 108.9, "text": " functions and chain the results together.", "tokens": [51489, 6828, 293, 5021, 264, 3542, 1214, 13, 51595], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 35, "seek": 8430, "start": 109.32, "end": 112.25999999999999, "text": " This has also been called task ports in the past.", "tokens": [51616, 639, 575, 611, 668, 1219, 5633, 18160, 294, 264, 1791, 13, 51763], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 36, "seek": 8430, "start": 112.66, "end": 113.86, "text": " And if you've.", "tokens": [51783, 400, 498, 291, 600, 13, 51843], "temperature": 0.0, "avg_logprob": -0.2426739717141176, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.01469446625560522}, {"id": 37, "seek": 11386, "start": 113.86, "end": 121.8, "text": " If you've used Elm Pages v3 and you're familiar with backend task, it is basically a standalone", "tokens": [50365, 759, 291, 600, 1143, 2699, 76, 430, 1660, 371, 18, 293, 291, 434, 4963, 365, 38087, 5633, 11, 309, 307, 1936, 257, 37454, 50762], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 38, "seek": 11386, "start": 121.8, "end": 124.36, "text": " or more or less a standalone implementation of that.", "tokens": [50762, 420, 544, 420, 1570, 257, 37454, 11420, 295, 300, 13, 50890], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 39, "seek": 11386, "start": 124.6, "end": 126.1, "text": " So, yeah.", "tokens": [50902, 407, 11, 1338, 13, 50977], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 40, "seek": 11386, "start": 126.36, "end": 126.92, "text": " Right.", "tokens": [50990, 1779, 13, 51018], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 41, "seek": 11386, "start": 127.2, "end": 133.3, "text": " And one that can be run on the front end too, which backend tasks as the name implies, cannot be.", "tokens": [51032, 400, 472, 300, 393, 312, 1190, 322, 264, 1868, 917, 886, 11, 597, 38087, 9608, 382, 264, 1315, 18779, 11, 2644, 312, 13, 51337], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 42, "seek": 11386, "start": 135.76, "end": 136.14, "text": " True.", "tokens": [51460, 13587, 13, 51479], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 43, "seek": 11386, "start": 136.36, "end": 136.74, "text": " Yes.", "tokens": [51490, 1079, 13, 51509], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 44, "seek": 11386, "start": 137.16, "end": 141.22, "text": " But I will add, it was fully inspired by backend task.", "tokens": [51530, 583, 286, 486, 909, 11, 309, 390, 4498, 7547, 538, 38087, 5633, 13, 51733], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 45, "seek": 11386, "start": 141.28, "end": 143.8, "text": " You gave me the idea, Dylan, and I just took him around with it.", "tokens": [51736, 509, 2729, 385, 264, 1558, 11, 28160, 11, 293, 286, 445, 1890, 796, 926, 365, 309, 13, 51862], "temperature": 0.0, "avg_logprob": -0.3771330734779095, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.002082164166495204}, {"id": 46, "seek": 14380, "start": 143.8, "end": 144.04000000000002, "text": " So.", "tokens": [50365, 407, 13, 50377], "temperature": 0.0, "avg_logprob": -0.40424624660558867, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001596919260919094}, {"id": 47, "seek": 14380, "start": 144.04000000000002, "end": 145.04000000000002, "text": " Fantastic.", "tokens": [50377, 21320, 13, 50427], "temperature": 0.0, "avg_logprob": -0.40424624660558867, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001596919260919094}, {"id": 48, "seek": 14380, "start": 145.04000000000002, "end": 146.04000000000002, "text": " I love that.", "tokens": [50427, 286, 959, 300, 13, 50477], "temperature": 0.0, "avg_logprob": -0.40424624660558867, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001596919260919094}, {"id": 49, "seek": 14380, "start": 146.04000000000002, "end": 146.14000000000001, "text": " Yeah.", "tokens": [50477, 865, 13, 50482], "temperature": 0.0, "avg_logprob": -0.40424624660558867, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001596919260919094}, {"id": 50, "seek": 14380, "start": 146.14000000000001, "end": 173.14000000000001, "text": " And I, on the point of running concurrently rather than sequentially, I just want to maybe give people an example to, to motivate that a little bit because, you know, it sounds, it sounds pretty in the weeds, but if you think about it, if you say, if you have two tasks defined for making HTTP requests, you know, to get the user users profile information and get the current weather.", "tokens": [50482, 400, 286, 11, 322, 264, 935, 295, 2614, 37702, 356, 2831, 813, 5123, 3137, 11, 286, 445, 528, 281, 1310, 976, 561, 364, 1365, 281, 11, 281, 28497, 300, 257, 707, 857, 570, 11, 291, 458, 11, 309, 3263, 11, 309, 3263, 1238, 294, 264, 26370, 11, 457, 498, 291, 519, 466, 309, 11, 498, 291, 584, 11, 498, 291, 362, 732, 9608, 7642, 337, 1455, 33283, 12475, 11, 291, 458, 11, 281, 483, 264, 4195, 5022, 7964, 1589, 293, 483, 264, 2190, 5503, 13, 51832], "temperature": 0.0, "avg_logprob": -0.40424624660558867, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001596919260919094}, {"id": 51, "seek": 14380, "start": 173.14000000000001, "end": 173.24, "text": " Yeah.", "tokens": [51832, 865, 13, 51837], "temperature": 0.0, "avg_logprob": -0.40424624660558867, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001596919260919094}, {"id": 52, "seek": 14380, "start": 173.24, "end": 173.5, "text": " Yeah.", "tokens": [51837, 865, 13, 51850], "temperature": 0.0, "avg_logprob": -0.40424624660558867, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001596919260919094}, {"id": 53, "seek": 17380, "start": 173.8, "end": 174.16000000000003, "text": " Right.", "tokens": [50365, 1779, 13, 50383], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 54, "seek": 17380, "start": 175.12, "end": 177.08, "text": " There's some latency for that.", "tokens": [50431, 821, 311, 512, 27043, 337, 300, 13, 50529], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 55, "seek": 17380, "start": 177.08, "end": 183.86, "text": " And, you know, maybe it takes some time to process the request on, on these different respective backends.", "tokens": [50529, 400, 11, 291, 458, 11, 1310, 309, 2516, 512, 565, 281, 1399, 264, 5308, 322, 11, 322, 613, 819, 23649, 646, 2581, 13, 50868], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 56, "seek": 17380, "start": 184.08, "end": 187.78, "text": " If you do test on map two, it's going to run the first one.", "tokens": [50879, 759, 291, 360, 1500, 322, 4471, 732, 11, 309, 311, 516, 281, 1190, 264, 700, 472, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 57, "seek": 17380, "start": 187.98000000000002, "end": 192.78, "text": " And then once that's done, it's going to go and run the second one, which is not great.", "tokens": [51074, 400, 550, 1564, 300, 311, 1096, 11, 309, 311, 516, 281, 352, 293, 1190, 264, 1150, 472, 11, 597, 307, 406, 869, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 58, "seek": 17380, "start": 193.16000000000003, "end": 195.98000000000002, "text": " So it's a serious problem.", "tokens": [51333, 407, 309, 311, 257, 3156, 1154, 13, 51474], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 59, "seek": 17380, "start": 196.28, "end": 199.06, "text": " Why isn't that great in this case?", "tokens": [51489, 1545, 1943, 380, 300, 869, 294, 341, 1389, 30, 51628], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 60, "seek": 17380, "start": 199.82000000000002, "end": 203.78, "text": " Well, because you, you stack the latency of the one on top of the other.", "tokens": [51666, 1042, 11, 570, 291, 11, 291, 8630, 264, 27043, 295, 264, 472, 322, 1192, 295, 264, 661, 13, 51864], "temperature": 0.0, "avg_logprob": -0.15450561337354707, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00032756594009697437}, {"id": 61, "seek": 20380, "start": 203.8, "end": 208.96, "text": " You, you turn it into a, you turn something that doesn't need to be a waterfall, i.e.", "tokens": [50365, 509, 11, 291, 1261, 309, 666, 257, 11, 291, 1261, 746, 300, 1177, 380, 643, 281, 312, 257, 27848, 11, 741, 13, 68, 13, 50623], "temperature": 0.0, "avg_logprob": -0.1360731289304536, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.07153703272342682}, {"id": 62, "seek": 20380, "start": 209.28, "end": 212.74, "text": " stacked one in front of the other finishing before the next one starts.", "tokens": [50639, 28867, 472, 294, 1868, 295, 264, 661, 12693, 949, 264, 958, 472, 3719, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1360731289304536, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.07153703272342682}, {"id": 63, "seek": 20380, "start": 213.24, "end": 216.10000000000002, "text": " You, you could just fire them off both at the same time.", "tokens": [50837, 509, 11, 291, 727, 445, 2610, 552, 766, 1293, 412, 264, 912, 565, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1360731289304536, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.07153703272342682}, {"id": 64, "seek": 20380, "start": 216.18, "end": 225.0, "text": " And that's sort of like the basic structure of, of web browsers is designed and, and of JavaScript of the JavaScript runtime.", "tokens": [50984, 400, 300, 311, 1333, 295, 411, 264, 3875, 3877, 295, 11, 295, 3670, 36069, 307, 4761, 293, 11, 293, 295, 15778, 295, 264, 15778, 34474, 13, 51425], "temperature": 0.0, "avg_logprob": -0.1360731289304536, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.07153703272342682}, {"id": 65, "seek": 20380, "start": 225.34, "end": 229.88000000000002, "text": " It's designed to be really good at doing things asynchronously.", "tokens": [51442, 467, 311, 4761, 281, 312, 534, 665, 412, 884, 721, 42642, 5098, 13, 51669], "temperature": 0.0, "avg_logprob": -0.1360731289304536, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.07153703272342682}, {"id": 66, "seek": 20380, "start": 230.88000000000002, "end": 233.58, "text": " So it's at its best when you can sort of.", "tokens": [51719, 407, 309, 311, 412, 1080, 1151, 562, 291, 393, 1333, 295, 13, 51854], "temperature": 0.0, "avg_logprob": -0.1360731289304536, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.07153703272342682}, {"id": 67, "seek": 23380, "start": 233.8, "end": 239.62, "text": " Fire things off and let them asynchronously come back and continue on.", "tokens": [50365, 7652, 721, 766, 293, 718, 552, 42642, 5098, 808, 646, 293, 2354, 322, 13, 50656], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 68, "seek": 23380, "start": 240.4, "end": 248.08, "text": " It's not good when you hold up the event loop or, or waterfall things too much.", "tokens": [50695, 467, 311, 406, 665, 562, 291, 1797, 493, 264, 2280, 6367, 420, 11, 420, 27848, 721, 886, 709, 13, 51079], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 69, "seek": 23380, "start": 249.08, "end": 249.58, "text": " Yeah.", "tokens": [51129, 865, 13, 51154], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 70, "seek": 23380, "start": 249.68, "end": 253.9, "text": " I actually recently learned that all the map functions are sequential.", "tokens": [51159, 286, 767, 3938, 3264, 300, 439, 264, 4471, 6828, 366, 42881, 13, 51370], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 71, "seek": 23380, "start": 254.3, "end": 257.26, "text": " Like I thought they were concurrent, but no.", "tokens": [51390, 1743, 286, 1194, 436, 645, 37702, 11, 457, 572, 13, 51538], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 72, "seek": 23380, "start": 258.40000000000003, "end": 259.40000000000003, "text": " It's surprising, right?", "tokens": [51595, 467, 311, 8830, 11, 558, 30, 51645], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 73, "seek": 23380, "start": 259.40000000000003, "end": 259.72, "text": " Yeah.", "tokens": [51645, 865, 13, 51661], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 74, "seek": 23380, "start": 259.74, "end": 260.16, "text": " Yeah.", "tokens": [51662, 865, 13, 51683], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 75, "seek": 23380, "start": 260.24, "end": 263.7, "text": " So in the example of the.", "tokens": [51687, 407, 294, 264, 1365, 295, 264, 13, 51860], "temperature": 0.0, "avg_logprob": -0.31704221745972994, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.0007611534674651921}, {"id": 76, "seek": 26380, "start": 263.8, "end": 269.46000000000004, "text": " You make two requests, one to get the time or the weather and one another to get some other information.", "tokens": [50365, 509, 652, 732, 12475, 11, 472, 281, 483, 264, 565, 420, 264, 5503, 293, 472, 1071, 281, 483, 512, 661, 1589, 13, 50648], "temperature": 0.2, "avg_logprob": -0.29883372455561924, "compression_ratio": 1.66015625, "no_speech_prob": 0.0023552526254206896}, {"id": 77, "seek": 26380, "start": 269.88, "end": 270.90000000000003, "text": " Doesn't matter too much.", "tokens": [50669, 12955, 380, 1871, 886, 709, 13, 50720], "temperature": 0.2, "avg_logprob": -0.29883372455561924, "compression_ratio": 1.66015625, "no_speech_prob": 0.0023552526254206896}, {"id": 78, "seek": 26380, "start": 270.90000000000003, "end": 274.32, "text": " It's just slower, which has some impact obviously.", "tokens": [50720, 467, 311, 445, 14009, 11, 597, 575, 512, 2712, 2745, 13, 50891], "temperature": 0.2, "avg_logprob": -0.29883372455561924, "compression_ratio": 1.66015625, "no_speech_prob": 0.0023552526254206896}, {"id": 79, "seek": 26380, "start": 274.62, "end": 291.84000000000003, "text": " But if you like, you make a get request and another request to a post request or put a request to actively do something and you expect those two to, to be done, even if the first one fails, then that changes the behavior.", "tokens": [50906, 583, 498, 291, 411, 11, 291, 652, 257, 483, 5308, 293, 1071, 5308, 281, 257, 2183, 5308, 420, 829, 257, 5308, 281, 13022, 360, 746, 293, 291, 2066, 729, 732, 281, 11, 281, 312, 1096, 11, 754, 498, 264, 700, 472, 18199, 11, 550, 300, 2962, 264, 5223, 13, 51767], "temperature": 0.2, "avg_logprob": -0.29883372455561924, "compression_ratio": 1.66015625, "no_speech_prob": 0.0023552526254206896}, {"id": 80, "seek": 26380, "start": 292.1, "end": 292.56, "text": " Right?", "tokens": [51780, 1779, 30, 51803], "temperature": 0.2, "avg_logprob": -0.29883372455561924, "compression_ratio": 1.66015625, "no_speech_prob": 0.0023552526254206896}, {"id": 81, "seek": 26380, "start": 293.0, "end": 293.44, "text": " True.", "tokens": [51825, 13587, 13, 51847], "temperature": 0.2, "avg_logprob": -0.29883372455561924, "compression_ratio": 1.66015625, "no_speech_prob": 0.0023552526254206896}, {"id": 82, "seek": 26380, "start": 293.44, "end": 293.74, "text": " Although.", "tokens": [51847, 5780, 13, 51862], "temperature": 0.2, "avg_logprob": -0.29883372455561924, "compression_ratio": 1.66015625, "no_speech_prob": 0.0023552526254206896}, {"id": 83, "seek": 29380, "start": 293.8, "end": 296.8, "text": " You could always, um, Hmm.", "tokens": [50365, 509, 727, 1009, 11, 1105, 11, 8239, 13, 50515], "temperature": 0.0, "avg_logprob": -0.2722848256429036, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0007435492007061839}, {"id": 84, "seek": 29380, "start": 297.8, "end": 306.94, "text": " You know, I'm not even sure what task dot map two and in the Elm core task definitions would do, but yeah, maybe it would not fire the second one.", "tokens": [50565, 509, 458, 11, 286, 478, 406, 754, 988, 437, 5633, 5893, 4471, 732, 293, 294, 264, 2699, 76, 4965, 5633, 21988, 576, 360, 11, 457, 1338, 11, 1310, 309, 576, 406, 2610, 264, 1150, 472, 13, 51022], "temperature": 0.0, "avg_logprob": -0.2722848256429036, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0007435492007061839}, {"id": 85, "seek": 29380, "start": 307.06, "end": 307.84000000000003, "text": " You could always do.", "tokens": [51028, 509, 727, 1009, 360, 13, 51067], "temperature": 0.0, "avg_logprob": -0.2722848256429036, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0007435492007061839}, {"id": 86, "seek": 29380, "start": 307.84000000000003, "end": 314.74, "text": " It doesn't, it doesn't like it, it actually does like a task that, and then on the first one, right.", "tokens": [51067, 467, 1177, 380, 11, 309, 1177, 380, 411, 309, 11, 309, 767, 775, 411, 257, 5633, 300, 11, 293, 550, 322, 264, 700, 472, 11, 558, 13, 51412], "temperature": 0.0, "avg_logprob": -0.2722848256429036, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0007435492007061839}, {"id": 87, "seek": 29380, "start": 314.86, "end": 316.12, "text": " And then it runs the second one.", "tokens": [51418, 400, 550, 309, 6676, 264, 1150, 472, 13, 51481], "temperature": 0.0, "avg_logprob": -0.2722848256429036, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0007435492007061839}, {"id": 88, "seek": 29380, "start": 316.3, "end": 322.36, "text": " So nothing gets sent out or triggered before you, before the first one completes.", "tokens": [51490, 407, 1825, 2170, 2279, 484, 420, 21710, 949, 291, 11, 949, 264, 700, 472, 36362, 13, 51793], "temperature": 0.0, "avg_logprob": -0.2722848256429036, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0007435492007061839}, {"id": 89, "seek": 29380, "start": 322.76, "end": 323.54, "text": " Um, we.", "tokens": [51813, 3301, 11, 321, 13, 51852], "temperature": 0.0, "avg_logprob": -0.2722848256429036, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0007435492007061839}, {"id": 90, "seek": 32354, "start": 323.54, "end": 334.88, "text": " We just had a new simplification in the arm review, simplify that if you have like task dot map three and the second element, uh, is clearly something that will fail.", "tokens": [50365, 492, 445, 632, 257, 777, 6883, 3774, 294, 264, 3726, 3131, 11, 20460, 300, 498, 291, 362, 411, 5633, 5893, 4471, 1045, 293, 264, 1150, 4478, 11, 2232, 11, 307, 4448, 746, 300, 486, 3061, 13, 50932], "temperature": 0.0, "avg_logprob": -0.2841110892917799, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0007316127885133028}, {"id": 91, "seek": 32354, "start": 335.24, "end": 342.5, "text": " Then the map three gets changed to a map two and we remove the third or the fourth argument.", "tokens": [50950, 1396, 264, 4471, 1045, 2170, 3105, 281, 257, 4471, 732, 293, 321, 4159, 264, 2636, 420, 264, 6409, 6770, 13, 51313], "temperature": 0.0, "avg_logprob": -0.2841110892917799, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0007316127885133028}, {"id": 92, "seek": 32354, "start": 342.86, "end": 344.96000000000004, "text": " Like if it's a task dot fail or whatever.", "tokens": [51331, 1743, 498, 309, 311, 257, 5633, 5893, 3061, 420, 2035, 13, 51436], "temperature": 0.0, "avg_logprob": -0.2841110892917799, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0007316127885133028}, {"id": 93, "seek": 32354, "start": 345.56, "end": 345.98, "text": " Yeah.", "tokens": [51466, 865, 13, 51487], "temperature": 0.0, "avg_logprob": -0.2841110892917799, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0007316127885133028}, {"id": 94, "seek": 32354, "start": 346.34000000000003, "end": 353.06, "text": " We, which is not going to happen very often, but it's now you will have like something that tells you like, Hey, this is simple.", "tokens": [51505, 492, 11, 597, 307, 406, 516, 281, 1051, 588, 2049, 11, 457, 309, 311, 586, 291, 486, 362, 411, 746, 300, 5112, 291, 411, 11, 1911, 11, 341, 307, 2199, 13, 51841], "temperature": 0.0, "avg_logprob": -0.2841110892917799, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0007316127885133028}, {"id": 95, "seek": 32354, "start": 353.06, "end": 353.3, "text": " For.", "tokens": [51841, 1171, 13, 51853], "temperature": 0.0, "avg_logprob": -0.2841110892917799, "compression_ratio": 1.683206106870229, "no_speech_prob": 0.0007316127885133028}, {"id": 96, "seek": 35354, "start": 353.54, "end": 357.02000000000004, "text": " Um, simplifiable or here's maybe something you didn't expect.", "tokens": [50365, 3301, 11, 6883, 30876, 420, 510, 311, 1310, 746, 291, 994, 380, 2066, 13, 50539], "temperature": 0.0, "avg_logprob": -0.24052585745757482, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0007150922901928425}, {"id": 97, "seek": 35354, "start": 357.48, "end": 358.16, "text": " That is interesting.", "tokens": [50562, 663, 307, 1880, 13, 50596], "temperature": 0.0, "avg_logprob": -0.24052585745757482, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0007150922901928425}, {"id": 98, "seek": 35354, "start": 358.3, "end": 358.46000000000004, "text": " Yeah.", "tokens": [50603, 865, 13, 50611], "temperature": 0.0, "avg_logprob": -0.24052585745757482, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0007150922901928425}, {"id": 99, "seek": 35354, "start": 358.46000000000004, "end": 365.96000000000004, "text": " Cause generally the Elm standard libraries have very intuitive semantics, but this is definitely one that's surprising.", "tokens": [50611, 10865, 5101, 264, 2699, 76, 3832, 15148, 362, 588, 21769, 4361, 45298, 11, 457, 341, 307, 2138, 472, 300, 311, 8830, 13, 50986], "temperature": 0.0, "avg_logprob": -0.24052585745757482, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0007150922901928425}, {"id": 100, "seek": 35354, "start": 366.14000000000004, "end": 379.04, "text": " And it is interesting because it is technically possible to define, and there are packages out there that define a way to use the Elm task package to to do parallel tasks.", "tokens": [50995, 400, 309, 307, 1880, 570, 309, 307, 12120, 1944, 281, 6964, 11, 293, 456, 366, 17401, 484, 456, 300, 6964, 257, 636, 281, 764, 264, 2699, 76, 5633, 7372, 281, 281, 360, 8952, 9608, 13, 51640], "temperature": 0.0, "avg_logprob": -0.24052585745757482, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0007150922901928425}, {"id": 101, "seek": 35354, "start": 379.22, "end": 383.48, "text": " Um, so that is technically possible, but but yeah, so, so.", "tokens": [51649, 3301, 11, 370, 300, 307, 12120, 1944, 11, 457, 457, 1338, 11, 370, 11, 370, 13, 51862], "temperature": 0.0, "avg_logprob": -0.24052585745757482, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0007150922901928425}, {"id": 102, "seek": 38354, "start": 383.54, "end": 390.42, "text": " So the FFI part of Elm concurrent task, I think is also really crucial.", "tokens": [50365, 407, 264, 479, 38568, 644, 295, 2699, 76, 37702, 5633, 11, 286, 519, 307, 611, 534, 11462, 13, 50709], "temperature": 0.0, "avg_logprob": -0.16917945543924967, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.011592802591621876}, {"id": 103, "seek": 38354, "start": 390.5, "end": 397.66, "text": " So, and, and now the idea here you talk about in the read me is, is a hack free of way of doing FFI.", "tokens": [50713, 407, 11, 293, 11, 293, 586, 264, 1558, 510, 291, 751, 466, 294, 264, 1401, 385, 307, 11, 307, 257, 10339, 1737, 295, 636, 295, 884, 479, 38568, 13, 51071], "temperature": 0.0, "avg_logprob": -0.16917945543924967, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.011592802591621876}, {"id": 104, "seek": 38354, "start": 397.94, "end": 408.78000000000003, "text": " I think sometimes in the Elm community, FFI is sort of looked at as a dirty word, but I mean, we need to interact with things outside of the sandbox of Elm in some way.", "tokens": [51085, 286, 519, 2171, 294, 264, 2699, 76, 1768, 11, 479, 38568, 307, 1333, 295, 2956, 412, 382, 257, 9360, 1349, 11, 457, 286, 914, 11, 321, 643, 281, 4648, 365, 721, 2380, 295, 264, 42115, 295, 2699, 76, 294, 512, 636, 13, 51627], "temperature": 0.0, "avg_logprob": -0.16917945543924967, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.011592802591621876}, {"id": 105, "seek": 38354, "start": 408.8, "end": 412.58000000000004, "text": " So it's a question of how do we do that?", "tokens": [51628, 407, 309, 311, 257, 1168, 295, 577, 360, 321, 360, 300, 30, 51817], "temperature": 0.0, "avg_logprob": -0.16917945543924967, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.011592802591621876}, {"id": 106, "seek": 38354, "start": 412.58000000000004, "end": 413.52000000000004, "text": " So, so what does that.", "tokens": [51817, 407, 11, 370, 437, 775, 300, 13, 51864], "temperature": 0.0, "avg_logprob": -0.16917945543924967, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.011592802591621876}, {"id": 107, "seek": 41352, "start": 413.52, "end": 415.52, "text": " Look like an Elm concurrent task?", "tokens": [50365, 2053, 411, 364, 2699, 76, 37702, 5633, 30, 50465], "temperature": 0.0, "avg_logprob": -0.24952275936420149, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0046433801762759686}, {"id": 108, "seek": 41352, "start": 416.08, "end": 417.08, "text": " What does that look like?", "tokens": [50493, 708, 775, 300, 574, 411, 30, 50543], "temperature": 0.0, "avg_logprob": -0.24952275936420149, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0046433801762759686}, {"id": 109, "seek": 41352, "start": 417.12, "end": 426.76, "text": " So very similar to backend task, funnily enough, the idea is you would define a function name in Elm.", "tokens": [50545, 407, 588, 2531, 281, 38087, 5633, 11, 1019, 77, 953, 1547, 11, 264, 1558, 307, 291, 576, 6964, 257, 2445, 1315, 294, 2699, 76, 13, 51027], "temperature": 0.0, "avg_logprob": -0.24952275936420149, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0046433801762759686}, {"id": 110, "seek": 41352, "start": 426.9, "end": 442.2, "text": " So like a string, like the, the string function name, you give it, uh, a decoder effectively, like what you expect to come back from the function, some way of interpreting the errors too, and then an encoded set of arguments.", "tokens": [51034, 407, 411, 257, 6798, 11, 411, 264, 11, 264, 6798, 2445, 1315, 11, 291, 976, 309, 11, 2232, 11, 257, 979, 19866, 8659, 11, 411, 437, 291, 2066, 281, 808, 646, 490, 264, 2445, 11, 512, 636, 295, 37395, 264, 13603, 886, 11, 293, 550, 364, 2058, 12340, 992, 295, 12869, 13, 51799], "temperature": 0.0, "avg_logprob": -0.24952275936420149, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0046433801762759686}, {"id": 111, "seek": 41352, "start": 442.76, "end": 443.12, "text": " And.", "tokens": [51827, 400, 13, 51845], "temperature": 0.0, "avg_logprob": -0.24952275936420149, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0046433801762759686}, {"id": 112, "seek": 44352, "start": 443.52, "end": 458.88, "text": " On the other side, the JavaScript side, there's a, there's a JavaScript package that goes alongside the Elm package that just looks up the stringified name in effectively like an object of functions that you provide on the JavaScript side.", "tokens": [50365, 1282, 264, 661, 1252, 11, 264, 15778, 1252, 11, 456, 311, 257, 11, 456, 311, 257, 15778, 7372, 300, 1709, 12385, 264, 2699, 76, 7372, 300, 445, 1542, 493, 264, 6798, 2587, 1315, 294, 8659, 411, 364, 2657, 295, 6828, 300, 291, 2893, 322, 264, 15778, 1252, 13, 51133], "temperature": 0.0, "avg_logprob": -0.147100635937282, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.13803277909755707}, {"id": 113, "seek": 44352, "start": 459.03999999999996, "end": 465.18, "text": " So it's when I saw it in backend test, cause I was like, that's such a simple, elegant idea.", "tokens": [51141, 407, 309, 311, 562, 286, 1866, 309, 294, 38087, 1500, 11, 3082, 286, 390, 411, 11, 300, 311, 1270, 257, 2199, 11, 21117, 1558, 13, 51448], "temperature": 0.0, "avg_logprob": -0.147100635937282, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.13803277909755707}, {"id": 114, "seek": 44352, "start": 465.35999999999996, "end": 469.78, "text": " And it actually has quite a nice, very nice kind of back and forth between the two packages.", "tokens": [51457, 400, 309, 767, 575, 1596, 257, 1481, 11, 588, 1481, 733, 295, 646, 293, 5220, 1296, 264, 732, 17401, 13, 51678], "temperature": 0.0, "avg_logprob": -0.147100635937282, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.13803277909755707}, {"id": 115, "seek": 44352, "start": 470.2, "end": 472.88, "text": " And I think the main, the main safety.", "tokens": [51699, 400, 286, 519, 264, 2135, 11, 264, 2135, 4514, 13, 51833], "temperature": 0.0, "avg_logprob": -0.147100635937282, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.13803277909755707}, {"id": 116, "seek": 47352, "start": 473.52, "end": 486.52, "text": " That you provide is like library authors is telling Elm when things like the function hasn't been defined or like if it throws an exception or doesn't give you back what you expect to give back.", "tokens": [50365, 663, 291, 2893, 307, 411, 6405, 16552, 307, 3585, 2699, 76, 562, 721, 411, 264, 2445, 6132, 380, 668, 7642, 420, 411, 498, 309, 19251, 364, 11183, 420, 1177, 380, 976, 291, 646, 437, 291, 2066, 281, 976, 646, 13, 51015], "temperature": 0.0, "avg_logprob": -0.1956975688105044, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0014773322036489844}, {"id": 117, "seek": 47352, "start": 486.52, "end": 495.06, "text": " You're sort of baking all of that bits where if you did it in an application without some of the wiring, you could get, you know, unpleasant errors, things that go wrong.", "tokens": [51015, 509, 434, 1333, 295, 12102, 439, 295, 300, 9239, 689, 498, 291, 630, 309, 294, 364, 3861, 1553, 512, 295, 264, 27520, 11, 291, 727, 483, 11, 291, 458, 11, 29128, 13603, 11, 721, 300, 352, 2085, 13, 51442], "temperature": 0.0, "avg_logprob": -0.1956975688105044, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0014773322036489844}, {"id": 118, "seek": 47352, "start": 495.56, "end": 497.47999999999996, "text": " They use your stuff when you're interacting with ports.", "tokens": [51467, 814, 764, 428, 1507, 562, 291, 434, 18017, 365, 18160, 13, 51563], "temperature": 0.0, "avg_logprob": -0.1956975688105044, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0014773322036489844}, {"id": 119, "seek": 47352, "start": 498.47999999999996, "end": 499.0, "text": " Right.", "tokens": [51613, 1779, 13, 51639], "temperature": 0.0, "avg_logprob": -0.1956975688105044, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0014773322036489844}, {"id": 120, "seek": 47352, "start": 499.32, "end": 499.71999999999997, "text": " Right.", "tokens": [51655, 1779, 13, 51675], "temperature": 0.0, "avg_logprob": -0.1956975688105044, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0014773322036489844}, {"id": 121, "seek": 47352, "start": 499.82, "end": 503.47999999999996, "text": " And if you contrast that with defining a.", "tokens": [51680, 400, 498, 291, 8712, 300, 365, 17827, 257, 13, 51863], "temperature": 0.0, "avg_logprob": -0.1956975688105044, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0014773322036489844}, {"id": 122, "seek": 50352, "start": 503.52, "end": 523.86, "text": " Port in, in Elm and running that as a command or a subscription, if you, let's say define an outgoing port to set an item in local storage, I believe if you, if you throw an exception in a port in Elm, it's just throwing that exception in, in JavaScript.", "tokens": [50365, 6733, 294, 11, 294, 2699, 76, 293, 2614, 300, 382, 257, 5622, 420, 257, 17231, 11, 498, 291, 11, 718, 311, 584, 6964, 364, 41565, 2436, 281, 992, 364, 3174, 294, 2654, 6725, 11, 286, 1697, 498, 291, 11, 498, 291, 3507, 364, 11183, 294, 257, 2436, 294, 2699, 76, 11, 309, 311, 445, 10238, 300, 11183, 294, 11, 294, 15778, 13, 51382], "temperature": 0.0, "avg_logprob": -0.14178848266601562, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.001727014547213912}, {"id": 123, "seek": 50352, "start": 523.86, "end": 524.26, "text": " Right.", "tokens": [51382, 1779, 13, 51402], "temperature": 0.0, "avg_logprob": -0.14178848266601562, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.001727014547213912}, {"id": 124, "seek": 50352, "start": 524.28, "end": 528.72, "text": " So it's not, there are no guardrails that are going to prevent that unless you add them in yourself.", "tokens": [51403, 407, 309, 311, 406, 11, 456, 366, 572, 6290, 424, 4174, 300, 366, 516, 281, 4871, 300, 5969, 291, 909, 552, 294, 1803, 13, 51625], "temperature": 0.0, "avg_logprob": -0.14178848266601562, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.001727014547213912}, {"id": 125, "seek": 50352, "start": 529.1, "end": 529.5799999999999, "text": " Exactly.", "tokens": [51644, 7587, 13, 51668], "temperature": 0.0, "avg_logprob": -0.14178848266601562, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.001727014547213912}, {"id": 126, "seek": 50352, "start": 530.0799999999999, "end": 531.72, "text": " So what would be the impact here?", "tokens": [51693, 407, 437, 576, 312, 264, 2712, 510, 30, 51775], "temperature": 0.0, "avg_logprob": -0.14178848266601562, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.001727014547213912}, {"id": 127, "seek": 50352, "start": 531.72, "end": 533.5, "text": " Would it cancel all the other?", "tokens": [51775, 6068, 309, 10373, 439, 264, 661, 30, 51864], "temperature": 0.0, "avg_logprob": -0.14178848266601562, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.001727014547213912}, {"id": 128, "seek": 53352, "start": 533.52, "end": 535.14, "text": " Tasks that are remaining?", "tokens": [50365, 27293, 1694, 300, 366, 8877, 30, 50446], "temperature": 0.0, "avg_logprob": -0.1795921669350014, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.00163489009719342}, {"id": 129, "seek": 53352, "start": 535.48, "end": 540.96, "text": " There's some, so it would, there's, there's actually two kinds of errors that you can get back.", "tokens": [50463, 821, 311, 512, 11, 370, 309, 576, 11, 456, 311, 11, 456, 311, 767, 732, 3685, 295, 13603, 300, 291, 393, 483, 646, 13, 50737], "temperature": 0.0, "avg_logprob": -0.1795921669350014, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.00163489009719342}, {"id": 130, "seek": 53352, "start": 541.02, "end": 552.28, "text": " You'd, you'd receive an error back through via concurrent task, and you can define certain tasks where you expect an exception to be thrown.", "tokens": [50740, 509, 1116, 11, 291, 1116, 4774, 364, 6713, 646, 807, 5766, 37702, 5633, 11, 293, 291, 393, 6964, 1629, 9608, 689, 291, 2066, 364, 11183, 281, 312, 11732, 13, 51303], "temperature": 0.0, "avg_logprob": -0.1795921669350014, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.00163489009719342}, {"id": 131, "seek": 53352, "start": 552.3, "end": 556.56, "text": " You can say like, catch the exception and then lift it into an error type.", "tokens": [51304, 509, 393, 584, 411, 11, 3745, 264, 11183, 293, 550, 5533, 309, 666, 364, 6713, 2010, 13, 51517], "temperature": 0.0, "avg_logprob": -0.1795921669350014, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.00163489009719342}, {"id": 132, "seek": 53352, "start": 557.34, "end": 563.46, "text": " Or you can, there's, there's actually kind of a lower level, which are called unexpected errors where like.", "tokens": [51556, 1610, 291, 393, 11, 456, 311, 11, 456, 311, 767, 733, 295, 257, 3126, 1496, 11, 597, 366, 1219, 13106, 13603, 689, 411, 13, 51862], "temperature": 0.0, "avg_logprob": -0.1795921669350014, "compression_ratio": 1.7729083665338645, "no_speech_prob": 0.00163489009719342}, {"id": 133, "seek": 56352, "start": 563.52, "end": 571.68, "text": " If you're, if you're not expecting this, this JavaScript function to throw an exception and then something happens, it's kind of like it will cancel everything.", "tokens": [50365, 759, 291, 434, 11, 498, 291, 434, 406, 9650, 341, 11, 341, 15778, 2445, 281, 3507, 364, 11183, 293, 550, 746, 2314, 11, 309, 311, 733, 295, 411, 309, 486, 10373, 1203, 13, 50773], "temperature": 0.0, "avg_logprob": -0.20105799865722657, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.0013878644676879048}, {"id": 134, "seek": 56352, "start": 571.68, "end": 575.88, "text": " And then you get a descriptive error message on the other side.", "tokens": [50773, 400, 550, 291, 483, 257, 42585, 6713, 3636, 322, 264, 661, 1252, 13, 50983], "temperature": 0.0, "avg_logprob": -0.20105799865722657, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.0013878644676879048}, {"id": 135, "seek": 56352, "start": 576.28, "end": 582.84, "text": " So yeah, that's, that's I've been, I've, I've tried it out in a couple of, a couple of applications.", "tokens": [51003, 407, 1338, 11, 300, 311, 11, 300, 311, 286, 600, 668, 11, 286, 600, 11, 286, 600, 3031, 309, 484, 294, 257, 1916, 295, 11, 257, 1916, 295, 5821, 13, 51331], "temperature": 0.0, "avg_logprob": -0.20105799865722657, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.0013878644676879048}, {"id": 136, "seek": 56352, "start": 583.28, "end": 585.52, "text": " That's pattern seems to have worked quite nicely.", "tokens": [51353, 663, 311, 5102, 2544, 281, 362, 2732, 1596, 9594, 13, 51465], "temperature": 0.0, "avg_logprob": -0.20105799865722657, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.0013878644676879048}, {"id": 137, "seek": 56352, "start": 585.64, "end": 587.78, "text": " So I'm a little bit confused about something.", "tokens": [51471, 407, 286, 478, 257, 707, 857, 9019, 466, 746, 13, 51578], "temperature": 0.0, "avg_logprob": -0.20105799865722657, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.0013878644676879048}, {"id": 138, "seek": 56352, "start": 588.18, "end": 593.48, "text": " What happens if in one of the ports through concurrent task or just plain.", "tokens": [51598, 708, 2314, 498, 294, 472, 295, 264, 18160, 807, 37702, 5633, 420, 445, 11121, 13, 51863], "temperature": 0.0, "avg_logprob": -0.20105799865722657, "compression_ratio": 1.67003367003367, "no_speech_prob": 0.0013878644676879048}, {"id": 139, "seek": 59348, "start": 593.48, "end": 600.08, "text": " Regular ports on JavaScript side, what happens if there's an exception that is being thrown synchronously?", "tokens": [50365, 45659, 18160, 322, 15778, 1252, 11, 437, 2314, 498, 456, 311, 364, 11183, 300, 307, 885, 11732, 19331, 5098, 30, 50695], "temperature": 0.0, "avg_logprob": -0.2956210545131138, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0024288378190249205}, {"id": 140, "seek": 59348, "start": 600.38, "end": 612.08, "text": " Does that then gets does that then cancel all the other tasks that are related to, or in the same batch of commands being sent?", "tokens": [50710, 4402, 300, 550, 2170, 775, 300, 550, 10373, 439, 264, 661, 9608, 300, 366, 4077, 281, 11, 420, 294, 264, 912, 15245, 295, 16901, 885, 2279, 30, 51295], "temperature": 0.0, "avg_logprob": -0.2956210545131138, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0024288378190249205}, {"id": 141, "seek": 59348, "start": 612.98, "end": 613.72, "text": " Are those canceled?", "tokens": [51340, 2014, 729, 24839, 30, 51377], "temperature": 0.0, "avg_logprob": -0.2956210545131138, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0024288378190249205}, {"id": 142, "seek": 59348, "start": 613.72, "end": 615.32, "text": " Do you know that by any chance?", "tokens": [51377, 1144, 291, 458, 300, 538, 604, 2931, 30, 51457], "temperature": 0.0, "avg_logprob": -0.2956210545131138, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0024288378190249205}, {"id": 143, "seek": 59348, "start": 615.6800000000001, "end": 622.88, "text": " So with the, if you've got a batch, like a batch that's gone out and one of them throws.", "tokens": [51475, 407, 365, 264, 11, 498, 291, 600, 658, 257, 15245, 11, 411, 257, 15245, 300, 311, 2780, 484, 293, 472, 295, 552, 19251, 13, 51835], "temperature": 0.0, "avg_logprob": -0.2956210545131138, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0024288378190249205}, {"id": 144, "seek": 59348, "start": 623.34, "end": 623.44, "text": " Yeah.", "tokens": [51858, 865, 13, 51863], "temperature": 0.0, "avg_logprob": -0.2956210545131138, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0024288378190249205}, {"id": 145, "seek": 62348, "start": 623.64, "end": 626.04, "text": " It's not clever enough to like stop that.", "tokens": [50373, 467, 311, 406, 13494, 1547, 281, 411, 1590, 300, 13, 50493], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 146, "seek": 62348, "start": 626.48, "end": 636.6800000000001, "text": " The, the ones that are in flight at that moment, they've already gone through the port, but any, any, any followup ones from that will be canceled.", "tokens": [50515, 440, 11, 264, 2306, 300, 366, 294, 7018, 412, 300, 1623, 11, 436, 600, 1217, 2780, 807, 264, 2436, 11, 457, 604, 11, 604, 11, 604, 1524, 1010, 2306, 490, 300, 486, 312, 24839, 13, 51025], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 147, "seek": 62348, "start": 637.1, "end": 644.9, "text": " So those that are like, and then, or map on that one will be canceled, but the other batch commands will just go through.", "tokens": [51046, 407, 729, 300, 366, 411, 11, 293, 550, 11, 420, 4471, 322, 300, 472, 486, 312, 24839, 11, 457, 264, 661, 15245, 16901, 486, 445, 352, 807, 13, 51436], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 148, "seek": 62348, "start": 644.9, "end": 645.86, "text": " So yeah.", "tokens": [51436, 407, 1338, 13, 51484], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 149, "seek": 62348, "start": 645.86, "end": 647.9, "text": " Which I think makes sense.", "tokens": [51484, 3013, 286, 519, 1669, 2020, 13, 51586], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 150, "seek": 62348, "start": 647.94, "end": 648.44, "text": " Like that.", "tokens": [51588, 1743, 300, 13, 51613], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 151, "seek": 62348, "start": 648.96, "end": 651.96, "text": " There's no reason that maybe they're disconnected.", "tokens": [51639, 821, 311, 572, 1778, 300, 1310, 436, 434, 29426, 13, 51789], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 152, "seek": 62348, "start": 652.2, "end": 653.34, "text": " They I'm doing.", "tokens": [51801, 814, 286, 478, 884, 13, 51858], "temperature": 0.0, "avg_logprob": -0.22446811805337163, "compression_ratio": 1.7346938775510203, "no_speech_prob": 0.0012242418015375733}, {"id": 153, "seek": 65334, "start": 653.34, "end": 656.02, "text": " One thing and another totally unrelated thing.", "tokens": [50365, 1485, 551, 293, 1071, 3879, 38967, 551, 13, 50499], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 154, "seek": 65334, "start": 656.58, "end": 660.72, "text": " And if the first one fails, then I don't want that to be affected.", "tokens": [50527, 400, 498, 264, 700, 472, 18199, 11, 550, 286, 500, 380, 528, 300, 281, 312, 8028, 13, 50734], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 155, "seek": 65334, "start": 661.1600000000001, "end": 661.38, "text": " Yeah.", "tokens": [50756, 865, 13, 50767], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 156, "seek": 65334, "start": 661.38, "end": 668.2800000000001, "text": " It depends on the case, but I guess having the possibility to say, well, these are connected and these are not connected.", "tokens": [50767, 467, 5946, 322, 264, 1389, 11, 457, 286, 2041, 1419, 264, 7959, 281, 584, 11, 731, 11, 613, 366, 4582, 293, 613, 366, 406, 4582, 13, 51112], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 157, "seek": 65334, "start": 668.86, "end": 671.1800000000001, "text": " Let's have them be separate just in case.", "tokens": [51141, 961, 311, 362, 552, 312, 4994, 445, 294, 1389, 13, 51257], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 158, "seek": 65334, "start": 671.1800000000001, "end": 671.94, "text": " Who knows?", "tokens": [51257, 2102, 3255, 30, 51295], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 159, "seek": 65334, "start": 672.22, "end": 672.48, "text": " Okay.", "tokens": [51309, 1033, 13, 51322], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 160, "seek": 65334, "start": 672.48, "end": 673.94, "text": " So that's, that's interesting.", "tokens": [51322, 407, 300, 311, 11, 300, 311, 1880, 13, 51395], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 161, "seek": 65334, "start": 674.32, "end": 683.2, "text": " If, if it's something that folks are actually interested in this definitely using like timeouts and HTTP, because one of the things that to do to make it compatible.", "tokens": [51414, 759, 11, 498, 309, 311, 746, 300, 4024, 366, 767, 3102, 294, 341, 2138, 1228, 411, 565, 7711, 293, 33283, 11, 570, 472, 295, 264, 721, 300, 281, 360, 281, 652, 309, 18218, 13, 51858], "temperature": 0.0, "avg_logprob": -0.24043226975661058, "compression_ratio": 1.6622073578595318, "no_speech_prob": 0.0008824995602481067}, {"id": 162, "seek": 68320, "start": 683.2, "end": 694.2, "text": " With well fill in the functionality of existing Elm core tasks, because that's, that's one thing that they're not actually compatible with Elm cores tasks, because they're from", "tokens": [50365, 2022, 731, 2836, 294, 264, 14980, 295, 6741, 2699, 76, 4965, 9608, 11, 570, 300, 311, 11, 300, 311, 472, 551, 300, 436, 434, 406, 767, 18218, 365, 2699, 76, 24826, 9608, 11, 570, 436, 434, 490, 50915], "temperature": 0.0, "avg_logprob": -0.27489223814847175, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.001751375151798129}, {"id": 163, "seek": 68320, "start": 694.2, "end": 701.8000000000001, "text": " many different types, but re-implementing HTTP timeouts, there's something called an abort signal in the fetch API.", "tokens": [50915, 867, 819, 3467, 11, 457, 319, 12, 332, 43704, 278, 33283, 565, 7711, 11, 456, 311, 746, 1219, 364, 38117, 6358, 294, 264, 23673, 9362, 13, 51295], "temperature": 0.0, "avg_logprob": -0.27489223814847175, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.001751375151798129}, {"id": 164, "seek": 68320, "start": 702.1, "end": 711.58, "text": " So that may be, that may be a way of like, if people want cancelable, say like you've got a batch of like a hundred tests that have gone out.", "tokens": [51310, 407, 300, 815, 312, 11, 300, 815, 312, 257, 636, 295, 411, 11, 498, 561, 528, 10373, 712, 11, 584, 411, 291, 600, 658, 257, 15245, 295, 411, 257, 3262, 6921, 300, 362, 2780, 484, 13, 51784], "temperature": 0.0, "avg_logprob": -0.27489223814847175, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.001751375151798129}, {"id": 165, "seek": 68320, "start": 711.7800000000001, "end": 712.72, "text": " One of them fails.", "tokens": [51794, 1485, 295, 552, 18199, 13, 51841], "temperature": 0.0, "avg_logprob": -0.27489223814847175, "compression_ratio": 1.6593406593406594, "no_speech_prob": 0.001751375151798129}, {"id": 166, "seek": 71272, "start": 712.72, "end": 715.5400000000001, "text": " And you actually don't want any of the other ones to carry on.", "tokens": [50365, 400, 291, 767, 500, 380, 528, 604, 295, 264, 661, 2306, 281, 3985, 322, 13, 50506], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 167, "seek": 71272, "start": 715.58, "end": 720.34, "text": " You could, then maybe there's a way of sending them an abort signal to say, just stop.", "tokens": [50508, 509, 727, 11, 550, 1310, 456, 311, 257, 636, 295, 7750, 552, 364, 38117, 6358, 281, 584, 11, 445, 1590, 13, 50746], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 168, "seek": 71272, "start": 720.34, "end": 727.3000000000001, "text": " Like, so you kind of reclaim memory or they may be they're expensive HTTP requests that you want to stop.", "tokens": [50746, 1743, 11, 370, 291, 733, 295, 40074, 4675, 420, 436, 815, 312, 436, 434, 5124, 33283, 12475, 300, 291, 528, 281, 1590, 13, 51094], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 169, "seek": 71272, "start": 727.5400000000001, "end": 735.34, "text": " But I, I totally agree with you saying like, it's probably a case by case basis, like necessarily want that on every single task you define.", "tokens": [51106, 583, 286, 11, 286, 3879, 3986, 365, 291, 1566, 411, 11, 309, 311, 1391, 257, 1389, 538, 1389, 5143, 11, 411, 4725, 528, 300, 322, 633, 2167, 5633, 291, 6964, 13, 51496], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 170, "seek": 71272, "start": 736.1800000000001, "end": 736.5400000000001, "text": " Yeah.", "tokens": [51538, 865, 13, 51556], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 171, "seek": 71272, "start": 737.14, "end": 738.7, "text": " So you said something interesting there.", "tokens": [51586, 407, 291, 848, 746, 1880, 456, 13, 51664], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 172, "seek": 71272, "start": 738.7, "end": 741.4, "text": " You had to re-implement HTTP task.", "tokens": [51664, 509, 632, 281, 319, 12, 332, 43704, 33283, 5633, 13, 51799], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 173, "seek": 71272, "start": 741.78, "end": 742.5, "text": " What is that?", "tokens": [51818, 708, 307, 300, 30, 51854], "temperature": 0.0, "avg_logprob": -0.20636168327040344, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.00028232153272256255}, {"id": 174, "seek": 74250, "start": 742.5, "end": 743.88, "text": " What is that all about?", "tokens": [50365, 708, 307, 300, 439, 466, 30, 50434], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 175, "seek": 74250, "start": 744.26, "end": 754.8, "text": " So they, the concurrent task, like task type is not, uh, unfortunately it's like, it's not the same as the Elm core task type.", "tokens": [50453, 407, 436, 11, 264, 37702, 5633, 11, 411, 5633, 2010, 307, 406, 11, 2232, 11, 7015, 309, 311, 411, 11, 309, 311, 406, 264, 912, 382, 264, 2699, 76, 4965, 5633, 2010, 13, 50980], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 176, "seek": 74250, "start": 755.16, "end": 763.98, "text": " So if you wanted to use like time dot now or process dot sleep from Elm cores library, they wouldn't, they wouldn't fit in.", "tokens": [50998, 407, 498, 291, 1415, 281, 764, 411, 565, 5893, 586, 420, 1399, 5893, 2817, 490, 2699, 76, 24826, 6405, 11, 436, 2759, 380, 11, 436, 2759, 380, 3318, 294, 13, 51439], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 177, "seek": 74250, "start": 764.5, "end": 771.82, "text": " So the it's actually, I mean, I took the idea from Dylan as well, who did the same with backend task and that you, you write.", "tokens": [51465, 407, 264, 309, 311, 767, 11, 286, 914, 11, 286, 1890, 264, 1558, 490, 28160, 382, 731, 11, 567, 630, 264, 912, 365, 38087, 5633, 293, 300, 291, 11, 291, 2464, 13, 51831], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 178, "seek": 74250, "start": 771.82, "end": 771.9, "text": " Yeah.", "tokens": [51831, 865, 13, 51835], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 179, "seek": 74250, "start": 771.9, "end": 771.96, "text": " Yeah.", "tokens": [51835, 865, 13, 51838], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 180, "seek": 74250, "start": 771.96, "end": 772.02, "text": " Yeah.", "tokens": [51838, 865, 13, 51841], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 181, "seek": 74250, "start": 772.02, "end": 772.08, "text": " Yeah.", "tokens": [51841, 865, 13, 51844], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 182, "seek": 74250, "start": 772.08, "end": 772.14, "text": " Yeah.", "tokens": [51844, 865, 13, 51847], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 183, "seek": 74250, "start": 772.14, "end": 772.2, "text": " Yeah.", "tokens": [51847, 865, 13, 51850], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 184, "seek": 74250, "start": 772.2, "end": 772.22, "text": " Yeah.", "tokens": [51850, 865, 13, 51851], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 185, "seek": 74250, "start": 772.22, "end": 772.24, "text": " Yeah.", "tokens": [51851, 865, 13, 51852], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 186, "seek": 74250, "start": 772.24, "end": 772.26, "text": " Yeah.", "tokens": [51852, 865, 13, 51853], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 187, "seek": 74250, "start": 772.26, "end": 772.32, "text": " Yeah.", "tokens": [51853, 865, 13, 51856], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 188, "seek": 74250, "start": 772.32, "end": 772.36, "text": " Yeah.", "tokens": [51856, 865, 13, 51858], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 189, "seek": 74250, "start": 772.36, "end": 772.38, "text": " Yeah.", "tokens": [51858, 865, 13, 51859], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 190, "seek": 74250, "start": 772.38, "end": 772.44, "text": " Yeah.", "tokens": [51859, 865, 13, 51862], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 191, "seek": 74250, "start": 772.44, "end": 772.48, "text": " Yeah.", "tokens": [51862, 865, 13, 51864], "temperature": 0.0, "avg_logprob": -0.3013409304331584, "compression_ratio": 2.0166666666666666, "no_speech_prob": 0.0013112036976963282}, {"id": 192, "seek": 77250, "start": 772.5, "end": 772.62, "text": " Yeah.", "tokens": [50365, 865, 13, 50371], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 193, "seek": 77250, "start": 772.62, "end": 772.64, "text": " Yeah.", "tokens": [50371, 865, 13, 50372], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 194, "seek": 77250, "start": 772.7, "end": 772.88, "text": " Yeah.", "tokens": [50375, 865, 13, 50384], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 195, "seek": 77250, "start": 772.88, "end": 772.94, "text": " Yeah.", "tokens": [50384, 865, 13, 50387], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 196, "seek": 77250, "start": 772.94, "end": 773.86, "text": " Yeah.", "tokens": [50387, 865, 13, 50433], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 197, "seek": 77250, "start": 779.44, "end": 780.38, "text": " Yeah.", "tokens": [50712, 865, 13, 50759], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 198, "seek": 77250, "start": 780.38, "end": 780.56, "text": " Yeah.", "tokens": [50759, 865, 13, 50768], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 199, "seek": 77250, "start": 780.56, "end": 781.46, "text": " Yeah.", "tokens": [50768, 865, 13, 50813], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 200, "seek": 77250, "start": 781.46, "end": 781.96, "text": " Yeah.", "tokens": [50813, 865, 13, 50838], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 201, "seek": 77250, "start": 782.54, "end": 782.78, "text": " Yeah.", "tokens": [50867, 865, 13, 50879], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 202, "seek": 77250, "start": 782.78, "end": 782.88, "text": " Yeah.", "tokens": [50879, 865, 13, 50884], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 203, "seek": 77250, "start": 782.88, "end": 784.82, "text": " Yeah.", "tokens": [50884, 865, 13, 50981], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 204, "seek": 77250, "start": 784.82, "end": 785.26, "text": " Yeah.", "tokens": [50981, 865, 13, 51003], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 205, "seek": 77250, "start": 785.26, "end": 785.34, "text": " Yeah.", "tokens": [51003, 865, 13, 51007], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 206, "seek": 77250, "start": 785.34, "end": 786.06, "text": " Yeah.", "tokens": [51007, 865, 13, 51043], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 207, "seek": 77250, "start": 786.24, "end": 786.52, "text": " Yeah.", "tokens": [51052, 865, 13, 51066], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 208, "seek": 77250, "start": 786.52, "end": 786.6, "text": " Yeah.", "tokens": [51066, 865, 13, 51070], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 209, "seek": 77250, "start": 786.6, "end": 786.78, "text": " Yeah.", "tokens": [51070, 865, 13, 51079], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 210, "seek": 77250, "start": 787.4, "end": 787.96, "text": " Yeah.", "tokens": [51110, 865, 13, 51138], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 211, "seek": 77250, "start": 787.96, "end": 788.16, "text": " Yeah.", "tokens": [51138, 865, 13, 51148], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 212, "seek": 77250, "start": 788.16, "end": 788.5, "text": " Yeah.", "tokens": [51148, 865, 13, 51165], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 213, "seek": 77250, "start": 788.5, "end": 788.56, "text": " Yeah.", "tokens": [51165, 865, 13, 51168], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 214, "seek": 77250, "start": 788.56, "end": 793.98, "text": " So at the concurrent task HTTP actually calls the fetch API underneath under the hood,", "tokens": [51168, 407, 412, 264, 37702, 5633, 33283, 767, 5498, 264, 23673, 9362, 7223, 833, 264, 13376, 11, 51439], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 215, "seek": 77250, "start": 794.2, "end": 802.34, "text": " which is actually, which is actually nice because, you know, the built in Elm HTTP package uses,", "tokens": [51450, 597, 307, 767, 11, 597, 307, 767, 1481, 570, 11, 291, 458, 11, 264, 3094, 294, 2699, 76, 33283, 7372, 4960, 11, 51857], "temperature": 0.8, "avg_logprob": -0.9990452853116122, "compression_ratio": 2.25, "no_speech_prob": 0.01614447496831417}, {"id": 216, "seek": 80234, "start": 802.34, "end": 808.4200000000001, "text": " uses XHR requests, which is kind of an outdated standard.", "tokens": [50365, 4960, 1783, 39, 49, 12475, 11, 597, 307, 733, 295, 364, 36313, 3832, 13, 50669], "temperature": 0.0, "avg_logprob": -0.1787747372402234, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.027151959016919136}, {"id": 217, "seek": 80234, "start": 808.88, "end": 812.24, "text": " And there are some performance improvements and modernizations.", "tokens": [50692, 400, 456, 366, 512, 3389, 13797, 293, 4363, 14455, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1787747372402234, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.027151959016919136}, {"id": 218, "seek": 80234, "start": 812.6, "end": 816.6800000000001, "text": " They're subtle, but there are some minor performance improvements", "tokens": [50878, 814, 434, 13743, 11, 457, 456, 366, 512, 6696, 3389, 13797, 51082], "temperature": 0.0, "avg_logprob": -0.1787747372402234, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.027151959016919136}, {"id": 219, "seek": 80234, "start": 816.6800000000001, "end": 818.46, "text": " and general improvements in fetch.", "tokens": [51082, 293, 2674, 13797, 294, 23673, 13, 51171], "temperature": 0.0, "avg_logprob": -0.1787747372402234, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.027151959016919136}, {"id": 220, "seek": 80234, "start": 818.72, "end": 820.26, "text": " So that's kind of a nice feature.", "tokens": [51184, 407, 300, 311, 733, 295, 257, 1481, 4111, 13, 51261], "temperature": 0.0, "avg_logprob": -0.1787747372402234, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.027151959016919136}, {"id": 221, "seek": 80234, "start": 820.44, "end": 825.1800000000001, "text": " Okay, so that means that you had to implement some of the things in Elm", "tokens": [51270, 1033, 11, 370, 300, 1355, 300, 291, 632, 281, 4445, 512, 295, 264, 721, 294, 2699, 76, 51507], "temperature": 0.0, "avg_logprob": -0.1787747372402234, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.027151959016919136}, {"id": 222, "seek": 80234, "start": 825.1800000000001, "end": 827.26, "text": " and some things in JavaScript.", "tokens": [51507, 293, 512, 721, 294, 15778, 13, 51611], "temperature": 0.0, "avg_logprob": -0.1787747372402234, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.027151959016919136}, {"id": 223, "seek": 82726, "start": 827.26, "end": 833.48, "text": " And does that mean that your Elm package does not work without the JavaScript package?", "tokens": [50365, 400, 775, 300, 914, 300, 428, 2699, 76, 7372, 775, 406, 589, 1553, 264, 15778, 7372, 30, 50676], "temperature": 0.0, "avg_logprob": -0.1532910413081103, "compression_ratio": 1.8177966101694916, "no_speech_prob": 9.459140710532665e-05}, {"id": 224, "seek": 82726, "start": 833.74, "end": 840.08, "text": " Like, for instance, I thought that the JavaScript package was if you wanted to enable task ports,", "tokens": [50689, 1743, 11, 337, 5197, 11, 286, 1194, 300, 264, 15778, 7372, 390, 498, 291, 1415, 281, 9528, 5633, 18160, 11, 51006], "temperature": 0.0, "avg_logprob": -0.1532910413081103, "compression_ratio": 1.8177966101694916, "no_speech_prob": 9.459140710532665e-05}, {"id": 225, "seek": 82726, "start": 840.4, "end": 845.38, "text": " but it just doesn't work without setting up that JavaScript library.", "tokens": [51022, 457, 309, 445, 1177, 380, 589, 1553, 3287, 493, 300, 15778, 6405, 13, 51271], "temperature": 0.0, "avg_logprob": -0.1532910413081103, "compression_ratio": 1.8177966101694916, "no_speech_prob": 9.459140710532665e-05}, {"id": 226, "seek": 82726, "start": 845.72, "end": 848.3199999999999, "text": " That's right, yeah, you do need the JavaScript library too,", "tokens": [51288, 663, 311, 558, 11, 1338, 11, 291, 360, 643, 264, 15778, 6405, 886, 11, 51418], "temperature": 0.0, "avg_logprob": -0.1532910413081103, "compression_ratio": 1.8177966101694916, "no_speech_prob": 9.459140710532665e-05}, {"id": 227, "seek": 82726, "start": 848.86, "end": 854.2, "text": " because the JavaScript library will wire in the ports that you provide from Elm.", "tokens": [51445, 570, 264, 15778, 6405, 486, 6234, 294, 264, 18160, 300, 291, 2893, 490, 2699, 76, 13, 51712], "temperature": 0.0, "avg_logprob": -0.1532910413081103, "compression_ratio": 1.8177966101694916, "no_speech_prob": 9.459140710532665e-05}, {"id": 228, "seek": 82726, "start": 854.42, "end": 856.58, "text": " It's actually the thing that runs.", "tokens": [51723, 467, 311, 767, 264, 551, 300, 6676, 13, 51831], "temperature": 0.0, "avg_logprob": -0.1532910413081103, "compression_ratio": 1.8177966101694916, "no_speech_prob": 9.459140710532665e-05}, {"id": 229, "seek": 85726, "start": 857.26, "end": 859.68, "text": " It runs the tasks themselves.", "tokens": [50365, 467, 6676, 264, 9608, 2969, 13, 50486], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 230, "seek": 85726, "start": 860.58, "end": 866.34, "text": " Right, so even if you wanted to run two HTTP requests", "tokens": [50531, 1779, 11, 370, 754, 498, 291, 1415, 281, 1190, 732, 33283, 12475, 50819], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 231, "seek": 85726, "start": 866.34, "end": 871.9399999999999, "text": " or two time.now functions that call out to the core one,", "tokens": [50819, 420, 732, 565, 13, 3785, 6828, 300, 818, 484, 281, 264, 4965, 472, 11, 51099], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 232, "seek": 85726, "start": 872.4399999999999, "end": 874.78, "text": " you wouldn't be able to run them concurrently", "tokens": [51124, 291, 2759, 380, 312, 1075, 281, 1190, 552, 37702, 356, 51241], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 233, "seek": 85726, "start": 874.78, "end": 877.9399999999999, "text": " because you need a new primitive to run them concurrently.", "tokens": [51241, 570, 291, 643, 257, 777, 28540, 281, 1190, 552, 37702, 356, 13, 51399], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 234, "seek": 85726, "start": 878.4, "end": 878.72, "text": " Gotcha.", "tokens": [51422, 42109, 13, 51438], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 235, "seek": 85726, "start": 878.72, "end": 884.2, "text": " Yeah, so it's basically using the exact same mechanism", "tokens": [51438, 865, 11, 370, 309, 311, 1936, 1228, 264, 1900, 912, 7513, 51712], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 236, "seek": 85726, "start": 884.2, "end": 886.72, "text": " for these internal primitives.", "tokens": [51712, 337, 613, 6920, 2886, 38970, 13, 51838], "temperature": 0.0, "avg_logprob": -0.17123903698391385, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0001420160406269133}, {"id": 237, "seek": 88726, "start": 887.26, "end": 892.46, "text": " Primitives of, you know, concurrent task.now and HTTP", "tokens": [50365, 19671, 38970, 295, 11, 291, 458, 11, 37702, 5633, 13, 3785, 293, 33283, 50625], "temperature": 0.0, "avg_logprob": -0.11741956075032552, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.0047547644935548306}, {"id": 238, "seek": 88726, "start": 892.46, "end": 899.76, "text": " as a user would use to define their own custom task definitions in JavaScript.", "tokens": [50625, 382, 257, 4195, 576, 764, 281, 6964, 641, 1065, 2375, 5633, 21988, 294, 15778, 13, 50990], "temperature": 0.0, "avg_logprob": -0.11741956075032552, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.0047547644935548306}, {"id": 239, "seek": 88726, "start": 900.68, "end": 905.98, "text": " Basically, the only difference is that when you call the code in JavaScript", "tokens": [51036, 8537, 11, 264, 787, 2649, 307, 300, 562, 291, 818, 264, 3089, 294, 15778, 51301], "temperature": 0.0, "avg_logprob": -0.11741956075032552, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.0047547644935548306}, {"id": 240, "seek": 88726, "start": 905.98, "end": 911.04, "text": " to set up your additional definitions for concurrent tasks,", "tokens": [51301, 281, 992, 493, 428, 4497, 21988, 337, 37702, 9608, 11, 51554], "temperature": 0.0, "avg_logprob": -0.11741956075032552, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.0047547644935548306}, {"id": 241, "seek": 88726, "start": 911.04, "end": 917.24, "text": " it already comes with some sort of pre-installed sort of core concurrent task definition.", "tokens": [51554, 309, 1217, 1487, 365, 512, 1333, 295, 659, 12, 13911, 8907, 1333, 295, 4965, 37702, 5633, 7123, 13, 51864], "temperature": 0.0, "avg_logprob": -0.11741956075032552, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.0047547644935548306}, {"id": 242, "seek": 91726, "start": 917.26, "end": 919.06, "text": " That's the only difference, I think.", "tokens": [50365, 663, 311, 264, 787, 2649, 11, 286, 519, 13, 50455], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 243, "seek": 91726, "start": 919.24, "end": 924.18, "text": " So how do you actually make those tasks concurrent in JavaScript?", "tokens": [50464, 407, 577, 360, 291, 767, 652, 729, 9608, 37702, 294, 15778, 30, 50711], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 244, "seek": 91726, "start": 925.24, "end": 929.24, "text": " Do you wrap everything in a set immediate callback", "tokens": [50764, 1144, 291, 7019, 1203, 294, 257, 992, 11629, 818, 3207, 50964], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 245, "seek": 91726, "start": 929.24, "end": 931.9399999999999, "text": " or I don't even remember the name of the function?", "tokens": [50964, 420, 286, 500, 380, 754, 1604, 264, 1315, 295, 264, 2445, 30, 51099], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 246, "seek": 91726, "start": 932.7, "end": 935.4, "text": " So it's basically you're sending out,", "tokens": [51137, 407, 309, 311, 1936, 291, 434, 7750, 484, 11, 51272], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 247, "seek": 91726, "start": 935.8, "end": 937.9399999999999, "text": " I call them internally, I call them task definitions.", "tokens": [51292, 286, 818, 552, 19501, 11, 286, 818, 552, 5633, 21988, 13, 51399], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 248, "seek": 91726, "start": 938.3199999999999, "end": 941.04, "text": " So it's like that record of the function name,", "tokens": [51418, 407, 309, 311, 411, 300, 2136, 295, 264, 2445, 1315, 11, 51554], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 249, "seek": 91726, "start": 941.6, "end": 944.1, "text": " the arguments that you're sending.", "tokens": [51582, 264, 12869, 300, 291, 434, 7750, 13, 51707], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 250, "seek": 91726, "start": 944.64, "end": 946.26, "text": " You send those all out in a command.batch.", "tokens": [51734, 509, 2845, 729, 439, 484, 294, 257, 5622, 13, 65, 852, 13, 51815], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 251, "seek": 91726, "start": 946.26, "end": 946.58, "text": " Okay.", "tokens": [51815, 1033, 13, 51831], "temperature": 0.0, "avg_logprob": -0.21757277126969962, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.0013989851577207446}, {"id": 252, "seek": 94726, "start": 947.26, "end": 951.24, "text": " So it's like everything goes out the port whenever it's ready to be executed.", "tokens": [50365, 407, 309, 311, 411, 1203, 1709, 484, 264, 2436, 5699, 309, 311, 1919, 281, 312, 17577, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13921008507410684, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00046549917897209525}, {"id": 253, "seek": 94726, "start": 951.62, "end": 955.58, "text": " And then the JavaScript runner will just run them.", "tokens": [50583, 400, 550, 264, 15778, 24376, 486, 445, 1190, 552, 13, 50781], "temperature": 0.0, "avg_logprob": -0.13921008507410684, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00046549917897209525}, {"id": 254, "seek": 94726, "start": 955.58, "end": 962.78, "text": " And then once it's got the results, it calls an incoming port with an ID.", "tokens": [50781, 400, 550, 1564, 309, 311, 658, 264, 3542, 11, 309, 5498, 364, 22341, 2436, 365, 364, 7348, 13, 51141], "temperature": 0.0, "avg_logprob": -0.13921008507410684, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00046549917897209525}, {"id": 255, "seek": 94726, "start": 963.22, "end": 968.84, "text": " The main challenge of that package and the core bit of it", "tokens": [51163, 440, 2135, 3430, 295, 300, 7372, 293, 264, 4965, 857, 295, 309, 51444], "temperature": 0.0, "avg_logprob": -0.13921008507410684, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00046549917897209525}, {"id": 256, "seek": 94726, "start": 968.84, "end": 971.88, "text": " is actually just attaching IDs to all of the tasks", "tokens": [51444, 307, 767, 445, 39074, 48212, 281, 439, 295, 264, 9608, 51596], "temperature": 0.0, "avg_logprob": -0.13921008507410684, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00046549917897209525}, {"id": 257, "seek": 94726, "start": 971.88, "end": 975.86, "text": " and then sending them back through and being able to reassociate them.", "tokens": [51596, 293, 550, 7750, 552, 646, 807, 293, 885, 1075, 281, 319, 49146, 473, 552, 13, 51795], "temperature": 0.0, "avg_logprob": -0.13921008507410684, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.00046549917897209525}, {"id": 258, "seek": 97586, "start": 975.86, "end": 981.8000000000001, "text": " So because Elm, as it stands, can run lots of things concurrently,", "tokens": [50365, 407, 570, 2699, 76, 11, 382, 309, 7382, 11, 393, 1190, 3195, 295, 721, 37702, 356, 11, 50662], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 259, "seek": 97586, "start": 982.04, "end": 987.54, "text": " like command.batch will let you run as many things as you can imagine", "tokens": [50674, 411, 5622, 13, 65, 852, 486, 718, 291, 1190, 382, 867, 721, 382, 291, 393, 3811, 50949], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 260, "seek": 97586, "start": 987.54, "end": 990.34, "text": " or that the program will take.", "tokens": [50949, 420, 300, 264, 1461, 486, 747, 13, 51089], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 261, "seek": 97586, "start": 990.6800000000001, "end": 994.76, "text": " But the problem is actually being able to associate them with the previous call.", "tokens": [51106, 583, 264, 1154, 307, 767, 885, 1075, 281, 14644, 552, 365, 264, 3894, 818, 13, 51310], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 262, "seek": 97586, "start": 995.3000000000001, "end": 998.94, "text": " So that's all the wiring that this package is doing", "tokens": [51337, 407, 300, 311, 439, 264, 27520, 300, 341, 7372, 307, 884, 51519], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 263, "seek": 97586, "start": 998.94, "end": 1002.0600000000001, "text": " to make it appear like they're just happening one after the other.", "tokens": [51519, 281, 652, 309, 4204, 411, 436, 434, 445, 2737, 472, 934, 264, 661, 13, 51675], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 264, "seek": 97586, "start": 1002.46, "end": 1002.76, "text": " Right.", "tokens": [51695, 1779, 13, 51710], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 265, "seek": 97586, "start": 1002.76, "end": 1005.4, "text": " And as far as set timeout and things like that,", "tokens": [51710, 400, 382, 1400, 382, 992, 565, 346, 293, 721, 411, 300, 11, 51842], "temperature": 0.0, "avg_logprob": -0.14525970252784523, "compression_ratio": 1.6484375, "no_speech_prob": 0.020327657461166382}, {"id": 266, "seek": 100586, "start": 1005.86, "end": 1008.2, "text": " the way that JavaScript works,", "tokens": [50365, 264, 636, 300, 15778, 1985, 11, 50482], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 267, "seek": 100586, "start": 1008.52, "end": 1013.64, "text": " things are asynchronous in the event loop by default.", "tokens": [50498, 721, 366, 49174, 294, 264, 2280, 6367, 538, 7576, 13, 50754], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 268, "seek": 100586, "start": 1013.64, "end": 1015.94, "text": " So when you do fetch,", "tokens": [50754, 407, 562, 291, 360, 23673, 11, 50869], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 269, "seek": 100586, "start": 1016.3000000000001, "end": 1020.64, "text": " you don't have to do anything for that to be an asynchronous task.", "tokens": [50887, 291, 500, 380, 362, 281, 360, 1340, 337, 300, 281, 312, 364, 49174, 5633, 13, 51104], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 270, "seek": 100586, "start": 1021.58, "end": 1025.08, "text": " You just fire it off and it's going to be running", "tokens": [51151, 509, 445, 2610, 309, 766, 293, 309, 311, 516, 281, 312, 2614, 51326], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 271, "seek": 100586, "start": 1025.08, "end": 1027.2, "text": " and then continue the event loop.", "tokens": [51326, 293, 550, 2354, 264, 2280, 6367, 13, 51432], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 272, "seek": 100586, "start": 1027.4, "end": 1030.38, "text": " But the set timeout thing is more of a hack", "tokens": [51442, 583, 264, 992, 565, 346, 551, 307, 544, 295, 257, 10339, 51591], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 273, "seek": 100586, "start": 1030.38, "end": 1032.7, "text": " or the run immediate or whatever.", "tokens": [51591, 420, 264, 1190, 11629, 420, 2035, 13, 51707], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 274, "seek": 100586, "start": 1032.84, "end": 1035.84, "text": " Those hacks are more for getting the event loop,", "tokens": [51714, 3950, 33617, 366, 544, 337, 1242, 264, 2280, 6367, 11, 51864], "temperature": 0.0, "avg_logprob": -0.13253496225597788, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.0015010853530839086}, {"id": 275, "seek": 103586, "start": 1035.86, "end": 1041.24, "text": " to tick, to try to hack the processing of things in the event loop.", "tokens": [50365, 281, 5204, 11, 281, 853, 281, 10339, 264, 9007, 295, 721, 294, 264, 2280, 6367, 13, 50634], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 276, "seek": 103586, "start": 1041.74, "end": 1041.9399999999998, "text": " Yeah.", "tokens": [50659, 865, 13, 50669], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 277, "seek": 103586, "start": 1042.1799999999998, "end": 1047.24, "text": " So the example that I had in mind where things are not async by default", "tokens": [50681, 407, 264, 1365, 300, 286, 632, 294, 1575, 689, 721, 366, 406, 382, 34015, 538, 7576, 50934], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 278, "seek": 103586, "start": 1047.24, "end": 1050.6799999999998, "text": " is the time.now command", "tokens": [50934, 307, 264, 565, 13, 3785, 5622, 51106], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 279, "seek": 103586, "start": 1050.6799999999998, "end": 1053.8999999999999, "text": " because that just calls date.now in JavaScript", "tokens": [51106, 570, 300, 445, 5498, 4002, 13, 3785, 294, 15778, 51267], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 280, "seek": 103586, "start": 1053.8999999999999, "end": 1058.6399999999999, "text": " and sends it back to the port or to the message in Elm.", "tokens": [51267, 293, 14790, 309, 646, 281, 264, 2436, 420, 281, 264, 3636, 294, 2699, 76, 13, 51504], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 281, "seek": 103586, "start": 1059.28, "end": 1062.52, "text": " So that one is not asynchronous by default.", "tokens": [51536, 407, 300, 472, 307, 406, 49174, 538, 7576, 13, 51698], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 282, "seek": 103586, "start": 1062.52, "end": 1065.6399999999999, "text": " So do you wrap that one back into a set timeout?", "tokens": [51698, 407, 360, 291, 7019, 300, 472, 646, 666, 257, 992, 565, 346, 30, 51854], "temperature": 0.0, "avg_logprob": -0.14820968294606626, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0008359244093298912}, {"id": 283, "seek": 106586, "start": 1065.86, "end": 1068.9399999999998, "text": " Or is it fine if it's just synchronous?", "tokens": [50365, 1610, 307, 309, 2489, 498, 309, 311, 445, 44743, 30, 50519], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 284, "seek": 106586, "start": 1069.8999999999999, "end": 1072.58, "text": " It's not currently wrapped in a set timeout.", "tokens": [50567, 467, 311, 406, 4362, 14226, 294, 257, 992, 565, 346, 13, 50701], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 285, "seek": 106586, "start": 1072.58, "end": 1079.76, "text": " I think there's an inevitable very small amount of lag probably", "tokens": [50701, 286, 519, 456, 311, 364, 21451, 588, 1359, 2372, 295, 8953, 1391, 51060], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 286, "seek": 106586, "start": 1079.76, "end": 1081.1, "text": " in that it has to be sent.", "tokens": [51060, 294, 300, 309, 575, 281, 312, 2279, 13, 51127], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 287, "seek": 106586, "start": 1081.5, "end": 1085.9199999999998, "text": " It probably does this just by default of it having to go out of the port", "tokens": [51147, 467, 1391, 775, 341, 445, 538, 7576, 295, 309, 1419, 281, 352, 484, 295, 264, 2436, 51368], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 288, "seek": 106586, "start": 1085.9199999999998, "end": 1087.0, "text": " and then back in.", "tokens": [51368, 293, 550, 646, 294, 13, 51422], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 289, "seek": 106586, "start": 1087.3999999999999, "end": 1089.58, "text": " There'll be a very small...", "tokens": [51442, 821, 603, 312, 257, 588, 1359, 485, 51551], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 290, "seek": 106586, "start": 1089.58, "end": 1095.32, "text": " I don't know actually how I'd measure the lag that it's got on it.", "tokens": [51551, 286, 500, 380, 458, 767, 577, 286, 1116, 3481, 264, 8953, 300, 309, 311, 658, 322, 309, 13, 51838], "temperature": 0.0, "avg_logprob": -0.15706275059626654, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.002303153043612838}, {"id": 291, "seek": 109586, "start": 1095.86, "end": 1099.3999999999999, "text": " I mean, it's also like, we're not...", "tokens": [50365, 286, 914, 11, 309, 311, 611, 411, 11, 321, 434, 406, 485, 50542], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 292, "seek": 109586, "start": 1099.3999999999999, "end": 1104.6399999999999, "text": " I mean, it's not like we're interested in the moment of when the update function returns, right?", "tokens": [50542, 286, 914, 11, 309, 311, 406, 411, 321, 434, 3102, 294, 264, 1623, 295, 562, 264, 5623, 2445, 11247, 11, 558, 30, 50804], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 293, "seek": 109586, "start": 1105.3999999999999, "end": 1107.1599999999999, "text": " It's when you get a message back.", "tokens": [50842, 467, 311, 562, 291, 483, 257, 3636, 646, 13, 50930], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 294, "seek": 109586, "start": 1107.28, "end": 1109.8, "text": " Like you will have some time between the two,", "tokens": [50936, 1743, 291, 486, 362, 512, 565, 1296, 264, 732, 11, 51062], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 295, "seek": 109586, "start": 1110.1599999999999, "end": 1113.82, "text": " between the return of the update and the start of the update function again.", "tokens": [51080, 1296, 264, 2736, 295, 264, 5623, 293, 264, 722, 295, 264, 5623, 2445, 797, 13, 51263], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 296, "seek": 109586, "start": 1114.8799999999999, "end": 1116.6399999999999, "text": " And thankfully, we're not that...", "tokens": [51316, 400, 27352, 11, 321, 434, 406, 300, 485, 51404], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 297, "seek": 109586, "start": 1116.6399999999999, "end": 1119.8999999999999, "text": " We don't need to be that precise in browsers usually, hopefully.", "tokens": [51404, 492, 500, 380, 643, 281, 312, 300, 13600, 294, 36069, 2673, 11, 4696, 13, 51567], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 298, "seek": 109586, "start": 1120.34, "end": 1120.7199999999998, "text": " Yeah.", "tokens": [51589, 865, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 299, "seek": 109586, "start": 1121.3799999999999, "end": 1122.3999999999999, "text": " Not often at least.", "tokens": [51641, 1726, 2049, 412, 1935, 13, 51692], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 300, "seek": 109586, "start": 1122.3999999999999, "end": 1122.74, "text": " True.", "tokens": [51692, 13587, 13, 51709], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 301, "seek": 109586, "start": 1124.2199999999998, "end": 1124.58, "text": " Yeah.", "tokens": [51783, 865, 13, 51801], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 302, "seek": 109586, "start": 1124.62, "end": 1125.3999999999999, "text": " I think there are like...", "tokens": [51803, 286, 519, 456, 366, 411, 485, 51842], "temperature": 0.0, "avg_logprob": -0.2728631857669715, "compression_ratio": 1.755813953488372, "no_speech_prob": 0.0007783021428622305}, {"id": 303, "seek": 112586, "start": 1126.5, "end": 1130.08, "text": " There's asynchronous for like things performing in parallel.", "tokens": [50397, 821, 311, 49174, 337, 411, 721, 10205, 294, 8952, 13, 50576], "temperature": 0.0, "avg_logprob": -0.14521178793399891, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.001911486149765551}, {"id": 304, "seek": 112586, "start": 1130.08, "end": 1135.12, "text": " And then there are like the semantics of the sort of chain of concurrent tasks.", "tokens": [50576, 400, 550, 456, 366, 411, 264, 4361, 45298, 295, 264, 1333, 295, 5021, 295, 37702, 9608, 13, 50828], "temperature": 0.0, "avg_logprob": -0.14521178793399891, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.001911486149765551}, {"id": 305, "seek": 112586, "start": 1135.58, "end": 1140.58, "text": " And if, you know, assuming that it's similar under the hood to Elm Pages,", "tokens": [50851, 400, 498, 11, 291, 458, 11, 11926, 300, 309, 311, 2531, 833, 264, 13376, 281, 2699, 76, 430, 1660, 11, 51101], "temperature": 0.0, "avg_logprob": -0.14521178793399891, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.001911486149765551}, {"id": 306, "seek": 112586, "start": 1140.6999999999998, "end": 1144.56, "text": " it's basically like able to just resolve to check,", "tokens": [51107, 309, 311, 1936, 411, 1075, 281, 445, 14151, 281, 1520, 11, 51300], "temperature": 0.0, "avg_logprob": -0.14521178793399891, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.001911486149765551}, {"id": 307, "seek": 112586, "start": 1144.9599999999998, "end": 1150.9599999999998, "text": " is this task complete and continue the chain of remaining tasks", "tokens": [51320, 307, 341, 5633, 3566, 293, 2354, 264, 5021, 295, 8877, 9608, 51620], "temperature": 0.0, "avg_logprob": -0.14521178793399891, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.001911486149765551}, {"id": 308, "seek": 112586, "start": 1150.9599999999998, "end": 1155.84, "text": " based on which ones needed to be completed before it continues.", "tokens": [51620, 2361, 322, 597, 2306, 2978, 281, 312, 7365, 949, 309, 6515, 13, 51864], "temperature": 0.0, "avg_logprob": -0.14521178793399891, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.001911486149765551}, {"id": 309, "seek": 115586, "start": 1155.86, "end": 1158.9599999999998, "text": " So it's not...", "tokens": [50365, 407, 309, 311, 406, 485, 50520], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 310, "seek": 115586, "start": 1158.9599999999998, "end": 1163.1, "text": " That part of it is not using this sort of JavaScript event loop.", "tokens": [50520, 663, 644, 295, 309, 307, 406, 1228, 341, 1333, 295, 15778, 2280, 6367, 13, 50727], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 311, "seek": 115586, "start": 1163.1799999999998, "end": 1167.3999999999999, "text": " It's just checking whenever it can, if it's ready for the next task.", "tokens": [50731, 467, 311, 445, 8568, 5699, 309, 393, 11, 498, 309, 311, 1919, 337, 264, 958, 5633, 13, 50942], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 312, "seek": 115586, "start": 1168.08, "end": 1169.86, "text": " So for the...", "tokens": [50976, 407, 337, 264, 485, 51065], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 313, "seek": 115586, "start": 1170.6599999999999, "end": 1175.26, "text": " Another comparison between commands and tasks,", "tokens": [51105, 3996, 9660, 1296, 16901, 293, 9608, 11, 51335], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 314, "seek": 115586, "start": 1175.52, "end": 1181.04, "text": " I always find that that's one of these kind of almost rough edges in Elm", "tokens": [51348, 286, 1009, 915, 300, 300, 311, 472, 295, 613, 733, 295, 1920, 5903, 8819, 294, 2699, 76, 51624], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 315, "seek": 115586, "start": 1181.04, "end": 1185.28, "text": " is like there are these two similar but different ways of doing things.", "tokens": [51624, 307, 411, 456, 366, 613, 732, 2531, 457, 819, 2098, 295, 884, 721, 13, 51836], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 316, "seek": 115586, "start": 1185.28, "end": 1185.3, "text": " Yeah.", "tokens": [51836, 865, 13, 51837], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 317, "seek": 115586, "start": 1185.3, "end": 1185.34, "text": " Yeah.", "tokens": [51837, 865, 13, 51839], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 318, "seek": 115586, "start": 1185.34, "end": 1185.3799999999999, "text": " Yeah.", "tokens": [51839, 865, 13, 51841], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 319, "seek": 115586, "start": 1185.3799999999999, "end": 1185.4199999999998, "text": " Yeah.", "tokens": [51841, 865, 13, 51843], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 320, "seek": 115586, "start": 1185.4199999999998, "end": 1185.4599999999998, "text": " Yeah.", "tokens": [51843, 865, 13, 51845], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 321, "seek": 115586, "start": 1185.4599999999998, "end": 1185.52, "text": " Yeah.", "tokens": [51845, 865, 13, 51848], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 322, "seek": 115586, "start": 1185.52, "end": 1185.54, "text": " Yeah.", "tokens": [51848, 865, 13, 51849], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 323, "seek": 115586, "start": 1185.54, "end": 1185.8, "text": " Yeah.", "tokens": [51849, 865, 13, 51862], "temperature": 0.0, "avg_logprob": -0.2581601867675781, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.003735850565135479}, {"id": 324, "seek": 118586, "start": 1185.86, "end": 1187.9599999999998, "text": " There are different ways of executing things.", "tokens": [50365, 821, 366, 819, 2098, 295, 32368, 721, 13, 50470], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 325, "seek": 118586, "start": 1188.62, "end": 1191.1999999999998, "text": " And the semantics are a little bit different.", "tokens": [50503, 400, 264, 4361, 45298, 366, 257, 707, 857, 819, 13, 50632], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 326, "seek": 118586, "start": 1191.58, "end": 1197.02, "text": " Like a, you know, task has an error type and they're chainable", "tokens": [50651, 1743, 257, 11, 291, 458, 11, 5633, 575, 364, 6713, 2010, 293, 436, 434, 5021, 712, 50923], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 327, "seek": 118586, "start": 1197.02, "end": 1202.0, "text": " and they're sequential, but they can technically be done in parallel.", "tokens": [50923, 293, 436, 434, 42881, 11, 457, 436, 393, 12120, 312, 1096, 294, 8952, 13, 51172], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 328, "seek": 118586, "start": 1202.0, "end": 1203.0, "text": " And like...", "tokens": [51172, 400, 411, 485, 51222], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 329, "seek": 118586, "start": 1203.0, "end": 1204.8999999999999, "text": " And you might get a response, which might not...", "tokens": [51222, 400, 291, 1062, 483, 257, 4134, 11, 597, 1062, 406, 485, 51317], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 330, "seek": 118586, "start": 1204.8999999999999, "end": 1206.0, "text": " Which is not the case for...", "tokens": [51317, 3013, 307, 406, 264, 1389, 337, 485, 51372], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 331, "seek": 118586, "start": 1206.0, "end": 1209.36, "text": " Well, you will always get a response, which is not the case for commands.", "tokens": [51372, 1042, 11, 291, 486, 1009, 483, 257, 4134, 11, 597, 307, 406, 264, 1389, 337, 16901, 13, 51540], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 332, "seek": 118586, "start": 1210.04, "end": 1210.56, "text": " Right.", "tokens": [51574, 1779, 13, 51600], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 333, "seek": 118586, "start": 1211.1399999999999, "end": 1212.4399999999998, "text": " That's an interesting one.", "tokens": [51629, 663, 311, 364, 1880, 472, 13, 51694], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 334, "seek": 118586, "start": 1212.84, "end": 1215.54, "text": " I think that's like the main reason why you don't do that.", "tokens": [51714, 286, 519, 300, 311, 411, 264, 2135, 1778, 983, 291, 500, 380, 360, 300, 13, 51849], "temperature": 0.8, "avg_logprob": -0.26774456626490545, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0023960648104548454}, {"id": 335, "seek": 121554, "start": 1215.54, "end": 1221.04, "text": " have what commands and tasks are different and then the other strange thing is like there are", "tokens": [50365, 362, 437, 16901, 293, 9608, 366, 819, 293, 550, 264, 661, 5861, 551, 307, 411, 456, 366, 50640], "temperature": 0.0, "avg_logprob": -0.04969006517659063, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.00734412157908082}, {"id": 336, "seek": 121554, "start": 1221.04, "end": 1230.24, "text": " only a handful of things in elm that you can use to create like a native task like http and some of", "tokens": [50640, 787, 257, 16458, 295, 721, 294, 806, 76, 300, 291, 393, 764, 281, 1884, 411, 257, 8470, 5633, 411, 37428, 293, 512, 295, 51100], "temperature": 0.0, "avg_logprob": -0.04969006517659063, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.00734412157908082}, {"id": 337, "seek": 121554, "start": 1230.24, "end": 1238.34, "text": " the you know dom operations give you tasks but then a lot of things there just is no task version", "tokens": [51100, 264, 291, 458, 3285, 7705, 976, 291, 9608, 457, 550, 257, 688, 295, 721, 456, 445, 307, 572, 5633, 3037, 51505], "temperature": 0.0, "avg_logprob": -0.04969006517659063, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.00734412157908082}, {"id": 338, "seek": 121554, "start": 1238.34, "end": 1244.62, "text": " of you just do a command for example ports like you define a port and it gives you you know a task", "tokens": [51505, 295, 291, 445, 360, 257, 5622, 337, 1365, 18160, 411, 291, 6964, 257, 2436, 293, 309, 2709, 291, 291, 458, 257, 5633, 51819], "temperature": 0.0, "avg_logprob": -0.04969006517659063, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.00734412157908082}, {"id": 339, "seek": 124462, "start": 1244.62, "end": 1251.2199999999998, "text": " for an outgoing port but i think a lot of people say well but i want to be able to", "tokens": [50365, 337, 364, 41565, 2436, 457, 741, 519, 257, 688, 295, 561, 584, 731, 457, 741, 528, 281, 312, 1075, 281, 50695], "temperature": 1.0, "avg_logprob": -0.1480864693136776, "compression_ratio": 1.901098901098901, "no_speech_prob": 8.883992995833978e-05}, {"id": 340, "seek": 124462, "start": 1251.2199999999998, "end": 1259.9599999999998, "text": " have a perform an http request and then take some decoded data from that http request and", "tokens": [50695, 362, 257, 2042, 364, 37428, 5308, 293, 550, 747, 512, 979, 12340, 1412, 490, 300, 37428, 5308, 293, 51132], "temperature": 1.0, "avg_logprob": -0.1480864693136776, "compression_ratio": 1.901098901098901, "no_speech_prob": 8.883992995833978e-05}, {"id": 341, "seek": 124462, "start": 1259.9599999999998, "end": 1266.8799999999999, "text": " write that to local storage and then i want to go and perform an http post request and then i want", "tokens": [51132, 2464, 300, 281, 2654, 6725, 293, 550, 741, 528, 281, 352, 293, 2042, 364, 37428, 2183, 5308, 293, 550, 741, 528, 51478], "temperature": 1.0, "avg_logprob": -0.1480864693136776, "compression_ratio": 1.901098901098901, "no_speech_prob": 8.883992995833978e-05}, {"id": 342, "seek": 124462, "start": 1266.8799999999999, "end": 1274.4799999999998, "text": " to do this so like but you can't chain an http request with a port because", "tokens": [51478, 281, 360, 341, 370, 411, 457, 291, 393, 380, 5021, 364, 37428, 5308, 365, 257, 2436, 570, 51858], "temperature": 1.0, "avg_logprob": -0.1480864693136776, "compression_ratio": 1.901098901098901, "no_speech_prob": 8.883992995833978e-05}, {"id": 343, "seek": 127462, "start": 1274.62, "end": 1275.2399999999998, "text": " It's a command.", "tokens": [50365, 467, 311, 257, 5622, 13, 50396], "temperature": 0.0, "avg_logprob": -0.1393530643903292, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.005218531005084515}, {"id": 344, "seek": 127462, "start": 1275.9599999999998, "end": 1283.4799999999998, "text": " So I've always felt like sometimes when people are complaining about Elm being very limited", "tokens": [50432, 407, 286, 600, 1009, 2762, 411, 2171, 562, 561, 366, 20740, 466, 2699, 76, 885, 588, 5567, 50808], "temperature": 0.0, "avg_logprob": -0.1393530643903292, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.005218531005084515}, {"id": 345, "seek": 127462, "start": 1283.4799999999998, "end": 1287.6, "text": " in the way you can do FFI, that there's no FFI in Elm.", "tokens": [50808, 294, 264, 636, 291, 393, 360, 479, 38568, 11, 300, 456, 311, 572, 479, 38568, 294, 2699, 76, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1393530643903292, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.005218531005084515}, {"id": 346, "seek": 127462, "start": 1287.6599999999999, "end": 1292.6599999999999, "text": " You can only do ports or web components, right?", "tokens": [51017, 509, 393, 787, 360, 18160, 420, 3670, 6677, 11, 558, 30, 51267], "temperature": 0.0, "avg_logprob": -0.1393530643903292, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.005218531005084515}, {"id": 347, "seek": 127462, "start": 1292.6599999999999, "end": 1298.56, "text": " Those are like sort of the two standard ways to do communication with JavaScript and Elm.", "tokens": [51267, 3950, 366, 411, 1333, 295, 264, 732, 3832, 2098, 281, 360, 6101, 365, 15778, 293, 2699, 76, 13, 51562], "temperature": 0.0, "avg_logprob": -0.1393530643903292, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.005218531005084515}, {"id": 348, "seek": 127462, "start": 1298.9599999999998, "end": 1304.12, "text": " Well, I feel like one of the biggest pain points of that, at least for me personally,", "tokens": [51582, 1042, 11, 286, 841, 411, 472, 295, 264, 3880, 1822, 2793, 295, 300, 11, 412, 1935, 337, 385, 5665, 11, 51840], "temperature": 0.0, "avg_logprob": -0.1393530643903292, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.005218531005084515}, {"id": 349, "seek": 130412, "start": 1304.12, "end": 1308.04, "text": " is just that it's very awkward to do that with a command.", "tokens": [50365, 307, 445, 300, 309, 311, 588, 11411, 281, 360, 300, 365, 257, 5622, 13, 50561], "temperature": 0.0, "avg_logprob": -0.09157893342791863, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.000153555694851093}, {"id": 350, "seek": 130412, "start": 1308.4799999999998, "end": 1312.52, "text": " I want a sort of chainable task style way of doing that.", "tokens": [50583, 286, 528, 257, 1333, 295, 5021, 712, 5633, 3758, 636, 295, 884, 300, 13, 50785], "temperature": 0.0, "avg_logprob": -0.09157893342791863, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.000153555694851093}, {"id": 351, "seek": 130412, "start": 1312.7199999999998, "end": 1318.34, "text": " I want to be able to include that in a chain with HTTP requests and other types of tasks.", "tokens": [50795, 286, 528, 281, 312, 1075, 281, 4090, 300, 294, 257, 5021, 365, 33283, 12475, 293, 661, 3467, 295, 9608, 13, 51076], "temperature": 0.0, "avg_logprob": -0.09157893342791863, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.000153555694851093}, {"id": 352, "seek": 130412, "start": 1318.34, "end": 1324.84, "text": " So I think it's very cool that you've sort of built this abstraction that with minimal", "tokens": [51076, 407, 286, 519, 309, 311, 588, 1627, 300, 291, 600, 1333, 295, 3094, 341, 37765, 300, 365, 13206, 51401], "temperature": 0.0, "avg_logprob": -0.09157893342791863, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.000153555694851093}, {"id": 353, "seek": 130412, "start": 1324.84, "end": 1331.3799999999999, "text": " hacks gives you a way to kind of do these different things all in that same chainable", "tokens": [51401, 33617, 2709, 291, 257, 636, 281, 733, 295, 360, 613, 819, 721, 439, 294, 300, 912, 5021, 712, 51728], "temperature": 0.0, "avg_logprob": -0.09157893342791863, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.000153555694851093}, {"id": 354, "seek": 130412, "start": 1331.3799999999999, "end": 1331.8799999999999, "text": " paradigm.", "tokens": [51728, 24709, 13, 51753], "temperature": 0.0, "avg_logprob": -0.09157893342791863, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.000153555694851093}, {"id": 355, "seek": 130412, "start": 1332.34, "end": 1334.1, "text": " With minimal JavaScripts.", "tokens": [51776, 2022, 13206, 15778, 82, 13, 51864], "temperature": 0.0, "avg_logprob": -0.09157893342791863, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.000153555694851093}, {"id": 356, "seek": 133412, "start": 1334.12, "end": 1334.8999999999999, "text": " Set up.", "tokens": [50365, 8928, 493, 13, 50404], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 357, "seek": 133412, "start": 1335.56, "end": 1335.7199999999998, "text": " Right.", "tokens": [50437, 1779, 13, 50445], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 358, "seek": 133412, "start": 1335.82, "end": 1337.9199999999998, "text": " Would you say it's a hack, Andrew?", "tokens": [50450, 6068, 291, 584, 309, 311, 257, 10339, 11, 10110, 30, 50555], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 359, "seek": 133412, "start": 1338.4399999999998, "end": 1343.9399999999998, "text": " The only reason it's not a hack is comparing it to the other ways that you have been done", "tokens": [50581, 440, 787, 1778, 309, 311, 406, 257, 10339, 307, 15763, 309, 281, 264, 661, 2098, 300, 291, 362, 668, 1096, 50856], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 360, "seek": 133412, "start": 1343.9399999999998, "end": 1346.3799999999999, "text": " in the past to make this kind of thing work.", "tokens": [50856, 294, 264, 1791, 281, 652, 341, 733, 295, 551, 589, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 361, "seek": 133412, "start": 1346.7199999999998, "end": 1347.9599999999998, "text": " Those are definitely hacks.", "tokens": [50995, 3950, 366, 2138, 33617, 13, 51057], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 362, "seek": 133412, "start": 1348.6799999999998, "end": 1355.1, "text": " Why don't you explain a couple of those approaches that other tools have used?", "tokens": [51093, 1545, 500, 380, 291, 2903, 257, 1916, 295, 729, 11587, 300, 661, 3873, 362, 1143, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 363, "seek": 133412, "start": 1355.54, "end": 1355.9199999999998, "text": " Sure.", "tokens": [51436, 4894, 13, 51455], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 364, "seek": 133412, "start": 1356.4599999999998, "end": 1362.9799999999998, "text": " There was one that I used in the past on an application that did inspire some of the work", "tokens": [51482, 821, 390, 472, 300, 286, 1143, 294, 264, 1791, 322, 364, 3861, 300, 630, 15638, 512, 295, 264, 589, 51808], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 365, "seek": 133412, "start": 1362.9799999999998, "end": 1364.06, "text": " on this package.", "tokens": [51808, 322, 341, 7372, 13, 51862], "temperature": 0.0, "avg_logprob": -0.1576530996677095, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.00039200641913339496}, {"id": 366, "seek": 136412, "start": 1364.12, "end": 1366.9399999999998, "text": " There's a package called Elm Taskport.", "tokens": [50365, 821, 311, 257, 7372, 1219, 2699, 76, 30428, 2707, 13, 50506], "temperature": 0.0, "avg_logprob": -0.28410837727208293, "compression_ratio": 1.4955752212389382, "no_speech_prob": 0.004706290550529957}, {"id": 367, "seek": 136412, "start": 1367.62, "end": 1370.1399999999999, "text": " And it's a very clever hack.", "tokens": [50540, 400, 309, 311, 257, 588, 13494, 10339, 13, 50666], "temperature": 0.0, "avg_logprob": -0.28410837727208293, "compression_ratio": 1.4955752212389382, "no_speech_prob": 0.004706290550529957}, {"id": 368, "seek": 136412, "start": 1370.82, "end": 1376.0, "text": " And the idea is that you monkey patch XML HTTP requests.", "tokens": [50700, 400, 264, 1558, 307, 300, 291, 17847, 9972, 43484, 33283, 12475, 13, 50959], "temperature": 0.0, "avg_logprob": -0.28410837727208293, "compression_ratio": 1.4955752212389382, "no_speech_prob": 0.004706290550529957}, {"id": 369, "seek": 136412, "start": 1377.1399999999999, "end": 1384.62, "text": " So you add, like, you modify that global object to whenever you're sending off a HTTP request", "tokens": [51016, 407, 291, 909, 11, 411, 11, 291, 16927, 300, 4338, 2657, 281, 5699, 291, 434, 7750, 766, 257, 33283, 5308, 51390], "temperature": 0.0, "avg_logprob": -0.28410837727208293, "compression_ratio": 1.4955752212389382, "no_speech_prob": 0.004706290550529957}, {"id": 370, "seek": 136412, "start": 1384.62, "end": 1388.3999999999999, "text": " with, say, you've got like a special URL scheme.", "tokens": [51390, 365, 11, 584, 11, 291, 600, 658, 411, 257, 2121, 12905, 12232, 13, 51579], "temperature": 0.0, "avg_logprob": -0.28410837727208293, "compression_ratio": 1.4955752212389382, "no_speech_prob": 0.004706290550529957}, {"id": 371, "seek": 136412, "start": 1388.3999999999999, "end": 1391.34, "text": " You might be like Elm custom function.", "tokens": [51579, 509, 1062, 312, 411, 2699, 76, 2375, 2445, 13, 51726], "temperature": 0.0, "avg_logprob": -0.28410837727208293, "compression_ratio": 1.4955752212389382, "no_speech_prob": 0.004706290550529957}, {"id": 372, "seek": 136412, "start": 1391.9799999999998, "end": 1393.7199999999998, "text": " Your monkey packs version will.", "tokens": [51758, 2260, 17847, 19403, 3037, 486, 13, 51845], "temperature": 0.0, "avg_logprob": -0.28410837727208293, "compression_ratio": 1.4955752212389382, "no_speech_prob": 0.004706290550529957}, {"id": 373, "seek": 139412, "start": 1394.12, "end": 1395.1, "text": " Check for that.", "tokens": [50365, 6881, 337, 300, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 374, "seek": 139412, "start": 1395.4799999999998, "end": 1396.4199999999998, "text": " That URL.", "tokens": [50433, 663, 12905, 13, 50480], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 375, "seek": 139412, "start": 1396.76, "end": 1400.1599999999999, "text": " And then you can call custom JavaScript from inside there.", "tokens": [50497, 400, 550, 291, 393, 818, 2375, 15778, 490, 1854, 456, 13, 50667], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 376, "seek": 139412, "start": 1400.54, "end": 1402.8999999999999, "text": " And it works really well.", "tokens": [50686, 400, 309, 1985, 534, 731, 13, 50804], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 377, "seek": 139412, "start": 1403.08, "end": 1403.6999999999998, "text": " It's clever.", "tokens": [50813, 467, 311, 13494, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 378, "seek": 139412, "start": 1403.9599999999998, "end": 1407.32, "text": " But you're modifying, like, global objects, which is a little bit dodgy.", "tokens": [50857, 583, 291, 434, 42626, 11, 411, 11, 4338, 6565, 11, 597, 307, 257, 707, 857, 13886, 1480, 13, 51025], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 379, "seek": 139412, "start": 1407.56, "end": 1411.02, "text": " So I think that's, like, that's without a doubt a hack.", "tokens": [51037, 407, 286, 519, 300, 311, 11, 411, 11, 300, 311, 1553, 257, 6385, 257, 10339, 13, 51210], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 380, "seek": 139412, "start": 1411.6999999999998, "end": 1411.86, "text": " Yeah.", "tokens": [51244, 865, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 381, "seek": 139412, "start": 1412.0, "end": 1413.7399999999998, "text": " Don't let your mother know about this.", "tokens": [51259, 1468, 380, 718, 428, 2895, 458, 466, 341, 13, 51346], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 382, "seek": 139412, "start": 1414.26, "end": 1414.7399999999998, "text": " Exactly.", "tokens": [51372, 7587, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 383, "seek": 139412, "start": 1415.56, "end": 1417.6599999999999, "text": " But what your mother don't know won't hurt her.", "tokens": [51437, 583, 437, 428, 2895, 500, 380, 458, 1582, 380, 4607, 720, 13, 51542], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 384, "seek": 139412, "start": 1418.3, "end": 1418.78, "text": " Well.", "tokens": [51574, 1042, 13, 51598], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 385, "seek": 139412, "start": 1418.78, "end": 1423.8799999999999, "text": " I mean, if you whack your mother in the back of her.", "tokens": [51598, 286, 914, 11, 498, 291, 42877, 428, 2895, 294, 264, 646, 295, 720, 13, 51853], "temperature": 0.0, "avg_logprob": -0.2618149684025691, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.0014546895399689674}, {"id": 386, "seek": 142412, "start": 1424.12, "end": 1424.6599999999999, "text": " Of her head.", "tokens": [50365, 2720, 720, 1378, 13, 50392], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 387, "seek": 142412, "start": 1424.82, "end": 1425.62, "text": " She won't know.", "tokens": [50400, 1240, 1582, 380, 458, 13, 50440], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 388, "seek": 142412, "start": 1426.1999999999998, "end": 1427.3999999999999, "text": " Yes, it will hurt her.", "tokens": [50469, 1079, 11, 309, 486, 4607, 720, 13, 50529], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 389, "seek": 142412, "start": 1429.2199999999998, "end": 1430.4399999999998, "text": " What is that idiom?", "tokens": [50620, 708, 307, 300, 18014, 298, 30, 50681], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 390, "seek": 142412, "start": 1430.58, "end": 1432.34, "text": " Like, that makes no sense.", "tokens": [50688, 1743, 11, 300, 1669, 572, 2020, 13, 50776], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 391, "seek": 142412, "start": 1433.56, "end": 1435.1399999999999, "text": " Don't modify global objects.", "tokens": [50837, 1468, 380, 16927, 4338, 6565, 13, 50916], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 392, "seek": 142412, "start": 1435.4599999999998, "end": 1436.28, "text": " If you can help it.", "tokens": [50932, 759, 291, 393, 854, 309, 13, 50973], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 393, "seek": 142412, "start": 1437.4399999999998, "end": 1439.2399999999998, "text": " Just don't hurt your mother, Dylan.", "tokens": [51031, 1449, 500, 380, 4607, 428, 2895, 11, 28160, 13, 51121], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 394, "seek": 142412, "start": 1444.9799999999998, "end": 1448.3799999999999, "text": " But that is the interesting thing about this approach is that.", "tokens": [51408, 583, 300, 307, 264, 1880, 551, 466, 341, 3109, 307, 300, 13, 51578], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 395, "seek": 142412, "start": 1448.3799999999999, "end": 1451.1, "text": " I mean, your read me.", "tokens": [51578, 286, 914, 11, 428, 1401, 385, 13, 51714], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 396, "seek": 142412, "start": 1451.2199999999998, "end": 1453.58, "text": " Andrew talks about this as FFI.", "tokens": [51720, 10110, 6686, 466, 341, 382, 479, 38568, 13, 51838], "temperature": 0.0, "avg_logprob": -0.29598880767822267, "compression_ratio": 1.4563106796116505, "no_speech_prob": 0.00047265246394090354}, {"id": 397, "seek": 145412, "start": 1454.12, "end": 1455.6799999999998, "text": " And conceptually it is.", "tokens": [50365, 400, 3410, 671, 309, 307, 13, 50443], "temperature": 0.0, "avg_logprob": -0.20698296106778657, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.04020596668124199}, {"id": 398, "seek": 145412, "start": 1456.1, "end": 1463.62, "text": " But it's actually just doing this, like, accepted thing in Elm, which is like passing in some flags.", "tokens": [50464, 583, 309, 311, 767, 445, 884, 341, 11, 411, 11, 9035, 551, 294, 2699, 76, 11, 597, 307, 411, 8437, 294, 512, 23265, 13, 50840], "temperature": 0.0, "avg_logprob": -0.20698296106778657, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.04020596668124199}, {"id": 399, "seek": 145412, "start": 1463.86, "end": 1469.26, "text": " And, you know, it's not using any frowned upon techniques to do these things.", "tokens": [50852, 400, 11, 291, 458, 11, 309, 311, 406, 1228, 604, 431, 14683, 3564, 7512, 281, 360, 613, 721, 13, 51122], "temperature": 0.0, "avg_logprob": -0.20698296106778657, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.04020596668124199}, {"id": 400, "seek": 145412, "start": 1469.26, "end": 1475.2199999999998, "text": " It's just an abstraction that makes it feel a little bit more natural to interrupt with JavaScript.", "tokens": [51122, 467, 311, 445, 364, 37765, 300, 1669, 309, 841, 257, 707, 857, 544, 3303, 281, 12729, 365, 15778, 13, 51420], "temperature": 0.0, "avg_logprob": -0.20698296106778657, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.04020596668124199}, {"id": 401, "seek": 145412, "start": 1475.58, "end": 1476.06, "text": " Exactly.", "tokens": [51438, 7587, 13, 51462], "temperature": 0.0, "avg_logprob": -0.20698296106778657, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.04020596668124199}, {"id": 402, "seek": 145412, "start": 1476.06, "end": 1484.0, "text": " I think the main thing that really I found very interesting with that approach was you can use it.", "tokens": [51462, 286, 519, 264, 2135, 551, 300, 534, 286, 1352, 588, 1880, 365, 300, 3109, 390, 291, 393, 764, 309, 13, 51859], "temperature": 0.0, "avg_logprob": -0.20698296106778657, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.04020596668124199}, {"id": 403, "seek": 148400, "start": 1484.0, "end": 1487.06, "text": " You can use it in Node or in the browser.", "tokens": [50365, 509, 393, 764, 309, 294, 38640, 420, 294, 264, 11185, 13, 50518], "temperature": 0.0, "avg_logprob": -0.27551569837204953, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0010235104709863663}, {"id": 404, "seek": 148400, "start": 1487.54, "end": 1494.32, "text": " Whereas if you've got if you're relying on the hacks like XMLHttpQuest, there's another hack with service workers that you can do.", "tokens": [50542, 13813, 498, 291, 600, 658, 498, 291, 434, 24140, 322, 264, 33617, 411, 43484, 39, 6319, 79, 8547, 377, 11, 456, 311, 1071, 10339, 365, 2643, 5600, 300, 291, 393, 360, 13, 50881], "temperature": 0.0, "avg_logprob": -0.27551569837204953, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0010235104709863663}, {"id": 405, "seek": 148400, "start": 1494.56, "end": 1496.7, "text": " Like they work great on the browser.", "tokens": [50893, 1743, 436, 589, 869, 322, 264, 11185, 13, 51000], "temperature": 0.0, "avg_logprob": -0.27551569837204953, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0010235104709863663}, {"id": 406, "seek": 148400, "start": 1496.72, "end": 1507.62, "text": " But if you wanted to run it in like, say, you wanted to run it in Dino or Node, like you have to add polyfills in and you're getting into a real fun situation at that point.", "tokens": [51001, 583, 498, 291, 1415, 281, 1190, 309, 294, 411, 11, 584, 11, 291, 1415, 281, 1190, 309, 294, 413, 2982, 420, 38640, 11, 411, 291, 362, 281, 909, 6754, 69, 2565, 294, 293, 291, 434, 1242, 666, 257, 957, 1019, 2590, 412, 300, 935, 13, 51546], "temperature": 0.0, "avg_logprob": -0.27551569837204953, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0010235104709863663}, {"id": 407, "seek": 148400, "start": 1507.94, "end": 1513.8, "text": " And now let's imagine that's Elm got a new release of ElmHttp, which now uses fetch.", "tokens": [51562, 400, 586, 718, 311, 3811, 300, 311, 2699, 76, 658, 257, 777, 4374, 295, 2699, 76, 39, 6319, 79, 11, 597, 586, 4960, 23673, 13, 51855], "temperature": 0.0, "avg_logprob": -0.27551569837204953, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0010235104709863663}, {"id": 408, "seek": 148400, "start": 1513.8, "end": 1513.9, "text": " Yeah.", "tokens": [51855, 865, 13, 51860], "temperature": 0.0, "avg_logprob": -0.27551569837204953, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0010235104709863663}, {"id": 409, "seek": 148400, "start": 1513.9, "end": 1513.98, "text": " Yeah.", "tokens": [51860, 865, 13, 51864], "temperature": 0.0, "avg_logprob": -0.27551569837204953, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0010235104709863663}, {"id": 410, "seek": 151398, "start": 1513.98, "end": 1516.68, "text": " Well, now you can't use that hack anymore.", "tokens": [50365, 1042, 11, 586, 291, 393, 380, 764, 300, 10339, 3602, 13, 50500], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 411, "seek": 151398, "start": 1517.1, "end": 1520.46, "text": " So aren't you glad that you can that it uses XMLHttpQuest?", "tokens": [50521, 407, 3212, 380, 291, 5404, 300, 291, 393, 300, 309, 4960, 43484, 39, 6319, 79, 8547, 377, 30, 50689], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 412, "seek": 151398, "start": 1520.76, "end": 1521.3600000000001, "text": " Exactly.", "tokens": [50704, 7587, 13, 50734], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 413, "seek": 151398, "start": 1522.56, "end": 1523.0, "text": " True.", "tokens": [50794, 13587, 13, 50816], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 414, "seek": 151398, "start": 1523.4, "end": 1536.92, "text": " And then actually, if you want to use an Elm platform worker with Node.js, you also have to use a sort of polyfill to make XHR requests work in Node.js with Elm.", "tokens": [50836, 400, 550, 767, 11, 498, 291, 528, 281, 764, 364, 2699, 76, 3663, 11346, 365, 38640, 13, 25530, 11, 291, 611, 362, 281, 764, 257, 1333, 295, 6754, 31072, 281, 652, 1783, 39, 49, 12475, 589, 294, 38640, 13, 25530, 365, 2699, 76, 13, 51512], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 415, "seek": 151398, "start": 1537.48, "end": 1539.1200000000001, "text": " So this one in a way.", "tokens": [51540, 407, 341, 472, 294, 257, 636, 13, 51622], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 416, "seek": 151398, "start": 1539.64, "end": 1540.2, "text": " Yeah.", "tokens": [51648, 865, 13, 51676], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 417, "seek": 151398, "start": 1541.0, "end": 1542.96, "text": " Otherwise, HTTP requests will fail.", "tokens": [51716, 10328, 11, 33283, 12475, 486, 3061, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 418, "seek": 151398, "start": 1543.5, "end": 1543.88, "text": " Oh.", "tokens": [51841, 876, 13, 51860], "temperature": 0.0, "avg_logprob": -0.3268471095872962, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.00014088681200519204}, {"id": 419, "seek": 154388, "start": 1543.88, "end": 1555.8000000000002, "text": " I was like, I mean, if that's the case, and I would have known about this, but because Elm reviews is using Node.js, I'm not making any HTTP requests from the Elm app.", "tokens": [50365, 286, 390, 411, 11, 286, 914, 11, 498, 300, 311, 264, 1389, 11, 293, 286, 576, 362, 2570, 466, 341, 11, 457, 570, 2699, 76, 10229, 307, 1228, 38640, 13, 25530, 11, 286, 478, 406, 1455, 604, 33283, 12475, 490, 264, 2699, 76, 724, 13, 50961], "temperature": 0.0, "avg_logprob": -0.17491050388502039, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0003005221369676292}, {"id": 420, "seek": 154388, "start": 1556.1000000000001, "end": 1556.8000000000002, "text": " So there you go.", "tokens": [50976, 407, 456, 291, 352, 13, 51011], "temperature": 0.0, "avg_logprob": -0.17491050388502039, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0003005221369676292}, {"id": 421, "seek": 154388, "start": 1556.92, "end": 1559.92, "text": " That's why I didn't know where I forgot about it.", "tokens": [51017, 663, 311, 983, 286, 994, 380, 458, 689, 286, 5298, 466, 309, 13, 51167], "temperature": 0.0, "avg_logprob": -0.17491050388502039, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0003005221369676292}, {"id": 422, "seek": 154388, "start": 1560.4, "end": 1560.5400000000002, "text": " Yeah.", "tokens": [51191, 865, 13, 51198], "temperature": 0.0, "avg_logprob": -0.17491050388502039, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0003005221369676292}, {"id": 423, "seek": 154388, "start": 1561.16, "end": 1565.5400000000002, "text": " So if we go back to that weirdness between commands and tasks.", "tokens": [51229, 407, 498, 321, 352, 646, 281, 300, 3657, 1287, 1296, 16901, 293, 9608, 13, 51448], "temperature": 0.0, "avg_logprob": -0.17491050388502039, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0003005221369676292}, {"id": 424, "seek": 154388, "start": 1565.5400000000002, "end": 1573.2600000000002, "text": " So the way that I always understood it is with a tasker, you always have the you always have.", "tokens": [51448, 407, 264, 636, 300, 286, 1009, 7320, 309, 307, 365, 257, 5633, 260, 11, 291, 1009, 362, 264, 291, 1009, 362, 13, 51834], "temperature": 0.0, "avg_logprob": -0.17491050388502039, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0003005221369676292}, {"id": 425, "seek": 157326, "start": 1573.26, "end": 1577.58, "text": " A response guaranteed or someone guaranteed.", "tokens": [50365, 316, 4134, 18031, 420, 1580, 18031, 13, 50581], "temperature": 0.0, "avg_logprob": -0.2733879986931296, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002376867923885584}, {"id": 426, "seek": 157326, "start": 1578.18, "end": 1585.0, "text": " So if you call time that now, then it will respond right away with the date.", "tokens": [50611, 407, 498, 291, 818, 565, 300, 586, 11, 550, 309, 486, 4196, 558, 1314, 365, 264, 4002, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2733879986931296, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002376867923885584}, {"id": 427, "seek": 157326, "start": 1585.74, "end": 1593.6, "text": " If you do HTTP request, then it will respond when the response comes back or after a time out.", "tokens": [50989, 759, 291, 360, 33283, 5308, 11, 550, 309, 486, 4196, 562, 264, 4134, 1487, 646, 420, 934, 257, 565, 484, 13, 51382], "temperature": 0.0, "avg_logprob": -0.2733879986931296, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002376867923885584}, {"id": 428, "seek": 157326, "start": 1594.02, "end": 1600.74, "text": " So one way or another, it will always come back unless the user exits the page before end.", "tokens": [51403, 407, 472, 636, 420, 1071, 11, 309, 486, 1009, 808, 646, 5969, 264, 4195, 44183, 264, 3028, 949, 917, 13, 51739], "temperature": 0.0, "avg_logprob": -0.2733879986931296, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002376867923885584}, {"id": 429, "seek": 157326, "start": 1601.28, "end": 1603.24, "text": " But the problem with commands is especially.", "tokens": [51766, 583, 264, 1154, 365, 16901, 307, 2318, 13, 51864], "temperature": 0.0, "avg_logprob": -0.2733879986931296, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.002376867923885584}, {"id": 430, "seek": 160326, "start": 1603.26, "end": 1605.78, "text": " With ports, you don't you don't have the guarantee.", "tokens": [50365, 2022, 18160, 11, 291, 500, 380, 291, 500, 380, 362, 264, 10815, 13, 50491], "temperature": 0.0, "avg_logprob": -0.1969946643762421, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0007091800216585398}, {"id": 431, "seek": 160326, "start": 1606.02, "end": 1611.54, "text": " So you can't make a request to a port and expect a response to come back.", "tokens": [50503, 407, 291, 393, 380, 652, 257, 5308, 281, 257, 2436, 293, 2066, 257, 4134, 281, 808, 646, 13, 50779], "temperature": 0.0, "avg_logprob": -0.1969946643762421, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0007091800216585398}, {"id": 432, "seek": 160326, "start": 1611.96, "end": 1623.56, "text": " I don't know if it practice it matters much because I don't know if it's a problem if there is a task hanging waiting for a response which will never come back.", "tokens": [50800, 286, 500, 380, 458, 498, 309, 3124, 309, 7001, 709, 570, 286, 500, 380, 458, 498, 309, 311, 257, 1154, 498, 456, 307, 257, 5633, 8345, 3806, 337, 257, 4134, 597, 486, 1128, 808, 646, 13, 51380], "temperature": 0.0, "avg_logprob": -0.1969946643762421, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0007091800216585398}, {"id": 433, "seek": 160326, "start": 1624.08, "end": 1624.3799999999999, "text": " Yeah.", "tokens": [51406, 865, 13, 51421], "temperature": 0.0, "avg_logprob": -0.1969946643762421, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0007091800216585398}, {"id": 434, "seek": 160326, "start": 1624.3799999999999, "end": 1630.16, "text": " As soon as some things are implemented by the user, you don't have the guarantee that it will come back.", "tokens": [51421, 1018, 2321, 382, 512, 721, 366, 12270, 538, 264, 4195, 11, 291, 500, 380, 362, 264, 10815, 300, 309, 486, 808, 646, 13, 51710], "temperature": 0.0, "avg_logprob": -0.1969946643762421, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0007091800216585398}, {"id": 435, "seek": 160326, "start": 1630.98, "end": 1632.78, "text": " But what I can imagine is that.", "tokens": [51751, 583, 437, 286, 393, 3811, 307, 300, 13, 51841], "temperature": 0.0, "avg_logprob": -0.1969946643762421, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0007091800216585398}, {"id": 436, "seek": 163278, "start": 1632.78, "end": 1650.44, "text": " If you do like if we if we know that everyone uses concurrent task and that scheme and everything is set up as it should, then the line between what tasks and commands are useful for slims down a lot.", "tokens": [50365, 759, 291, 360, 411, 498, 321, 498, 321, 458, 300, 1518, 4960, 37702, 5633, 293, 300, 12232, 293, 1203, 307, 992, 493, 382, 309, 820, 11, 550, 264, 1622, 1296, 437, 9608, 293, 16901, 366, 4420, 337, 1061, 18857, 760, 257, 688, 13, 51248], "temperature": 0.0, "avg_logprob": -0.17580714592566857, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0027108832728117704}, {"id": 437, "seek": 163278, "start": 1650.44, "end": 1657.18, "text": " And I don't look don't know if there's actually any reason not to have the same API for both.", "tokens": [51248, 400, 286, 500, 380, 574, 500, 380, 458, 498, 456, 311, 767, 604, 1778, 406, 281, 362, 264, 912, 9362, 337, 1293, 13, 51585], "temperature": 0.0, "avg_logprob": -0.17580714592566857, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0027108832728117704}, {"id": 438, "seek": 163278, "start": 1658.1, "end": 1662.62, "text": " Well, one thing that I'm going to test doesn't help is sending a request.", "tokens": [51631, 1042, 11, 472, 551, 300, 286, 478, 516, 281, 1500, 1177, 380, 854, 307, 7750, 257, 5308, 13, 51857], "temperature": 0.0, "avg_logprob": -0.17580714592566857, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0027108832728117704}, {"id": 439, "seek": 166278, "start": 1662.78, "end": 1664.82, "text": " And not expecting a response.", "tokens": [50365, 400, 406, 9650, 257, 4134, 13, 50467], "temperature": 0.0, "avg_logprob": -0.2827619455628476, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.001695998478680849}, {"id": 440, "seek": 166278, "start": 1665.18, "end": 1668.46, "text": " That is the only command is the only one that can do that.", "tokens": [50485, 663, 307, 264, 787, 5622, 307, 264, 787, 472, 300, 393, 360, 300, 13, 50649], "temperature": 0.0, "avg_logprob": -0.2827619455628476, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.001695998478680849}, {"id": 441, "seek": 166278, "start": 1668.84, "end": 1670.28, "text": " Not not with many things, though.", "tokens": [50668, 1726, 406, 365, 867, 721, 11, 1673, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2827619455628476, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.001695998478680849}, {"id": 442, "seek": 166278, "start": 1670.28, "end": 1676.76, "text": " But yeah, I think the what you could do in our current task seems like the pattern.", "tokens": [50740, 583, 1338, 11, 286, 519, 264, 437, 291, 727, 360, 294, 527, 2190, 5633, 2544, 411, 264, 5102, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2827619455628476, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.001695998478680849}, {"id": 443, "seek": 166278, "start": 1676.76, "end": 1683.24, "text": " If you say you do something like log to the console, like you just return unit an empty tuple.", "tokens": [51064, 759, 291, 584, 291, 360, 746, 411, 3565, 281, 264, 11076, 11, 411, 291, 445, 2736, 4985, 364, 6707, 2604, 781, 13, 51388], "temperature": 0.0, "avg_logprob": -0.2827619455628476, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.001695998478680849}, {"id": 444, "seek": 166278, "start": 1683.56, "end": 1686.22, "text": " It is kind of like it's sort of a response.", "tokens": [51404, 467, 307, 733, 295, 411, 309, 311, 1333, 295, 257, 4134, 13, 51537], "temperature": 0.0, "avg_logprob": -0.2827619455628476, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.001695998478680849}, {"id": 445, "seek": 166278, "start": 1686.22, "end": 1692.54, "text": " But yeah, that's what you do in a lot of things with the Elms tasks API as well.", "tokens": [51537, 583, 1338, 11, 300, 311, 437, 291, 360, 294, 257, 688, 295, 721, 365, 264, 2699, 2592, 9608, 9362, 382, 731, 13, 51853], "temperature": 0.0, "avg_logprob": -0.2827619455628476, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.001695998478680849}, {"id": 446, "seek": 169278, "start": 1692.78, "end": 1698.18, "text": " But I don't know if there's a real reason for all of them responding.", "tokens": [50365, 583, 286, 500, 380, 458, 498, 456, 311, 257, 957, 1778, 337, 439, 295, 552, 16670, 13, 50635], "temperature": 0.0, "avg_logprob": -0.20180218436501243, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.00039150752127170563}, {"id": 447, "seek": 169278, "start": 1698.66, "end": 1699.16, "text": " Right.", "tokens": [50659, 1779, 13, 50684], "temperature": 0.0, "avg_logprob": -0.20180218436501243, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.00039150752127170563}, {"id": 448, "seek": 169278, "start": 1699.56, "end": 1714.74, "text": " Maybe it's just like, well, if we have a way to send a task request and not get a response and some of the APIs get weird, maybe that's it's like, oh, well, we just did this entire HTTP chain function.", "tokens": [50704, 2704, 309, 311, 445, 411, 11, 731, 11, 498, 321, 362, 257, 636, 281, 2845, 257, 5633, 5308, 293, 406, 483, 257, 4134, 293, 512, 295, 264, 21445, 483, 3657, 11, 1310, 300, 311, 309, 311, 411, 11, 1954, 11, 731, 11, 321, 445, 630, 341, 2302, 33283, 5021, 2445, 13, 51463], "temperature": 0.0, "avg_logprob": -0.20180218436501243, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.00039150752127170563}, {"id": 449, "seek": 169278, "start": 1714.98, "end": 1720.1399999999999, "text": " And then at the end, because you use this function now, we won't get a response.", "tokens": [51475, 400, 550, 412, 264, 917, 11, 570, 291, 764, 341, 2445, 586, 11, 321, 1582, 380, 483, 257, 4134, 13, 51733], "temperature": 0.0, "avg_logprob": -0.20180218436501243, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.00039150752127170563}, {"id": 450, "seek": 169278, "start": 1720.68, "end": 1722.56, "text": " And that's unexpected.", "tokens": [51760, 400, 300, 311, 13106, 13, 51854], "temperature": 0.0, "avg_logprob": -0.20180218436501243, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.00039150752127170563}, {"id": 451, "seek": 169278, "start": 1722.6, "end": 1722.76, "text": " Or.", "tokens": [51856, 1610, 13, 51864], "temperature": 0.0, "avg_logprob": -0.20180218436501243, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.00039150752127170563}, {"id": 452, "seek": 172278, "start": 1722.78, "end": 1725.68, "text": " That's just like, oh, that's a bit weird.", "tokens": [50365, 663, 311, 445, 411, 11, 1954, 11, 300, 311, 257, 857, 3657, 13, 50510], "temperature": 0.0, "avg_logprob": -0.2532556357503939, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0008826411794871092}, {"id": 453, "seek": 172278, "start": 1725.68, "end": 1727.3999999999999, "text": " Like it's a foot gun is what I mean.", "tokens": [50510, 1743, 309, 311, 257, 2671, 3874, 307, 437, 286, 914, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2532556357503939, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0008826411794871092}, {"id": 454, "seek": 172278, "start": 1728.12, "end": 1729.26, "text": " Yeah, you're right.", "tokens": [50632, 865, 11, 291, 434, 558, 13, 50689], "temperature": 0.0, "avg_logprob": -0.2532556357503939, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0008826411794871092}, {"id": 455, "seek": 172278, "start": 1729.3999999999999, "end": 1736.28, "text": " It does give you the option to before like there's always the guarantee that tasks will complete.", "tokens": [50696, 467, 775, 976, 291, 264, 3614, 281, 949, 411, 456, 311, 1009, 264, 10815, 300, 9608, 486, 3566, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2532556357503939, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0008826411794871092}, {"id": 456, "seek": 172278, "start": 1736.52, "end": 1741.66, "text": " Whereas you're not if you if you can write them yourself, you can break those guarantees.", "tokens": [51052, 13813, 291, 434, 406, 498, 291, 498, 291, 393, 2464, 552, 1803, 11, 291, 393, 1821, 729, 32567, 13, 51309], "temperature": 0.0, "avg_logprob": -0.2532556357503939, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0008826411794871092}, {"id": 457, "seek": 172278, "start": 1741.8, "end": 1748.28, "text": " Yeah. Although there's there's always sort of an illusion of guarantees any time you're working with JavaScript, right?", "tokens": [51316, 865, 13, 5780, 456, 311, 456, 311, 1009, 1333, 295, 364, 18854, 295, 32567, 604, 565, 291, 434, 1364, 365, 15778, 11, 558, 30, 51640], "temperature": 0.0, "avg_logprob": -0.2532556357503939, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0008826411794871092}, {"id": 458, "seek": 172278, "start": 1748.3999999999999, "end": 1752.6, "text": " Like JavaScript is inherently something that you can't explain.", "tokens": [51646, 1743, 15778, 307, 27993, 746, 300, 291, 393, 380, 2903, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2532556357503939, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0008826411794871092}, {"id": 459, "seek": 175260, "start": 1752.6, "end": 1754.32, "text": " You can't expect to be well behaved.", "tokens": [50365, 509, 393, 380, 2066, 281, 312, 731, 48249, 13, 50451], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 460, "seek": 175260, "start": 1755.08, "end": 1759.6799999999998, "text": " And as anybody who has used it knows, I think.", "tokens": [50489, 400, 382, 4472, 567, 575, 1143, 309, 3255, 11, 286, 519, 13, 50719], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 461, "seek": 175260, "start": 1760.0, "end": 1766.78, "text": " And so, for example, it can throw exceptions like that's just a thing that JavaScript code can do.", "tokens": [50735, 400, 370, 11, 337, 1365, 11, 309, 393, 3507, 22847, 411, 300, 311, 445, 257, 551, 300, 15778, 3089, 393, 360, 13, 51074], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 462, "seek": 175260, "start": 1767.56, "end": 1770.24, "text": " And so you have to handle that for that.", "tokens": [51113, 400, 370, 291, 362, 281, 4813, 300, 337, 300, 13, 51247], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 463, "seek": 175260, "start": 1770.3, "end": 1773.48, "text": " And it may also time out.", "tokens": [51250, 400, 309, 815, 611, 565, 484, 13, 51409], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 464, "seek": 175260, "start": 1773.6599999999999, "end": 1778.9399999999998, "text": " That's like another class of poorly behaved behavior it could fall into.", "tokens": [51418, 663, 311, 411, 1071, 1508, 295, 22271, 48249, 5223, 309, 727, 2100, 666, 13, 51682], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 465, "seek": 175260, "start": 1779.08, "end": 1779.6, "text": " Right.", "tokens": [51689, 1779, 13, 51715], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 466, "seek": 175260, "start": 1779.82, "end": 1782.24, "text": " But you could have a framework like.", "tokens": [51726, 583, 291, 727, 362, 257, 8388, 411, 13, 51847], "temperature": 0.2, "avg_logprob": -0.19936926058023283, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.002050623530521989}, {"id": 467, "seek": 178260, "start": 1782.6, "end": 1789.52, "text": " So you could have a framework like Elm concurrent task, wrap the thing into a try catch.", "tokens": [50365, 407, 291, 727, 362, 257, 8388, 411, 2699, 76, 37702, 5633, 11, 7019, 264, 551, 666, 257, 853, 3745, 13, 50711], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 468, "seek": 178260, "start": 1789.74, "end": 1790.24, "text": " Exactly.", "tokens": [50722, 7587, 13, 50747], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 469, "seek": 178260, "start": 1790.24, "end": 1792.02, "text": " And have a timeout.", "tokens": [50747, 400, 362, 257, 565, 346, 13, 50836], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 470, "seek": 178260, "start": 1792.08, "end": 1792.58, "text": " That's right.", "tokens": [50839, 663, 311, 558, 13, 50864], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 471, "seek": 178260, "start": 1792.62, "end": 1793.36, "text": " Yeah, exactly.", "tokens": [50866, 865, 11, 2293, 13, 50903], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 472, "seek": 178260, "start": 1793.36, "end": 1795.12, "text": " And then you're sure to have a response.", "tokens": [50903, 400, 550, 291, 434, 988, 281, 362, 257, 4134, 13, 50991], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 473, "seek": 178260, "start": 1795.36, "end": 1795.86, "text": " Yes.", "tokens": [51003, 1079, 13, 51028], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 474, "seek": 178260, "start": 1796.08, "end": 1797.48, "text": " Will the timeout be long enough?", "tokens": [51039, 3099, 264, 565, 346, 312, 938, 1547, 30, 51109], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 475, "seek": 178260, "start": 1797.48, "end": 1799.52, "text": " That's a different question.", "tokens": [51109, 663, 311, 257, 819, 1168, 13, 51211], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 476, "seek": 178260, "start": 1799.52, "end": 1800.02, "text": " But.", "tokens": [51211, 583, 13, 51236], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 477, "seek": 178260, "start": 1800.36, "end": 1800.86, "text": " Right.", "tokens": [51253, 1779, 13, 51278], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 478, "seek": 178260, "start": 1800.9599999999998, "end": 1806.9599999999998, "text": " And like, do you also want a response that is a timeout?", "tokens": [51283, 400, 411, 11, 360, 291, 611, 528, 257, 4134, 300, 307, 257, 565, 346, 30, 51583], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 479, "seek": 178260, "start": 1807.32, "end": 1811.1, "text": " Plenty of cases like I think you would prefer not have the response.", "tokens": [51601, 2149, 4179, 295, 3331, 411, 286, 519, 291, 576, 4382, 406, 362, 264, 4134, 13, 51790], "temperature": 0.2, "avg_logprob": -0.6356801063783707, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0013655429938808084}, {"id": 480, "seek": 181110, "start": 1811.1, "end": 1814.6, "text": " But you would prefer to have a timeout or at least it could be possible.", "tokens": [50365, 583, 291, 576, 4382, 281, 362, 257, 565, 346, 420, 412, 1935, 309, 727, 312, 1944, 13, 50540], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 481, "seek": 181110, "start": 1814.6, "end": 1815.6, "text": " If I know.", "tokens": [50540, 759, 286, 458, 13, 50590], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 482, "seek": 181110, "start": 1815.6, "end": 1824.6, "text": " So one thing you mentioned as well is when you're getting back the data from JavaScript, you have a decoder.", "tokens": [50590, 407, 472, 551, 291, 2835, 382, 731, 307, 562, 291, 434, 1242, 646, 264, 1412, 490, 15778, 11, 291, 362, 257, 979, 19866, 13, 51040], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 483, "seek": 181110, "start": 1824.6, "end": 1825.6, "text": " How does that work?", "tokens": [51040, 1012, 775, 300, 589, 30, 51090], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 484, "seek": 181110, "start": 1825.6, "end": 1830.6, "text": " And what happens if the decoding fails or do you need to do?", "tokens": [51090, 400, 437, 2314, 498, 264, 979, 8616, 18199, 420, 360, 291, 643, 281, 360, 30, 51340], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 485, "seek": 181110, "start": 1830.6, "end": 1835.6, "text": " Do you just get a JSON encode value and you need to decode it manually?", "tokens": [51340, 1144, 291, 445, 483, 257, 31828, 2058, 1429, 2158, 293, 291, 643, 281, 979, 1429, 309, 16945, 30, 51590], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 486, "seek": 181110, "start": 1835.6, "end": 1836.6, "text": " Exactly.", "tokens": [51590, 7587, 13, 51640], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 487, "seek": 181110, "start": 1836.6, "end": 1839.6, "text": " So you get back a JSON encode value.", "tokens": [51640, 407, 291, 483, 646, 257, 31828, 2058, 1429, 2158, 13, 51790], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 488, "seek": 181110, "start": 1839.6, "end": 1840.6, "text": " There.", "tokens": [51790, 821, 13, 51840], "temperature": 0.0, "avg_logprob": -0.46340287037384814, "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0009374398505315185}, {"id": 489, "seek": 184060, "start": 1840.6, "end": 1842.6, "text": " Right under the hood, it will be a JSON encode value.", "tokens": [50365, 1779, 833, 264, 13376, 11, 309, 486, 312, 257, 31828, 2058, 1429, 2158, 13, 50465], "temperature": 0.0, "avg_logprob": -0.20031953514169115, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0012014913372695446}, {"id": 490, "seek": 184060, "start": 1842.6, "end": 1846.1, "text": " The JSON decoder gets run on it.", "tokens": [50465, 440, 31828, 979, 19866, 2170, 1190, 322, 309, 13, 50640], "temperature": 0.0, "avg_logprob": -0.20031953514169115, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0012014913372695446}, {"id": 491, "seek": 184060, "start": 1846.1, "end": 1850.1, "text": " If it succeeds, you get the value back through your.", "tokens": [50640, 759, 309, 49263, 11, 291, 483, 264, 2158, 646, 807, 428, 13, 50840], "temperature": 0.0, "avg_logprob": -0.20031953514169115, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0012014913372695446}, {"id": 492, "seek": 184060, "start": 1850.1, "end": 1861.6, "text": " I've called it in the readme like the task flow, which is like your kind of regular error success flow that you do when you're chaining tasks together.", "tokens": [50840, 286, 600, 1219, 309, 294, 264, 1401, 1398, 411, 264, 5633, 3095, 11, 597, 307, 411, 428, 733, 295, 3890, 6713, 2245, 3095, 300, 291, 360, 562, 291, 434, 417, 3686, 9608, 1214, 13, 51415], "temperature": 0.0, "avg_logprob": -0.20031953514169115, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0012014913372695446}, {"id": 493, "seek": 184060, "start": 1861.6, "end": 1870.6, "text": " If there's some error handlers, there's some functions that you can use to say, like, if you get a decode error.", "tokens": [51415, 759, 456, 311, 512, 6713, 1011, 11977, 11, 456, 311, 512, 6828, 300, 291, 393, 764, 281, 584, 11, 411, 11, 498, 291, 483, 257, 979, 1429, 6713, 13, 51865], "temperature": 0.0, "avg_logprob": -0.20031953514169115, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0012014913372695446}, {"id": 494, "seek": 187060, "start": 1870.6, "end": 1872.1, "text": " Do something with it.", "tokens": [50365, 1144, 746, 365, 309, 13, 50440], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 495, "seek": 187060, "start": 1872.1, "end": 1878.6, "text": " You can either, like, fail with a custom error or you could, like, recover it and lift it into your, like, success type.", "tokens": [50440, 509, 393, 2139, 11, 411, 11, 3061, 365, 257, 2375, 6713, 420, 291, 727, 11, 411, 11, 8114, 309, 293, 5533, 309, 666, 428, 11, 411, 11, 2245, 2010, 13, 50765], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 496, "seek": 187060, "start": 1878.6, "end": 1892.6, "text": " But if you don't add that in, there's what's called, like, an unexpected error gets returned out through the on response callback, like a message that you provide.", "tokens": [50765, 583, 498, 291, 500, 380, 909, 300, 294, 11, 456, 311, 437, 311, 1219, 11, 411, 11, 364, 13106, 6713, 2170, 8752, 484, 807, 264, 322, 4134, 818, 3207, 11, 411, 257, 3636, 300, 291, 2893, 13, 51465], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 497, "seek": 187060, "start": 1892.6, "end": 1894.6, "text": " And that stops everything.", "tokens": [51465, 400, 300, 10094, 1203, 13, 51565], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 498, "seek": 187060, "start": 1894.6, "end": 1897.6, "text": " You mean it gets sent back to JavaScript again?", "tokens": [51565, 509, 914, 309, 2170, 2279, 646, 281, 15778, 797, 30, 51715], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 499, "seek": 187060, "start": 1897.6, "end": 1898.6, "text": " No.", "tokens": [51715, 883, 13, 51765], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 500, "seek": 187060, "start": 1898.6, "end": 1899.6, "text": " So this is all Elm side.", "tokens": [51765, 407, 341, 307, 439, 2699, 76, 1252, 13, 51815], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 501, "seek": 187060, "start": 1899.6, "end": 1900.6, "text": " Like, it's not.", "tokens": [51815, 1743, 11, 309, 311, 406, 13, 51865], "temperature": 0.0, "avg_logprob": -0.21223149299621583, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.009601944126188755}, {"id": 502, "seek": 190060, "start": 1900.6, "end": 1901.6, "text": " Oh, yeah.", "tokens": [50365, 876, 11, 1338, 13, 50415], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 503, "seek": 190060, "start": 1901.6, "end": 1902.6, "text": " On the Elm side.", "tokens": [50415, 1282, 264, 2699, 76, 1252, 13, 50465], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 504, "seek": 190060, "start": 1902.6, "end": 1903.6, "text": " Yeah.", "tokens": [50465, 865, 13, 50515], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 505, "seek": 190060, "start": 1903.6, "end": 1907.6, "text": " On the JavaScript side, it's all, like, it doesn't really know anything about it.", "tokens": [50515, 1282, 264, 15778, 1252, 11, 309, 311, 439, 11, 411, 11, 309, 1177, 380, 534, 458, 1340, 466, 309, 13, 50715], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 506, "seek": 190060, "start": 1907.6, "end": 1912.6, "text": " And the ports are just, like, just send me, like, JSON encode values back through.", "tokens": [50715, 400, 264, 18160, 366, 445, 11, 411, 11, 445, 2845, 385, 11, 411, 11, 31828, 2058, 1429, 4190, 646, 807, 13, 50965], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 507, "seek": 190060, "start": 1912.6, "end": 1921.6, "text": " So it's very those ports are very safe from, like, I think you'd have a very hard time to make them blow up.", "tokens": [50965, 407, 309, 311, 588, 729, 18160, 366, 588, 3273, 490, 11, 411, 11, 286, 519, 291, 1116, 362, 257, 588, 1152, 565, 281, 652, 552, 6327, 493, 13, 51415], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 508, "seek": 190060, "start": 1921.6, "end": 1927.6, "text": " But then on the Elm side, it's all handling the, like, what do you expect to come back from the task?", "tokens": [51415, 583, 550, 322, 264, 2699, 76, 1252, 11, 309, 311, 439, 13175, 264, 11, 411, 11, 437, 360, 291, 2066, 281, 808, 646, 490, 264, 5633, 30, 51715], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 509, "seek": 190060, "start": 1927.6, "end": 1929.6, "text": " And if it does something weird, it's, like, stop.", "tokens": [51715, 400, 498, 309, 775, 746, 3657, 11, 309, 311, 11, 411, 11, 1590, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 510, "seek": 190060, "start": 1929.6, "end": 1930.6, "text": " Abort.", "tokens": [51815, 2847, 477, 13, 51865], "temperature": 0.0, "avg_logprob": -0.09173540274302165, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.0068350620567798615}, {"id": 511, "seek": 193060, "start": 1930.6, "end": 1931.6, "text": " Like, abort the task chain.", "tokens": [50365, 1743, 11, 38117, 264, 5633, 5021, 13, 50415], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 512, "seek": 193060, "start": 1931.6, "end": 1936.6, "text": " Like, don't send any more values back out through JavaScript at that point.", "tokens": [50415, 1743, 11, 500, 380, 2845, 604, 544, 4190, 646, 484, 807, 15778, 412, 300, 935, 13, 50665], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 513, "seek": 193060, "start": 1936.6, "end": 1937.6, "text": " Mm-hmm.", "tokens": [50665, 8266, 12, 10250, 13, 50715], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 514, "seek": 193060, "start": 1937.6, "end": 1946.6, "text": " So if somebody wants to wire this into an app, what does that involve kind of getting it set up with the boilerplate?", "tokens": [50715, 407, 498, 2618, 2738, 281, 6234, 341, 666, 364, 724, 11, 437, 775, 300, 9494, 733, 295, 1242, 309, 992, 493, 365, 264, 39228, 37008, 30, 51165], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 515, "seek": 193060, "start": 1946.6, "end": 1948.6, "text": " What do you mean?", "tokens": [51165, 708, 360, 291, 914, 30, 51265], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 516, "seek": 193060, "start": 1948.6, "end": 1949.6, "text": " Sorry.", "tokens": [51265, 4919, 13, 51315], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 517, "seek": 193060, "start": 1949.6, "end": 1952.6, "text": " Basically, what do you need in your model?", "tokens": [51315, 8537, 11, 437, 360, 291, 643, 294, 428, 2316, 30, 51465], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 518, "seek": 193060, "start": 1952.6, "end": 1955.6, "text": " What do you need on the JavaScript side to wire it in?", "tokens": [51465, 708, 360, 291, 643, 322, 264, 15778, 1252, 281, 6234, 309, 294, 30, 51615], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 519, "seek": 193060, "start": 1955.6, "end": 1958.6, "text": " What other messages and all that sort of thing do you need?", "tokens": [51615, 708, 661, 7897, 293, 439, 300, 1333, 295, 551, 360, 291, 643, 30, 51765], "temperature": 0.0, "avg_logprob": -0.08118830056026063, "compression_ratio": 1.648, "no_speech_prob": 0.001314224093221128}, {"id": 520, "seek": 195860, "start": 1958.6, "end": 1962.6, "text": " So you need a couple of things.", "tokens": [50365, 407, 291, 643, 257, 1916, 295, 721, 13, 50565], "temperature": 0.0, "avg_logprob": -0.10027378909992722, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.005790197290480137}, {"id": 521, "seek": 195860, "start": 1962.6, "end": 1968.6, "text": " There's in your model, there's what's effectively the task, the tasks model.", "tokens": [50565, 821, 311, 294, 428, 2316, 11, 456, 311, 437, 311, 8659, 264, 5633, 11, 264, 9608, 2316, 13, 50865], "temperature": 0.0, "avg_logprob": -0.10027378909992722, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.005790197290480137}, {"id": 522, "seek": 195860, "start": 1968.6, "end": 1970.6, "text": " I've called it a task pool.", "tokens": [50865, 286, 600, 1219, 309, 257, 5633, 7005, 13, 50965], "temperature": 0.0, "avg_logprob": -0.10027378909992722, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.005790197290480137}, {"id": 523, "seek": 195860, "start": 1970.6, "end": 1972.6, "text": " The name could change.", "tokens": [50965, 440, 1315, 727, 1319, 13, 51065], "temperature": 0.0, "avg_logprob": -0.10027378909992722, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.005790197290480137}, {"id": 524, "seek": 195860, "start": 1972.6, "end": 1976.6, "text": " But it's an idea that you can have multiple.", "tokens": [51065, 583, 309, 311, 364, 1558, 300, 291, 393, 362, 3866, 13, 51265], "temperature": 0.0, "avg_logprob": -0.10027378909992722, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.005790197290480137}, {"id": 525, "seek": 195860, "start": 1976.6, "end": 1980.6, "text": " Yeah, you could have, like, multiple tasks running at the same time.", "tokens": [51265, 865, 11, 291, 727, 362, 11, 411, 11, 3866, 9608, 2614, 412, 264, 912, 565, 13, 51465], "temperature": 0.0, "avg_logprob": -0.10027378909992722, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.005790197290480137}, {"id": 526, "seek": 195860, "start": 1980.6, "end": 1987.6, "text": " So it's, like, this is something that's managing all of the state internally of the in-flight tasks.", "tokens": [51465, 407, 309, 311, 11, 411, 11, 341, 307, 746, 300, 311, 11642, 439, 295, 264, 1785, 19501, 295, 264, 294, 12, 43636, 9608, 13, 51815], "temperature": 0.0, "avg_logprob": -0.10027378909992722, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.005790197290480137}, {"id": 527, "seek": 198760, "start": 1987.6, "end": 1990.6, "text": " And you might want to swim in it, a task pool.", "tokens": [50365, 400, 291, 1062, 528, 281, 7110, 294, 309, 11, 257, 5633, 7005, 13, 50515], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 528, "seek": 198760, "start": 1990.6, "end": 1991.6, "text": " Exactly.", "tokens": [50515, 7587, 13, 50565], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 529, "seek": 198760, "start": 1991.6, "end": 1992.6, "text": " Yeah, exactly.", "tokens": [50565, 865, 11, 2293, 13, 50615], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 530, "seek": 198760, "start": 1992.6, "end": 1993.6, "text": " Every time.", "tokens": [50615, 2048, 565, 13, 50665], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 531, "seek": 198760, "start": 1993.6, "end": 1994.6, "text": " Yeah.", "tokens": [50665, 865, 13, 50715], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 532, "seek": 198760, "start": 1994.6, "end": 1996.6, "text": " So you need the task pool.", "tokens": [50715, 407, 291, 643, 264, 5633, 7005, 13, 50815], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 533, "seek": 198760, "start": 1996.6, "end": 1998.6, "text": " There's two messages.", "tokens": [50815, 821, 311, 732, 7897, 13, 50915], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 534, "seek": 198760, "start": 1998.6, "end": 2001.6, "text": " So there's one that I've called, like, on progress.", "tokens": [50915, 407, 456, 311, 472, 300, 286, 600, 1219, 11, 411, 11, 322, 4205, 13, 51065], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 535, "seek": 198760, "start": 2001.6, "end": 2010.6, "text": " So that's kind of, like, the internal wiring that's, like, every time you get, like, a response through the port, it's, like, kind of funneling that back through.", "tokens": [51065, 407, 300, 311, 733, 295, 11, 411, 11, 264, 6920, 27520, 300, 311, 11, 411, 11, 633, 565, 291, 483, 11, 411, 11, 257, 4134, 807, 264, 2436, 11, 309, 311, 11, 411, 11, 733, 295, 24515, 278, 300, 646, 807, 13, 51515], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 536, "seek": 198760, "start": 2010.6, "end": 2016.6, "text": " It's just, like, updating the progress of the task and then giving the next command to be performed.", "tokens": [51515, 467, 311, 445, 11, 411, 11, 25113, 264, 4205, 295, 264, 5633, 293, 550, 2902, 264, 958, 5622, 281, 312, 10332, 13, 51815], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 537, "seek": 198760, "start": 2016.6, "end": 2017.6, "text": " Yeah.", "tokens": [51815, 865, 13, 51865], "temperature": 0.0, "avg_logprob": -0.07973963236637252, "compression_ratio": 1.8582995951417005, "no_speech_prob": 0.0006870098295621574}, {"id": 538, "seek": 201760, "start": 2017.6, "end": 2030.6, "text": " And then there's one message which is, like, when everything's done, like, you've got, like, a final result from the task, which is either, like, an unexpected error or the success.", "tokens": [50365, 400, 550, 456, 311, 472, 3636, 597, 307, 11, 411, 11, 562, 1203, 311, 1096, 11, 411, 11, 291, 600, 658, 11, 411, 11, 257, 2572, 1874, 490, 264, 5633, 11, 597, 307, 2139, 11, 411, 11, 364, 13106, 6713, 420, 264, 2245, 13, 51015], "temperature": 0.0, "avg_logprob": -0.05068008716289814, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0018656561151146889}, {"id": 539, "seek": 201760, "start": 2030.6, "end": 2031.6, "text": " Okay.", "tokens": [51015, 1033, 13, 51065], "temperature": 0.0, "avg_logprob": -0.05068008716289814, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0018656561151146889}, {"id": 540, "seek": 201760, "start": 2031.6, "end": 2036.6, "text": " And then there's just some subscriptions to wire all of that up together.", "tokens": [51065, 400, 550, 456, 311, 445, 512, 44951, 281, 6234, 439, 295, 300, 493, 1214, 13, 51315], "temperature": 0.0, "avg_logprob": -0.05068008716289814, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0018656561151146889}, {"id": 541, "seek": 201760, "start": 2036.6, "end": 2039.6, "text": " So it is a little bit of boilerplate.", "tokens": [51315, 407, 309, 307, 257, 707, 857, 295, 39228, 37008, 13, 51465], "temperature": 0.0, "avg_logprob": -0.05068008716289814, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0018656561151146889}, {"id": 542, "seek": 201760, "start": 2039.6, "end": 2046.6, "text": " The advantage of the core task or command is all that, like, wiring is hidden away from you.", "tokens": [51465, 440, 5002, 295, 264, 4965, 5633, 420, 5622, 307, 439, 300, 11, 411, 11, 27520, 307, 7633, 1314, 490, 291, 13, 51815], "temperature": 0.0, "avg_logprob": -0.05068008716289814, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0018656561151146889}, {"id": 543, "seek": 204660, "start": 2046.6, "end": 2052.6, "text": " So you can just fire the task and then give it, like, a callback, like a success callback.", "tokens": [50365, 407, 291, 393, 445, 2610, 264, 5633, 293, 550, 976, 309, 11, 411, 11, 257, 818, 3207, 11, 411, 257, 2245, 818, 3207, 13, 50665], "temperature": 0.0, "avg_logprob": -0.08215303744299937, "compression_ratio": 1.496, "no_speech_prob": 0.0033481349237263203}, {"id": 544, "seek": 204660, "start": 2052.6, "end": 2058.6, "text": " Then in, like, a command's case, it's just, like, just fire it and then stuff will happen.", "tokens": [50665, 1396, 294, 11, 411, 11, 257, 5622, 311, 1389, 11, 309, 311, 445, 11, 411, 11, 445, 2610, 309, 293, 550, 1507, 486, 1051, 13, 50965], "temperature": 0.0, "avg_logprob": -0.08215303744299937, "compression_ratio": 1.496, "no_speech_prob": 0.0033481349237263203}, {"id": 545, "seek": 204660, "start": 2058.6, "end": 2060.6, "text": " Yeah.", "tokens": [50965, 865, 13, 51065], "temperature": 0.0, "avg_logprob": -0.08215303744299937, "compression_ratio": 1.496, "no_speech_prob": 0.0033481349237263203}, {"id": 546, "seek": 206060, "start": 2060.6, "end": 2066.6, "text": " So, like, you abstract away a few of the details of the JavaScript wiring.", "tokens": [50365, 407, 11, 411, 11, 291, 12649, 1314, 257, 1326, 295, 264, 4365, 295, 264, 15778, 27520, 13, 50665], "temperature": 0.0, "avg_logprob": -0.09995201022125953, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.09703528881072998}, {"id": 547, "seek": 206060, "start": 2066.6, "end": 2075.6, "text": " So, like, you initialize your Elm application and then you can, you pass in a JavaScript object with all of your port definitions.", "tokens": [50665, 407, 11, 411, 11, 291, 5883, 1125, 428, 2699, 76, 3861, 293, 550, 291, 393, 11, 291, 1320, 294, 257, 15778, 2657, 365, 439, 295, 428, 2436, 21988, 13, 51115], "temperature": 0.0, "avg_logprob": -0.09995201022125953, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.09703528881072998}, {"id": 548, "seek": 206060, "start": 2075.6, "end": 2086.6, "text": " And your port definitions are, you know, instead of, like, going through that dance of the incoming and outgoing ports and wiring those in and adding subscriptions.", "tokens": [51115, 400, 428, 2436, 21988, 366, 11, 291, 458, 11, 2602, 295, 11, 411, 11, 516, 807, 300, 4489, 295, 264, 22341, 293, 41565, 18160, 293, 27520, 729, 294, 293, 5127, 44951, 13, 51665], "temperature": 0.0, "avg_logprob": -0.09995201022125953, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.09703528881072998}, {"id": 549, "seek": 208660, "start": 2086.6, "end": 2100.6, "text": " And then if you have a subscriptions port that you stop using and then the Elm compiler or the JavaScript code crashes because it's no longer defining this port because it's unused code in your Elm code.", "tokens": [50365, 400, 550, 498, 291, 362, 257, 44951, 2436, 300, 291, 1590, 1228, 293, 550, 264, 2699, 76, 31958, 420, 264, 15778, 3089, 28642, 570, 309, 311, 572, 2854, 17827, 341, 2436, 570, 309, 311, 44383, 3089, 294, 428, 2699, 76, 3089, 13, 51065], "temperature": 0.0, "avg_logprob": -0.19427204132080078, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012737022479996085}, {"id": 550, "seek": 208660, "start": 2100.6, "end": 2102.6, "text": " And all these, like, rough edges kind of go away.", "tokens": [51065, 400, 439, 613, 11, 411, 11, 5903, 8819, 733, 295, 352, 1314, 13, 51165], "temperature": 0.0, "avg_logprob": -0.19427204132080078, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012737022479996085}, {"id": 551, "seek": 208660, "start": 2102.6, "end": 2112.6, "text": " And instead you just are defining these ports, these tasks for Elm concurrent task in a set of key value pairs.", "tokens": [51165, 400, 2602, 291, 445, 366, 17827, 613, 18160, 11, 613, 9608, 337, 2699, 76, 37702, 5633, 294, 257, 992, 295, 2141, 2158, 15494, 13, 51665], "temperature": 0.0, "avg_logprob": -0.19427204132080078, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012737022479996085}, {"id": 552, "seek": 208660, "start": 2112.6, "end": 2113.6, "text": " You give it a name.", "tokens": [51665, 509, 976, 309, 257, 1315, 13, 51715], "temperature": 0.0, "avg_logprob": -0.19427204132080078, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012737022479996085}, {"id": 553, "seek": 208660, "start": 2113.6, "end": 2114.6, "text": " And you give it an async function or a synchronous function.", "tokens": [51715, 400, 291, 976, 309, 364, 382, 34015, 2445, 420, 257, 44743, 2445, 13, 51765], "temperature": 0.0, "avg_logprob": -0.19427204132080078, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012737022479996085}, {"id": 554, "seek": 208660, "start": 2114.6, "end": 2115.6, "text": " And then you can just go back and do it.", "tokens": [51765, 400, 550, 291, 393, 445, 352, 646, 293, 360, 309, 13, 51815], "temperature": 0.0, "avg_logprob": -0.19427204132080078, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012737022479996085}, {"id": 555, "seek": 208660, "start": 2115.6, "end": 2116.6, "text": " Okay.", "tokens": [51815, 1033, 13, 51865], "temperature": 0.0, "avg_logprob": -0.19427204132080078, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012737022479996085}, {"id": 556, "seek": 211660, "start": 2116.6, "end": 2118.6, "text": " So, you can do a synchronous function if you want in JavaScript.", "tokens": [50365, 407, 11, 291, 393, 360, 257, 44743, 2445, 498, 291, 528, 294, 15778, 13, 50465], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 557, "seek": 211660, "start": 2118.6, "end": 2122.6, "text": " And return some data and give it a decoder.", "tokens": [50465, 400, 2736, 512, 1412, 293, 976, 309, 257, 979, 19866, 13, 50665], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 558, "seek": 211660, "start": 2122.6, "end": 2129.6, "text": " So, like, that feels like the right mental model for my brain to interact with that.", "tokens": [50665, 407, 11, 411, 11, 300, 3417, 411, 264, 558, 4973, 2316, 337, 452, 3567, 281, 4648, 365, 300, 13, 51015], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 559, "seek": 211660, "start": 2129.6, "end": 2133.6, "text": " So, it's a little bit of wiring, but it cleans up a lot of things as well.", "tokens": [51015, 407, 11, 309, 311, 257, 707, 857, 295, 27520, 11, 457, 309, 16912, 493, 257, 688, 295, 721, 382, 731, 13, 51215], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 560, "seek": 211660, "start": 2133.6, "end": 2135.6, "text": " That's good to hear.", "tokens": [51215, 663, 311, 665, 281, 1568, 13, 51315], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 561, "seek": 211660, "start": 2135.6, "end": 2137.6, "text": " It's good to hear the mental model fits.", "tokens": [51315, 467, 311, 665, 281, 1568, 264, 4973, 2316, 9001, 13, 51415], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 562, "seek": 211660, "start": 2137.6, "end": 2143.6, "text": " I've tried to abstract as many of those wiring details away.", "tokens": [51415, 286, 600, 3031, 281, 12649, 382, 867, 295, 729, 27520, 4365, 1314, 13, 51715], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 563, "seek": 211660, "start": 2143.6, "end": 2145.6, "text": " But there is inevitably a bit of.", "tokens": [51715, 583, 456, 307, 28171, 257, 857, 295, 13, 51815], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 564, "seek": 211660, "start": 2145.6, "end": 2146.6, "text": " Spoiler plate there.", "tokens": [51815, 45011, 5441, 5924, 456, 13, 51865], "temperature": 0.2, "avg_logprob": -0.1620525178455171, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.0010397880105301738}, {"id": 565, "seek": 214660, "start": 2146.6, "end": 2147.6, "text": " You just can't escape.", "tokens": [50365, 509, 445, 393, 380, 7615, 13, 50415], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 566, "seek": 214660, "start": 2147.6, "end": 2148.6, "text": " Yeah.", "tokens": [50415, 865, 13, 50465], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 567, "seek": 214660, "start": 2148.6, "end": 2149.6, "text": " Definitely.", "tokens": [50465, 12151, 13, 50515], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 568, "seek": 214660, "start": 2149.6, "end": 2155.6, "text": " In this community, we have long accepted boilerplate as being okay.", "tokens": [50515, 682, 341, 1768, 11, 321, 362, 938, 9035, 39228, 37008, 382, 885, 1392, 13, 50815], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 569, "seek": 214660, "start": 2155.6, "end": 2160.6, "text": " Like, sure, if we could use less, we would not say no.", "tokens": [50815, 1743, 11, 988, 11, 498, 321, 727, 764, 1570, 11, 321, 576, 406, 584, 572, 13, 51065], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 570, "seek": 214660, "start": 2160.6, "end": 2163.6, "text": " But, I mean, we're okay with it, right?", "tokens": [51065, 583, 11, 286, 914, 11, 321, 434, 1392, 365, 309, 11, 558, 30, 51215], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 571, "seek": 214660, "start": 2163.6, "end": 2164.6, "text": " Yeah.", "tokens": [51215, 865, 13, 51265], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 572, "seek": 214660, "start": 2164.6, "end": 2170.6, "text": " I think for me, it's like as long as I don't have to think too much about the boilerplate, then I'm happy with it.", "tokens": [51265, 286, 519, 337, 385, 11, 309, 311, 411, 382, 938, 382, 286, 500, 380, 362, 281, 519, 886, 709, 466, 264, 39228, 37008, 11, 550, 286, 478, 2055, 365, 309, 13, 51565], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 573, "seek": 214660, "start": 2170.6, "end": 2174.6, "text": " Like, I don't want to be figuring out complicated logic.", "tokens": [51565, 1743, 11, 286, 500, 380, 528, 281, 312, 15213, 484, 6179, 9952, 13, 51765], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 574, "seek": 214660, "start": 2174.6, "end": 2175.6, "text": " It's just like.", "tokens": [51765, 467, 311, 445, 411, 13, 51815], "temperature": 0.0, "avg_logprob": -0.12212885750664605, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0012219829950481653}, {"id": 575, "seek": 217560, "start": 2175.6, "end": 2180.6, "text": " Give me a list of things I need to provide and I'll do it.", "tokens": [50365, 5303, 385, 257, 1329, 295, 721, 286, 643, 281, 2893, 293, 286, 603, 360, 309, 13, 50615], "temperature": 0.0, "avg_logprob": -0.1241966420953924, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.003602077718824148}, {"id": 576, "seek": 217560, "start": 2180.6, "end": 2183.6, "text": " Actually, like, there's one thing that I'm wondering about.", "tokens": [50615, 5135, 11, 411, 11, 456, 311, 472, 551, 300, 286, 478, 6359, 466, 13, 50765], "temperature": 0.0, "avg_logprob": -0.1241966420953924, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.003602077718824148}, {"id": 577, "seek": 217560, "start": 2183.6, "end": 2191.6, "text": " So, in the JavaScript, you set a, you call a function called concurrent task dot register.", "tokens": [50765, 407, 11, 294, 264, 15778, 11, 291, 992, 257, 11, 291, 818, 257, 2445, 1219, 37702, 5633, 5893, 7280, 13, 51165], "temperature": 0.0, "avg_logprob": -0.1241966420953924, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.003602077718824148}, {"id": 578, "seek": 217560, "start": 2191.6, "end": 2196.6, "text": " And you give it your ports, your input ports and your output port.", "tokens": [51165, 400, 291, 976, 309, 428, 18160, 11, 428, 4846, 18160, 293, 428, 5598, 2436, 13, 51415], "temperature": 0.0, "avg_logprob": -0.1241966420953924, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.003602077718824148}, {"id": 579, "seek": 217560, "start": 2196.6, "end": 2197.6, "text": " And a list of tasks.", "tokens": [51415, 400, 257, 1329, 295, 9608, 13, 51465], "temperature": 0.0, "avg_logprob": -0.1241966420953924, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.003602077718824148}, {"id": 580, "seek": 217560, "start": 2197.6, "end": 2204.6, "text": " So, those are the, that is the object which contains names of the functions and then what to do with the arguments.", "tokens": [51465, 407, 11, 729, 366, 264, 11, 300, 307, 264, 2657, 597, 8306, 5288, 295, 264, 6828, 293, 550, 437, 281, 360, 365, 264, 12869, 13, 51815], "temperature": 0.0, "avg_logprob": -0.1241966420953924, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.003602077718824148}, {"id": 581, "seek": 220460, "start": 2204.6, "end": 2206.6, "text": " That were sent.", "tokens": [50365, 663, 645, 2279, 13, 50465], "temperature": 0.0, "avg_logprob": -0.09637719934636896, "compression_ratio": 1.5491071428571428, "no_speech_prob": 0.0015212580328807235}, {"id": 582, "seek": 220460, "start": 2206.6, "end": 2216.6, "text": " And every time you need to ask to add a new task, you need to both do it in the JavaScript side and do it in the Elm side.", "tokens": [50465, 400, 633, 565, 291, 643, 281, 1029, 281, 909, 257, 777, 5633, 11, 291, 643, 281, 1293, 360, 309, 294, 264, 15778, 1252, 293, 360, 309, 294, 264, 2699, 76, 1252, 13, 50965], "temperature": 0.0, "avg_logprob": -0.09637719934636896, "compression_ratio": 1.5491071428571428, "no_speech_prob": 0.0015212580328807235}, {"id": 583, "seek": 220460, "start": 2216.6, "end": 2220.6, "text": " And you have to make sure that those are always in sync, right?", "tokens": [50965, 400, 291, 362, 281, 652, 988, 300, 729, 366, 1009, 294, 20271, 11, 558, 30, 51165], "temperature": 0.0, "avg_logprob": -0.09637719934636896, "compression_ratio": 1.5491071428571428, "no_speech_prob": 0.0015212580328807235}, {"id": 584, "seek": 220460, "start": 2220.6, "end": 2223.6, "text": " Have you had any problems so far with them?", "tokens": [51165, 3560, 291, 632, 604, 2740, 370, 1400, 365, 552, 30, 51315], "temperature": 0.0, "avg_logprob": -0.09637719934636896, "compression_ratio": 1.5491071428571428, "no_speech_prob": 0.0015212580328807235}, {"id": 585, "seek": 220460, "start": 2223.6, "end": 2230.6, "text": " Like, oh, I forgot to set up one up or I had a typo in one of them.", "tokens": [51315, 1743, 11, 1954, 11, 286, 5298, 281, 992, 493, 472, 493, 420, 286, 632, 257, 2125, 78, 294, 472, 295, 552, 13, 51665], "temperature": 0.0, "avg_logprob": -0.09637719934636896, "compression_ratio": 1.5491071428571428, "no_speech_prob": 0.0015212580328807235}, {"id": 586, "seek": 220460, "start": 2230.6, "end": 2232.6, "text": " I'm guessing not, but.", "tokens": [51665, 286, 478, 17939, 406, 11, 457, 13, 51765], "temperature": 0.0, "avg_logprob": -0.09637719934636896, "compression_ratio": 1.5491071428571428, "no_speech_prob": 0.0015212580328807235}, {"id": 587, "seek": 220460, "start": 2232.6, "end": 2233.6, "text": " Well, it.", "tokens": [51765, 1042, 11, 309, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09637719934636896, "compression_ratio": 1.5491071428571428, "no_speech_prob": 0.0015212580328807235}, {"id": 588, "seek": 223360, "start": 2233.6, "end": 2235.6, "text": " I mean, that inevitably happened.", "tokens": [50365, 286, 914, 11, 300, 28171, 2011, 13, 50465], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 589, "seek": 223360, "start": 2235.6, "end": 2243.6, "text": " I spent a bit of time wiring it into, like, an existing, like, running application that I've got.", "tokens": [50465, 286, 4418, 257, 857, 295, 565, 27520, 309, 666, 11, 411, 11, 364, 6741, 11, 411, 11, 2614, 3861, 300, 286, 600, 658, 13, 50865], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 590, "seek": 223360, "start": 2243.6, "end": 2245.6, "text": " And you spell the names wrong.", "tokens": [50865, 400, 291, 9827, 264, 5288, 2085, 13, 50965], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 591, "seek": 223360, "start": 2245.6, "end": 2247.6, "text": " You're like, oh, damn, I missed that one.", "tokens": [50965, 509, 434, 411, 11, 1954, 11, 8151, 11, 286, 6721, 300, 472, 13, 51065], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 592, "seek": 223360, "start": 2247.6, "end": 2257.6, "text": " It gives you, so it will give you back an unexpected error at that point, which is, like, it says, like, I couldn't find, I couldn't find a registered task with this name.", "tokens": [51065, 467, 2709, 291, 11, 370, 309, 486, 976, 291, 646, 364, 13106, 6713, 412, 300, 935, 11, 597, 307, 11, 411, 11, 309, 1619, 11, 411, 11, 286, 2809, 380, 915, 11, 286, 2809, 380, 915, 257, 13968, 5633, 365, 341, 1315, 13, 51565], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 593, "seek": 223360, "start": 2257.6, "end": 2259.6, "text": " So, it's not perfect.", "tokens": [51565, 407, 11, 309, 311, 406, 2176, 13, 51665], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 594, "seek": 223360, "start": 2259.6, "end": 2261.6, "text": " You can, like, you can mismatch the names.", "tokens": [51665, 509, 393, 11, 411, 11, 291, 393, 23220, 852, 264, 5288, 13, 51765], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 595, "seek": 223360, "start": 2261.6, "end": 2263.6, "text": " But it does give you some hints to it.", "tokens": [51765, 583, 309, 775, 976, 291, 512, 27271, 281, 309, 13, 51865], "temperature": 0.0, "avg_logprob": -0.08701589448111398, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0013337768614292145}, {"id": 596, "seek": 226360, "start": 2263.6, "end": 2271.6, "text": " You can say, like, you might have a typo or maybe you forgot to add it into the JavaScript object of, like, your list of tasks.", "tokens": [50365, 509, 393, 584, 11, 411, 11, 291, 1062, 362, 257, 2125, 78, 420, 1310, 291, 5298, 281, 909, 309, 666, 264, 15778, 2657, 295, 11, 411, 11, 428, 1329, 295, 9608, 13, 50765], "temperature": 0.2, "avg_logprob": -0.10997845807413417, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00033772786264307797}, {"id": 597, "seek": 226360, "start": 2271.6, "end": 2272.6, "text": " Yes.", "tokens": [50765, 1079, 13, 50815], "temperature": 0.2, "avg_logprob": -0.10997845807413417, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00033772786264307797}, {"id": 598, "seek": 226360, "start": 2272.6, "end": 2275.6, "text": " So, this is, like, one of the things that I would like to do with Elm review.", "tokens": [50815, 407, 11, 341, 307, 11, 411, 11, 472, 295, 264, 721, 300, 286, 576, 411, 281, 360, 365, 2699, 76, 3131, 13, 50965], "temperature": 0.2, "avg_logprob": -0.10997845807413417, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00033772786264307797}, {"id": 599, "seek": 226360, "start": 2275.6, "end": 2286.6, "text": " And I put it on pause, but it's something that I'm pretty close to having done, which is having Elm review look at both Elm code and other files.", "tokens": [50965, 400, 286, 829, 309, 322, 10465, 11, 457, 309, 311, 746, 300, 286, 478, 1238, 1998, 281, 1419, 1096, 11, 597, 307, 1419, 2699, 76, 3131, 574, 412, 1293, 2699, 76, 3089, 293, 661, 7098, 13, 51515], "temperature": 0.2, "avg_logprob": -0.10997845807413417, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00033772786264307797}, {"id": 600, "seek": 226360, "start": 2286.6, "end": 2291.6, "text": " So, you could have, in theory, or it would work with this new feature that I'm working on.", "tokens": [51515, 407, 11, 291, 727, 362, 11, 294, 5261, 11, 420, 309, 576, 589, 365, 341, 777, 4111, 300, 286, 478, 1364, 322, 13, 51765], "temperature": 0.2, "avg_logprob": -0.10997845807413417, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00033772786264307797}, {"id": 601, "seek": 229160, "start": 2291.6, "end": 2296.6, "text": " That I have somewhere stashed up in a Git branch.", "tokens": [50365, 663, 286, 362, 4079, 342, 12219, 493, 294, 257, 16939, 9819, 13, 50615], "temperature": 0.0, "avg_logprob": -0.1597647156034197, "compression_ratio": 1.7302904564315353, "no_speech_prob": 0.0020619069691747427}, {"id": 602, "seek": 229160, "start": 2296.6, "end": 2308.6, "text": " You could have a rule that says, I want to look at all the Elm code, but I also want to look at this specific JavaScript file or just all JavaScript files.", "tokens": [50615, 509, 727, 362, 257, 4978, 300, 1619, 11, 286, 528, 281, 574, 412, 439, 264, 2699, 76, 3089, 11, 457, 286, 611, 528, 281, 574, 412, 341, 2685, 15778, 3991, 420, 445, 439, 15778, 7098, 13, 51215], "temperature": 0.0, "avg_logprob": -0.1597647156034197, "compression_ratio": 1.7302904564315353, "no_speech_prob": 0.0020619069691747427}, {"id": 603, "seek": 229160, "start": 2308.6, "end": 2314.6, "text": " And it could look at the list of tasks that you define in your call to concurrent register.", "tokens": [51215, 400, 309, 727, 574, 412, 264, 1329, 295, 9608, 300, 291, 6964, 294, 428, 818, 281, 37702, 7280, 13, 51515], "temperature": 0.0, "avg_logprob": -0.1597647156034197, "compression_ratio": 1.7302904564315353, "no_speech_prob": 0.0020619069691747427}, {"id": 604, "seek": 229160, "start": 2314.6, "end": 2318.6, "text": " And then it can compare that with what you have in Elm on the Elm side.", "tokens": [51515, 400, 550, 309, 393, 6794, 300, 365, 437, 291, 362, 294, 2699, 76, 322, 264, 2699, 76, 1252, 13, 51715], "temperature": 0.0, "avg_logprob": -0.1597647156034197, "compression_ratio": 1.7302904564315353, "no_speech_prob": 0.0020619069691747427}, {"id": 605, "seek": 229160, "start": 2318.6, "end": 2320.6, "text": " And it could figure out, like, oh, I have this.", "tokens": [51715, 400, 309, 727, 2573, 484, 11, 411, 11, 1954, 11, 286, 362, 341, 13, 51815], "temperature": 0.0, "avg_logprob": -0.1597647156034197, "compression_ratio": 1.7302904564315353, "no_speech_prob": 0.0020619069691747427}, {"id": 606, "seek": 232060, "start": 2320.6, "end": 2325.6, "text": " It could figure out, like, oh, well, the one you call on the Elm side does not exist in JavaScript.", "tokens": [50365, 467, 727, 2573, 484, 11, 411, 11, 1954, 11, 731, 11, 264, 472, 291, 818, 322, 264, 2699, 76, 1252, 775, 406, 2514, 294, 15778, 13, 50615], "temperature": 0.0, "avg_logprob": -0.050502391815185545, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00024137433501891792}, {"id": 607, "seek": 232060, "start": 2325.6, "end": 2334.6, "text": " Or even, like, oh, you have a task on the JavaScript side that is defined, but that is never used in Elm land.", "tokens": [50615, 1610, 754, 11, 411, 11, 1954, 11, 291, 362, 257, 5633, 322, 264, 15778, 1252, 300, 307, 7642, 11, 457, 300, 307, 1128, 1143, 294, 2699, 76, 2117, 13, 51065], "temperature": 0.0, "avg_logprob": -0.050502391815185545, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00024137433501891792}, {"id": 608, "seek": 232060, "start": 2334.6, "end": 2338.6, "text": " So, that's something that I would like to see happen at some point.", "tokens": [51065, 407, 11, 300, 311, 746, 300, 286, 576, 411, 281, 536, 1051, 412, 512, 935, 13, 51265], "temperature": 0.0, "avg_logprob": -0.050502391815185545, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00024137433501891792}, {"id": 609, "seek": 232060, "start": 2338.6, "end": 2341.6, "text": " Just, like, an extra layer of guarantees.", "tokens": [51265, 1449, 11, 411, 11, 364, 2857, 4583, 295, 32567, 13, 51415], "temperature": 0.0, "avg_logprob": -0.050502391815185545, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00024137433501891792}, {"id": 610, "seek": 232060, "start": 2341.6, "end": 2346.6, "text": " Like, you're sure that you have boilerplate, but it's done correctly for sure.", "tokens": [51415, 1743, 11, 291, 434, 988, 300, 291, 362, 39228, 37008, 11, 457, 309, 311, 1096, 8944, 337, 988, 13, 51665], "temperature": 0.0, "avg_logprob": -0.050502391815185545, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00024137433501891792}, {"id": 611, "seek": 232060, "start": 2346.6, "end": 2348.6, "text": " I would like to see that.", "tokens": [51665, 286, 576, 411, 281, 536, 300, 13, 51765], "temperature": 0.0, "avg_logprob": -0.050502391815185545, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00024137433501891792}, {"id": 612, "seek": 232060, "start": 2348.6, "end": 2349.6, "text": " Yeah.", "tokens": [51765, 865, 13, 51815], "temperature": 0.0, "avg_logprob": -0.050502391815185545, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.00024137433501891792}, {"id": 613, "seek": 234960, "start": 2349.6, "end": 2356.6, "text": " Because that case where you, that's an interesting one that you mentioned where you can't know whether it's unused.", "tokens": [50365, 1436, 300, 1389, 689, 291, 11, 300, 311, 364, 1880, 472, 300, 291, 2835, 689, 291, 393, 380, 458, 1968, 309, 311, 44383, 13, 50715], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 614, "seek": 234960, "start": 2356.6, "end": 2364.6, "text": " Even if they do match up, you're not sure if there's no way of figuring out unless you've got something like Elm review to tell you, like, you can get rid of these two.", "tokens": [50715, 2754, 498, 436, 360, 2995, 493, 11, 291, 434, 406, 988, 498, 456, 311, 572, 636, 295, 15213, 484, 5969, 291, 600, 658, 746, 411, 2699, 76, 3131, 281, 980, 291, 11, 411, 11, 291, 393, 483, 3973, 295, 613, 732, 13, 51115], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 615, "seek": 234960, "start": 2364.6, "end": 2366.6, "text": " Like, they're never called.", "tokens": [51115, 1743, 11, 436, 434, 1128, 1219, 13, 51215], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 616, "seek": 234960, "start": 2366.6, "end": 2368.6, "text": " The less JavaScript, the better.", "tokens": [51215, 440, 1570, 15778, 11, 264, 1101, 13, 51315], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 617, "seek": 234960, "start": 2370.6, "end": 2375.6, "text": " As long as you put as many rules as possible, then it's fine.", "tokens": [51415, 1018, 938, 382, 291, 829, 382, 867, 4474, 382, 1944, 11, 550, 309, 311, 2489, 13, 51665], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 618, "seek": 234960, "start": 2375.6, "end": 2376.6, "text": " Yeah.", "tokens": [51665, 865, 13, 51715], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 619, "seek": 234960, "start": 2376.6, "end": 2377.6, "text": " Just exchange, like, 10 lines of JavaScript for 10 lines of JavaScript.", "tokens": [51715, 1449, 7742, 11, 411, 11, 1266, 3876, 295, 15778, 337, 1266, 3876, 295, 15778, 13, 51765], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 620, "seek": 234960, "start": 2377.6, "end": 2378.6, "text": " Yeah.", "tokens": [51765, 865, 13, 51815], "temperature": 0.0, "avg_logprob": -0.12969709697522616, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.0022823777981102467}, {"id": 621, "seek": 237860, "start": 2378.6, "end": 2383.6, "text": " And then you can just write it for 10 Elm review rules and, yeah, it will be worth it, right?", "tokens": [50365, 400, 550, 291, 393, 445, 2464, 309, 337, 1266, 2699, 76, 3131, 4474, 293, 11, 1338, 11, 309, 486, 312, 3163, 309, 11, 558, 30, 50615], "temperature": 0.0, "avg_logprob": -0.20183683122907367, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0005785583052784204}, {"id": 622, "seek": 237860, "start": 2383.6, "end": 2384.6, "text": " Exactly.", "tokens": [50615, 7587, 13, 50665], "temperature": 0.0, "avg_logprob": -0.20183683122907367, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0005785583052784204}, {"id": 623, "seek": 237860, "start": 2384.6, "end": 2385.6, "text": " It's great.", "tokens": [50665, 467, 311, 869, 13, 50715], "temperature": 0.0, "avg_logprob": -0.20183683122907367, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0005785583052784204}, {"id": 624, "seek": 237860, "start": 2385.6, "end": 2390.6, "text": " I'm obviously biased.", "tokens": [50715, 286, 478, 2745, 28035, 13, 50965], "temperature": 0.0, "avg_logprob": -0.20183683122907367, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0005785583052784204}, {"id": 625, "seek": 237860, "start": 2390.6, "end": 2393.6, "text": " Something doesn't sound right here.", "tokens": [50965, 6595, 1177, 380, 1626, 558, 510, 13, 51115], "temperature": 0.0, "avg_logprob": -0.20183683122907367, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0005785583052784204}, {"id": 626, "seek": 237860, "start": 2393.6, "end": 2399.6, "text": " So, this might be less of a 1.0 thing, Andrew, and more of a future thing.", "tokens": [51115, 407, 11, 341, 1062, 312, 1570, 295, 257, 502, 13, 15, 551, 11, 10110, 11, 293, 544, 295, 257, 2027, 551, 13, 51415], "temperature": 0.0, "avg_logprob": -0.20183683122907367, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0005785583052784204}, {"id": 627, "seek": 237860, "start": 2399.6, "end": 2407.6, "text": " But I'm curious, especially as a maintainer of a similar mechanism who's been thinking about these things as well.", "tokens": [51415, 583, 286, 478, 6369, 11, 2318, 382, 257, 6909, 260, 295, 257, 2531, 7513, 567, 311, 668, 1953, 466, 613, 721, 382, 731, 13, 51815], "temperature": 0.0, "avg_logprob": -0.20183683122907367, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0005785583052784204}, {"id": 628, "seek": 240760, "start": 2407.6, "end": 2413.6, "text": " What are your thoughts on unit testing something like this?", "tokens": [50365, 708, 366, 428, 4598, 322, 4985, 4997, 746, 411, 341, 30, 50665], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 629, "seek": 240760, "start": 2413.6, "end": 2422.6, "text": " I think there are ways to make it doable in a pretty nice way, but it takes a little bit of introducing some abstractions to make that possible.", "tokens": [50665, 286, 519, 456, 366, 2098, 281, 652, 309, 41183, 294, 257, 1238, 1481, 636, 11, 457, 309, 2516, 257, 707, 857, 295, 15424, 512, 12649, 626, 281, 652, 300, 1944, 13, 51115], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 630, "seek": 240760, "start": 2422.6, "end": 2424.6, "text": " Is that something you've thought about?", "tokens": [51115, 1119, 300, 746, 291, 600, 1194, 466, 30, 51215], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 631, "seek": 240760, "start": 2424.6, "end": 2426.6, "text": " Very interesting you mentioned that.", "tokens": [51215, 4372, 1880, 291, 2835, 300, 13, 51315], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 632, "seek": 240760, "start": 2426.6, "end": 2428.6, "text": " So, I had the same thought.", "tokens": [51315, 407, 11, 286, 632, 264, 912, 1194, 13, 51415], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 633, "seek": 240760, "start": 2428.6, "end": 2434.6, "text": " I was like, I really wish I had a way to try these in isolation and, like, what are they doing in the real world?", "tokens": [51415, 286, 390, 411, 11, 286, 534, 3172, 286, 632, 257, 636, 281, 853, 613, 294, 16001, 293, 11, 411, 11, 437, 366, 436, 884, 294, 264, 957, 1002, 30, 51715], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 634, "seek": 240760, "start": 2434.6, "end": 2435.6, "text": " And I wrote for that runs on CI.", "tokens": [51715, 400, 286, 4114, 337, 300, 6676, 322, 37777, 13, 51765], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 635, "seek": 240760, "start": 2435.6, "end": 2436.6, "text": " And I wrote for that runs on CI.", "tokens": [51765, 400, 286, 4114, 337, 300, 6676, 322, 37777, 13, 51815], "temperature": 0.0, "avg_logprob": -0.1247211870296981, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01540821697562933}, {"id": 636, "seek": 243660, "start": 2436.6, "end": 2438.6, "text": " And I wrote for that runs on CI.", "tokens": [50365, 400, 286, 4114, 337, 300, 6676, 322, 37777, 13, 50465], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 637, "seek": 243660, "start": 2438.6, "end": 2440.6, "text": " And then they changed what I'm doing.", "tokens": [50465, 400, 550, 436, 3105, 437, 286, 478, 884, 13, 50565], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 638, "seek": 243660, "start": 2440.6, "end": 2443.6, "text": " But I've got, like, a little integration test runner that's, like, a custom.", "tokens": [50565, 583, 286, 600, 658, 11, 411, 11, 257, 707, 10980, 1500, 24376, 300, 311, 11, 411, 11, 257, 2375, 13, 50715], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 639, "seek": 243660, "start": 2443.6, "end": 2445.6, "text": " It's kind of a custom program for it.", "tokens": [50715, 467, 311, 733, 295, 257, 2375, 1461, 337, 309, 13, 50815], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 640, "seek": 243660, "start": 2445.6, "end": 2450.6, "text": " And it's not fancy, but it, like, it does the trick.", "tokens": [50815, 400, 309, 311, 406, 10247, 11, 457, 309, 11, 411, 11, 309, 775, 264, 4282, 13, 51065], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 641, "seek": 243660, "start": 2450.6, "end": 2457.6, "text": " Like, it's kind of a simple way of saying, like, I expect this value to be a success and you can match on this.", "tokens": [51065, 1743, 11, 309, 311, 733, 295, 257, 2199, 636, 295, 1566, 11, 411, 11, 286, 2066, 341, 2158, 281, 312, 257, 2245, 293, 291, 393, 2995, 322, 341, 13, 51415], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 642, "seek": 243660, "start": 2457.6, "end": 2460.6, "text": " You can assert on, like, how long the tasks take.", "tokens": [51415, 509, 393, 19810, 322, 11, 411, 11, 577, 938, 264, 9608, 747, 13, 51565], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 643, "seek": 243660, "start": 2460.6, "end": 2464.6, "text": " So, I've got one that it was actually very hard to test the batch.", "tokens": [51565, 407, 11, 286, 600, 658, 472, 300, 309, 390, 767, 588, 1152, 281, 1500, 264, 15245, 13, 51765], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 644, "seek": 243660, "start": 2464.6, "end": 2465.6, "text": " It was very hard.", "tokens": [51765, 467, 390, 588, 1152, 13, 51815], "temperature": 0.0, "avg_logprob": -0.14964566359648834, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0017767396057024598}, {"id": 645, "seek": 246560, "start": 2465.6, "end": 2466.6, "text": " It was very hard.", "tokens": [50365, 467, 390, 588, 1152, 13, 50415], "temperature": 0.0, "avg_logprob": -0.17250055624238142, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.0020310268737375736}, {"id": 646, "seek": 246560, "start": 2466.6, "end": 2467.6, "text": " It was very hard.", "tokens": [50415, 467, 390, 588, 1152, 13, 50465], "temperature": 0.0, "avg_logprob": -0.17250055624238142, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.0020310268737375736}, {"id": 647, "seek": 246560, "start": 2467.6, "end": 2477.6, "text": " So, this batch implementation of, like, running a very large, very large list of tasks, running that in Elm test for some reason, which I haven't figured out, it does not enjoy it.", "tokens": [50465, 407, 11, 341, 15245, 11420, 295, 11, 411, 11, 2614, 257, 588, 2416, 11, 588, 2416, 1329, 295, 9608, 11, 2614, 300, 294, 2699, 76, 1500, 337, 512, 1778, 11, 597, 286, 2378, 380, 8932, 484, 11, 309, 775, 406, 2103, 309, 13, 50965], "temperature": 0.0, "avg_logprob": -0.17250055624238142, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.0020310268737375736}, {"id": 648, "seek": 246560, "start": 2477.6, "end": 2479.6, "text": " It really, really struggles.", "tokens": [50965, 467, 534, 11, 534, 17592, 13, 51065], "temperature": 0.0, "avg_logprob": -0.17250055624238142, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.0020310268737375736}, {"id": 649, "seek": 246560, "start": 2479.6, "end": 2485.6, "text": " But if you're actually running it in a real application, it goes very quickly and doesn't have any problems.", "tokens": [51065, 583, 498, 291, 434, 767, 2614, 309, 294, 257, 957, 3861, 11, 309, 1709, 588, 2661, 293, 1177, 380, 362, 604, 2740, 13, 51365], "temperature": 0.0, "avg_logprob": -0.17250055624238142, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.0020310268737375736}, {"id": 650, "seek": 246560, "start": 2485.6, "end": 2489.6, "text": " If anybody knows why, that would be, I'm all open to suggestions.", "tokens": [51365, 759, 4472, 3255, 983, 11, 300, 576, 312, 11, 286, 478, 439, 1269, 281, 13396, 13, 51565], "temperature": 0.0, "avg_logprob": -0.17250055624238142, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.0020310268737375736}, {"id": 651, "seek": 246560, "start": 2489.6, "end": 2493.6, "text": " But having that confidence, being able to actually run the tasks in, like, a test suite was really, really, really hard.", "tokens": [51565, 583, 1419, 300, 6687, 11, 885, 1075, 281, 767, 1190, 264, 9608, 294, 11, 411, 11, 257, 1500, 14205, 390, 534, 11, 534, 11, 534, 1152, 13, 51765], "temperature": 0.0, "avg_logprob": -0.17250055624238142, "compression_ratio": 1.8215488215488216, "no_speech_prob": 0.0020310268737375736}, {"id": 652, "seek": 249360, "start": 2493.6, "end": 2505.6, "text": " I mean, I think that the fact that I had to run all of my tasks in, like, a test suite was really, really valuable to make sure that I didn't mess any of the wiring up or stuff was coming back strange.", "tokens": [50365, 286, 914, 11, 286, 519, 300, 264, 1186, 300, 286, 632, 281, 1190, 439, 295, 452, 9608, 294, 11, 411, 11, 257, 1500, 14205, 390, 534, 11, 534, 8263, 281, 652, 988, 300, 286, 994, 380, 2082, 604, 295, 264, 27520, 493, 420, 1507, 390, 1348, 646, 5861, 13, 50965], "temperature": 0.4, "avg_logprob": -0.3224612742930919, "compression_ratio": 1.664, "no_speech_prob": 0.001398380845785141}, {"id": 653, "seek": 249360, "start": 2505.6, "end": 2511.6, "text": " So, that is a JavaScript test suite, right, which runs the Elm or partial Elm application maybe?", "tokens": [50965, 407, 11, 300, 307, 257, 15778, 1500, 14205, 11, 558, 11, 597, 6676, 264, 2699, 76, 420, 14641, 2699, 76, 3861, 1310, 30, 51265], "temperature": 0.4, "avg_logprob": -0.3224612742930919, "compression_ratio": 1.664, "no_speech_prob": 0.001398380845785141}, {"id": 654, "seek": 249360, "start": 2511.6, "end": 2514.6, "text": " So, it's actually just an Elm application.", "tokens": [51265, 407, 11, 309, 311, 767, 445, 364, 2699, 76, 3861, 13, 51415], "temperature": 0.4, "avg_logprob": -0.3224612742930919, "compression_ratio": 1.664, "no_speech_prob": 0.001398380845785141}, {"id": 655, "seek": 249360, "start": 2514.6, "end": 2521.6, "text": " And I've written, it's like a single, like, a platform worker application.", "tokens": [51415, 400, 286, 600, 3720, 11, 309, 311, 411, 257, 2167, 11, 411, 11, 257, 3663, 11346, 3861, 13, 51765], "temperature": 0.4, "avg_logprob": -0.3224612742930919, "compression_ratio": 1.664, "no_speech_prob": 0.001398380845785141}, {"id": 656, "seek": 252160, "start": 2521.6, "end": 2525.6, "text": " And you define, like, a list of tasks in your, like, in your test file.", "tokens": [50365, 400, 291, 6964, 11, 411, 11, 257, 1329, 295, 9608, 294, 428, 11, 411, 11, 294, 428, 1500, 3991, 13, 50565], "temperature": 0.0, "avg_logprob": -0.09052951676504953, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0006158032338134944}, {"id": 657, "seek": 252160, "start": 2525.6, "end": 2528.6, "text": " And then you're just passing those to the runner and it runs them.", "tokens": [50565, 400, 550, 291, 434, 445, 8437, 729, 281, 264, 24376, 293, 309, 6676, 552, 13, 50715], "temperature": 0.0, "avg_logprob": -0.09052951676504953, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0006158032338134944}, {"id": 658, "seek": 252160, "start": 2528.6, "end": 2533.6, "text": " And then you can, there's some Elm functions to match on the results that come back.", "tokens": [50715, 400, 550, 291, 393, 11, 456, 311, 512, 2699, 76, 6828, 281, 2995, 322, 264, 3542, 300, 808, 646, 13, 50965], "temperature": 0.0, "avg_logprob": -0.09052951676504953, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0006158032338134944}, {"id": 659, "seek": 252160, "start": 2533.6, "end": 2536.6, "text": " It's not, it's not packaged up at the moment.", "tokens": [50965, 467, 311, 406, 11, 309, 311, 406, 38162, 493, 412, 264, 1623, 13, 51115], "temperature": 0.0, "avg_logprob": -0.09052951676504953, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0006158032338134944}, {"id": 660, "seek": 252160, "start": 2536.6, "end": 2546.6, "text": " But if it's something that folks are interested in, like, if there's a need for it, I could definitely think about extracting it out into something that's a bit more usable in other projects.", "tokens": [51115, 583, 498, 309, 311, 746, 300, 4024, 366, 3102, 294, 11, 411, 11, 498, 456, 311, 257, 643, 337, 309, 11, 286, 727, 2138, 519, 466, 49844, 309, 484, 666, 746, 300, 311, 257, 857, 544, 29975, 294, 661, 4455, 13, 51615], "temperature": 0.0, "avg_logprob": -0.09052951676504953, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0006158032338134944}, {"id": 661, "seek": 252160, "start": 2546.6, "end": 2550.6, "text": " My initial thought would be to use Elm program test for this.", "tokens": [51615, 1222, 5883, 1194, 576, 312, 281, 764, 2699, 76, 1461, 1500, 337, 341, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09052951676504953, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0006158032338134944}, {"id": 662, "seek": 252160, "start": 2550.6, "end": 2551.6, "text": " Yeah.", "tokens": [51815, 865, 13, 51865], "temperature": 0.0, "avg_logprob": -0.09052951676504953, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0006158032338134944}, {"id": 663, "seek": 255160, "start": 2551.6, "end": 2559.6, "text": " And then have a library that makes Elm concur task work for Elm program test.", "tokens": [50365, 400, 550, 362, 257, 6405, 300, 1669, 2699, 76, 23702, 5633, 589, 337, 2699, 76, 1461, 1500, 13, 50765], "temperature": 0.0, "avg_logprob": -0.09343543052673339, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0010216865921393037}, {"id": 664, "seek": 255160, "start": 2559.6, "end": 2567.6, "text": " And where you would mock the JavaScript responses or the HTTP responses and so on.", "tokens": [50765, 400, 689, 291, 576, 17362, 264, 15778, 13019, 420, 264, 33283, 13019, 293, 370, 322, 13, 51165], "temperature": 0.0, "avg_logprob": -0.09343543052673339, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0010216865921393037}, {"id": 665, "seek": 255160, "start": 2567.6, "end": 2569.6, "text": " And I think that would work quite well.", "tokens": [51165, 400, 286, 519, 300, 576, 589, 1596, 731, 13, 51265], "temperature": 0.0, "avg_logprob": -0.09343543052673339, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0010216865921393037}, {"id": 666, "seek": 255160, "start": 2569.6, "end": 2571.6, "text": " And it would probably work in Elm test.", "tokens": [51265, 400, 309, 576, 1391, 589, 294, 2699, 76, 1500, 13, 51365], "temperature": 0.0, "avg_logprob": -0.09343543052673339, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0010216865921393037}, {"id": 667, "seek": 255160, "start": 2571.6, "end": 2577.6, "text": " Maybe you should open an issue for that problem you've had if you haven't already.", "tokens": [51365, 2704, 291, 820, 1269, 364, 2734, 337, 300, 1154, 291, 600, 632, 498, 291, 2378, 380, 1217, 13, 51665], "temperature": 0.0, "avg_logprob": -0.09343543052673339, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0010216865921393037}, {"id": 668, "seek": 255160, "start": 2577.6, "end": 2580.6, "text": " But, yeah, you're right that you're mocking the JavaScript side.", "tokens": [51665, 583, 11, 1338, 11, 291, 434, 558, 300, 291, 434, 49792, 264, 15778, 1252, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09343543052673339, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0010216865921393037}, {"id": 669, "seek": 258060, "start": 2580.6, "end": 2587.6, "text": " So it wouldn't be as reliable as your integration tests.", "tokens": [50365, 407, 309, 2759, 380, 312, 382, 12924, 382, 428, 10980, 6921, 13, 50715], "temperature": 0.0, "avg_logprob": -0.1549430813705712, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.002816766267642379}, {"id": 670, "seek": 258060, "start": 2587.6, "end": 2592.6, "text": " I mean, integration tests are usually more reliable, just not as precise as unit tests.", "tokens": [50715, 286, 914, 11, 10980, 6921, 366, 2673, 544, 12924, 11, 445, 406, 382, 13600, 382, 4985, 6921, 13, 50965], "temperature": 0.0, "avg_logprob": -0.1549430813705712, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.002816766267642379}, {"id": 671, "seek": 258060, "start": 2592.6, "end": 2593.6, "text": " Yeah.", "tokens": [50965, 865, 13, 51015], "temperature": 0.0, "avg_logprob": -0.1549430813705712, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.002816766267642379}, {"id": 672, "seek": 258060, "start": 2593.6, "end": 2595.6, "text": " That's a very interesting idea.", "tokens": [51015, 663, 311, 257, 588, 1880, 1558, 13, 51115], "temperature": 0.0, "avg_logprob": -0.1549430813705712, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.002816766267642379}, {"id": 673, "seek": 258060, "start": 2595.6, "end": 2596.6, "text": " Yeah.", "tokens": [51115, 865, 13, 51165], "temperature": 0.0, "avg_logprob": -0.1549430813705712, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.002816766267642379}, {"id": 674, "seek": 258060, "start": 2596.6, "end": 2598.6, "text": " It makes me think of, like, Elm program test, obviously.", "tokens": [51165, 467, 1669, 385, 519, 295, 11, 411, 11, 2699, 76, 1461, 1500, 11, 2745, 13, 51265], "temperature": 0.0, "avg_logprob": -0.1549430813705712, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.002816766267642379}, {"id": 675, "seek": 258060, "start": 2598.6, "end": 2609.6, "text": " And then Martin Stewart's Elm program test for Landerer where you get responses from the backend, interactions with the backend, with the frontend, and all those kinds of things that you could simulate.", "tokens": [51265, 400, 550, 9184, 25951, 311, 2699, 76, 1461, 1500, 337, 441, 4483, 260, 689, 291, 483, 13019, 490, 264, 38087, 11, 13280, 365, 264, 38087, 11, 365, 264, 1868, 521, 11, 293, 439, 729, 3685, 295, 721, 300, 291, 727, 27817, 13, 51815], "temperature": 0.0, "avg_logprob": -0.1549430813705712, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.002816766267642379}, {"id": 676, "seek": 260960, "start": 2609.6, "end": 2613.6, "text": " And that's, since it's all just Elm code, it's fine.", "tokens": [50365, 400, 300, 311, 11, 1670, 309, 311, 439, 445, 2699, 76, 3089, 11, 309, 311, 2489, 13, 50565], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 677, "seek": 260960, "start": 2613.6, "end": 2617.6, "text": " I haven't seen that before, the Elm program test for Landerer.", "tokens": [50565, 286, 2378, 380, 1612, 300, 949, 11, 264, 2699, 76, 1461, 1500, 337, 441, 4483, 260, 13, 50765], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 678, "seek": 260960, "start": 2617.6, "end": 2618.6, "text": " It's worth checking out.", "tokens": [50765, 467, 311, 3163, 8568, 484, 13, 50815], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 679, "seek": 260960, "start": 2618.6, "end": 2626.6, "text": " There's a great talk where Martin walks through using the tool and shows the visual runner with all the connected clients.", "tokens": [50815, 821, 311, 257, 869, 751, 689, 9184, 12896, 807, 1228, 264, 2290, 293, 3110, 264, 5056, 24376, 365, 439, 264, 4582, 6982, 13, 51215], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 680, "seek": 260960, "start": 2626.6, "end": 2627.6, "text": " It's very cool.", "tokens": [51215, 467, 311, 588, 1627, 13, 51265], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 681, "seek": 260960, "start": 2627.6, "end": 2632.6, "text": " And, yeah, it gets really interesting, too, if you had some way.", "tokens": [51265, 400, 11, 1338, 11, 309, 2170, 534, 1880, 11, 886, 11, 498, 291, 632, 512, 636, 13, 51515], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 682, "seek": 260960, "start": 2632.6, "end": 2634.6, "text": " I really like the Elm program test idea as well.", "tokens": [51515, 286, 534, 411, 264, 2699, 76, 1461, 1500, 1558, 382, 731, 13, 51615], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 683, "seek": 260960, "start": 2634.6, "end": 2638.6, "text": " And, like, if you had some way to even have a few sort of clients.", "tokens": [51615, 400, 11, 411, 11, 498, 291, 632, 512, 636, 281, 754, 362, 257, 1326, 1333, 295, 6982, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09179171225182096, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.0008228250662796199}, {"id": 684, "seek": 263860, "start": 2638.6, "end": 2639.6, "text": " Yeah.", "tokens": [50365, 865, 13, 50415], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 685, "seek": 263860, "start": 2639.6, "end": 2640.6, "text": " I mean, I think it's a really good idea.", "tokens": [50415, 286, 914, 11, 286, 519, 309, 311, 257, 534, 665, 1558, 13, 50465], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 686, "seek": 263860, "start": 2640.6, "end": 2650.6, "text": " And I think it's a really good idea to have a few sort of core implementations for things like local storage where you could, you know, you could imagine if you wanted to get really fancy with this.", "tokens": [50465, 400, 286, 519, 309, 311, 257, 534, 665, 1558, 281, 362, 257, 1326, 1333, 295, 4965, 4445, 763, 337, 721, 411, 2654, 6725, 689, 291, 727, 11, 291, 458, 11, 291, 727, 3811, 498, 291, 1415, 281, 483, 534, 10247, 365, 341, 13, 50965], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 687, "seek": 263860, "start": 2650.6, "end": 2651.6, "text": " Oh.", "tokens": [50965, 876, 13, 51015], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 688, "seek": 263860, "start": 2651.6, "end": 2659.6, "text": " You could say, given this initial value in local storage and then let it actually simulate for the setting and getting in local storage.", "tokens": [51015, 509, 727, 584, 11, 2212, 341, 5883, 2158, 294, 2654, 6725, 293, 550, 718, 309, 767, 27817, 337, 264, 3287, 293, 1242, 294, 2654, 6725, 13, 51415], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 689, "seek": 263860, "start": 2659.6, "end": 2661.6, "text": " Let it actually simulate that.", "tokens": [51415, 961, 309, 767, 27817, 300, 13, 51515], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 690, "seek": 263860, "start": 2661.6, "end": 2664.6, "text": " And now you could have Elm program test.", "tokens": [51515, 400, 586, 291, 727, 362, 2699, 76, 1461, 1500, 13, 51665], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 691, "seek": 263860, "start": 2664.6, "end": 2667.6, "text": " You could even have a way to expand some of these definitions.", "tokens": [51665, 509, 727, 754, 362, 257, 636, 281, 5268, 512, 295, 613, 21988, 13, 51815], "temperature": 0.4, "avg_logprob": -0.30359996448863635, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.0010811338433995843}, {"id": 692, "seek": 266760, "start": 2667.6, "end": 2677.6, "text": " Where there's, like, a certain set of core web platform primitives that you're able to simulate in a fairly realistic way.", "tokens": [50365, 2305, 456, 311, 11, 411, 11, 257, 1629, 992, 295, 4965, 3670, 3663, 2886, 38970, 300, 291, 434, 1075, 281, 27817, 294, 257, 6457, 12465, 636, 13, 50865], "temperature": 0.0, "avg_logprob": -0.13230151546244717, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.007284554652869701}, {"id": 693, "seek": 266760, "start": 2677.6, "end": 2679.6, "text": " So, it's really interesting, man.", "tokens": [50865, 407, 11, 309, 311, 534, 1880, 11, 587, 13, 50965], "temperature": 0.0, "avg_logprob": -0.13230151546244717, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.007284554652869701}, {"id": 694, "seek": 266760, "start": 2679.6, "end": 2682.6, "text": " I think you could go one or two or ten steps further.", "tokens": [50965, 286, 519, 291, 727, 352, 472, 420, 732, 420, 2064, 4439, 3052, 13, 51115], "temperature": 0.0, "avg_logprob": -0.13230151546244717, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.007284554652869701}, {"id": 695, "seek": 266760, "start": 2682.6, "end": 2689.6, "text": " Like, if you try to simulate HTTP, you just simulate a browser and DNS systems and everything.", "tokens": [51115, 1743, 11, 498, 291, 853, 281, 27817, 33283, 11, 291, 445, 27817, 257, 11185, 293, 35153, 3652, 293, 1203, 13, 51465], "temperature": 0.0, "avg_logprob": -0.13230151546244717, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.007284554652869701}, {"id": 696, "seek": 266760, "start": 2689.6, "end": 2696.6, "text": " And, I mean, how hard can it just be to simulate Google.com?", "tokens": [51465, 400, 11, 286, 914, 11, 577, 1152, 393, 309, 445, 312, 281, 27817, 3329, 13, 1112, 30, 51815], "temperature": 0.0, "avg_logprob": -0.13230151546244717, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.007284554652869701}, {"id": 697, "seek": 269660, "start": 2696.6, "end": 2697.6, "text": " Any Elm package?", "tokens": [50365, 2639, 2699, 76, 7372, 30, 50415], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 698, "seek": 269660, "start": 2697.6, "end": 2698.6, "text": " Like, come on.", "tokens": [50415, 1743, 11, 808, 322, 13, 50465], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 699, "seek": 269660, "start": 2698.6, "end": 2699.6, "text": " You can do it, Andrew.", "tokens": [50465, 509, 393, 360, 309, 11, 10110, 13, 50515], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 700, "seek": 269660, "start": 2699.6, "end": 2700.6, "text": " Exactly.", "tokens": [50515, 7587, 13, 50565], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 701, "seek": 269660, "start": 2700.6, "end": 2701.6, "text": " I will say it too after this call.", "tokens": [50565, 286, 486, 584, 309, 886, 934, 341, 818, 13, 50615], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 702, "seek": 269660, "start": 2701.6, "end": 2706.6, "text": " Assuming that there's no free will and that the universe is deterministic and not probabilistic, in theory you could simulate the entire world and all its behaviors in a pure way.", "tokens": [50615, 6281, 24919, 300, 456, 311, 572, 1737, 486, 293, 300, 264, 6445, 307, 15957, 3142, 293, 406, 31959, 3142, 11, 294, 5261, 291, 727, 27817, 264, 2302, 1002, 293, 439, 1080, 15501, 294, 257, 6075, 636, 13, 50865], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 703, "seek": 269660, "start": 2706.6, "end": 2707.6, "text": " Just as a pure function.", "tokens": [50865, 1449, 382, 257, 6075, 2445, 13, 50915], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 704, "seek": 269660, "start": 2707.6, "end": 2708.6, "text": " So, that's if you want to go a few steps further than that.", "tokens": [50915, 407, 11, 300, 311, 498, 291, 528, 281, 352, 257, 1326, 4439, 3052, 813, 300, 13, 50965], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 705, "seek": 269660, "start": 2708.6, "end": 2709.6, "text": " I think that's a good idea.", "tokens": [50965, 286, 519, 300, 311, 257, 665, 1558, 13, 51015], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 706, "seek": 269660, "start": 2709.6, "end": 2710.6, "text": " I think that's a good idea.", "tokens": [51015, 286, 519, 300, 311, 257, 665, 1558, 13, 51065], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 707, "seek": 269660, "start": 2710.6, "end": 2711.6, "text": " Yeah.", "tokens": [51065, 865, 13, 51115], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 708, "seek": 269660, "start": 2711.6, "end": 2712.6, "text": " I think that's a good idea.", "tokens": [51115, 286, 519, 300, 311, 257, 665, 1558, 13, 51165], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 709, "seek": 269660, "start": 2712.6, "end": 2713.6, "text": " I think that's a good idea.", "tokens": [51165, 286, 519, 300, 311, 257, 665, 1558, 13, 51215], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 710, "seek": 269660, "start": 2713.6, "end": 2714.6, "text": " Yeah.", "tokens": [51215, 865, 13, 51265], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 711, "seek": 269660, "start": 2714.6, "end": 2715.6, "text": " I think that's a good idea.", "tokens": [51265, 286, 519, 300, 311, 257, 665, 1558, 13, 51315], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 712, "seek": 269660, "start": 2715.6, "end": 2716.6, "text": " Yeah.", "tokens": [51315, 865, 13, 51365], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 713, "seek": 269660, "start": 2716.6, "end": 2717.6, "text": " I think that's a good idea.", "tokens": [51365, 286, 519, 300, 311, 257, 665, 1558, 13, 51415], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 714, "seek": 269660, "start": 2717.6, "end": 2718.6, "text": " Yeah.", "tokens": [51415, 865, 13, 51465], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 715, "seek": 269660, "start": 2718.6, "end": 2719.6, "text": " I think that's a good idea.", "tokens": [51465, 286, 519, 300, 311, 257, 665, 1558, 13, 51515], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 716, "seek": 269660, "start": 2719.6, "end": 2720.6, "text": " Yeah.", "tokens": [51515, 865, 13, 51565], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 717, "seek": 269660, "start": 2720.6, "end": 2721.6, "text": " Yeah.", "tokens": [51565, 865, 13, 51615], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 718, "seek": 269660, "start": 2721.6, "end": 2722.6, "text": " Yeah.", "tokens": [51615, 865, 13, 51665], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 719, "seek": 269660, "start": 2722.6, "end": 2723.6, "text": " Yeah.", "tokens": [51665, 865, 13, 51715], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 720, "seek": 269660, "start": 2723.6, "end": 2724.6, "text": " Yeah.", "tokens": [51715, 865, 13, 51765], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 721, "seek": 269660, "start": 2724.6, "end": 2725.6, "text": " Yeah.", "tokens": [51765, 865, 13, 51815], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 722, "seek": 269660, "start": 2725.6, "end": 2726.6, "text": " Yeah.", "tokens": [51815, 865, 13, 51865], "temperature": 0.2, "avg_logprob": -0.2561724251563396, "compression_ratio": 2.358490566037736, "no_speech_prob": 0.0016223767306655645}, {"id": 723, "seek": 272660, "start": 2726.6, "end": 2727.6, "text": " Yeah.", "tokens": [50365, 865, 13, 50415], "temperature": 0.6, "avg_logprob": -0.3743750415270842, "compression_ratio": 1.3905325443786982, "no_speech_prob": 0.0013331678928807378}, {"id": 724, "seek": 272660, "start": 2727.6, "end": 2728.6, "text": " Yeah.", "tokens": [50415, 865, 13, 50465], "temperature": 0.6, "avg_logprob": -0.3743750415270842, "compression_ratio": 1.3905325443786982, "no_speech_prob": 0.0013331678928807378}, {"id": 725, "seek": 272660, "start": 2728.6, "end": 2733.6, "text": " I feel like you went from giving him, like, oh, this is not for V1 but this could be for V2.", "tokens": [50465, 286, 841, 411, 291, 1437, 490, 2902, 796, 11, 411, 11, 1954, 11, 341, 307, 406, 337, 691, 16, 457, 341, 727, 312, 337, 691, 17, 13, 50715], "temperature": 0.6, "avg_logprob": -0.3743750415270842, "compression_ratio": 1.3905325443786982, "no_speech_prob": 0.0013331678928807378}, {"id": 726, "seek": 272660, "start": 2733.6, "end": 2740.6, "text": " And now we're, like, can you just reimplement all of computer science please?", "tokens": [50715, 400, 586, 321, 434, 11, 411, 11, 393, 291, 445, 33433, 43704, 439, 295, 3820, 3497, 1767, 30, 51065], "temperature": 0.6, "avg_logprob": -0.3743750415270842, "compression_ratio": 1.3905325443786982, "no_speech_prob": 0.0013331678928807378}, {"id": 727, "seek": 272660, "start": 2740.6, "end": 2748.56, "text": " PR is welcome.", "tokens": [51065, 11568, 307, 2928, 13, 51463], "temperature": 0.6, "avg_logprob": -0.3743750415270842, "compression_ratio": 1.3905325443786982, "no_speech_prob": 0.0013331678928807378}, {"id": 728, "seek": 272660, "start": 2748.56, "end": 2754.68, "text": " Yeah.", "tokens": [51463, 865, 13, 51769], "temperature": 0.6, "avg_logprob": -0.3743750415270842, "compression_ratio": 1.3905325443786982, "no_speech_prob": 0.0013331678928807378}, {"id": 729, "seek": 272660, "start": 2754.68, "end": 2756.6, "text": " So, one thing we haven't asked.", "tokens": [51769, 407, 11, 472, 551, 321, 2378, 380, 2351, 13, 51865], "temperature": 0.6, "avg_logprob": -0.3743750415270842, "compression_ratio": 1.3905325443786982, "no_speech_prob": 0.0013331678928807378}, {"id": 730, "seek": 275660, "start": 2756.6, "end": 2762.6, "text": " you about or maybe not enough like what kind of use cases did you actually use this uh", "tokens": [50365, 291, 466, 420, 1310, 406, 1547, 411, 437, 733, 295, 764, 3331, 630, 291, 767, 764, 341, 2232, 50665], "temperature": 0.0, "avg_logprob": -0.07129152872229136, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.030644729733467102}, {"id": 731, "seek": 275660, "start": 2763.18, "end": 2770.88, "text": " package for like when did you feel the need for something like this yeah that's a good question", "tokens": [50694, 7372, 337, 411, 562, 630, 291, 841, 264, 643, 337, 746, 411, 341, 1338, 300, 311, 257, 665, 1168, 51079], "temperature": 0.0, "avg_logprob": -0.07129152872229136, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.030644729733467102}, {"id": 732, "seek": 275660, "start": 2770.88, "end": 2779.52, "text": " i so my my initial need for it is about a year and a bit ago i was working on a small medical app", "tokens": [51079, 741, 370, 452, 452, 5883, 643, 337, 309, 307, 466, 257, 1064, 293, 257, 857, 2057, 741, 390, 1364, 322, 257, 1359, 4625, 724, 51511], "temperature": 0.0, "avg_logprob": -0.07129152872229136, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.030644729733467102}, {"id": 733, "seek": 275660, "start": 2779.52, "end": 2786.0, "text": " and i wrote the back end in elm as an experiment and thought this is great it's lovely just just", "tokens": [51511, 293, 741, 4114, 264, 646, 917, 294, 806, 76, 382, 364, 5120, 293, 1194, 341, 307, 869, 309, 311, 7496, 445, 445, 51835], "temperature": 0.0, "avg_logprob": -0.07129152872229136, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.030644729733467102}, {"id": 734, "seek": 278600, "start": 2786.0, "end": 2793.52, "text": " such a nice pleasant experience and i'd set it up where each hdp endpoint was effectively a task", "tokens": [50365, 1270, 257, 1481, 16232, 1752, 293, 741, 1116, 992, 309, 493, 689, 1184, 276, 67, 79, 35795, 390, 8659, 257, 5633, 50741], "temperature": 0.0, "avg_logprob": -0.05637614777747621, "compression_ratio": 1.7544642857142858, "no_speech_prob": 6.401292921509594e-05}, {"id": 735, "seek": 278600, "start": 2793.52, "end": 2799.86, "text": " and i was using elm task port then and everything it was working great i was really really happy with", "tokens": [50741, 293, 741, 390, 1228, 806, 76, 5633, 2436, 550, 293, 1203, 309, 390, 1364, 869, 741, 390, 534, 534, 2055, 365, 51058], "temperature": 0.0, "avg_logprob": -0.05637614777747621, "compression_ratio": 1.7544642857142858, "no_speech_prob": 6.401292921509594e-05}, {"id": 736, "seek": 278600, "start": 2799.86, "end": 2808.2, "text": " the with the the readability and like how easy it was to change stuff but the only problem i found", "tokens": [51058, 264, 365, 264, 264, 1401, 2310, 293, 411, 577, 1858, 309, 390, 281, 1319, 1507, 457, 264, 787, 1154, 741, 1352, 51475], "temperature": 0.0, "avg_logprob": -0.05637614777747621, "compression_ratio": 1.7544642857142858, "no_speech_prob": 6.401292921509594e-05}, {"id": 737, "seek": 278600, "start": 2808.2, "end": 2815.98, "text": " was that these tasks were performed one after the other so all of the subtasks and there were a", "tokens": [51475, 390, 300, 613, 9608, 645, 10332, 472, 934, 264, 661, 370, 439, 295, 264, 7257, 296, 1694, 293, 456, 645, 257, 51864], "temperature": 0.0, "avg_logprob": -0.05637614777747621, "compression_ratio": 1.7544642857142858, "no_speech_prob": 6.401292921509594e-05}, {"id": 738, "seek": 281598, "start": 2815.98, "end": 2821.04, "text": " couple of things that i had to do to get it up and there were a couple of i had it in a cli application", "tokens": [50365, 1916, 295, 721, 300, 741, 632, 281, 360, 281, 483, 309, 493, 293, 456, 645, 257, 1916, 295, 741, 632, 309, 294, 257, 596, 72, 3861, 50618], "temperature": 0.0, "avg_logprob": -0.1488319772188781, "compression_ratio": 1.9291338582677164, "no_speech_prob": 0.000882327847648412}, {"id": 739, "seek": 281598, "start": 2821.04, "end": 2826.76, "text": " as well that was kind of doing some background stuff on like there was some sort of like work", "tokens": [50618, 382, 731, 300, 390, 733, 295, 884, 512, 3678, 1507, 322, 411, 456, 390, 512, 1333, 295, 411, 589, 50904], "temperature": 0.0, "avg_logprob": -0.1488319772188781, "compression_ratio": 1.9291338582677164, "no_speech_prob": 0.000882327847648412}, {"id": 740, "seek": 281598, "start": 2826.76, "end": 2832.64, "text": " processes that was running and it just made me think this would be so much nicer if i could run", "tokens": [50904, 7555, 300, 390, 2614, 293, 309, 445, 1027, 385, 519, 341, 576, 312, 370, 709, 22842, 498, 741, 727, 1190, 51198], "temperature": 0.0, "avg_logprob": -0.1488319772188781, "compression_ratio": 1.9291338582677164, "no_speech_prob": 0.000882327847648412}, {"id": 741, "seek": 281598, "start": 2832.64, "end": 2839.18, "text": " a lot if i could batch up a lot of these tasks at the same time so and then i did that and now", "tokens": [51198, 257, 688, 498, 741, 727, 15245, 493, 257, 688, 295, 613, 9608, 412, 264, 912, 565, 370, 293, 550, 741, 630, 300, 293, 586, 51525], "temperature": 0.0, "avg_logprob": -0.1488319772188781, "compression_ratio": 1.9291338582677164, "no_speech_prob": 0.000882327847648412}, {"id": 742, "seek": 281598, "start": 2839.18, "end": 2845.68, "text": " that's all really fast and works exactly the same so well almost exactly the same it's yeah so that's", "tokens": [51525, 300, 311, 439, 534, 2370, 293, 1985, 2293, 264, 912, 370, 731, 1920, 2293, 264, 912, 309, 311, 1338, 370, 300, 311, 51850], "temperature": 0.0, "avg_logprob": -0.1488319772188781, "compression_ratio": 1.9291338582677164, "no_speech_prob": 0.000882327847648412}, {"id": 743, "seek": 284598, "start": 2845.98, "end": 2851.98, "text": " i from my perspective i'd say like these kind of bucket like if you're using kind of a back end", "tokens": [50365, 741, 490, 452, 4585, 741, 1116, 584, 411, 613, 733, 295, 13058, 411, 498, 291, 434, 1228, 733, 295, 257, 646, 917, 50665], "temperature": 0.0, "avg_logprob": -0.09979373043023267, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.0003377614775672555}, {"id": 744, "seek": 284598, "start": 2851.98, "end": 2859.92, "text": " as like hdp endpoints this like clear sequence of tasks i've found and like a cli application if you", "tokens": [50665, 382, 411, 276, 67, 79, 917, 20552, 341, 411, 1850, 8310, 295, 9608, 741, 600, 1352, 293, 411, 257, 596, 72, 3861, 498, 291, 51062], "temperature": 0.0, "avg_logprob": -0.09979373043023267, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.0003377614775672555}, {"id": 745, "seek": 284598, "start": 2859.92, "end": 2866.2, "text": " got a cli command like very much like elm pages scripts it's the same kind of idea like those", "tokens": [51062, 658, 257, 596, 72, 5622, 411, 588, 709, 411, 806, 76, 7183, 23294, 309, 311, 264, 912, 733, 295, 1558, 411, 729, 51376], "temperature": 0.0, "avg_logprob": -0.09979373043023267, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.0003377614775672555}, {"id": 746, "seek": 284598, "start": 2866.2, "end": 2873.1, "text": " i found those use cases really they fit really nicely there yeah because you want your server", "tokens": [51376, 741, 1352, 729, 764, 3331, 534, 436, 3318, 534, 9594, 456, 1338, 570, 291, 528, 428, 7154, 51721], "temperature": 0.0, "avg_logprob": -0.09979373043023267, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.0003377614775672555}, {"id": 747, "seek": 284598, "start": 2873.1, "end": 2875.44, "text": " to be as stateless as possible", "tokens": [51721, 281, 312, 382, 2219, 4272, 382, 1944, 51838], "temperature": 0.0, "avg_logprob": -0.09979373043023267, "compression_ratio": 1.8122270742358078, "no_speech_prob": 0.0003377614775672555}, {"id": 748, "seek": 287598, "start": 2875.98, "end": 2882.68, "text": " ideally or each endpoint but then because you have to chain tasks that go through", "tokens": [50365, 22915, 420, 1184, 35795, 457, 550, 570, 291, 362, 281, 5021, 9608, 300, 352, 807, 50700], "temperature": 0.0, "avg_logprob": -0.0916874607404073, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0007278152625076473}, {"id": 749, "seek": 287598, "start": 2882.68, "end": 2889.98, "text": " transform to command now you need to store the state of ongoing requests in the model and yeah", "tokens": [50700, 4088, 281, 5622, 586, 291, 643, 281, 3531, 264, 1785, 295, 10452, 12475, 294, 264, 2316, 293, 1338, 51065], "temperature": 0.0, "avg_logprob": -0.0916874607404073, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0007278152625076473}, {"id": 750, "seek": 287598, "start": 2889.98, "end": 2896.3, "text": " that doesn't feel very good you can imagine i definitely tried that to start with and quickly", "tokens": [51065, 300, 1177, 380, 841, 588, 665, 291, 393, 3811, 741, 2138, 3031, 300, 281, 722, 365, 293, 2661, 51381], "temperature": 0.0, "avg_logprob": -0.0916874607404073, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0007278152625076473}, {"id": 751, "seek": 287598, "start": 2896.3, "end": 2901.72, "text": " got oh my goodness this is it's just not a nice it's definitely not a nice way of writing for", "tokens": [51381, 658, 1954, 452, 8387, 341, 307, 309, 311, 445, 406, 257, 1481, 309, 311, 2138, 406, 257, 1481, 636, 295, 3579, 337, 51652], "temperature": 0.0, "avg_logprob": -0.0916874607404073, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0007278152625076473}, {"id": 752, "seek": 287598, "start": 2901.72, "end": 2905.68, "text": " exactly as you say like a stateless a stateless process", "tokens": [51652, 2293, 382, 291, 584, 411, 257, 2219, 4272, 257, 2219, 4272, 1399, 51850], "temperature": 0.0, "avg_logprob": -0.0916874607404073, "compression_ratio": 1.7573221757322175, "no_speech_prob": 0.0007278152625076473}, {"id": 753, "seek": 290598, "start": 2905.98, "end": 2913.36, "text": " so it's it's basically the the pattern of a functional core imperative shell which i think", "tokens": [50365, 370, 309, 311, 309, 311, 1936, 264, 264, 5102, 295, 257, 11745, 4965, 32490, 8720, 597, 741, 519, 50734], "temperature": 0.0, "avg_logprob": -0.05747925957968069, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.010551807470619678}, {"id": 754, "seek": 290598, "start": 2913.36, "end": 2921.12, "text": " is a really nice model and it's like so elm is a really good sort of central processing unit", "tokens": [50734, 307, 257, 534, 1481, 2316, 293, 309, 311, 411, 370, 806, 76, 307, 257, 534, 665, 1333, 295, 5777, 9007, 4985, 51122], "temperature": 0.0, "avg_logprob": -0.05747925957968069, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.010551807470619678}, {"id": 755, "seek": 290598, "start": 2921.12, "end": 2928.58, "text": " brain for like the the hub of everything and then the messy details as needed you can", "tokens": [51122, 3567, 337, 411, 264, 264, 11838, 295, 1203, 293, 550, 264, 16191, 4365, 382, 2978, 291, 393, 51495], "temperature": 0.0, "avg_logprob": -0.05747925957968069, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.010551807470619678}, {"id": 756, "seek": 290598, "start": 2928.58, "end": 2935.08, "text": " delegate out to javascript code and it can do whatever you need it to do and you can define", "tokens": [51495, 40999, 484, 281, 361, 37331, 5944, 3089, 293, 309, 393, 360, 2035, 291, 643, 309, 281, 360, 293, 291, 393, 6964, 51820], "temperature": 0.0, "avg_logprob": -0.05747925957968069, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.010551807470619678}, {"id": 757, "seek": 293598, "start": 2935.98, "end": 2943.36, "text": " an asynchronous code that's you know just going and doing what it does best like firing something", "tokens": [50365, 364, 49174, 3089, 300, 311, 291, 458, 445, 516, 293, 884, 437, 309, 775, 1151, 411, 16045, 746, 50734], "temperature": 0.8, "avg_logprob": -0.13660938865260075, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.006477192044258118}, {"id": 758, "seek": 293598, "start": 2943.36, "end": 2948.92, "text": " off and coming back on the event loop when it's done and not holding up the event loop from", "tokens": [50734, 766, 293, 1348, 646, 322, 264, 2280, 6367, 562, 309, 311, 1096, 293, 406, 5061, 493, 264, 2280, 6367, 490, 51012], "temperature": 0.8, "avg_logprob": -0.13660938865260075, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.006477192044258118}, {"id": 759, "seek": 293598, "start": 2948.92, "end": 2955.88, "text": " continuing while it processes those things so yeah actually like elm is pretty nice for for", "tokens": [51012, 9289, 1339, 309, 7555, 729, 721, 370, 1338, 767, 411, 806, 76, 307, 1238, 1481, 337, 337, 51360], "temperature": 0.8, "avg_logprob": -0.13660938865260075, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.006477192044258118}, {"id": 760, "seek": 293598, "start": 2955.88, "end": 2962.12, "text": " using for these sorts of uh functional core imperative shell ways of scripting doing some", "tokens": [51360, 1228, 337, 613, 7527, 295, 2232, 11745, 4965, 32490, 8720, 2098, 295, 5755, 278, 884, 512, 51672], "temperature": 0.8, "avg_logprob": -0.13660938865260075, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.006477192044258118}, {"id": 761, "seek": 293598, "start": 2962.12, "end": 2965.78, "text": " sort of back-end work or or running in your front end", "tokens": [51672, 1333, 295, 646, 12, 521, 589, 420, 420, 2614, 294, 428, 1868, 917, 51855], "temperature": 0.8, "avg_logprob": -0.13660938865260075, "compression_ratio": 1.7206477732793521, "no_speech_prob": 0.006477192044258118}, {"id": 762, "seek": 296598, "start": 2965.98, "end": 2970.3, "text": " So why did you call it Elm concurrent task and not Elm script?", "tokens": [50365, 407, 983, 630, 291, 818, 309, 2699, 76, 37702, 5633, 293, 406, 2699, 76, 5755, 30, 50581], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 763, "seek": 296598, "start": 2970.98, "end": 2972.52, "text": " Like, just like JavaScript.", "tokens": [50615, 1743, 11, 445, 411, 15778, 13, 50692], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 764, "seek": 296598, "start": 2975.42, "end": 2976.4, "text": " Elm script.", "tokens": [50837, 2699, 76, 5755, 13, 50886], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 765, "seek": 296598, "start": 2977.2400000000002, "end": 2978.04, "text": " Elm script, yeah.", "tokens": [50928, 2699, 76, 5755, 11, 1338, 13, 50968], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 766, "seek": 296598, "start": 2979.06, "end": 2983.02, "text": " I actually wouldn't be surprised if there was already a project called that.", "tokens": [51019, 286, 767, 2759, 380, 312, 6100, 498, 456, 390, 1217, 257, 1716, 1219, 300, 13, 51217], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 767, "seek": 296598, "start": 2984.02, "end": 2985.38, "text": " Yeah, probably, actually.", "tokens": [51267, 865, 11, 1391, 11, 767, 13, 51335], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 768, "seek": 296598, "start": 2986.2, "end": 2989.5, "text": " Sorry if I don't remember you, author of Elm script.", "tokens": [51376, 4919, 498, 286, 500, 380, 1604, 291, 11, 3793, 295, 2699, 76, 5755, 13, 51541], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 769, "seek": 296598, "start": 2991.32, "end": 2994.46, "text": " I am curious about the word concurrent.", "tokens": [51632, 286, 669, 6369, 466, 264, 1349, 37702, 13, 51789], "temperature": 0.0, "avg_logprob": -0.25531785206128194, "compression_ratio": 1.5643564356435644, "no_speech_prob": 0.006373334676027298}, {"id": 770, "seek": 299446, "start": 2994.46, "end": 2997.52, "text": " So it's the same thing as parallel.", "tokens": [50365, 407, 309, 311, 264, 912, 551, 382, 8952, 13, 50518], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 771, "seek": 299446, "start": 2997.76, "end": 2999.2400000000002, "text": " The exact same thing.", "tokens": [50530, 440, 1900, 912, 551, 13, 50604], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 772, "seek": 299446, "start": 2999.96, "end": 3000.2400000000002, "text": " Yes.", "tokens": [50640, 1079, 13, 50654], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 773, "seek": 299446, "start": 3000.34, "end": 3004.7, "text": " Let's open up that can of worms, Andrew, if you'd be so kind.", "tokens": [50659, 961, 311, 1269, 493, 300, 393, 295, 28271, 11, 10110, 11, 498, 291, 1116, 312, 370, 733, 13, 50877], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 774, "seek": 299446, "start": 3009.3, "end": 3015.08, "text": " To be honest with you, my initial calling of concurrent task is there's already Elm", "tokens": [51107, 1407, 312, 3245, 365, 291, 11, 452, 5883, 5141, 295, 37702, 5633, 307, 456, 311, 1217, 2699, 76, 51396], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 775, "seek": 299446, "start": 3015.08, "end": 3017.38, "text": " task parallel as a package out there.", "tokens": [51396, 5633, 8952, 382, 257, 7372, 484, 456, 13, 51511], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 776, "seek": 299446, "start": 3019.0, "end": 3021.14, "text": " One that would be very confusing.", "tokens": [51592, 1485, 300, 576, 312, 588, 13181, 13, 51699], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 777, "seek": 299446, "start": 3021.46, "end": 3023.2400000000002, "text": " Like, which one do I use?", "tokens": [51715, 1743, 11, 597, 472, 360, 286, 764, 30, 51804], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 778, "seek": 299446, "start": 3023.2400000000002, "end": 3024.44, "text": " I think the other.", "tokens": [51804, 286, 519, 264, 661, 13, 51864], "temperature": 0.0, "avg_logprob": -0.21659122813831677, "compression_ratio": 1.4908256880733946, "no_speech_prob": 1.2804424841306172e-05}, {"id": 779, "seek": 302446, "start": 3024.46, "end": 3029.92, "text": " The reason I add on the side of calling it concurrent was that technically", "tokens": [50365, 440, 1778, 286, 909, 322, 264, 1252, 295, 5141, 309, 37702, 390, 300, 12120, 50638], "temperature": 0.0, "avg_logprob": -0.37339290618896487, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.002924378262832761}, {"id": 780, "seek": 302446, "start": 3029.92, "end": 3032.68, "text": " JavaScript is single threaded.", "tokens": [50638, 15778, 307, 2167, 47493, 13, 50776], "temperature": 0.0, "avg_logprob": -0.37339290618896487, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.002924378262832761}, {"id": 781, "seek": 302446, "start": 3032.7400000000002, "end": 3039.76, "text": " So it's a, it's a, it's a, it's a technicality that, um, yeah, is that", "tokens": [50779, 407, 309, 311, 257, 11, 309, 311, 257, 11, 309, 311, 257, 11, 309, 311, 257, 6191, 507, 300, 11, 1105, 11, 1338, 11, 307, 300, 51130], "temperature": 0.0, "avg_logprob": -0.37339290618896487, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.002924378262832761}, {"id": 782, "seek": 302446, "start": 3039.88, "end": 3045.14, "text": " technically things in that environment cannot run in parallel.", "tokens": [51136, 12120, 721, 294, 300, 2823, 2644, 1190, 294, 8952, 13, 51399], "temperature": 0.0, "avg_logprob": -0.37339290618896487, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.002924378262832761}, {"id": 783, "seek": 302446, "start": 3045.32, "end": 3047.98, "text": " Like true parallelism is like on different cores.", "tokens": [51408, 1743, 2074, 8952, 1434, 307, 411, 322, 819, 24826, 13, 51541], "temperature": 0.0, "avg_logprob": -0.37339290618896487, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.002924378262832761}, {"id": 784, "seek": 302446, "start": 3048.46, "end": 3053.38, "text": " Let's say, but they're not just parallel, but yeah, I thought the", "tokens": [51565, 961, 311, 584, 11, 457, 436, 434, 406, 445, 8952, 11, 457, 1338, 11, 286, 1194, 264, 51811], "temperature": 0.0, "avg_logprob": -0.37339290618896487, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.002924378262832761}, {"id": 785, "seek": 302446, "start": 3053.86, "end": 3054.34, "text": " parallel.", "tokens": [51835, 8952, 13, 51859], "temperature": 0.0, "avg_logprob": -0.37339290618896487, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.002924378262832761}, {"id": 786, "seek": 305434, "start": 3054.34, "end": 3058.7000000000003, "text": " That educational means doing the same task on different data at the same", "tokens": [50365, 663, 10189, 1355, 884, 264, 912, 5633, 322, 819, 1412, 412, 264, 912, 50583], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 787, "seek": 305434, "start": 3059.34, "end": 3063.98, "text": " time and concurrent just like two things are running at the same time,", "tokens": [50615, 565, 293, 37702, 445, 411, 732, 721, 366, 2614, 412, 264, 912, 565, 11, 50847], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 788, "seek": 305434, "start": 3064.3, "end": 3066.44, "text": " but they might be doing it different things.", "tokens": [50863, 457, 436, 1062, 312, 884, 309, 819, 721, 13, 50970], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 789, "seek": 305434, "start": 3067.28, "end": 3068.3, "text": " I feel like.", "tokens": [51012, 286, 841, 411, 13, 51063], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 790, "seek": 305434, "start": 3068.86, "end": 3070.88, "text": " I'm right, but I also feel like people are", "tokens": [51091, 286, 478, 558, 11, 457, 286, 611, 841, 411, 561, 366, 51192], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 791, "seek": 305434, "start": 3070.88, "end": 3071.6000000000004, "text": " just gonna shut up.", "tokens": [51192, 445, 799, 5309, 493, 13, 51228], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 792, "seek": 305434, "start": 3071.6000000000004, "end": 3072.2200000000003, "text": " Me?", "tokens": [51228, 1923, 30, 51259], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 793, "seek": 305434, "start": 3072.7200000000003, "end": 3073.5, "text": " Sorry, listener.", "tokens": [51284, 4919, 11, 31569, 13, 51323], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 794, "seek": 305434, "start": 3073.76, "end": 3076.7400000000002, "text": " is, there's a thread.", "tokens": [51336, 307, 11, 456, 311, 257, 7207, 13, 51485], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 795, "seek": 305434, "start": 3076.76, "end": 3078.56, "text": " There's a long thread on own Slack.", "tokens": [51486, 821, 311, 257, 938, 7207, 322, 1065, 37211, 13, 51576], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 796, "seek": 305434, "start": 3078.6400000000003, "end": 3079.94, "text": " If it's still there, I", "tokens": [51580, 759, 309, 311, 920, 456, 11, 286, 51645], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 797, "seek": 305434, "start": 3079.96, "end": 3080.82, "text": " could, .", "tokens": [51646, 727, 11, 2411, 51689], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 798, "seek": 305434, "start": 3080.84, "end": 3081.2000000000003, "text": " Yeah.", "tokens": [51690, 865, 13, 51708], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 799, "seek": 305434, "start": 3081.6400000000003, "end": 3082.28, "text": " I feel like it made the same point.", "tokens": [51730, 286, 841, 411, 309, 1027, 264, 912, 935, 13, 51762], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 800, "seek": 305434, "start": 3082.28, "end": 3083.34, "text": " And then people corrected.", "tokens": [51762, 400, 550, 561, 31687, 13, 51815], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 801, "seek": 305434, "start": 3083.36, "end": 3083.92, "text": " Me there.", "tokens": [51816, 1923, 456, 13, 51844], "temperature": 1.0, "avg_logprob": -1.3712584583471852, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0018166527152061462}, {"id": 802, "seek": 308392, "start": 3083.92, "end": 3085.48, "text": " There's a thread, just one thread,", "tokens": [50365, 821, 311, 257, 7207, 11, 445, 472, 7207, 11, 50443], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 803, "seek": 308392, "start": 3085.54, "end": 3087.0, "text": " or are there multiple threads on it?", "tokens": [50446, 420, 366, 456, 3866, 19314, 322, 309, 30, 50519], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 804, "seek": 308392, "start": 3089.8, "end": 3093.66, "text": " I would have to reread it and process all that.", "tokens": [50659, 286, 576, 362, 281, 46453, 345, 309, 293, 1399, 439, 300, 13, 50852], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 805, "seek": 308392, "start": 3097.14, "end": 3099.26, "text": " It's much of a muchness, I would say.", "tokens": [51026, 467, 311, 709, 295, 257, 709, 1287, 11, 286, 576, 584, 13, 51132], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 806, "seek": 308392, "start": 3099.7200000000003, "end": 3102.7400000000002, "text": " You could call it either.", "tokens": [51155, 509, 727, 818, 309, 2139, 13, 51306], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 807, "seek": 308392, "start": 3102.96, "end": 3104.36, "text": " You could call it parallel, concurrent.", "tokens": [51317, 509, 727, 818, 309, 8952, 11, 37702, 13, 51387], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 808, "seek": 308392, "start": 3104.84, "end": 3107.14, "text": " To be honest with you, I think my main motivator is", "tokens": [51411, 1407, 312, 3245, 365, 291, 11, 286, 519, 452, 2135, 5426, 1639, 307, 51526], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 809, "seek": 308392, "start": 3107.14, "end": 3109.38, "text": " I didn't want it to clash with the other package", "tokens": [51526, 286, 994, 380, 528, 309, 281, 36508, 365, 264, 661, 7372, 51638], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 810, "seek": 308392, "start": 3109.38, "end": 3110.7400000000002, "text": " that's already out there.", "tokens": [51638, 300, 311, 1217, 484, 456, 13, 51706], "temperature": 0.0, "avg_logprob": -0.22736046200706844, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.021801942959427834}, {"id": 811, "seek": 311074, "start": 3110.74, "end": 3115.6, "text": " Just because it's like if you're reaching for that same name,", "tokens": [50365, 1449, 570, 309, 311, 411, 498, 291, 434, 9906, 337, 300, 912, 1315, 11, 50608], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 812, "seek": 311074, "start": 3116.08, "end": 3119.54, "text": " it's a small thing, but it's likely to cause a bit of confusion.", "tokens": [50632, 309, 311, 257, 1359, 551, 11, 457, 309, 311, 3700, 281, 3082, 257, 857, 295, 15075, 13, 50805], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 813, "seek": 311074, "start": 3120.2999999999997, "end": 3123.8399999999997, "text": " But I think your name is technically correct anyway.", "tokens": [50843, 583, 286, 519, 428, 1315, 307, 12120, 3006, 4033, 13, 51020], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 814, "seek": 311074, "start": 3124.52, "end": 3124.74, "text": " Technically.", "tokens": [51054, 42494, 13, 51065], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 815, "seek": 311074, "start": 3125.9199999999996, "end": 3128.7599999999998, "text": " I think it is.", "tokens": [51124, 286, 519, 309, 307, 13, 51266], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 816, "seek": 311074, "start": 3129.5, "end": 3132.7999999999997, "text": " So technically correct is the best kind of correct.", "tokens": [51303, 407, 12120, 3006, 307, 264, 1151, 733, 295, 3006, 13, 51468], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 817, "seek": 311074, "start": 3136.2999999999997, "end": 3138.3399999999997, "text": " Yeah, so I've definitely thought about,", "tokens": [51643, 865, 11, 370, 286, 600, 2138, 1194, 466, 11, 51745], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 818, "seek": 311074, "start": 3138.3399999999997, "end": 3140.14, "text": " Andrew and I have discussed whether", "tokens": [51745, 10110, 293, 286, 362, 7152, 1968, 51835], "temperature": 0.0, "avg_logprob": -0.25358707302219263, "compression_ratio": 1.6105769230769231, "no_speech_prob": 6.501682219095528e-05}, {"id": 819, "seek": 314014, "start": 3140.14, "end": 3145.0, "text": " backend task would be a nice fit for sort of using", "tokens": [50365, 38087, 5633, 576, 312, 257, 1481, 3318, 337, 1333, 295, 1228, 50608], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 820, "seek": 314014, "start": 3145.0, "end": 3148.2799999999997, "text": " the concurrent task API under the hood", "tokens": [50608, 264, 37702, 5633, 9362, 833, 264, 13376, 50772], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 821, "seek": 314014, "start": 3148.2799999999997, "end": 3150.3799999999997, "text": " and maybe even making it interoperable.", "tokens": [50772, 293, 1310, 754, 1455, 309, 728, 7192, 712, 13, 50877], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 822, "seek": 314014, "start": 3150.72, "end": 3153.68, "text": " But it's definitely a similar paradigm.", "tokens": [50894, 583, 309, 311, 2138, 257, 2531, 24709, 13, 51042], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 823, "seek": 314014, "start": 3154.3599999999997, "end": 3158.44, "text": " And it makes me think also like as a framework,", "tokens": [51076, 400, 309, 1669, 385, 519, 611, 411, 382, 257, 8388, 11, 51280], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 824, "seek": 314014, "start": 3158.94, "end": 3161.7, "text": " Elm pages would have the ability to give you", "tokens": [51305, 2699, 76, 7183, 576, 362, 264, 3485, 281, 976, 291, 51443], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 825, "seek": 314014, "start": 3161.7, "end": 3163.8599999999997, "text": " a little bit of wiring,", "tokens": [51443, 257, 707, 857, 295, 27520, 11, 51551], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 826, "seek": 314014, "start": 3164.08, "end": 3168.46, "text": " like all of the boilerplate for adding the right thing to your model", "tokens": [51562, 411, 439, 295, 264, 39228, 37008, 337, 5127, 264, 558, 551, 281, 428, 2316, 51781], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 827, "seek": 314014, "start": 3168.46, "end": 3169.2, "text": " and defining.", "tokens": [51781, 293, 17827, 13, 51818], "temperature": 0.0, "avg_logprob": -0.21012602912055123, "compression_ratio": 1.5569620253164558, "no_speech_prob": 0.04334437847137451}, {"id": 828, "seek": 316920, "start": 3169.2, "end": 3170.2, "text": " And then also, you know,", "tokens": [50365, 400, 550, 611, 11, 291, 458, 11, 50415], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 829, "seek": 316920, "start": 3170.2, "end": 3172.2, "text": " you could be adding an incoming and outgoing port and.", "tokens": [50415, 291, 727, 312, 5127, 364, 22341, 293, 41565, 2436, 293, 13, 50515], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 830, "seek": 316920, "start": 3172.2, "end": 3176.2, "text": " And making sure that there's no typo in the names of.", "tokens": [50515, 400, 1455, 988, 300, 456, 311, 572, 2125, 78, 294, 264, 5288, 295, 13, 50715], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 831, "seek": 316920, "start": 3176.2, "end": 3177.2, "text": " Right.", "tokens": [50715, 1779, 13, 50765], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 832, "seek": 316920, "start": 3177.2, "end": 3178.2, "text": " Yeah, exactly.", "tokens": [50765, 865, 11, 2293, 13, 50815], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 833, "seek": 316920, "start": 3178.2, "end": 3180.2, "text": " All these little details and right.", "tokens": [50815, 1057, 613, 707, 4365, 293, 558, 13, 50915], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 834, "seek": 316920, "start": 3180.2, "end": 3181.2, "text": " Adding the right message.", "tokens": [50915, 31204, 264, 558, 3636, 13, 50965], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 835, "seek": 316920, "start": 3181.2, "end": 3183.2, "text": " They're just like a handful of little things.", "tokens": [50965, 814, 434, 445, 411, 257, 16458, 295, 707, 721, 13, 51065], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 836, "seek": 316920, "start": 3183.2, "end": 3186.2, "text": " But Elm pages could build in something like this for you.", "tokens": [51065, 583, 2699, 76, 7183, 727, 1322, 294, 746, 411, 341, 337, 291, 13, 51215], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 837, "seek": 316920, "start": 3186.2, "end": 3190.2, "text": " Because in my opinion, like this is kind of like the ideal paradigm", "tokens": [51215, 1436, 294, 452, 4800, 11, 411, 341, 307, 733, 295, 411, 264, 7157, 24709, 51415], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 838, "seek": 316920, "start": 3190.2, "end": 3192.2, "text": " for using these things.", "tokens": [51415, 337, 1228, 613, 721, 13, 51515], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 839, "seek": 316920, "start": 3192.2, "end": 3194.2, "text": " So that could be an interesting thing to explore as well.", "tokens": [51515, 407, 300, 727, 312, 364, 1880, 551, 281, 6839, 382, 731, 13, 51615], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 840, "seek": 316920, "start": 3194.2, "end": 3195.2, "text": " Definitely.", "tokens": [51615, 12151, 13, 51665], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 841, "seek": 316920, "start": 3195.2, "end": 3198.2, "text": " It's just I know it's a lot of work actually.", "tokens": [51665, 467, 311, 445, 286, 458, 309, 311, 257, 688, 295, 589, 767, 13, 51815], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 842, "seek": 316920, "start": 3198.2, "end": 3199.2, "text": " Yeah.", "tokens": [51815, 865, 13, 51865], "temperature": 0.0, "avg_logprob": -0.4257040023803711, "compression_ratio": 1.7370129870129871, "no_speech_prob": 0.007995632477104664}, {"id": 843, "seek": 319920, "start": 3199.2, "end": 3202.2, "text": " I think it's really important to be able to actually rewiring all of it in", "tokens": [50365, 286, 519, 309, 311, 534, 1021, 281, 312, 1075, 281, 767, 319, 86, 5057, 439, 295, 309, 294, 50515], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 844, "seek": 319920, "start": 3202.2, "end": 3207.2, "text": " because it's yeah, you've like Elm pages is an expansive API.", "tokens": [50515, 570, 309, 311, 1338, 11, 291, 600, 411, 2699, 76, 7183, 307, 364, 46949, 9362, 13, 50765], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 845, "seek": 319920, "start": 3207.2, "end": 3209.2, "text": " It's very impressive what it's doing.", "tokens": [50765, 467, 311, 588, 8992, 437, 309, 311, 884, 13, 50865], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 846, "seek": 319920, "start": 3209.2, "end": 3214.2, "text": " But I yeah, it would be a lot of work actually wiring it in.", "tokens": [50865, 583, 286, 1338, 11, 309, 576, 312, 257, 688, 295, 589, 767, 27520, 309, 294, 13, 51115], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 847, "seek": 319920, "start": 3214.2, "end": 3218.2, "text": " So it's only like 50 modules.", "tokens": [51115, 407, 309, 311, 787, 411, 2625, 16679, 13, 51315], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 848, "seek": 319920, "start": 3218.2, "end": 3220.2, "text": " Yeah, right.", "tokens": [51315, 865, 11, 558, 13, 51415], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 849, "seek": 319920, "start": 3220.2, "end": 3221.2, "text": " I know.", "tokens": [51415, 286, 458, 13, 51465], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 850, "seek": 319920, "start": 3221.2, "end": 3224.2, "text": " And it only took him like a month to release or something.", "tokens": [51465, 400, 309, 787, 1890, 796, 411, 257, 1618, 281, 4374, 420, 746, 13, 51615], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 851, "seek": 319920, "start": 3226.2, "end": 3227.2, "text": " Something like that.", "tokens": [51715, 6595, 411, 300, 13, 51765], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 852, "seek": 319920, "start": 3227.2, "end": 3228.2, "text": " Yeah.", "tokens": [51765, 865, 13, 51815], "temperature": 0.2, "avg_logprob": -0.2921276921811311, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.002062736777588725}, {"id": 853, "seek": 322820, "start": 3228.2, "end": 3230.2, "text": " I think it's a bunch of credits.", "tokens": [50365, 286, 519, 309, 311, 257, 3840, 295, 16816, 13, 50465], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 854, "seek": 322820, "start": 3230.2, "end": 3232.2, "text": " It all goes to his head.", "tokens": [50465, 467, 439, 1709, 281, 702, 1378, 13, 50565], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 855, "seek": 322820, "start": 3232.2, "end": 3239.2, "text": " I wonder if there is a way we could do it incrementally like to get some useful feedback", "tokens": [50565, 286, 2441, 498, 456, 307, 257, 636, 321, 727, 360, 309, 26200, 379, 411, 281, 483, 512, 4420, 5824, 50915], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 856, "seek": 322820, "start": 3239.2, "end": 3244.2, "text": " on like is it providing the functionality that you need or are there some like weird", "tokens": [50915, 322, 411, 307, 309, 6530, 264, 14980, 300, 291, 643, 420, 366, 456, 512, 411, 3657, 51165], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 857, "seek": 322820, "start": 3244.2, "end": 3245.2, "text": " edge cases?", "tokens": [51165, 4691, 3331, 30, 51215], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 858, "seek": 322820, "start": 3245.2, "end": 3250.2, "text": " We don't have to like reimplement everything under the hood with that.", "tokens": [51215, 492, 500, 380, 362, 281, 411, 33433, 43704, 1203, 833, 264, 13376, 365, 300, 13, 51465], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 859, "seek": 322820, "start": 3250.2, "end": 3252.2, "text": " I'm not sure how to do it.", "tokens": [51465, 286, 478, 406, 988, 577, 281, 360, 309, 13, 51565], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 860, "seek": 322820, "start": 3252.2, "end": 3256.2, "text": " That's the tricky thing when it's like such a backbone of it and there are all these like", "tokens": [51565, 663, 311, 264, 12414, 551, 562, 309, 311, 411, 1270, 257, 34889, 295, 309, 293, 456, 366, 439, 613, 411, 51765], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 861, "seek": 322820, "start": 3256.2, "end": 3257.2, "text": " things that are.", "tokens": [51765, 721, 300, 366, 13, 51815], "temperature": 0.0, "avg_logprob": -0.24079372843758004, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.003938540816307068}, {"id": 862, "seek": 325720, "start": 3257.2, "end": 3264.2, "text": " Deeply integrated but but it would be so nice if like the community could sort of coalesce", "tokens": [50365, 14895, 356, 10919, 457, 457, 309, 576, 312, 370, 1481, 498, 411, 264, 1768, 727, 1333, 295, 598, 4229, 384, 50715], "temperature": 0.0, "avg_logprob": -0.12648569189983866, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.005017546471208334}, {"id": 863, "seek": 325720, "start": 3264.2, "end": 3269.2, "text": " on like one nice way to do this and yeah, I definitely think you've hit up hit upon", "tokens": [50715, 322, 411, 472, 1481, 636, 281, 360, 341, 293, 1338, 11, 286, 2138, 519, 291, 600, 2045, 493, 2045, 3564, 50965], "temperature": 0.0, "avg_logprob": -0.12648569189983866, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.005017546471208334}, {"id": 864, "seek": 325720, "start": 3269.2, "end": 3275.2, "text": " a really elegant formulation of this sort of thing that as a community we've been iterating", "tokens": [50965, 257, 534, 21117, 37642, 295, 341, 1333, 295, 551, 300, 382, 257, 1768, 321, 600, 668, 17138, 990, 51265], "temperature": 0.0, "avg_logprob": -0.12648569189983866, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.005017546471208334}, {"id": 865, "seek": 325720, "start": 3275.2, "end": 3281.2, "text": " on for a while in some way shape or form and starting to feel like it's really coming together", "tokens": [51265, 322, 337, 257, 1339, 294, 512, 636, 3909, 420, 1254, 293, 2891, 281, 841, 411, 309, 311, 534, 1348, 1214, 51565], "temperature": 0.0, "avg_logprob": -0.12648569189983866, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.005017546471208334}, {"id": 866, "seek": 325720, "start": 3281.2, "end": 3282.2, "text": " as a as a paradigm.", "tokens": [51565, 382, 257, 382, 257, 24709, 13, 51615], "temperature": 0.0, "avg_logprob": -0.12648569189983866, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.005017546471208334}, {"id": 867, "seek": 325720, "start": 3282.2, "end": 3283.2, "text": " Andrew.", "tokens": [51615, 10110, 13, 51665], "temperature": 0.0, "avg_logprob": -0.12648569189983866, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.005017546471208334}, {"id": 868, "seek": 325720, "start": 3283.2, "end": 3286.2, "text": " Have you by any chance heard about the Elm package JS?", "tokens": [51665, 3560, 291, 538, 604, 2931, 2198, 466, 264, 2699, 76, 7372, 33063, 30, 51815], "temperature": 0.0, "avg_logprob": -0.12648569189983866, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.005017546471208334}, {"id": 869, "seek": 328620, "start": 3286.2, "end": 3288.2, "text": " Approach from Lambda.", "tokens": [50365, 29551, 608, 490, 45691, 13, 50465], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 870, "seek": 328620, "start": 3288.2, "end": 3289.2, "text": " I haven't no.", "tokens": [50465, 286, 2378, 380, 572, 13, 50515], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 871, "seek": 328620, "start": 3289.2, "end": 3290.2, "text": " Okay, Dylan.", "tokens": [50515, 1033, 11, 28160, 13, 50565], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 872, "seek": 328620, "start": 3290.2, "end": 3292.2, "text": " Do you remember much about that?", "tokens": [50565, 1144, 291, 1604, 709, 466, 300, 30, 50665], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 873, "seek": 328620, "start": 3292.2, "end": 3299.2, "text": " Because I have yeah, I don't but I feel like there's some kind of overlap here where basically", "tokens": [50665, 1436, 286, 362, 1338, 11, 286, 500, 380, 457, 286, 841, 411, 456, 311, 512, 733, 295, 19959, 510, 689, 1936, 51015], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 874, "seek": 328620, "start": 3299.2, "end": 3302.2, "text": " Lambda is a full Elm JavaScript.", "tokens": [51015, 45691, 307, 257, 1577, 2699, 76, 15778, 13, 51165], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 875, "seek": 328620, "start": 3302.2, "end": 3304.2, "text": " Sorry full Elm framework.", "tokens": [51165, 4919, 1577, 2699, 76, 8388, 13, 51265], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 876, "seek": 328620, "start": 3304.2, "end": 3309.2, "text": " So back ends and front end isn't in Elm and there are you can't use ports or at least", "tokens": [51265, 407, 646, 5314, 293, 1868, 917, 1943, 380, 294, 2699, 76, 293, 456, 366, 291, 393, 380, 764, 18160, 420, 412, 1935, 51515], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 877, "seek": 328620, "start": 3309.2, "end": 3310.2, "text": " at some point you can use ports.", "tokens": [51515, 412, 512, 935, 291, 393, 764, 18160, 13, 51565], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 878, "seek": 328620, "start": 3310.2, "end": 3312.2, "text": " So there's no way to do that.", "tokens": [51565, 407, 456, 311, 572, 636, 281, 360, 300, 13, 51665], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 879, "seek": 328620, "start": 3312.2, "end": 3314.2, "text": " But I mean, I think there is a whole range of possibilities.", "tokens": [51665, 583, 286, 914, 11, 286, 519, 456, 307, 257, 1379, 3613, 295, 12178, 13, 51765], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 880, "seek": 328620, "start": 3314.2, "end": 3315.2, "text": " Yeah.", "tokens": [51765, 865, 13, 51815], "temperature": 0.6, "avg_logprob": -0.5735798856637774, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.0003400141140446067}, {"id": 881, "seek": 331520, "start": 3315.2, "end": 3324.0, "text": " point you couldn't use sports but in some cases like it's necessary even just to use more recent", "tokens": [50365, 935, 291, 2809, 380, 764, 6573, 457, 294, 512, 3331, 411, 309, 311, 4818, 754, 445, 281, 764, 544, 5162, 50805], "temperature": 0.0, "avg_logprob": -0.12088657827938304, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.015592261217534542}, {"id": 882, "seek": 331520, "start": 3324.64, "end": 3336.56, "text": " browser functionalities so mario mario rojic suggested an approach called elm package yes pkg", "tokens": [50837, 11185, 11745, 1088, 370, 1849, 1004, 1849, 1004, 744, 73, 299, 10945, 364, 3109, 1219, 806, 76, 7372, 2086, 280, 18302, 51433], "temperature": 0.0, "avg_logprob": -0.12088657827938304, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.015592261217534542}, {"id": 883, "seek": 331520, "start": 3337.12, "end": 3342.72, "text": " to make that work and i don't remember how it works i wonder whether it works somewhat similarly", "tokens": [51461, 281, 652, 300, 589, 293, 741, 500, 380, 1604, 577, 309, 1985, 741, 2441, 1968, 309, 1985, 8344, 14138, 51741], "temperature": 0.0, "avg_logprob": -0.12088657827938304, "compression_ratio": 1.559782608695652, "no_speech_prob": 0.015592261217534542}, {"id": 884, "seek": 334272, "start": 3342.72, "end": 3350.72, "text": " like there are names that you can call and they are available as elm values or ports or", "tokens": [50365, 411, 456, 366, 5288, 300, 291, 393, 818, 293, 436, 366, 2435, 382, 806, 76, 4190, 420, 18160, 420, 50765], "temperature": 0.0, "avg_logprob": -0.03671215220195491, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.0002886419533751905}, {"id": 885, "seek": 334272, "start": 3350.72, "end": 3356.8799999999997, "text": " i don't remember maybe something to look at yeah and there was also a similar like", "tokens": [50765, 741, 500, 380, 1604, 1310, 746, 281, 574, 412, 1338, 293, 456, 390, 611, 257, 2531, 411, 51073], "temperature": 0.0, "avg_logprob": -0.03671215220195491, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.0002886419533751905}, {"id": 886, "seek": 334272, "start": 3357.68, "end": 3363.68, "text": " a goal i think of giving a standard way of you know if if somebody implements uh", "tokens": [51113, 257, 3387, 741, 519, 295, 2902, 257, 3832, 636, 295, 291, 458, 498, 498, 2618, 704, 17988, 2232, 51413], "temperature": 0.0, "avg_logprob": -0.03671215220195491, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.0002886419533751905}, {"id": 887, "seek": 334272, "start": 3364.24, "end": 3371.6, "text": " something for local storage or copying to clipboard or these basic web platform adapters", "tokens": [51441, 746, 337, 2654, 6725, 420, 27976, 281, 7353, 3787, 420, 613, 3875, 3670, 3663, 23169, 1559, 51809], "temperature": 0.0, "avg_logprob": -0.03671215220195491, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.0002886419533751905}, {"id": 888, "seek": 334272, "start": 3371.6, "end": 3371.8399999999997, "text": " that", "tokens": [51809, 300, 51821], "temperature": 0.0, "avg_logprob": -0.03671215220195491, "compression_ratio": 1.6507177033492824, "no_speech_prob": 0.0002886419533751905}, {"id": 889, "seek": 337272, "start": 3372.72, "end": 3377.9199999999996, "text": " you can sort of plug and play with different adapters that the community is maintaining", "tokens": [50365, 291, 393, 1333, 295, 5452, 293, 862, 365, 819, 23169, 1559, 300, 264, 1768, 307, 14916, 50625], "temperature": 0.0, "avg_logprob": -0.0504470020532608, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.0003793122887145728}, {"id": 890, "seek": 337272, "start": 3377.9199999999996, "end": 3382.56, "text": " yeah and i mean actually like so elm packages would be a solution for that", "tokens": [50625, 1338, 293, 741, 914, 767, 411, 370, 806, 76, 17401, 576, 312, 257, 3827, 337, 300, 50857], "temperature": 0.0, "avg_logprob": -0.0504470020532608, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.0003793122887145728}, {"id": 891, "seek": 337272, "start": 3382.56, "end": 3386.8799999999997, "text": " i guess but i would have to look at it again but you could also potentially do that", "tokens": [50857, 741, 2041, 457, 741, 576, 362, 281, 574, 412, 309, 797, 457, 291, 727, 611, 7263, 360, 300, 51073], "temperature": 0.0, "avg_logprob": -0.0504470020532608, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.0003793122887145728}, {"id": 892, "seek": 337272, "start": 3387.4399999999996, "end": 3393.4399999999996, "text": " with elm taskport like you already have a javascript implementation that just adds", "tokens": [51101, 365, 806, 76, 5633, 2707, 411, 291, 1217, 362, 257, 361, 37331, 5944, 11420, 300, 445, 10860, 51401], "temperature": 0.0, "avg_logprob": -0.0504470020532608, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.0003793122887145728}, {"id": 893, "seek": 337272, "start": 3394.08, "end": 3402.08, "text": " arbitrary primitives and you make them available in elmland you you have the power to", "tokens": [51433, 23211, 2886, 38970, 293, 291, 652, 552, 2435, 294, 806, 76, 1661, 291, 291, 362, 264, 1347, 281, 51833], "temperature": 0.0, "avg_logprob": -0.0504470020532608, "compression_ratio": 1.7219917012448134, "no_speech_prob": 0.0003793122887145728}, {"id": 894, "seek": 340272, "start": 3402.72, "end": 3408.48, "text": " add new primitives like that that is what this framework enables right yeah i've been working", "tokens": [50365, 909, 777, 2886, 38970, 411, 300, 300, 307, 437, 341, 8388, 17077, 558, 1338, 741, 600, 668, 1364, 50653], "temperature": 0.6, "avg_logprob": -0.2978934566531561, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.0015317058423534036}, {"id": 895, "seek": 340272, "start": 3408.48, "end": 3416.24, "text": " on my own set of uh primitives that are useful for something like node i had it was interesting", "tokens": [50653, 322, 452, 1065, 992, 295, 2232, 2886, 38970, 300, 366, 4420, 337, 746, 411, 9984, 741, 632, 309, 390, 1880, 51041], "temperature": 0.6, "avg_logprob": -0.2978934566531561, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.0015317058423534036}, {"id": 896, "seek": 340272, "start": 3416.24, "end": 3423.3599999999997, "text": " i had um there's an example in the examples repo where oh sorry in the example in the examples in", "tokens": [51041, 741, 632, 1105, 456, 311, 364, 1365, 294, 264, 5110, 49040, 689, 1954, 2597, 294, 264, 1365, 294, 264, 5110, 294, 51397], "temperature": 0.6, "avg_logprob": -0.2978934566531561, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.0015317058423534036}, {"id": 897, "seek": 340272, "start": 3423.3599999999997, "end": 3429.6, "text": " the elm concurrent task repo there's like it's a little kind of pipeline worker so it's like it's", "tokens": [51397, 264, 806, 76, 37702, 5633, 49040, 456, 311, 411, 309, 311, 257, 707, 733, 295, 15517, 11346, 370, 309, 311, 411, 309, 311, 51709], "temperature": 0.6, "avg_logprob": -0.2978934566531561, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.0015317058423534036}, {"id": 898, "seek": 340272, "start": 3429.6, "end": 3432.56, "text": " like a platform platform worker that will like list all the different tools in the elm taskport", "tokens": [51709, 411, 257, 3663, 3663, 11346, 300, 486, 411, 1329, 439, 264, 819, 3873, 294, 264, 806, 76, 5633, 2707, 51857], "temperature": 0.6, "avg_logprob": -0.2978934566531561, "compression_ratio": 1.971311475409836, "no_speech_prob": 0.0015317058423534036}, {"id": 899, "seek": 343256, "start": 3432.56, "end": 3439.5, "text": " listen it's listening out for like sqs messages and then orchestrates uh sorry like a amazon sqs", "tokens": [50365, 2140, 309, 311, 4764, 484, 337, 411, 262, 80, 82, 7897, 293, 550, 14161, 12507, 2232, 2597, 411, 257, 47010, 262, 80, 82, 50712], "temperature": 0.0, "avg_logprob": -0.04190652810254143, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.08499909937381744}, {"id": 900, "seek": 343256, "start": 3439.5, "end": 3445.04, "text": " like a queue so it's like this yeah it's this thing like on a queue and then does like a bunch", "tokens": [50712, 411, 257, 18639, 370, 309, 311, 411, 341, 1338, 309, 311, 341, 551, 411, 322, 257, 18639, 293, 550, 775, 411, 257, 3840, 50989], "temperature": 0.0, "avg_logprob": -0.04190652810254143, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.08499909937381744}, {"id": 901, "seek": 343256, "start": 3445.04, "end": 3452.6, "text": " of a bunch of things with various aws services and i had like i was quite surprised at how pleasant", "tokens": [50989, 295, 257, 3840, 295, 721, 365, 3683, 1714, 82, 3328, 293, 741, 632, 411, 741, 390, 1596, 6100, 412, 577, 16232, 51367], "temperature": 0.0, "avg_logprob": -0.04190652810254143, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.08499909937381744}, {"id": 902, "seek": 343256, "start": 3452.6, "end": 3460.68, "text": " it was to write so there was some very minimal bindings to the aws sdk so i mean that's a much", "tokens": [51367, 309, 390, 281, 2464, 370, 456, 390, 512, 588, 13206, 14786, 1109, 281, 264, 1714, 82, 262, 67, 74, 370, 741, 914, 300, 311, 257, 709, 51771], "temperature": 0.0, "avg_logprob": -0.04190652810254143, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.08499909937381744}, {"id": 903, "seek": 346068, "start": 3460.68, "end": 3465.94, "text": " bigger example you don't want to go like full hog on that but actually having a set of like simple", "tokens": [50365, 3801, 1365, 291, 500, 380, 528, 281, 352, 411, 1577, 24855, 322, 300, 457, 767, 1419, 257, 992, 295, 411, 2199, 50628], "temperature": 0.0, "avg_logprob": -0.0698104389643265, "compression_ratio": 1.863799283154122, "no_speech_prob": 3.26961453538388e-05}, {"id": 904, "seek": 346068, "start": 3465.94, "end": 3471.18, "text": " primitives like you could have for the web platform or you could have with things on node like this", "tokens": [50628, 2886, 38970, 411, 291, 727, 362, 337, 264, 3670, 3663, 420, 291, 727, 362, 365, 721, 322, 9984, 411, 341, 50890], "temperature": 0.0, "avg_logprob": -0.0698104389643265, "compression_ratio": 1.863799283154122, "no_speech_prob": 3.26961453538388e-05}, {"id": 905, "seek": 346068, "start": 3471.18, "end": 3476.2799999999997, "text": " seems to work quite nicely and then you just use as dylan you were saying before you just use elm", "tokens": [50890, 2544, 281, 589, 1596, 9594, 293, 550, 291, 445, 764, 382, 274, 24691, 291, 645, 1566, 949, 291, 445, 764, 806, 76, 51145], "temperature": 0.0, "avg_logprob": -0.0698104389643265, "compression_ratio": 1.863799283154122, "no_speech_prob": 3.26961453538388e-05}, {"id": 906, "seek": 346068, "start": 3476.2799999999997, "end": 3482.56, "text": " as the brain effectively for orchestrating all of these together so it's interesting having a", "tokens": [51145, 382, 264, 3567, 8659, 337, 14161, 8754, 439, 295, 613, 1214, 370, 309, 311, 1880, 1419, 257, 51459], "temperature": 0.0, "avg_logprob": -0.0698104389643265, "compression_ratio": 1.863799283154122, "no_speech_prob": 3.26961453538388e-05}, {"id": 907, "seek": 346068, "start": 3482.56, "end": 3489.0, "text": " standard i like the idea of having a some kind of standard way of distributing those two two things", "tokens": [51459, 3832, 741, 411, 264, 1558, 295, 1419, 257, 512, 733, 295, 3832, 636, 295, 41406, 729, 732, 732, 721, 51781], "temperature": 0.0, "avg_logprob": -0.0698104389643265, "compression_ratio": 1.863799283154122, "no_speech_prob": 3.26961453538388e-05}, {"id": 908, "seek": 346068, "start": 3489.0, "end": 3490.66, "text": " alongside like the javascript", "tokens": [51781, 12385, 411, 264, 361, 37331, 5944, 51864], "temperature": 0.0, "avg_logprob": -0.0698104389643265, "compression_ratio": 1.863799283154122, "no_speech_prob": 3.26961453538388e-05}, {"id": 909, "seek": 349068, "start": 3490.68, "end": 3498.2, "text": " and the elm module yeah i mean like to me it's it's one of the things that's so at the heart of", "tokens": [50365, 293, 264, 806, 76, 10088, 1338, 741, 914, 411, 281, 385, 309, 311, 309, 311, 472, 295, 264, 721, 300, 311, 370, 412, 264, 1917, 295, 50741], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 910, "seek": 349068, "start": 3498.2, "end": 3506.7799999999997, "text": " elm and its design is this this separation of the clean pure elm sandbox from the rest of the world", "tokens": [50741, 806, 76, 293, 1080, 1715, 307, 341, 341, 14634, 295, 264, 2541, 6075, 806, 76, 42115, 490, 264, 1472, 295, 264, 1002, 51170], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 911, "seek": 349068, "start": 3506.7799999999997, "end": 3516.02, "text": " but then to me the the current design of ports as a command subscription sort of thing", "tokens": [51170, 457, 550, 281, 385, 264, 264, 2190, 1715, 295, 18160, 382, 257, 5622, 17231, 1333, 295, 551, 51632], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 912, "seek": 349068, "start": 3516.02, "end": 3519.98, "text": " i know there is this concept of like the actor", "tokens": [51632, 741, 458, 456, 307, 341, 3410, 295, 411, 264, 8747, 51830], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 913, "seek": 349068, "start": 3519.98, "end": 3520.54, "text": " actor", "tokens": [51830, 8747, 51858], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 914, "seek": 349068, "start": 3520.54, "end": 3520.58, "text": " actor", "tokens": [51858, 8747, 51860], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 915, "seek": 349068, "start": 3520.58, "end": 3520.66, "text": " actor", "tokens": [51860, 8747, 51864], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 916, "seek": 349068, "start": 3520.66, "end": 3520.68, "text": " actor", "tokens": [51864, 8747, 51865], "temperature": 0.0, "avg_logprob": -0.11584039529164632, "compression_ratio": 1.877659574468085, "no_speech_prob": 1.4367452649821644e-06}, {"id": 917, "seek": 352068, "start": 3520.68, "end": 3529.8599999999997, "text": " model and and kind of sending something out into the world and not necessarily tying that together", "tokens": [50365, 2316, 293, 293, 733, 295, 7750, 746, 484, 666, 264, 1002, 293, 406, 4725, 32405, 300, 1214, 50824], "temperature": 0.0, "avg_logprob": -0.06541004180908203, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.005093563348054886}, {"id": 918, "seek": 352068, "start": 3529.8599999999997, "end": 3536.1, "text": " to something that comes back in from the outside world and that that is an idea that i think evan", "tokens": [50824, 281, 746, 300, 1487, 646, 294, 490, 264, 2380, 1002, 293, 300, 300, 307, 364, 1558, 300, 741, 519, 1073, 282, 51136], "temperature": 0.0, "avg_logprob": -0.06541004180908203, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.005093563348054886}, {"id": 919, "seek": 352068, "start": 3536.1, "end": 3543.98, "text": " did design intentionally to like have it be maybe more fault tolerant like you know like elixir or", "tokens": [51136, 630, 1715, 22062, 281, 411, 362, 309, 312, 1310, 544, 7441, 45525, 411, 291, 458, 411, 806, 970, 347, 420, 51530], "temperature": 0.0, "avg_logprob": -0.06541004180908203, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.005093563348054886}, {"id": 920, "seek": 352068, "start": 3543.98, "end": 3550.54, "text": " um erlang's sort of actor model for for how the erlang vm works", "tokens": [51530, 1105, 1189, 25241, 311, 1333, 295, 8747, 2316, 337, 337, 577, 264, 1189, 25241, 371, 76, 1985, 51858], "temperature": 0.0, "avg_logprob": -0.06541004180908203, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.005093563348054886}, {"id": 921, "seek": 355054, "start": 3550.54, "end": 3557.56, "text": " but i to me the thing that makes that's essential to elm is the purity of the elm sandbox", "tokens": [50365, 457, 741, 281, 385, 264, 551, 300, 1669, 300, 311, 7115, 281, 806, 76, 307, 264, 34382, 295, 264, 806, 76, 42115, 50716], "temperature": 0.0, "avg_logprob": -0.056178620282341454, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.15003928542137146}, {"id": 922, "seek": 355054, "start": 3557.56, "end": 3564.2799999999997, "text": " and all bets are off for anything outside of that in the outside world and this preserves that but", "tokens": [50716, 293, 439, 39922, 366, 766, 337, 1340, 2380, 295, 300, 294, 264, 2380, 1002, 293, 341, 1183, 9054, 300, 457, 51052], "temperature": 0.0, "avg_logprob": -0.056178620282341454, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.15003928542137146}, {"id": 923, "seek": 355054, "start": 3564.2799999999997, "end": 3571.94, "text": " it makes it much more manageable to work with that and then similarly like if you can install", "tokens": [51052, 309, 1669, 309, 709, 544, 38798, 281, 589, 365, 300, 293, 550, 14138, 411, 498, 291, 393, 3625, 51435], "temperature": 0.0, "avg_logprob": -0.056178620282341454, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.15003928542137146}, {"id": 924, "seek": 355054, "start": 3571.94, "end": 3580.52, "text": " elm packages and they violate those guarantees and expectations that would go against the", "tokens": [51435, 806, 76, 17401, 293, 436, 37478, 729, 32567, 293, 9843, 300, 576, 352, 1970, 264, 51864], "temperature": 0.0, "avg_logprob": -0.056178620282341454, "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.15003928542137146}, {"id": 925, "seek": 358052, "start": 3580.52, "end": 3582.02, "text": " that core design to me", "tokens": [50365, 300, 4965, 1715, 281, 385, 50440], "temperature": 0.0, "avg_logprob": -0.2012126604715983, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.03938804194331169}, {"id": 926, "seek": 358052, "start": 3582.02, "end": 3595.72, "text": " um so if you could install a package and it has kernel code or it has ports defined but at the same time like we've been trying i think in the elm community for a long time to find a nice way to share a definition for", "tokens": [50440, 1105, 370, 498, 291, 727, 3625, 257, 7372, 293, 309, 575, 28256, 3089, 420, 309, 575, 18160, 7642, 457, 412, 264, 912, 565, 411, 321, 600, 668, 1382, 741, 519, 294, 264, 806, 76, 1768, 337, 257, 938, 565, 281, 915, 257, 1481, 636, 281, 2073, 257, 7123, 337, 51125], "temperature": 0.0, "avg_logprob": -0.2012126604715983, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.03938804194331169}, {"id": 927, "seek": 358052, "start": 3595.72, "end": 3603.64, "text": " how you define something for local storage and these basic things you know how you use the", "tokens": [51125, 577, 291, 6964, 746, 337, 2654, 6725, 293, 613, 3875, 721, 291, 458, 577, 291, 764, 264, 51521], "temperature": 0.0, "avg_logprob": -0.2012126604715983, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.03938804194331169}, {"id": 928, "seek": 358052, "start": 3603.64, "end": 3610.5, "text": " internationalization apis or apis that need to be built into the web platform they're important we need to use them for a lot of different things but i think it's a good idea to", "tokens": [51521, 5058, 2144, 1882, 271, 420, 1882, 271, 300, 643, 281, 312, 3094, 666, 264, 3670, 3663, 436, 434, 1021, 321, 643, 281, 764, 552, 337, 257, 688, 295, 819, 721, 457, 741, 519, 309, 311, 257, 665, 1558, 281, 51864], "temperature": 0.0, "avg_logprob": -0.2012126604715983, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.03938804194331169}, {"id": 929, "seek": 361050, "start": 3610.5, "end": 3617.08, "text": " as web developers um but how do you share them and the type definitions for them and stuff like that and we don't want", "tokens": [50365, 382, 3670, 8849, 1105, 457, 577, 360, 291, 2073, 552, 293, 264, 2010, 21988, 337, 552, 293, 1507, 411, 300, 293, 321, 500, 380, 528, 50694], "temperature": 0.6, "avg_logprob": -0.19140362739562988, "compression_ratio": 1.8453237410071943, "no_speech_prob": 0.016346525400877}, {"id": 930, "seek": 361050, "start": 3617.08, "end": 3626.26, "text": " ffi bindings that we can just trust but i feel like there is something like elm elm pkg js elm package js", "tokens": [50694, 283, 13325, 14786, 1109, 300, 321, 393, 445, 3361, 457, 741, 841, 411, 456, 307, 746, 411, 806, 76, 806, 76, 280, 18302, 42713, 806, 76, 7372, 42713, 51153], "temperature": 0.6, "avg_logprob": -0.19140362739562988, "compression_ratio": 1.8453237410071943, "no_speech_prob": 0.016346525400877}, {"id": 931, "seek": 361050, "start": 3626.26, "end": 3632.12, "text": " mario's specification is trying to address this problem in some way where it's like a shareable", "tokens": [51153, 1849, 1004, 311, 31256, 307, 1382, 281, 2985, 341, 1154, 294, 512, 636, 689, 309, 311, 411, 257, 2073, 712, 51446], "temperature": 0.6, "avg_logprob": -0.19140362739562988, "compression_ratio": 1.8453237410071943, "no_speech_prob": 0.016346525400877}, {"id": 932, "seek": 361050, "start": 3632.12, "end": 3640.08, "text": " definition that has an elm type associated with it and elm concurrent task does seem like it would", "tokens": [51446, 7123, 300, 575, 364, 806, 76, 2010, 6615, 365, 309, 293, 806, 76, 37702, 5633, 775, 1643, 411, 309, 576, 51844], "temperature": 0.6, "avg_logprob": -0.19140362739562988, "compression_ratio": 1.8453237410071943, "no_speech_prob": 0.016346525400877}, {"id": 933, "seek": 361050, "start": 3640.08, "end": 3640.48, "text": " pair really well with the elm package js but i think it's a good idea to use this as a way to", "tokens": [51844, 6119, 534, 731, 365, 264, 806, 76, 7372, 42713, 457, 741, 519, 309, 311, 257, 665, 1558, 281, 764, 341, 382, 257, 636, 281, 51864], "temperature": 0.6, "avg_logprob": -0.19140362739562988, "compression_ratio": 1.8453237410071943, "no_speech_prob": 0.016346525400877}, {"id": 934, "seek": 364050, "start": 3640.5, "end": 3647.5, "text": " nicely with that, like letting you define these tasks. And if you could like install a task,", "tokens": [50365, 9594, 365, 300, 11, 411, 8295, 291, 6964, 613, 9608, 13, 400, 498, 291, 727, 411, 3625, 257, 5633, 11, 50715], "temperature": 0.0, "avg_logprob": -0.06308297935975801, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.01001096423715353}, {"id": 935, "seek": 364050, "start": 3647.98, "end": 3653.02, "text": " that could certainly be very interesting. And again, other people may have different opinions", "tokens": [50739, 300, 727, 3297, 312, 588, 1880, 13, 400, 797, 11, 661, 561, 815, 362, 819, 11819, 50991], "temperature": 0.0, "avg_logprob": -0.06308297935975801, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.01001096423715353}, {"id": 936, "seek": 364050, "start": 3653.02, "end": 3658.66, "text": " on this, but to me, that preserves what's essential about the guarantees of Elm, which is", "tokens": [50991, 322, 341, 11, 457, 281, 385, 11, 300, 1183, 9054, 437, 311, 7115, 466, 264, 32567, 295, 2699, 76, 11, 597, 307, 51273], "temperature": 0.0, "avg_logprob": -0.06308297935975801, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.01001096423715353}, {"id": 937, "seek": 364050, "start": 3658.66, "end": 3663.04, "text": " you can't trust the outside world. You're still not trusting it, but you're making it a little", "tokens": [51273, 291, 393, 380, 3361, 264, 2380, 1002, 13, 509, 434, 920, 406, 28235, 309, 11, 457, 291, 434, 1455, 309, 257, 707, 51492], "temperature": 0.0, "avg_logprob": -0.06308297935975801, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.01001096423715353}, {"id": 938, "seek": 364050, "start": 3663.04, "end": 3668.38, "text": " more convenient to use it in a way where you're skeptical of it being safe.", "tokens": [51492, 544, 10851, 281, 764, 309, 294, 257, 636, 689, 291, 434, 28601, 295, 309, 885, 3273, 13, 51759], "temperature": 0.0, "avg_logprob": -0.06308297935975801, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.01001096423715353}, {"id": 939, "seek": 366838, "start": 3668.38, "end": 3672.42, "text": " The only problem is then everyone would have to install Elm concurrent task.", "tokens": [50365, 440, 787, 1154, 307, 550, 1518, 576, 362, 281, 3625, 2699, 76, 37702, 5633, 13, 50567], "temperature": 0.0, "avg_logprob": -0.18224136502135033, "compression_ratio": 1.545816733067729, "no_speech_prob": 3.0958864954300225e-05}, {"id": 940, "seek": 366838, "start": 3673.08, "end": 3674.8, "text": " It would be a quite critical dependency.", "tokens": [50600, 467, 576, 312, 257, 1596, 4924, 33621, 13, 50686], "temperature": 0.0, "avg_logprob": -0.18224136502135033, "compression_ratio": 1.545816733067729, "no_speech_prob": 3.0958864954300225e-05}, {"id": 941, "seek": 366838, "start": 3675.82, "end": 3676.06, "text": " True.", "tokens": [50737, 13587, 13, 50749], "temperature": 0.0, "avg_logprob": -0.18224136502135033, "compression_ratio": 1.545816733067729, "no_speech_prob": 3.0958864954300225e-05}, {"id": 942, "seek": 366838, "start": 3678.98, "end": 3685.58, "text": " But it is interesting. Like, yeah, you don't really want to be re-implementing a lot of the", "tokens": [50895, 583, 309, 307, 1880, 13, 1743, 11, 1338, 11, 291, 500, 380, 534, 528, 281, 312, 319, 12, 332, 43704, 278, 257, 688, 295, 264, 51225], "temperature": 0.0, "avg_logprob": -0.18224136502135033, "compression_ratio": 1.545816733067729, "no_speech_prob": 3.0958864954300225e-05}, {"id": 943, "seek": 366838, "start": 3685.58, "end": 3687.76, "text": " very low level stuff over and over again.", "tokens": [51225, 588, 2295, 1496, 1507, 670, 293, 670, 797, 13, 51334], "temperature": 0.0, "avg_logprob": -0.18224136502135033, "compression_ratio": 1.545816733067729, "no_speech_prob": 3.0958864954300225e-05}, {"id": 944, "seek": 366838, "start": 3688.32, "end": 3695.86, "text": " Right. Exactly. Well, Andrew, if somebody wants to get started playing around with Elm concurrent", "tokens": [51362, 1779, 13, 7587, 13, 1042, 11, 10110, 11, 498, 2618, 2738, 281, 483, 1409, 2433, 926, 365, 2699, 76, 37702, 51739], "temperature": 0.0, "avg_logprob": -0.18224136502135033, "compression_ratio": 1.545816733067729, "no_speech_prob": 3.0958864954300225e-05}, {"id": 945, "seek": 366838, "start": 3695.86, "end": 3697.94, "text": " task and learning more about it,", "tokens": [51739, 5633, 293, 2539, 544, 466, 309, 11, 51843], "temperature": 0.0, "avg_logprob": -0.18224136502135033, "compression_ratio": 1.545816733067729, "no_speech_prob": 3.0958864954300225e-05}, {"id": 946, "seek": 369794, "start": 3697.94, "end": 3699.3, "text": " what's a good place to do that?", "tokens": [50365, 437, 311, 257, 665, 1081, 281, 360, 300, 30, 50433], "temperature": 0.0, "avg_logprob": -0.13741616641773896, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.012425038032233715}, {"id": 947, "seek": 369794, "start": 3699.78, "end": 3703.32, "text": " Because if you search on the Elm package website, Elm concurrent task,", "tokens": [50457, 1436, 498, 291, 3164, 322, 264, 2699, 76, 7372, 3144, 11, 2699, 76, 37702, 5633, 11, 50634], "temperature": 0.0, "avg_logprob": -0.13741616641773896, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.012425038032233715}, {"id": 948, "seek": 369794, "start": 3703.82, "end": 3709.32, "text": " that should point you in the right direction. The readme has some instructions on", "tokens": [50659, 300, 820, 935, 291, 294, 264, 558, 3513, 13, 440, 1401, 1398, 575, 512, 9415, 322, 50934], "temperature": 0.0, "avg_logprob": -0.13741616641773896, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.012425038032233715}, {"id": 949, "seek": 369794, "start": 3709.32, "end": 3716.64, "text": " installing the Elm package and the NPM package. And yeah, if you've got, please get in touch", "tokens": [50934, 20762, 264, 2699, 76, 7372, 293, 264, 426, 18819, 7372, 13, 400, 1338, 11, 498, 291, 600, 658, 11, 1767, 483, 294, 2557, 51300], "temperature": 0.0, "avg_logprob": -0.13741616641773896, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.012425038032233715}, {"id": 950, "seek": 369794, "start": 3716.64, "end": 3720.32, "text": " if anything is unclear, like I'm available on Slack or GitHub.", "tokens": [51300, 498, 1340, 307, 25636, 11, 411, 286, 478, 2435, 322, 37211, 420, 23331, 13, 51484], "temperature": 0.0, "avg_logprob": -0.13741616641773896, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.012425038032233715}, {"id": 951, "seek": 369794, "start": 3720.84, "end": 3726.14, "text": " Wonderful. Yeah. And I definitely recommend there are some really nice examples there as well that", "tokens": [51510, 22768, 13, 865, 13, 400, 286, 2138, 2748, 456, 366, 512, 534, 1481, 5110, 456, 382, 731, 300, 51775], "temperature": 0.0, "avg_logprob": -0.13741616641773896, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.012425038032233715}, {"id": 952, "seek": 369794, "start": 3726.14, "end": 3727.92, "text": " are worth checking out. So yeah.", "tokens": [51775, 366, 3163, 8568, 484, 13, 407, 1338, 13, 51864], "temperature": 0.0, "avg_logprob": -0.13741616641773896, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.012425038032233715}, {"id": 953, "seek": 372794, "start": 3727.94, "end": 3730.88, "text": " Wonderful. Well, thanks again for coming on the show, Andrew. It was a pleasure having you.", "tokens": [50365, 22768, 13, 1042, 11, 3231, 797, 337, 1348, 322, 264, 855, 11, 10110, 13, 467, 390, 257, 6834, 1419, 291, 13, 50512], "temperature": 0.0, "avg_logprob": -0.24692030747731528, "compression_ratio": 1.347457627118644, "no_speech_prob": 0.0006118197343312204}, {"id": 954, "seek": 372794, "start": 3731.2000000000003, "end": 3731.96, "text": " Thanks for having me.", "tokens": [50528, 2561, 337, 1419, 385, 13, 50566], "temperature": 0.0, "avg_logprob": -0.24692030747731528, "compression_ratio": 1.347457627118644, "no_speech_prob": 0.0006118197343312204}, {"id": 955, "seek": 372794, "start": 3732.34, "end": 3733.98, "text": " And Jeroen. Until next time.", "tokens": [50585, 400, 508, 2032, 268, 13, 9088, 958, 565, 13, 50667], "temperature": 0.0, "avg_logprob": -0.24692030747731528, "compression_ratio": 1.347457627118644, "no_speech_prob": 0.0006118197343312204}, {"id": 956, "seek": 372794, "start": 3734.36, "end": 3735.04, "text": " Until next time.", "tokens": [50686, 9088, 958, 565, 13, 50720], "temperature": 0.0, "avg_logprob": -0.24692030747731528, "compression_ratio": 1.347457627118644, "no_speech_prob": 0.0006118197343312204}], "language": "en"}