{"text": " Hello, Jeroen. Hello, Dillon. And, well, it's feeling a little bit of holiday spirit in the air today, wouldn't you say? No, I would just say it's feeling very cold here. I've got three sweaters on me and I'm just cold. But yeah, there's also a little bit of Christmas feeling. Maybe it's not a holiday spirit, maybe it's a phantom type. Oh, I wish we had the three phantom types here for Christmas. The phantom type of Elm, Past, Present and Future? That would be very interesting. Well, let's find out. We've got a couple of guests here to ring in the new year with us. Our good friend Matt Griffith is here. Welcome, Matt. Thanks for having me. Good to be here. Thanks for being back. And of course, the one and only Mario Rojek is with us. Hello, Mario. Hello, hello. Thanks so much for being here. Great to have you too. Yeah, good to be here. It's good. It's a good time. It's a good time to be alive. So yeah, let's get into it. So today we're just kind of ringing in the new year, reflecting a little bit on this year in Elm and looking forward to the next year. So let's start out. What were some projects that you all worked on this year? Does anybody want to start anything that they shipped or were working on that they're really excited about? I shipped nothing this year. It's a year of lack of shipping. A year of preparation. Maybe that's not true. I can't remember when I... Did I release Elmcraft this year? I might have. I can't even remember. It feels like it. It feels like it's been... No, I did. Technically I did. You did announce Elmcraft. You absolutely announced Elmcraft this year. Yeah, February 20th. There we go. I released the thing. For people who don't know, I think you should pitch Elmcraft. Yes. For people who don't know. Well, you know what? I now no longer know. So I'm going to get rid of it. So I'm going to just read the about page from Elmcraft. So yeah, Elmcraft is an unofficial endeavor curated by a bunch of people who love and use Elm. So our goal is for Elmcraft to be the official, unofficial place for all things Elm. So yeah, it's pretty much me thinking that or feeling that there was a wealth of kind of knowledge, projects, initiatives, and people in the Elm community. And Elmcraft is kind of like a map to help you find your way amongst all those things. Is that Elmcraft.com? It's Elmcraft.com, Elmcraft.org, where it'll redirect to, but both domains are there. Oh, wow. Okay. Yeah. So yeah, that's a thing. That's great. That's a pretty cool post on the Elmcraft site, like a pretty interactive post, as I recall. That is true. Oh yeah. Jeroen released on Elmcraft as well. Yes. How could I forget? Even I forgot about that. Now relabeling this podcast, things we forgot we released in 2020. Exactly. We're not very good at self-promotion, are we? Yeah, right. It's been a very, very long 2020. Something or other. Maybe. 2020 and the 2020.2 release. Yeah. I wrote a blog post on Elmcraft about all the rules in ESLint, which is a lint of JavaScript, which didn't make sense for Elm. And basically that's like 90% of them don't make sense. So 87% more. 87%? Yeah. So that's a big, big chunk of Elm. Oh, actually. You were right. It's 87% of all the rules, but if you only narrow it to the recommended ESLint rules, then it jumps up to 92%. Not necessary in Elm. So yeah, I thought that would be a good pitch for Elm. I don't know if it worked, but it's a good statistic to add to Elm talks. Yeah. So I think the idea was for all that kind of stuff to pile up over time on Elmcraft, like a lot of the conversations that we kind of end up having in Slack and Discord that sometimes just seem really, really valuable. It became a trope this year because Martin Janicek, I'm not sure if I pronounced his surname right, but yeah, I pitched the idea of Elmcraft to him because I noticed he was answering so many questions and then he just kept answering questions. So I had to just keep DMing him being like, stop answering questions because we can't write Elmcraft articles as quickly as you're creating new knowledge. But yeah, he also has contributed a few things, so that's been cool. So yeah, next year I think I've got to figure out how to make the visibility of some of those kind of blog posts or knowledge things a bit more evident because right now they're not listed in the index, but they are accumulating slowly. So while we're talking about Elmcraft, how do people submit stories or articles or blog posts to Elmcraft? Yes, I suppose that's another thing that I forgot to announce is that Elmcraft is now open source on GitHub. It wasn't before when I released it, but now it is. But yeah, I think there's a discuss page on Elmcraft that basically just says, just join the Elmcraft discord and come have a chat about what your idea is. But yeah, I suppose you could do the same on the Elmcraft GitHub now. So yeah, I should probably definitely announce that. I'm going to announce that at some point. I think you just did. I think it's an ad. The more the merrier. Yeah, temporarily forgot I'm on a podcast recording. I still think you should announce it in a few places like discourse, Reddit, Twitter, Aston. Well, now I have to, right? Now it's on the podcast. The different discords. Which ones did I miss? So many places, lobsters, orange websites. Maybe like Elm News Federation, another project idea for next year. I have to admit, I have not seen these Martin Janicek articles being publicized on Elmcraft. I was not aware of them and they sound intriguing. I would really like to take a look. So we will definitely share some links in the show notes. Matt, what about you? What did you release this year? Yeah. Oh man, two things, I guess. So the first one was Elm CodeGen. So the idea there was that could we have CodeGen without having like, you know, Elm doesn't have macros, right? And I don't think we want macros, but like we still would like to benefit from some code generation. And what does that look like if you don't have like language support? So this is just a library and a little CLI tool to help you run stuff if you want, but these can be used sort of separately. And could you write a CodeGen library that was very intuitive and with, you know, that that was able to be sort of abstracted over that would allow a whole other class of tools for Elm that I sort of saw was kind of missing. Or if they were implemented like CodeGen tools, it was sort of like everybody had their own system and it was kind of harder. That ultimately, like the reason why I did that project was motivated because I had another project which is Elm GQL. And what Elm GQL was is we definitely wanted, we at Vendor, which is we use at Vendor just to pitch it a little bit, we have 600,000 lines of Elm code. And so the entire front end is Elm and we use a lot of GraphQL. And we were looking at tools to work with GraphQL. And one of the biggest challenging things with writing a tool like that is the code generation. So if you try to do just some sort of string template thing, you'll quickly like kind of run into some challenges. Like I know Dillon is probably intimately familiar with all of these challenges because he also has a GraphQL tool. And so yeah, so I wanted the Elm GQL to be out there and to pitch Elm GQL a little bit. It's a takes your GraphQL queries or mutations, parses them, checks them against your schema and generates some nice concise Elm code for you to use them or compose them together or whatever. And so yeah, those were the two main projects that were actually released. They are out there. You can use them now. Go check them out. And the other thing which unifies the two kind of is I gave a talk at Strange Loop called Code Gen with Types. And it was not only about the Code Gen tool, but it was about also how, what other things you could do with it. Yeah. That's my, that's my summary for the year. Actually you're really good about it. I'm really happy. It's been a big year for you, Matt. Yeah. And I will also note I've been since, since we recorded our Elm Code Gen episode, I've been using it quite a bit and it's been a joy. And I'm currently generating quite a lot of Elm pages V3 code in the V3 beta using Elm Code Gen. And it's, it's awesome. It's a, it's really better than Spring Template it in TypeScript was. So thank you. Well, thank you. I'm, I'm, I'm glad there've been a number. My hope with Elm Code Gen was that, you know, it's a tool for tool makers, right? So the hope is that there would be sort of a, a small set of very useful tools that would kind of leverage it. So I could see Elm pages using that. I could see something like Ryan Haskell-Glatz's Elmland potentially using it, obviously Elm GQL. And honestly, projects that I'm hoping to do this coming year. And this is why I was like, okay, I'm going to take time to do this Code Gen thing is because I'm like, there are a lot of things that I think I would like to generate specifically regarding like Elm UI stuff, like layout, CSS, whatever that I think would be very powerful. And it's cool to see people sort of pop up and write their own little tool in Elm Code Gen and generally have a pretty good experience. Code generation is hard as a concept. Like it's like just going into arbitrary code generation is challenging. So seeing people jump in and, oh yeah, I wrote this little tool to generate this thing and it took me 20 minutes. I'm like, oh my God, that's amazing. Yeah, it really is. Yeah. Having the right like set of primitives for, for generating code makes a huge difference. And it feels like so much more high level with Elm Code Gen. Yeah, I've, yeah, I agree, but I can't like my opinion only kind of matters, right? Yeah, I'm really excited to see it used in Elm Review rules as well. Yeah. I'm really curious to see how that looks. That should be really cool. Because you can do this review code, Jay, or is it, is there? So in Elm Review, you can have rules report errors and those errors provide a fix. And that fix is basically just a string, like insert a string, remove some texts. And as long as you can transform some Elm code to a string, which you can do with Elm Code Gen, right? Okay. Because it's just a library. You can use Elm Code Gen in Elm Review. Yeah. Very cool. That's, that's great. And speaking of vendor, VENDR, the company, not the technique of vendoring packages, our Elm at a billion dollar company, Elm Radio episode is like by far our most popular Elm Radio episode we've done. So thanks again to Aaron White for being an amazing guest and for VENDR for having like an awesome story for, you know, social proof for using Elm in an awesome way and building a successful company off of it. It's been amazing. It should be noted too, the Elm GQL library that I was able to write is like vendor inc slash Elm GQL. I did it basically entirely at VENDR and the Code Gen library as well, though it's under my name. So like the fact that they are, you know, pushing more resources to like give back to the community and stuff is, is one of the reasons why I'm very excited about working at VENDR just in general. Right. Yeah. I think there's a lot of opportunity and I'm hoping that there'll be some more, some more goodies in the future too. So yeah, that's huge. And so for people who want to know more about Elm GQL and Elm Code Gen, we also made a Elm Radio podcasts episodes with you, Matt. That's right. I was there. And they were all like, in the span of five episodes, we did three about things that were done at VENDR, including the VENDR specific episode. So I wouldn't say it's a busy year. It was almost like a busy month for you. Two months. Yeah. At least in terms of release cadences, because I'm guessing that the work has been lasting longer than that. Right. Yeah. There's still a lot of stuff that I want to get out. There are a lot of projects that I'm trying to get into shape where I can ship them. That's actually why Elm GQL and Elm Code Gen was, it was such a relief just because it's like, there's a lot of other stuff that I'm very excited to work on, but I can't do until those are shipped and using them. And so I'm very relieved those projects are out and generally just work for people. There's some improvements we want to bring. I know Dillon and I spoke actually about some cool Code Gen stuff that he ran into to improve the library. And I'm excited to bring that out there, but it seems like these projects are out there. People are using them and it's like, yeah, they work. Yeah, they work. Great. And so I'm like, okay, cool. They're in maintenance mode or at least like back burner a little bit where I can jump in on a weekend and improve them, but I can put my main focus to the future. Yeah. It's such a weight off your shoulders when you get something out there and it's shipped. You can always iterate, but it's shipped. Yeah. Jeroen, what have you shipped in 2022? What have you been working on this year? Something I started last year, at the end of last year, but I only got to, well, not release because you will understand why it's not released. I made a proposal called till recursion module cons for Elm Optimized Level 2. So that is basically like a change in the Elm language or the compilation of the Elm language where some functions that are not till recursive, meaning not very optimized, which can crash the stack and are just slower, can become till optimized and much faster. And that took a long time and I'm very excited because I think it works really well. The reason why it's not released yet is because it's still in a pull request to Elm Optimized Level 2. So as soon as Matt, you will have some time. I knew it. I knew this was coming. It's the whole point of this podcast really, just to have Jeroen to grant Matt on his time allocation. It's more of an intervention for my PR than a podcast really. That's what I'm hearing. Did you say PR? Did you mean PRs plural? Oh yeah. No, sure. Matt, this is actually not going live. We are just faking a podcast. It's not even recorded. It's just, no, I get it. Pressure you into- Oddly enough, not the first time this has happened, but. Yeah. So it's not released, but it was a very fun exploration and I'm very happy with the results. Other than that, plenty of new releases for Elm Review, improving performance, improving things you can do with it, data that you can have and quality of life improvements. I think you're also underselling the performance aspect of this. You had a major speed up. Yeah. Yeah. Yeah. Especially for fixes. Yeah. So the main one that you're thinking of is making fixes a lot faster. Some cases it's improved 13 times and I'm very happy because now at Vendor, where you have a very large code base, you can finally start using it normally instead of your Zamboni thing that ran every weekend or something. Which probably requires some context with people. So at Vendor, we have something called the Zamboni, which we love the Zamboni. We don't need no Zamboni hate. The Zamboni runs every weekend and opens a PR. In case you don't know what a Zamboni is, it's the little machine that re-does the surface of the ice for an ice rink. I wouldn't call it a little machine, but- It's a big machine. Yeah. A big machine. It's a big machine when there are people out there or else you might run them over. And mostly that was because we wanted rules that could be run that may take longer. And yeah, it definitely cut down the time of the average run for that. We haven't totally turned on Elm Review as far as something that would run in our build process, like when we're editing stuff. Build process meaning in dev, like when you're watching stuff. But I know there are a few motivated individuals at Vendor who are kind of looking into that. We did actually just write a new Elm Review rule that I need to actually review. So that's exciting. We did one for message naming conventions, just because as we grow, we realized that we were getting more Elm developers and that we had written a document for a while ago, and it's not a big document and it's fairly clear. People who see it are like, oh yeah, that makes sense. But you have no way to discover that. So the Elm Review rule is honestly really good. And Wolfgang put it together in like 15 minutes. It was great. Nice. Another performance optimization that I'm working on right now is, so it's not released yet, but I am working on it. So it's still kind of as part of 2022 and maybe would be released soon, is adding caching to Elm Review that would be persisted to the file system. And so far it looks like on my work project, it speeds it up by two times. So that's really nice. What is it caching specifically? Basically whenever you run Elm Review, it reports a lot of errors, right? And those would be persisted to the file system. So if you run it again and it noticed that a lot of these files haven't changed, then it doesn't need to recompute a lot of things. Got it. That makes total sense. Yeah. And that was quite tricky. So probably if I don't end up releasing it this year, it's because I'm running a very large blog post explaining it. Right. Well, if you don't release it this year, we'll just hold another podcast intervention and follow up. Exactly. Do you want to give the elevator pitch for extractors too? I'm very excited about that feature. Oh yeah. So there's now Elm Review extract, which is a new sub command or flag for Elm Review, where basically you can write rules which collect data about your project. That's what they do. And at the end, they report errors. The additional thing that you can now do is to take all the data that you've collected and transform into JSON that you can then pass to other projects, to scripts, or make graphs or images or documentation from those. So basically any data that you have in your Elm project, you can now collect and extract and do whatever you want with it. And I think that's very powerful. I don't know what people will do with it yet, but- I'm curious. I mean- Yeah. I mean, I guess you could make a metrics dashboard for your code base, right? Absolutely. Yeah. I'd be curious to see what you'd want to put in there. But yeah, and it wouldn't be that involved to actually do. Yeah. You just have something, like have a CI deployment or whatever, where you calculate this stuff, take out the JSON, put it to a little Elm app to display it. Yeah. A few things that you can already do with it is like compute the import module graph, find a list of licenses that you have in your dependencies, small things like that. Other things that I can imagine is people can write a rule that extracts the list of use CSS classes, and then they give it to a CSS pruner or something, things that remove all the unused CSS classes, things like that. It's a very generic feature. So how powerful it is, is related to what people do with it. Yeah. So many possibilities. Somebody was asking something about optimizations for an Elm pages site, about pulling in translations for a given page. And I was thinking, like, wouldn't it be cool? You've got this kind of JSON file of all these translations, and you have specific ones that you need for specific pages. And wouldn't it be cool if you just had a big data source in Elm pages of all the translations, but then if you had an Elm review extractor that looked at which keys are used for a given route and then massaged that data source to contain the subset. So basically, as a developer, you're not paying the cost of micromanaging which translation keys you're pulling in, doing some builder pattern to pull in only the translations you need, but then you can, under the hood, optimize it in your build step using this fancy rule. So there's just so many things I could imagine people building with it. It'll be really cool to see what people do with it. Yeah. If you want to start translating your application as well, you can just look at all the strings that are passed to HTML.txt, for instance, and then extract them and provide them to your translators. And they can now translate all those keys. Hmm. Brilliant. What about you, Dillon? What have you released this year? So I was gonna say before Jeroen says anything, because yes, it has been a year for me of a mega whale, not yet released stable Elm pages v3. I was not going to say anything. You were thinking. But I'm glad he has a PR open. I bet. Or two. So, yes, I do have a mega release. It has been a real joy and privilege to get to really dedicate most of the year to building this and no holds barred building exactly what I want in this. So I will say I did release the beta. That's out there. And people can install and use it. And they are. So I did release that. That's a release. That's a release. It's a beta release. It's a pre-release. And so, yeah, that's been amazing. Mario, I had this on my radar of wouldn't it be cool if instead of this sort of optimized decoder functionality that Elm pages v2 had, which was like one of the most complicated features I've ever built that I was super proud of. And then Mario's like, you know, it's pretty easy to just use Lambda to get the automatic serialization from the compiler. I'm like, really? It's like, yeah, you just like call these secret functions and it automatically serializes all your types. And so, yeah, there goes all my like most brilliant things I've ever built out the window. And now it's like a much better user experience, much more optimized byte data that's sent over the wire. So that's been amazing. Stripping out all my meticulously crafted features and replacing it with just Lambda serialization. So that's been amazing. I built a feature that I'm super excited about Elm pages script, which I think is so Matt was talking about this Elm code. What's the command in the CLI Elm code gen generate Elm code gen gen or run. Okay, cool. I'm coaching run. So Elm pages run is actually something we talked about on the Elm code gen episode with you. I was saying, I want to build Elm pages code gen where you can basically run like so Elm code gen run lets you run an Elm file where you can execute this Elm file. It runs Elm code gen and generate some files for you, which is great. But then maybe you need to pull in an environment variable. Maybe you need to read some files and pass them in and Elm code gen run. You can do that technically, but you have to maybe like strew together a few piped commands on the CLI to pass in that data as a flag. Right. So that's basically how Elm code gen run, it's basically just a little TypeScript little file that just sort of has a standard, you know, shape of an Elm app that it's expecting and and it hands it some flags, some initial data. And then it's expecting this Elm program to give sort of a list of files to generate. And there's a little bit of like logic as to where it would actually write those. But essentially all it can do is write files. Right. And yeah, so you it's very bare bones as far as supporting arbitrary CLI stuff. So writing something more to basically gather more data or maybe, you know, like for an example, Elm GQL does not use Elm code gen run. It has its own sort of custom made TypeScript harness that's made for Elm, the Elm GQL thing itself that does handle stuff a little differently. Right, exactly. So I think it's natural like Elm code gen, it's like if you try to build everything under the sun, what if you want to pull an HTTP data? What if you want to read environment variables? What if instead of writing files, you want to make an HTTP request to post something to an API or print it in the console or like it's a slippery slope and eventually you've built like a general purpose scripting. Yeah, right, right. Exactly. So basically like the Elm pages engine is a sort of general purpose way to bind to Node.js. So already with Elm pages data sources, you can read files, you can write files, you can read JSON files, you can read, you know, mark markdown content, you can do all these different things. But you can also create custom Node.js bindings with data source.port. So you can write files and you can build arbitrary bindings where you write a JSON encoder to send data into an async Node.js function. And then you return JSON in that async function and you write an Elm decoder to decode that response. So Elm pages script, you do the Elm pages run command and it will run a module in your script folder and execute that. And you can use that to scaffold things. So if you wanted to use Elm pages to scaffold a new route module for an Elm pages app, you can do that using Elm CodeGen. But you can also, like the sky's the limit because it's just an arbitrary scripting tool. So you can do, so all you do is you write a module that exposes a top level value called run and it's basically just a data source. So if you, so you can do like data source.http to get HTTP data. If that HTTP data fails, then your script just fails, which is fine. Like your script just stops and says, oops, I couldn't hit that HTTP endpoint. So I'm super excited about this. You can pull in environment variables, you can do all these things and you can use it with Elm CodeGen. So I kind of like, I built this thing that I kind of somewhat jokingly talked about in our Elm CodeGen episode of Elm pages CodeGen. Now we just need to get Elm review in there so we can do that extractors in a context where we have data sources. So yeah, I'm very excited about that. Excited to see what people do with it. Another thing that I shipped this year that I'm really excited about was OpenAI Whisper is this voice transcription service that works very well for Elm Radio. I tried several paid and free transcription services and they were pretty bad. Half a year ago, a year ago, even the paid ones, there were some things I'm like, I definitely don't want to put that up in a public place because that's a very embarrassing typo in the transcript. And then I tried OpenAI Whisper, this recently released like MIT licensed transcription project. And it's amazing. The quality is so good. So now Elm Radio, the ElmRadio.com website has transcripts powered by OpenAI Whisper. They're great. And you can, there's an audio player in there. You can click to a timestamp. You can share that timestamp. So pretty excited about that. That's really cool. I've been curious about the transcription stuff. Does Whisper do, does it track or is it able to distinguish like who's talking? Not right now. There are some, there are some experiments that try to build in that functionality. So I think we could see, I mean, the fact that it's open source, I think we could see some cool things emerging in the future for that. But right now there's no stable way to do that. I mean, this is evolving, just wait two weeks and it's there. Yeah, yeah, yeah, sure. Yeah, I think so. It's like for me, once that hits, I have an app idea that I don't want to talk about right now. Ooh, intriguing. We'll save that for 2023. Yeah, yeah, we will. It'll be good. Yeah. See, it's so good. And it, little details in your transcripts that a human would catch, but most transcription services won't, it nails. For example, if you have a false start in a sentence, it removes the false start and removes duplicate words and things like that and just figures it out. It's pretty brilliant. So interesting. That's been, that's been a lot of fun to build. Yeah. I'll have to play with that. That's great. Yeah. All right. Well, did anybody have any, any trends that they noticed happening in Elm in 2022? Any, any cool projects that they saw other people shipping? Any trends in the way people are building things in 2022 in Elm? I think if we, if we took a sample size of the Elm developers on this call, I'd say a hundred percent of Elm developers are making Elm tooling. That seems to be, I mean, if that's not the trend, then I'm not sure what is. Right. But yeah, no, I, I, I'm pretty, I'm pretty bullish on, on Elm tools, expanding the Elm language surface area, which I think is what has kind of been happening. And I'm not, I'm not sure it's like, I mean, I think it's, it feels to me like it's becoming increasingly obvious that that is a really cool way to go and we can get so much out of the Elm language. I wonder if maybe it wasn't so obvious originally as a community, maybe we kind of thought originally like, oh, it will, you know, Elm will evolve in really awesome ways once Evan does XYZ. And I'm not sure I, I don't feel that as much anymore. Like I think this year I've seen so many people kind of run with awesome tools that don't conflict with the Elm language, but to kind of empower it. I'm like, those really, really lovely primitives. I feel like it's been a year of discovering the joy of Elm's primitives in, in, in other contexts, at least for me. Yeah, totally. I mean, a decent number of my tools, or at least two of them that I can think of have that sort of could have been a, I'm irritated that Elm doesn't have this conversation, but turned into a like, oh, I made this project. It just works. You can have this benefit. So the two, two tools that talking about is Elm Optimize Level 2, which was actually, I believe last year. So that will optimize Elm generated JS a bit further and actually get some surprising results. This is the project that Yerun has something like 48 open PRs for. And then there's the second one, which is Elm Code Gen. Again, you'd be like, Elm doesn't have macros. Elm doesn't have a thing. It's like, oh, we could just have a library and like a little bit of node JS and like, oh, we have a thing. Actually turns out to be pretty nice. Okay, cool. We don't need a language change. In fact, we might even, well, you never say never, like, you know, who knows, but, but it's not obvious that having something like Code Gen built into the language would be the way to go. Yeah. But yeah. I've felt that way about this Elm pages script thing too. I mean, to me, that's just a more clear illustration of this more general thing, which is like Elm is a really good target for frameworks because you can define these effects as data and then execute them however you want and swap out optimizations under the hood. So it's very good for, for building frameworks and, and like, you don't need to wait for Elm to have some binding to node JS because well, like if, if you have effects as data, then you say, here's this, here's this type that represents an effect to do in node JS. And that's it. You don't need Elm to build it. You just need to build some glue that does some code generation and has a framework to abstract that away from the user. But Elm is a language is completely capable of that. Just like JavaScript as a language is capable of interfacing with C++ bindings through node JS. You don't need the JavaScript language. You don't need V8 to have bindings to executing C++ code. You just need a JavaScript language that has a way to, you know, run promises and, uh, and hook into that. So yeah, Elm, I like Mario's phrase that he's used in the past of Elm in unusual places. And I'm, I'm also very bullish on this. I think it's, I think we're, we're really seeing that come to fruition and I think there's going to be more of it. I think that for a long time, Elm developers were a bit scared of what would change in Elm 0.20. And as you said, Mark, Emmet, that we're waiting for Evan to do something. And now we're all realizing, well, actually we, we are seeing a lot of tools being released and they're very cool and they're very powerful. And we are actually have the freedom to do so. And we have the needs and we have, we have all those benefits from Elm, which allows us to make great tools. So let's do it. And yeah, that's what I'm noticing is that we are making more and more of these tools and the quality is amazing. Every time almost. One of them that I'm thinking of is Elm CodeGen, obviously, but also Elm Watch from Simon Leider. Yes, totally. That one is amazing. Really fast. And just because it's using Elm's primitives under the, or it's making use of all the promises that Elm makes and to make it really fast. Yeah. This is something that I want to see more and more of. I think we will. Another trend that I saw happening in 2022 is that I feel like we're seeing more and more tools to make applications. Like we've had Elm SPA, we have Elm Pages, Elm Static, and Elm Lend is also new, which hasn't been released. It's still a beta, I think, at the moment. And I feel like we're seeing more and more of these and it's starting to be like, huh, we need to make choices when we start an Elm application right now. And that worries me a little bit, but a little bit of computation is also quite nice and useful. Yeah. And LambdaRay, of course. And LambdaRay, obviously. LambdaRay is underlying, right? I'm not going to, I'm going to make this problem worse by releasing LambdaRay Elm Pages and LambdaRay Elm SPA or LambdaRay Elm Land. Yeah. But it's like a nice type safe battle royale, you know? Like whichever one wins is going to be lovely. So yeah. And similarly, I feel like we're seeing more and more UI frameworks as well. We've got ElmUI, ElmCSS, ElmTailwind, ElmHTML, all those variants with context. We've got accessibility packages and more, George Boris also came on the show to talk about one of those, I think. Elmbook. Elmbook. And he also released recently, but not, he didn't come on the show for this, ElmWidgets. So there's a lot of these UI frameworks and components collections, which is both nice and also like, again, like there's a little bit too much choice in my opinion, but that can be very useful for adoption at least, I think. And to get quick, quick started on a project. Yeah. And that leads into, I'm not sure if we want to transition to the future, but definitely leans into one of the projects I'm excited about. Go for it. Yeah. So one of the motivations for Elm CodeGen just in general was, had to do, so there's the Elm UI library, which you may or may not be familiar with, but essentially is a way to kind of write your layout and it turns into CSS, but I wanted a language or a library where the layout itself was tighter than CSS, where there were fewer basically situations where you just had no idea what was going on. I wanted to make it as type safe as possible, essentially. Like another way to think about this is if you could look at the code and know what it looked like, know what your layout was. For me, that's just insanely valuable. If you're able to read the code and just in your mind, you're able to compose what is actually visible. That's just incredibly valuable as far as speed of development. And I think that resonated with the community for sure, but it's missing a piece because I was like, okay, why don't people use Elm UI or why wouldn't they? I think there's a very valid argument of they want a design system and there's no design system built into Elm UI. Elm UI is sort of a replacement almost for CSS, right? Where it's like you can think of style this way, but there's no design system on top of it. There's no set of presets. And I've thought a lot and had a lot of experience at vendor building as we've gone through different versions of design systems at vendor and what is a good design system look like with a system that is roughly the shape of Elm UI. It doesn't have to be Elm UI, but if it makes some fundamental choices that are similar. And I was thinking like, okay, well I want a language for design, your design system, like a higher level language. And I think the challenge initially is that implementing that language in Elm code, like that you're writing by hand is challenging for a number of reasons because you have to basically, you have to be disciplined about what you're writing. Where it's like, okay, here's my, you gather a set of values that work well together, but you have to make all these like little tiny decisions that are kind of dumb. It's kind of like, where do I put my colors? Like how do I even organize my UI code? I would like some widgets. And even when I'm like looking at this design system, I would like to even be able to like have a few widgets already made for me. Nobody is going to create a button or module. Everyone's going to create like probably a number of very common UI elements. So I made Elm Code Gen with the idea, and this is basically still being formed, but I think the project is one that I want to get out because I want to use it this year is that you'd be able to sort of define what your design, like high level primitives for your design. And it would generate sort of a bunch of Elm Code, including possibly a handful of kind of widgets that everybody's going to use. And it would just be in like a really nice shape already and kind of ready to use. And because the experience I want, I think about who the ideal customer for Elm is as a language. And one thing that I think about is somebody who doesn't want to deal with kind of the mess of JavaScript and like the always iterating, you know, massive cognitive overhead of the ecosystem of JavaScript. They want something simple and they want to get something working very quickly. I think there's a big challenge there. If they don't have a widget library, they can boot up very quickly. It's actually very hard. It's actually like almost like a stopper where it's like someone's going to choose React because guess what? You can just pull down a design system easily. And even though you may conceptually really love the idea of purity and everything, like you really, your main focus may be on a backend implementation of something and you just need a UI for it. Anyway, so that's what I want to build. I've been thinking about it for a long time. I think I know what it looks like. Yeah, it's very exciting. And there are precedents out there for kind of design system palettes like Tailwind being the obvious one. That's not that complicated. It's like, what colors do you have to work with? What spacing units do you have to work with and generate some permutations of things? Right. I think the actual complexity of the project won't be, I think it'll be in the design decisions. So like, but not in the actual implementation. I think there are things to know about for each of those concepts that if you're aware of them, it makes things a lot easier. If you're not, then you're not. So an example being you're choosing a color palette and this is if you're not getting it from a designer who would have, hopefully, they would just know that have this on lockdown. But if you're designing a color palette yourself, if you don't know about color spaces that make it much easier to develop a palette, so this would be something based on what's called Lab, L-A-B, then you're going to have a much harder time with colors. If you don't know about it, well, if you do know about it, you'd probably start to use it, but even then it can be a little confusing. So an example of what Lab does that just so I'm not like throwing out terms and people are like, what the heck? You may be familiar with defining a color where it says like HSL, which is like hue, saturation, and lightness, right? The lightness parameter is actually, it's a lie. It's not lightness. Two colors that have the same lightness will be vastly different perceptual brightnesses. And what this means is that when you're developing a color palette, it means that the color space, the numbers you're using to define your colors, things that are numerically near each other are not perceptually near each other. And that means it's very frustrating to actually come to a higher level abstraction for anything. I think there, so it's like, okay, well, that's really interesting. Like one color space, again, that ultimately, because this is all mathematics, turns into something called Lab is called HSL UV. And that's where they basically adjust these things so that they are sort of numerical. Numerical closeness means perceptual closeness. But anyway, having insights like that, which are awesome. I think there are also insights around typography, around spacing, around visual weight of things, gradients, being able to construct nice gradients is a subtle thing that you could go spend a few weeks learning, maybe not a few weeks, but definitely like surprisingly more involved than it is immediately obvious. So like gathering all that design knowledge into one code base that kind of knows what the mental model should be, I think will be super valuable. That's amazing. I just want to say like, this is like one of my favorite things about Elm and the Elm community and the Elm ecosystem, I think is this like, not settling for the status quo and saying like, yeah, there are palettes out there for creating a design system and laying out these colors. It's like, well, what if we think this through from first principles? Where would we arrive? And I think like, I think Evan has really modeled and embodied that and that has shaped the Elm community and attracted a lot of people to the Elm community who care about that. I think Elm's data types and purity have created tools to help people do that and made that the path of least resistance to make impossible states impossible and make you really think through the options. And like Matt, I know you've personally inspired me the way you take that approach. And I like, that's what I love about Elm and the Elm community is that we take these principles and we say, yeah, we're not going to settle for like, people have figured out a pretty okay way to do this. We're like, let's think this through from the ground up from first principles and find a really great way to approach this. So very exciting stuff. Shall we go around the room and talk about plans? Is that exciting? Is that a thing? Absolutely. Absolutely. Okay. Who's got plans? Any, anything you say here, you are going to be held to legally. There may be interventions. There may be interventions. Legally and also like socially slash emotionally. Basically all of them. Any PRs that are mentioned here must be reviewed. Or they will be closed. Tell recursion monitor to close. I was going to say any PR that's mentioned here, I'm just going to go close. It's been reviewed. Well, I don't make any PRs for Lambda because it's all closed. So yeah, I guess I'll dive in. I am excited for my year long hustling towards a Lambda version 1.1. Funnily enough that the feature I'm most excited about isn't actually one that I primarily worked on anyway. So that's also kind of cool. So what I've been mostly working on is evergreen migration auto generation. So for those that don't know in Lambda the migrations and deployments are type safe, or at least my goal is for them to be 100% type safe. So if it compiles locally, you know it's going to deploy and you know it's going to migrate and everything's going to be lovely. And so one of the problems of that introduced is Elm. So what Elm does have is it has equality between records that are equally defined, right? So if you have a record that has first name and last name as strings, and then somewhere else in your project, you've defined identically the same record, and then you compare those somewhere, they will be equal. So there's kind of like the underlying reason for that is that there are really no records, all records are anonymous and you only have kind of type aliases for records. Whereas for custom types, that's not true. So a custom type is not an alias, a custom type is kind of like a concrete primitive, if you will. So if you create a custom type that has, you know, a custom type of ice cream flavors that has chocolate, vanilla and strawberry, and then in another file somewhere else, you create exactly the same type with the same name and the same variance, creating two values of those two different types and then comparing them, they won't, well, it'll be a type error, you can't even compare them. So the annoying thing about trying to apply the evergreen kind of philosophy to your apps is that basically between two different versions of your apps, we do snapshots to keep hold of what all your types were so that Lambdaera can know, well, you know, you've changed these types and this is exactly what's changing. And the frustrating thing has been that users kind of have to write out, even if you haven't changed a custom type, technically, according to Elm, it's changed, right? So you have to write this really big migration function to go, okay, every single old variant maps to every single new variant of this thing that, you know, hasn't changed. And it's a bit annoying. So yeah, it's one of those things where I'm like, I feel slightly annoyed because it's like, it's not a feature that's delightful in the sense of like, hey, look at this new thing you've got. And it's more like a, hey, look at this thing I fixed that was annoying and imposed by me. So it has been not a huge amount of joy in that because I'm like, oh, it was kind of just bad to start off with. And I've had to figure out a way to patch it. So it'll save people a lot of time. I think 99% of migrations will now get written for you. So I'm really excited about that. But most people would just probably, I'm assuming just be like, oh, great. Now I don't have to do that annoying thing that you made me do. And it's how it maybe should have been always. But the second part of it, and there's a whole slew of features in there, but the one I'm super, super excited about, and I think even non-Lambdera enthusiasts will be excited about this is Martin Stewart came up with an implementation of an idea that I'd kind of thought and toyed with, but he really kind of nailed it down. So basically in the next version of Lambdera, what you'll be able to do is you boot up Lambdera Live, you browse around anywhere in your project, you want to edit a particular piece of the UI, you've navigated somewhere deep or you've navigated into some complex state and you'll notice a borders off where there's something's wrong and you want to go there. And I think now the experience, at least for me, is I kind of jump into my editor and if I'm lucky, I'll kind of guess the name of the identifier that I'm kind of globally searching for and maybe I'll find it. Or maybe on the page, there's a bit of text and you're like, oh, that looks like a bit of unique text. Maybe I'll just copy that text and search for that and maybe I'll find the element. But a lot of the time, especially as a project I've not worked on for ages, you just, you can't remember where stuff is, right? And everything's slightly, it's just a little bit of friction. And so now instead of doing the thing that you were thinking like, oh, I need to do that, you've had to switch modes, switch gears entirely to where's the thing. And then when you find the thing, you're like, what was I doing again? And it's just a bit of mental kind of impediment. So yeah, now with the feature that Martin's worked on and we've kind of worked together to integrate into the compiler, you can hover over any UI element, hit a hotkey, it'll give you a dropdown of the kind of tree of functions that were involved in generating that particular piece of UI. And if you click on any of them, it'll try and find your current editor and open up like directly that line and like the column and the row of the exact expression that is generating that UI element. So it's kind of, I'm calling it UI source maps or like live UI source maps. So I'm, yeah, I'm phenomenally excited about this. And yeah, the moment that I started using it, I was like, how have I not had this? Like now I can't not have it. And the exciting thing for other Elm users will be because one of my goals of the Lambdaera compiler is for it to be backwards compatible. You should be able to, well, we have to figure this out how that'll integrate into other pipelines, but there should be a pathway for using this feature on just a regular Elm project, which I think would be really, really cool. But I think that as a whole release thing, I'm excited about that, but I think that segue is I'm going to just throw the baton over to Matt for the other project that we've been kind of on and off collaborating for this whole year, because yeah, this feature, I want to end up in that tool as well. Yeah, a thousand percent. So this is something I initially sort of announced at Strangeloop for my talk, Code Gen with Types. And you'll see a little demo at the end of something that's possible. If you go look for that talk and watch it, it's at the end, so you'll have to stay till the end. So this project is called ElmDev. There's a placeholder website at elm.dev. I actually got that domain, which I'm so happy about. What ElmDev is, is again, talking about how Dillon was like, well, in Elm, we sort of try to rethink things maybe from principles, first principles. Not that that's always the way, but like, it can be very nice. So ElmDev is basically support for editor tooling based on a version of the Elm compiler. So this is something that Mario and I have been talking about and have been working on for actually a decent amount of time. The idea is that, you know, the compiler actually knows a lot about your project. And a lot of that information could be surfaced in really interesting ways. Like yeah, you could have a mapping to like a view function. You could have like, I think actually navigation of your code base in different ways than files. You could have like information from the compiler about your code. So this would be like essentially like bringing the static analysis that the compiler already does and surfacing that when you can. And I was just very excited because it seemed like there were so many interesting avenues for editing that an Elm would be like ideal for a lot of this because of just how it's sort of put together that we'd be able, that I wanted to explore personally. And the main blocker seemed to be that, you know, there's this sort of this concept of that and Mario, let me know if I'm getting ahead of myself, but like that, you know, a batch compiler, a compiler that is meant to run, you know, you fire it and then it finishes is just going to be too slow for editor tooling. And of course, if you're Rust or if you're Haskell, this is probably true just because, but your language is vastly more complicated than Elms in a lot of ways. And also Evans put a lot of effort into making the Elm compiler efficient. It is very efficient. So what we've been working on is with our version of Elm dev, our version of the Elm compiler is what couldn't we cache in memory? Like can this be a running server that we talked to and can we cache some of these things to make it fast? And like, I, we think that that's, that's true from what we're seeing. We can't probably talk to specifics on that, but, and there's still a lot of work to be done, but very excited for what this opens up. And this is to give a little bit of context. When we had Simon Lidell on for the Elmwatch tool, he kind of hinted at this idea of like, okay, I can make the feedback cycle faster with this nice Elm live dev server environment that, you know, compiles things very quickly. But if I, like he was getting hooked on making it faster and he's like, well, if I really wanted to make it faster, I would have it in memory. And yeah, that's basically what you're talking about. Yeah. Yeah. Yep. I find it funny that I've been working very hard to make ElmReviews cache that is in memory, put it on the file system and you've been trying to do the opposite. The opposite. That's right. Yeah. Absolutely. So one, one anecdote there, and like Matt said, like there's some, a lot of this work is, I mean, it's looking pretty promising. I think Matt's currently been playing around with the kind of memory model work that I'm desperately trying to get back to, to finish off. But yeah, one anecdote that I found both really funny and very kind of encouraging, continuing to pursue this as I spent an entire weekend, kind of, I spent a weekend finishing off, finishing off basically this kind of a chunk of this in-memory caching feature, caching feature. And I was getting really excited to be like, okay, I'm going to now it's, you know, it's, it's, it's working, it's type checking. I'm going to do some tests. And I was really excited to send some results to Matt. And I started, I started putting together like all these tests and you know, I was having, this isn't in like a Haskell development mode, right? So it was slower than the normal Elm compiles would be anyway, but I was kind of seeing like 200, 300 millisecond compiles without caching like the normal ones. And then on my, my memory work, for some reason it was giving me like 600, 700 milliseconds. And I'm, I'm scratching my head. I'm going like, it doesn't, it doesn't make sense. I'm like, what if I deoptimize? And I was digging, digging, I spent hours, I think I spent the full, I think I spent the full of my Sunday kind of, kind of torpedoed into this. And eventually I got a, something, something, something crashed at some point and I couldn't figure it out. I was looking at, at, at the code and I had this really, I mean, it's all entirely my fault. I had this really, really janky code where I was using some, some Haskell, a low level library to do timing parsing. Right. And I had assumed in my little string parser of this timing result, things would come back to me in seconds or milliseconds. And I had this, this parsing crash. And then I look into it and I realized actually I hadn't handled the case where things are coming back in nanoseconds, but what was actually happening is that the memory cache compile was coming back in like 600 to 700, 800 nanoseconds, not milliseconds. And so actually everything was fine and there wasn't a problem. And I was just going around in circles for a day. So it's so good. Very, very excited when I was able to share that with Matt. I literally remember that day. It was, it was, yeah, it was awesome. I, yeah, and what we're using to sort of validate this is we actually are using the vendor code base, which again, 600,000 lines of code. So it's pretty big. You know, there may be bigger code bases, but it's definitely one of the top, you know, whatever, five, maybe of things that are out there. So if we can get it where it can in the editor, it feels like instantaneous feedback, which I, I feel is possible. And I, I have experienced actually with some of this stuff because when the stuff is cached and you haven't changed anything, you're just navigating around. Like it should be nearly instantaneous, like, you know, a millisecond or two to do some of these things. So the tricky part is when like, okay, you made an edit now, can we get stuff? But yeah, cannot wait. That's going to, that's going to be this year for sure. Because it's one of the projects I'm super, super excited about this year, 2022. Cool. This upcoming year. That's right. I'm closing your PRs right now. I have the window open. I'm leaving an emoji. Another middle finger one. So this is all very exciting. I'm not sure whether you guys want to get into, you're talking about like speed and the feedback cycle. I'm not sure if you, how much you want to get into things beyond that of like getting more rich information about Elm code here, but just wanted to give you the opportunity if you want to talk about it. Yeah, I can kind of talk to this a little bit. It's kind of been aware, like Mario's been working on the caching strategy and I've been working on iterating on a specific plugin for VS code to sort of try out these editing ideas that we sort of have. It's a, there's the basic premise, which is that the compiler can allow us to get information that is otherwise hard to compute, or you basically just end up into a situation where you're literally recreating the compiler in some other language, which is a pain. Yes. So it's like, okay, we have this information. So we have this information and also just the, you know, at vendor, I spent a lot of time thinking about product and how to develop a product that's actually useful and, you know, solves like for the user and everything. So we don't, it, what I am not pitching is a specific solution with this, but I am looking very hard at what do people actually do when they edit. And it's not obvious to me that our default modality of editing, and when I say default, I mean something like the standard VS code editing experience where you have a lot of information in hover cards or, you know, even like the, the language server, like we're going to flash red with all these intermediate compilation steps. Like it's not like that doesn't feel like the, that doesn't feel like the ideal. It doesn't to me. I think it's really cool. And there's, there's a lot of, there's some stuff there, but there's a lot of stuff just to learn of what information do you want at what time or what are you doing? You know, what are you trying to accomplish? We spend, I think way more time or I spend a lot more time in my code. It's not about reading code necessarily. It's literally about navigating and gathering enough information context. So it's, you don't read it from front to back. Like you read a book, you know, you, you, you go around, you gather clues of, of what you might, you gather references, right? You set your table with various things from all over the code base, and then you get a piece of clarity and you do something. So there's not too many, I mean, there are some tools, but there aren't too many tools that I think look at that concept at that primary interaction, which feels massively common and actually try to solve for it beyond just like, we're going to kind of give you everything at every point and hopefully something sticks. So I have some really cool experiments that I cannot wait to share, like once we have something going, but it's sort of in the stage of like, okay, I think I have a good idea what the problem is. Let's start trying out some ideas that address those things directly and hopefully it'll be successful, but you know, who knows? It's very exciting for me. It's every time I see these exciting things being done with AI code assist tools, like GitHub copilot or people using GPT chat to solve coding problems and, and stack overflow banning GPT chat answers in, you know, I'm like, ah, this feels like the wrong direction. And like, I would love as a, you know, pure FP community to be like, here's another path we could go down and it's actually really awesome. Let us show you how cool it can be. Yeah. I'm really curious. I mean, I think AI stuff, it's not going to be like a fad. It's going to be just like progressively moving forward. I think one of the challenges we're going to have just in general is not that right now there's a bifurcation, right? There's like, okay, here's something that was totally like human developed and here's something that was totally suggested by an AI, but like, there's going, there are going to be hybrid tools that are going to be more nuanced on both sides, but who knows what that looks like? Absolutely. Exactly. And like type directed, right? Like, okay, we can, we can calculate the types already. Type directed AI intervention of some sort. That's I don't know. It could be a thing. Yeah. And it should be like computers are good at understanding constraints and they have them. So let them do smart AI things within those constraints. Yeah, definitely. I'm pretty, I'm pretty convinced that it's not going to be, that it's not going to be AI long-term applied to directly spitting out code for us. It feels like there's going to be a better, a better human interface there, or just, just a better interface in general. And then maybe AI could target that interface or humans could direct AI to target that interface much better. Like I think the imagery, imagery is a pretty good view of how that goes, right? Like there's a, there's a, there's a visual target. It's a kind of softer visual target. Whereas I think programming right now is very, very precise and very, in a large amount of really pernickety ways. And I think Elm, like Elm is a really great example of that, right? Like it's like a tighter interface where we go, you know what, let's use more of what the computer is really, really good at. Like let's narrow in on that or let the computer do more of that stuff rather than us, you know, having those kinds of foot guns lying around all over the place. So yeah, we'll see. AI in Elm.dev for version two now, I think. I like that. I like that take, Mario. I can definitely imagine like AI tools to help assist you in generating like sample inputs and outputs. Cause it sees the pattern and it's like, well, okay, should I help you fill out your test suite? And then it could understand how it's tweaking of the code or generation of code influences the outputs as it runs that test suite and have that as part of its feedback loop. It could understand the constraints of the type system it's working in. And then it could iterate on that and start making those tests green and then show you, okay, here are five different ways to make this green. Like that's what I want. Those are the type of AI assists that I want. Help us help you machines. We're here for you. Always with tests, right? That's true. It always goes back to types and tests. That's a moral of the story. It's only going to work for you because you're the only one who writes tests first. In the typed world. Or like you could maybe take that further. You basically have a code and then have the AI ask you questions like, were you intending for this list to be reversed afterwards? That's interesting. I like that. Because you may not have noticed it, but it definitely is. You could get the AI to review PRs for you. And then you basically just confirm properties, right? So like you say, yeah, this list is reversed when you call this function. Great. Jeroen, do you have any plans for 2023? Any goals? Any hopes? I think I'm going to work on that. It's not just the one hope. It's just that one PR. That would be pretty cool. And then hopefully it gets backtracked to the Elm compiler. Because otherwise it's not very useful in practice. I think I'm going to work on Elm review. Probably. Surprising, yeah. The one thing that I would really like to get in there is to add type information. But for that I need to re-implant the Elm compiler. To ask Martin Janicek to do that for me. So if you're listening, do it. That would be amazing. That would open up for so many possibilities. So that would be cool. Yeah, maybe I'll... I'm really thinking about performance at the moment. So maybe I'll do a few more pull requests to Elm Optimizer 2. Who knows? Maybe we can look at those next year. Yeah. Then what I'm probably going to do with Elm review is just add more information to the project. Like being able to collect arbitrary files, like CSS files or arbitrary JSON, your translation files, whatever. Because the more information you give to the linter, the better it becomes. That's what I figured out when I was preparing talks this year. And then yeah. Because false positives is a lot of information. Yeah. And maybe to make it be able to do and maybe allow it to do more things like create files, delete files, basically do something like Elm Code Gen in a way. Elm Pages Code Gen review. Yeah, absolutely. Maybe without the pages. Matt, let's do something just you and me. It's always Elm Pages. Why is it always me who's the third wheel? Okay, hear me out here. Elm Pages Code Gen review Lambdaera. Boom. Now you're just trying to include Mario. No, actually I am very excited about the possibility. Well already Elm Pages v3 beta uses Lambdaera, but other way around could be really cool too. And we've definitely discussed possibilities in that area. 2025 all tools converge into one mega tool. That's what I'm predicting. Yes. Elm Pages is already using Elm Code Gen, Lambdaera and Elm Review. Right? All right. Exactly. Yeah, it uses Elm Review for several things, including helping do dead code elimination for the data sources. So they're stripped out of your client side bundle, which is very exciting. So second hot take there, Elm Pages does a hostile reverse takeover. This is all the other tools to be subservient to Elm Pages. I will neither confirm nor deny that. Anything else on your radar Jeroen? For now that's it. But like most things I will go through with whatever I feel like I'm interested in. Right? If I'm interested in performance, I will look at that if I'm interested in fixing security issues, I will do that again. If I'm interested in making Elm Review much faster, that's what I'll focus on. And it's probably going to be related to performance or Elm Review in some way. And all the while I will keep trying to convince the JavaScript community that they're wrong and that they should do things like we do it in the Elm community. But not very hopeful for that. The best way to convince someone is to tell them they're wrong, by the way. I know, I know. In all seriousness, I do think actually a lot of the things we've talked about here do reduce the barrier to entry for people coming into Elm and give a better on-ramp. Because I think we all know that once you're up the on-ramp of Elm, you fall in love with all these things, all the things that seem like just a pain, decoders and not having a bajillion NPM dependencies and things like that seem like a burden and how am I going to manage? But then you're like, actually, packages that do exist are amazing. There are all these things that I can very easily do without a package. It's so easy to manage my dependencies and it's so type safe to write these decoders in this way. You fall in love with it, but you need a good on-ramp. So I think we're talking about a lot of things here that do ease that on-ramp. I'm definitely excited about in 2023, first of all, getting the Elmpages v3 stable release is going to be a huge load off my back. I can't wait to finally be done with it, but also just have created this thing that I have really wanted to create for a long time for server-side rendering, dynamic server-side applications, a la Remix and Next.js. Very excited for there to be an answer to the question of how do you do that kind of thing in Elm. And I'm just, yeah, I think in 2023, I'm really excited to have that out there and do fun things with it and to continue to try to answer this question of how do we make great experiences for the web with Elm and how do we equip people with the tools to do that? And so, yeah, I'm also, I've been doing some pairing with Philip Kruger on Tailwind for some Tailwind v3 changes for Elm Tailwind modules to take all of these permutations of different Tailwind variants that get generated into a bajillion generated functions and parameterize color. So instead of bg red 500, bg red 600, bg red 700, you have bg with color and then theme dot red 600, for example. So that's another thing I'm excited about, shipping and getting a reduced period entry for people who, you know, I think that having great first principles tools like Elm UI is important. And I think people who say, well, we're on Tailwind, how do I do that in Elm is also super important. So I'm really excited to, like, we can have best in class ways to do that where it's like, okay, here's how you do Tailwind and react, but check out how you can do it in Elm. Check out how type safe it is and how high level it is, but you still are using Tailwind. So I'm super bullish on that and yeah, excited to see what's in store for us in 2023. Yeah. I also want a website for Elm review because Matt's been making a few websites for his Elm projects. This year is going to be the year of Matt writing so many websites. And I'm just going to fork one of them and make Elm review. Great. Yeah, I need to write it. You can do that right now, but they're very, it's like one page, but ultimately, I mean, yeah, there's a sequence here. It's like design system generator. Great. I get that. And then I'm able to use that to generate the stuff for the pages. And then, yeah, no, I got at least four websites that I want to make this year. We've got ElmUI.com. We got ElmGQL.com. We got ElmCodeGen.com. We got Elm.dev. I think I want to launch my own blog at some point. We'll see. Don't each of those have to have their own blog as well? Probably. Yes. Wait, your blog has a blog? Yeah. He blogging about the blog. It's the meta blog. Well, I can't wait for all of these wonderful projects. And thank you again for coming on to ring in the new year with us. Mario, Matt, thank you so much for coming on the show and happy 2023, everybody. Thanks for having us. This is great. Yeah. Happy 2023. And Jeroen, until next year. Until next year. Happy 2021.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.0, "text": " Hello, Jeroen.", "tokens": [2425, 11, 508, 2032, 268, 13], "temperature": 0.0, "avg_logprob": -0.33627072288876486, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.13448497653007507}, {"id": 1, "seek": 0, "start": 1.0, "end": 2.0, "text": " Hello, Dillon.", "tokens": [2425, 11, 28160, 13], "temperature": 0.0, "avg_logprob": -0.33627072288876486, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.13448497653007507}, {"id": 2, "seek": 0, "start": 2.0, "end": 9.200000000000001, "text": " And, well, it's feeling a little bit of holiday spirit in the air today, wouldn't you say?", "tokens": [400, 11, 731, 11, 309, 311, 2633, 257, 707, 857, 295, 9960, 3797, 294, 264, 1988, 965, 11, 2759, 380, 291, 584, 30], "temperature": 0.0, "avg_logprob": -0.33627072288876486, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.13448497653007507}, {"id": 3, "seek": 0, "start": 9.200000000000001, "end": 12.64, "text": " No, I would just say it's feeling very cold here.", "tokens": [883, 11, 286, 576, 445, 584, 309, 311, 2633, 588, 3554, 510, 13], "temperature": 0.0, "avg_logprob": -0.33627072288876486, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.13448497653007507}, {"id": 4, "seek": 0, "start": 12.64, "end": 17.8, "text": " I've got three sweaters on me and I'm just cold.", "tokens": [286, 600, 658, 1045, 11872, 433, 322, 385, 293, 286, 478, 445, 3554, 13], "temperature": 0.0, "avg_logprob": -0.33627072288876486, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.13448497653007507}, {"id": 5, "seek": 0, "start": 17.8, "end": 20.56, "text": " But yeah, there's also a little bit of Christmas feeling.", "tokens": [583, 1338, 11, 456, 311, 611, 257, 707, 857, 295, 5272, 2633, 13], "temperature": 0.0, "avg_logprob": -0.33627072288876486, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.13448497653007507}, {"id": 6, "seek": 0, "start": 20.56, "end": 24.88, "text": " Maybe it's not a holiday spirit, maybe it's a phantom type.", "tokens": [2704, 309, 311, 406, 257, 9960, 3797, 11, 1310, 309, 311, 257, 903, 25796, 2010, 13], "temperature": 0.0, "avg_logprob": -0.33627072288876486, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.13448497653007507}, {"id": 7, "seek": 2488, "start": 24.88, "end": 31.599999999999998, "text": " Oh, I wish we had the three phantom types here for Christmas.", "tokens": [876, 11, 286, 3172, 321, 632, 264, 1045, 903, 25796, 3467, 510, 337, 5272, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 8, "seek": 2488, "start": 31.599999999999998, "end": 35.04, "text": " The phantom type of Elm, Past, Present and Future?", "tokens": [440, 903, 25796, 2010, 295, 2699, 76, 11, 18408, 11, 33253, 293, 20805, 30], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 9, "seek": 2488, "start": 35.04, "end": 37.879999999999995, "text": " That would be very interesting.", "tokens": [663, 576, 312, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 10, "seek": 2488, "start": 37.879999999999995, "end": 39.12, "text": " Well, let's find out.", "tokens": [1042, 11, 718, 311, 915, 484, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 11, "seek": 2488, "start": 39.12, "end": 43.120000000000005, "text": " We've got a couple of guests here to ring in the new year with us.", "tokens": [492, 600, 658, 257, 1916, 295, 9804, 510, 281, 4875, 294, 264, 777, 1064, 365, 505, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 12, "seek": 2488, "start": 43.120000000000005, "end": 45.0, "text": " Our good friend Matt Griffith is here.", "tokens": [2621, 665, 1277, 7397, 23765, 355, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 13, "seek": 2488, "start": 45.0, "end": 46.0, "text": " Welcome, Matt.", "tokens": [4027, 11, 7397, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 14, "seek": 2488, "start": 46.0, "end": 47.0, "text": " Thanks for having me.", "tokens": [2561, 337, 1419, 385, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 15, "seek": 2488, "start": 47.0, "end": 48.0, "text": " Good to be here.", "tokens": [2205, 281, 312, 510, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 16, "seek": 2488, "start": 48.0, "end": 49.0, "text": " Thanks for being back.", "tokens": [2561, 337, 885, 646, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 17, "seek": 2488, "start": 49.0, "end": 52.28, "text": " And of course, the one and only Mario Rojek is with us.", "tokens": [400, 295, 1164, 11, 264, 472, 293, 787, 9343, 3101, 27023, 307, 365, 505, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 18, "seek": 2488, "start": 52.28, "end": 53.28, "text": " Hello, Mario.", "tokens": [2425, 11, 9343, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 19, "seek": 2488, "start": 53.28, "end": 54.28, "text": " Hello, hello.", "tokens": [2425, 11, 7751, 13], "temperature": 0.0, "avg_logprob": -0.2756612750067227, "compression_ratio": 1.6096654275092936, "no_speech_prob": 2.9758941309410147e-05}, {"id": 20, "seek": 5428, "start": 54.28, "end": 56.160000000000004, "text": " Thanks so much for being here.", "tokens": [2561, 370, 709, 337, 885, 510, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 21, "seek": 5428, "start": 56.160000000000004, "end": 57.160000000000004, "text": " Great to have you too.", "tokens": [3769, 281, 362, 291, 886, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 22, "seek": 5428, "start": 57.160000000000004, "end": 58.800000000000004, "text": " Yeah, good to be here.", "tokens": [865, 11, 665, 281, 312, 510, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 23, "seek": 5428, "start": 58.800000000000004, "end": 59.800000000000004, "text": " It's good.", "tokens": [467, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 24, "seek": 5428, "start": 59.800000000000004, "end": 62.68, "text": " It's a good time.", "tokens": [467, 311, 257, 665, 565, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 25, "seek": 5428, "start": 62.68, "end": 64.16, "text": " It's a good time to be alive.", "tokens": [467, 311, 257, 665, 565, 281, 312, 5465, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 26, "seek": 5428, "start": 64.16, "end": 66.72, "text": " So yeah, let's get into it.", "tokens": [407, 1338, 11, 718, 311, 483, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 27, "seek": 5428, "start": 66.72, "end": 72.24000000000001, "text": " So today we're just kind of ringing in the new year, reflecting a little bit on this", "tokens": [407, 965, 321, 434, 445, 733, 295, 18423, 294, 264, 777, 1064, 11, 23543, 257, 707, 857, 322, 341], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 28, "seek": 5428, "start": 72.24000000000001, "end": 75.44, "text": " year in Elm and looking forward to the next year.", "tokens": [1064, 294, 2699, 76, 293, 1237, 2128, 281, 264, 958, 1064, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 29, "seek": 5428, "start": 75.44, "end": 77.52000000000001, "text": " So let's start out.", "tokens": [407, 718, 311, 722, 484, 13], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 30, "seek": 5428, "start": 77.52000000000001, "end": 81.0, "text": " What were some projects that you all worked on this year?", "tokens": [708, 645, 512, 4455, 300, 291, 439, 2732, 322, 341, 1064, 30], "temperature": 0.0, "avg_logprob": -0.24144973439618575, "compression_ratio": 1.6785714285714286, "no_speech_prob": 5.502430667547742e-06}, {"id": 31, "seek": 8100, "start": 81.0, "end": 85.0, "text": " Does anybody want to start anything that they shipped or were working on that they're really", "tokens": [4402, 4472, 528, 281, 722, 1340, 300, 436, 25312, 420, 645, 1364, 322, 300, 436, 434, 534], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 32, "seek": 8100, "start": 85.0, "end": 86.0, "text": " excited about?", "tokens": [2919, 466, 30], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 33, "seek": 8100, "start": 86.0, "end": 87.72, "text": " I shipped nothing this year.", "tokens": [286, 25312, 1825, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 34, "seek": 8100, "start": 87.72, "end": 90.92, "text": " It's a year of lack of shipping.", "tokens": [467, 311, 257, 1064, 295, 5011, 295, 14122, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 35, "seek": 8100, "start": 90.92, "end": 93.56, "text": " A year of preparation.", "tokens": [316, 1064, 295, 13081, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 36, "seek": 8100, "start": 93.56, "end": 95.56, "text": " Maybe that's not true.", "tokens": [2704, 300, 311, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 37, "seek": 8100, "start": 95.56, "end": 96.88, "text": " I can't remember when I...", "tokens": [286, 393, 380, 1604, 562, 286, 485], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 38, "seek": 8100, "start": 96.88, "end": 98.52, "text": " Did I release Elmcraft this year?", "tokens": [2589, 286, 4374, 2699, 76, 5611, 341, 1064, 30], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 39, "seek": 8100, "start": 98.52, "end": 99.52, "text": " I might have.", "tokens": [286, 1062, 362, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 40, "seek": 8100, "start": 99.52, "end": 100.52, "text": " I can't even remember.", "tokens": [286, 393, 380, 754, 1604, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 41, "seek": 8100, "start": 100.52, "end": 101.52, "text": " It feels like it.", "tokens": [467, 3417, 411, 309, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 42, "seek": 8100, "start": 101.52, "end": 102.52, "text": " It feels like it's been...", "tokens": [467, 3417, 411, 309, 311, 668, 485], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 43, "seek": 8100, "start": 102.52, "end": 103.52, "text": " No, I did.", "tokens": [883, 11, 286, 630, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 44, "seek": 8100, "start": 103.52, "end": 104.52, "text": " Technically I did.", "tokens": [42494, 286, 630, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 45, "seek": 8100, "start": 104.52, "end": 105.52, "text": " You did announce Elmcraft.", "tokens": [509, 630, 7478, 2699, 76, 5611, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 46, "seek": 8100, "start": 105.52, "end": 106.52, "text": " You absolutely announced Elmcraft this year.", "tokens": [509, 3122, 7548, 2699, 76, 5611, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 47, "seek": 8100, "start": 106.52, "end": 107.52, "text": " Yeah, February 20th.", "tokens": [865, 11, 8711, 945, 392, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 48, "seek": 8100, "start": 107.52, "end": 108.52, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 49, "seek": 8100, "start": 108.52, "end": 110.16, "text": " I released the thing.", "tokens": [286, 4736, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.30659729096947647, "compression_ratio": 1.7793103448275862, "no_speech_prob": 3.08922153635649e-06}, {"id": 50, "seek": 11016, "start": 110.16, "end": 113.2, "text": " For people who don't know, I think you should pitch Elmcraft.", "tokens": [1171, 561, 567, 500, 380, 458, 11, 286, 519, 291, 820, 7293, 2699, 76, 5611, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 51, "seek": 11016, "start": 113.2, "end": 114.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 52, "seek": 11016, "start": 114.2, "end": 115.44, "text": " For people who don't know.", "tokens": [1171, 561, 567, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 53, "seek": 11016, "start": 115.44, "end": 116.72, "text": " Well, you know what?", "tokens": [1042, 11, 291, 458, 437, 30], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 54, "seek": 11016, "start": 116.72, "end": 119.39999999999999, "text": " I now no longer know.", "tokens": [286, 586, 572, 2854, 458, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 55, "seek": 11016, "start": 119.39999999999999, "end": 123.6, "text": " So I'm going to get rid of it.", "tokens": [407, 286, 478, 516, 281, 483, 3973, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 56, "seek": 11016, "start": 123.6, "end": 125.88, "text": " So I'm going to just read the about page from Elmcraft.", "tokens": [407, 286, 478, 516, 281, 445, 1401, 264, 466, 3028, 490, 2699, 76, 5611, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 57, "seek": 11016, "start": 125.88, "end": 130.72, "text": " So yeah, Elmcraft is an unofficial endeavor curated by a bunch of people who love and", "tokens": [407, 1338, 11, 2699, 76, 5611, 307, 364, 8526, 37661, 34975, 47851, 538, 257, 3840, 295, 561, 567, 959, 293], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 58, "seek": 11016, "start": 130.72, "end": 131.72, "text": " use Elm.", "tokens": [764, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 59, "seek": 11016, "start": 131.72, "end": 136.24, "text": " So our goal is for Elmcraft to be the official, unofficial place for all things Elm.", "tokens": [407, 527, 3387, 307, 337, 2699, 76, 5611, 281, 312, 264, 4783, 11, 8526, 37661, 1081, 337, 439, 721, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.30737703763521634, "compression_ratio": 1.7521739130434784, "no_speech_prob": 1.6796955151221482e-06}, {"id": 60, "seek": 13624, "start": 136.24, "end": 140.44, "text": " So yeah, it's pretty much me thinking that or feeling that there was a wealth of kind", "tokens": [407, 1338, 11, 309, 311, 1238, 709, 385, 1953, 300, 420, 2633, 300, 456, 390, 257, 7203, 295, 733], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 61, "seek": 13624, "start": 140.44, "end": 145.0, "text": " of knowledge, projects, initiatives, and people in the Elm community.", "tokens": [295, 3601, 11, 4455, 11, 16194, 11, 293, 561, 294, 264, 2699, 76, 1768, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 62, "seek": 13624, "start": 145.0, "end": 150.92000000000002, "text": " And Elmcraft is kind of like a map to help you find your way amongst all those things.", "tokens": [400, 2699, 76, 5611, 307, 733, 295, 411, 257, 4471, 281, 854, 291, 915, 428, 636, 12918, 439, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 63, "seek": 13624, "start": 150.92000000000002, "end": 153.28, "text": " Is that Elmcraft.com?", "tokens": [1119, 300, 2699, 76, 5611, 13, 1112, 30], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 64, "seek": 13624, "start": 153.28, "end": 158.68, "text": " It's Elmcraft.com, Elmcraft.org, where it'll redirect to, but both domains are there.", "tokens": [467, 311, 2699, 76, 5611, 13, 1112, 11, 2699, 76, 5611, 13, 4646, 11, 689, 309, 603, 29066, 281, 11, 457, 1293, 25514, 366, 456, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 65, "seek": 13624, "start": 158.68, "end": 159.68, "text": " Oh, wow.", "tokens": [876, 11, 6076, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 66, "seek": 13624, "start": 159.68, "end": 160.68, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 67, "seek": 13624, "start": 160.68, "end": 161.68, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 68, "seek": 13624, "start": 161.68, "end": 162.68, "text": " So yeah, that's a thing.", "tokens": [407, 1338, 11, 300, 311, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 69, "seek": 13624, "start": 162.68, "end": 163.68, "text": " That's great.", "tokens": [663, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.2547911432863192, "compression_ratio": 1.6334661354581674, "no_speech_prob": 7.5276611823937856e-06}, {"id": 70, "seek": 16368, "start": 163.68, "end": 168.96, "text": " That's a pretty cool post on the Elmcraft site, like a pretty interactive post, as I", "tokens": [663, 311, 257, 1238, 1627, 2183, 322, 264, 2699, 76, 5611, 3621, 11, 411, 257, 1238, 15141, 2183, 11, 382, 286], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 71, "seek": 16368, "start": 168.96, "end": 169.96, "text": " recall.", "tokens": [9901, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 72, "seek": 16368, "start": 169.96, "end": 170.96, "text": " That is true.", "tokens": [663, 307, 2074, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 73, "seek": 16368, "start": 170.96, "end": 171.96, "text": " Oh yeah.", "tokens": [876, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 74, "seek": 16368, "start": 171.96, "end": 173.52, "text": " Jeroen released on Elmcraft as well.", "tokens": [508, 2032, 268, 4736, 322, 2699, 76, 5611, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 75, "seek": 16368, "start": 173.52, "end": 174.52, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 76, "seek": 16368, "start": 174.52, "end": 175.52, "text": " How could I forget?", "tokens": [1012, 727, 286, 2870, 30], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 77, "seek": 16368, "start": 175.52, "end": 179.48000000000002, "text": " Even I forgot about that.", "tokens": [2754, 286, 5298, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 78, "seek": 16368, "start": 179.48000000000002, "end": 183.16, "text": " Now relabeling this podcast, things we forgot we released in 2020.", "tokens": [823, 1039, 455, 11031, 341, 7367, 11, 721, 321, 5298, 321, 4736, 294, 4808, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 79, "seek": 16368, "start": 183.16, "end": 184.16, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 80, "seek": 16368, "start": 184.16, "end": 186.92000000000002, "text": " We're not very good at self-promotion, are we?", "tokens": [492, 434, 406, 588, 665, 412, 2698, 12, 28722, 19228, 11, 366, 321, 30], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 81, "seek": 16368, "start": 186.92000000000002, "end": 187.92000000000002, "text": " Yeah, right.", "tokens": [865, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 82, "seek": 16368, "start": 187.92000000000002, "end": 192.16, "text": " It's been a very, very long 2020.", "tokens": [467, 311, 668, 257, 588, 11, 588, 938, 4808, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 83, "seek": 16368, "start": 192.16, "end": 193.16, "text": " Something or other.", "tokens": [6595, 420, 661, 13], "temperature": 0.0, "avg_logprob": -0.3553377309239897, "compression_ratio": 1.53515625, "no_speech_prob": 1.482323682466813e-06}, {"id": 84, "seek": 19316, "start": 193.16, "end": 194.16, "text": " Maybe.", "tokens": [2704, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 85, "seek": 19316, "start": 194.16, "end": 199.16, "text": " 2020 and the 2020.2 release.", "tokens": [4808, 293, 264, 4808, 13, 17, 4374, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 86, "seek": 19316, "start": 199.16, "end": 200.16, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 87, "seek": 19316, "start": 200.16, "end": 207.96, "text": " I wrote a blog post on Elmcraft about all the rules in ESLint, which is a lint of JavaScript,", "tokens": [286, 4114, 257, 6968, 2183, 322, 2699, 76, 5611, 466, 439, 264, 4474, 294, 12564, 43, 686, 11, 597, 307, 257, 287, 686, 295, 15778, 11], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 88, "seek": 19316, "start": 207.96, "end": 210.8, "text": " which didn't make sense for Elm.", "tokens": [597, 994, 380, 652, 2020, 337, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 89, "seek": 19316, "start": 210.8, "end": 214.92, "text": " And basically that's like 90% of them don't make sense.", "tokens": [400, 1936, 300, 311, 411, 4289, 4, 295, 552, 500, 380, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 90, "seek": 19316, "start": 214.92, "end": 215.92, "text": " So 87% more.", "tokens": [407, 27990, 4, 544, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 91, "seek": 19316, "start": 215.92, "end": 216.92, "text": " 87%?", "tokens": [27990, 4, 30], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 92, "seek": 19316, "start": 216.92, "end": 217.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 93, "seek": 19316, "start": 217.92, "end": 220.92, "text": " So that's a big, big chunk of Elm.", "tokens": [407, 300, 311, 257, 955, 11, 955, 16635, 295, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 94, "seek": 19316, "start": 220.92, "end": 221.92, "text": " Oh, actually.", "tokens": [876, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.3834494857100753, "compression_ratio": 1.4210526315789473, "no_speech_prob": 2.026065885729622e-06}, {"id": 95, "seek": 22192, "start": 221.92, "end": 224.76, "text": " You were right.", "tokens": [509, 645, 558, 13], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 96, "seek": 22192, "start": 224.76, "end": 229.79999999999998, "text": " It's 87% of all the rules, but if you only narrow it to the recommended ESLint rules,", "tokens": [467, 311, 27990, 4, 295, 439, 264, 4474, 11, 457, 498, 291, 787, 9432, 309, 281, 264, 9628, 12564, 43, 686, 4474, 11], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 97, "seek": 22192, "start": 229.79999999999998, "end": 231.79999999999998, "text": " then it jumps up to 92%.", "tokens": [550, 309, 16704, 493, 281, 28225, 6856], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 98, "seek": 22192, "start": 231.79999999999998, "end": 234.11999999999998, "text": " Not necessary in Elm.", "tokens": [1726, 4818, 294, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 99, "seek": 22192, "start": 234.11999999999998, "end": 237.48, "text": " So yeah, I thought that would be a good pitch for Elm.", "tokens": [407, 1338, 11, 286, 1194, 300, 576, 312, 257, 665, 7293, 337, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 100, "seek": 22192, "start": 237.48, "end": 244.92, "text": " I don't know if it worked, but it's a good statistic to add to Elm talks.", "tokens": [286, 500, 380, 458, 498, 309, 2732, 11, 457, 309, 311, 257, 665, 29588, 281, 909, 281, 2699, 76, 6686, 13], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 101, "seek": 22192, "start": 244.92, "end": 245.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 102, "seek": 22192, "start": 245.92, "end": 251.88, "text": " So I think the idea was for all that kind of stuff to pile up over time on Elmcraft,", "tokens": [407, 286, 519, 264, 1558, 390, 337, 439, 300, 733, 295, 1507, 281, 14375, 493, 670, 565, 322, 2699, 76, 5611, 11], "temperature": 0.0, "avg_logprob": -0.3334405866719909, "compression_ratio": 1.5081967213114753, "no_speech_prob": 1.3496198789653135e-06}, {"id": 103, "seek": 25188, "start": 251.88, "end": 255.35999999999999, "text": " like a lot of the conversations that we kind of end up having in Slack and Discord that", "tokens": [411, 257, 688, 295, 264, 7315, 300, 321, 733, 295, 917, 493, 1419, 294, 37211, 293, 32623, 300], "temperature": 0.0, "avg_logprob": -0.2793339574059775, "compression_ratio": 1.7184466019417475, "no_speech_prob": 2.7965761546511203e-05}, {"id": 104, "seek": 25188, "start": 255.35999999999999, "end": 259.04, "text": " sometimes just seem really, really valuable.", "tokens": [2171, 445, 1643, 534, 11, 534, 8263, 13], "temperature": 0.0, "avg_logprob": -0.2793339574059775, "compression_ratio": 1.7184466019417475, "no_speech_prob": 2.7965761546511203e-05}, {"id": 105, "seek": 25188, "start": 259.04, "end": 264.4, "text": " It became a trope this year because Martin Janicek, I'm not sure if I pronounced his", "tokens": [467, 3062, 257, 4495, 494, 341, 1064, 570, 9184, 4956, 573, 74, 11, 286, 478, 406, 988, 498, 286, 23155, 702], "temperature": 0.0, "avg_logprob": -0.2793339574059775, "compression_ratio": 1.7184466019417475, "no_speech_prob": 2.7965761546511203e-05}, {"id": 106, "seek": 25188, "start": 264.4, "end": 268.52, "text": " surname right, but yeah, I pitched the idea of Elmcraft to him because I noticed he was", "tokens": [50152, 558, 11, 457, 1338, 11, 286, 32994, 264, 1558, 295, 2699, 76, 5611, 281, 796, 570, 286, 5694, 415, 390], "temperature": 0.0, "avg_logprob": -0.2793339574059775, "compression_ratio": 1.7184466019417475, "no_speech_prob": 2.7965761546511203e-05}, {"id": 107, "seek": 25188, "start": 268.52, "end": 272.32, "text": " answering so many questions and then he just kept answering questions.", "tokens": [13430, 370, 867, 1651, 293, 550, 415, 445, 4305, 13430, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2793339574059775, "compression_ratio": 1.7184466019417475, "no_speech_prob": 2.7965761546511203e-05}, {"id": 108, "seek": 25188, "start": 272.32, "end": 276.68, "text": " So I had to just keep DMing him being like, stop answering questions because we can't", "tokens": [407, 286, 632, 281, 445, 1066, 15322, 278, 796, 885, 411, 11, 1590, 13430, 1651, 570, 321, 393, 380], "temperature": 0.0, "avg_logprob": -0.2793339574059775, "compression_ratio": 1.7184466019417475, "no_speech_prob": 2.7965761546511203e-05}, {"id": 109, "seek": 25188, "start": 276.68, "end": 280.92, "text": " write Elmcraft articles as quickly as you're creating new knowledge.", "tokens": [2464, 2699, 76, 5611, 11290, 382, 2661, 382, 291, 434, 4084, 777, 3601, 13], "temperature": 0.0, "avg_logprob": -0.2793339574059775, "compression_ratio": 1.7184466019417475, "no_speech_prob": 2.7965761546511203e-05}, {"id": 110, "seek": 28092, "start": 280.92, "end": 284.32, "text": " But yeah, he also has contributed a few things, so that's been cool.", "tokens": [583, 1338, 11, 415, 611, 575, 18434, 257, 1326, 721, 11, 370, 300, 311, 668, 1627, 13], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 111, "seek": 28092, "start": 284.32, "end": 288.12, "text": " So yeah, next year I think I've got to figure out how to make the visibility of some of", "tokens": [407, 1338, 11, 958, 1064, 286, 519, 286, 600, 658, 281, 2573, 484, 577, 281, 652, 264, 19883, 295, 512, 295], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 112, "seek": 28092, "start": 288.12, "end": 291.48, "text": " those kind of blog posts or knowledge things a bit more evident because right now they're", "tokens": [729, 733, 295, 6968, 12300, 420, 3601, 721, 257, 857, 544, 16371, 570, 558, 586, 436, 434], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 113, "seek": 28092, "start": 291.48, "end": 295.72, "text": " not listed in the index, but they are accumulating slowly.", "tokens": [406, 10052, 294, 264, 8186, 11, 457, 436, 366, 12989, 12162, 5692, 13], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 114, "seek": 28092, "start": 295.72, "end": 302.28000000000003, "text": " So while we're talking about Elmcraft, how do people submit stories or articles or blog", "tokens": [407, 1339, 321, 434, 1417, 466, 2699, 76, 5611, 11, 577, 360, 561, 10315, 3676, 420, 11290, 420, 6968], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 115, "seek": 28092, "start": 302.28000000000003, "end": 303.92, "text": " posts to Elmcraft?", "tokens": [12300, 281, 2699, 76, 5611, 30], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 116, "seek": 28092, "start": 303.92, "end": 308.72, "text": " Yes, I suppose that's another thing that I forgot to announce is that Elmcraft is now", "tokens": [1079, 11, 286, 7297, 300, 311, 1071, 551, 300, 286, 5298, 281, 7478, 307, 300, 2699, 76, 5611, 307, 586], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 117, "seek": 28092, "start": 308.72, "end": 310.04, "text": " open source on GitHub.", "tokens": [1269, 4009, 322, 23331, 13], "temperature": 0.0, "avg_logprob": -0.24059444315293255, "compression_ratio": 1.669871794871795, "no_speech_prob": 1.112417066906346e-05}, {"id": 118, "seek": 31004, "start": 310.04, "end": 312.68, "text": " It wasn't before when I released it, but now it is.", "tokens": [467, 2067, 380, 949, 562, 286, 4736, 309, 11, 457, 586, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 119, "seek": 31004, "start": 312.68, "end": 318.04, "text": " But yeah, I think there's a discuss page on Elmcraft that basically just says, just join", "tokens": [583, 1338, 11, 286, 519, 456, 311, 257, 2248, 3028, 322, 2699, 76, 5611, 300, 1936, 445, 1619, 11, 445, 3917], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 120, "seek": 31004, "start": 318.04, "end": 322.20000000000005, "text": " the Elmcraft discord and come have a chat about what your idea is.", "tokens": [264, 2699, 76, 5611, 32989, 293, 808, 362, 257, 5081, 466, 437, 428, 1558, 307, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 121, "seek": 31004, "start": 322.20000000000005, "end": 326.52000000000004, "text": " But yeah, I suppose you could do the same on the Elmcraft GitHub now.", "tokens": [583, 1338, 11, 286, 7297, 291, 727, 360, 264, 912, 322, 264, 2699, 76, 5611, 23331, 586, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 122, "seek": 31004, "start": 326.52000000000004, "end": 328.6, "text": " So yeah, I should probably definitely announce that.", "tokens": [407, 1338, 11, 286, 820, 1391, 2138, 7478, 300, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 123, "seek": 31004, "start": 328.6, "end": 330.96000000000004, "text": " I'm going to announce that at some point.", "tokens": [286, 478, 516, 281, 7478, 300, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 124, "seek": 31004, "start": 330.96000000000004, "end": 332.24, "text": " I think you just did.", "tokens": [286, 519, 291, 445, 630, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 125, "seek": 31004, "start": 332.24, "end": 334.6, "text": " I think it's an ad.", "tokens": [286, 519, 309, 311, 364, 614, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 126, "seek": 31004, "start": 334.6, "end": 335.6, "text": " The more the merrier.", "tokens": [440, 544, 264, 3551, 7326, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 127, "seek": 31004, "start": 335.6, "end": 338.6, "text": " Yeah, temporarily forgot I'm on a podcast recording.", "tokens": [865, 11, 23750, 5298, 286, 478, 322, 257, 7367, 6613, 13], "temperature": 0.0, "avg_logprob": -0.3009650859426945, "compression_ratio": 1.721830985915493, "no_speech_prob": 1.922238152474165e-05}, {"id": 128, "seek": 33860, "start": 338.6, "end": 347.24, "text": " I still think you should announce it in a few places like discourse, Reddit, Twitter,", "tokens": [286, 920, 519, 291, 820, 7478, 309, 294, 257, 1326, 3190, 411, 23938, 11, 32210, 11, 5794, 11], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 129, "seek": 33860, "start": 347.24, "end": 348.24, "text": " Aston.", "tokens": [316, 8805, 13], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 130, "seek": 33860, "start": 348.24, "end": 349.76000000000005, "text": " Well, now I have to, right?", "tokens": [1042, 11, 586, 286, 362, 281, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 131, "seek": 33860, "start": 349.76000000000005, "end": 350.76000000000005, "text": " Now it's on the podcast.", "tokens": [823, 309, 311, 322, 264, 7367, 13], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 132, "seek": 33860, "start": 350.76000000000005, "end": 353.32000000000005, "text": " The different discords.", "tokens": [440, 819, 2983, 5703, 13], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 133, "seek": 33860, "start": 353.32000000000005, "end": 355.84000000000003, "text": " Which ones did I miss?", "tokens": [3013, 2306, 630, 286, 1713, 30], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 134, "seek": 33860, "start": 355.84000000000003, "end": 360.36, "text": " So many places, lobsters, orange websites.", "tokens": [407, 867, 3190, 11, 450, 9690, 433, 11, 7671, 12891, 13], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 135, "seek": 33860, "start": 360.36, "end": 366.04, "text": " Maybe like Elm News Federation, another project idea for next year.", "tokens": [2704, 411, 2699, 76, 7987, 27237, 11, 1071, 1716, 1558, 337, 958, 1064, 13], "temperature": 0.0, "avg_logprob": -0.3791034824245579, "compression_ratio": 1.389908256880734, "no_speech_prob": 1.9033261651202338e-06}, {"id": 136, "seek": 36604, "start": 366.04, "end": 373.68, "text": " I have to admit, I have not seen these Martin Janicek articles being publicized on Elmcraft.", "tokens": [286, 362, 281, 9796, 11, 286, 362, 406, 1612, 613, 9184, 4956, 573, 74, 11290, 885, 1908, 1602, 322, 2699, 76, 5611, 13], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 137, "seek": 36604, "start": 373.68, "end": 375.8, "text": " I was not aware of them and they sound intriguing.", "tokens": [286, 390, 406, 3650, 295, 552, 293, 436, 1626, 32503, 13], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 138, "seek": 36604, "start": 375.8, "end": 377.14000000000004, "text": " I would really like to take a look.", "tokens": [286, 576, 534, 411, 281, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 139, "seek": 36604, "start": 377.14000000000004, "end": 381.48, "text": " So we will definitely share some links in the show notes.", "tokens": [407, 321, 486, 2138, 2073, 512, 6123, 294, 264, 855, 5570, 13], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 140, "seek": 36604, "start": 381.48, "end": 382.92, "text": " Matt, what about you?", "tokens": [7397, 11, 437, 466, 291, 30], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 141, "seek": 36604, "start": 382.92, "end": 384.32000000000005, "text": " What did you release this year?", "tokens": [708, 630, 291, 4374, 341, 1064, 30], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 142, "seek": 36604, "start": 384.32000000000005, "end": 385.32000000000005, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 143, "seek": 36604, "start": 385.32000000000005, "end": 387.76, "text": " Oh man, two things, I guess.", "tokens": [876, 587, 11, 732, 721, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 144, "seek": 36604, "start": 387.76, "end": 392.36, "text": " So the first one was Elm CodeGen.", "tokens": [407, 264, 700, 472, 390, 2699, 76, 15549, 26647, 13], "temperature": 0.0, "avg_logprob": -0.2621844230441872, "compression_ratio": 1.4937759336099585, "no_speech_prob": 3.8069183005973173e-07}, {"id": 145, "seek": 39236, "start": 392.36, "end": 399.92, "text": " So the idea there was that could we have CodeGen without having like, you know, Elm doesn't", "tokens": [407, 264, 1558, 456, 390, 300, 727, 321, 362, 15549, 26647, 1553, 1419, 411, 11, 291, 458, 11, 2699, 76, 1177, 380], "temperature": 0.0, "avg_logprob": -0.21678408554622106, "compression_ratio": 1.5960784313725491, "no_speech_prob": 5.988841280668566e-07}, {"id": 146, "seek": 39236, "start": 399.92, "end": 401.40000000000003, "text": " have macros, right?", "tokens": [362, 7912, 2635, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21678408554622106, "compression_ratio": 1.5960784313725491, "no_speech_prob": 5.988841280668566e-07}, {"id": 147, "seek": 39236, "start": 401.40000000000003, "end": 406.40000000000003, "text": " And I don't think we want macros, but like we still would like to benefit from some code", "tokens": [400, 286, 500, 380, 519, 321, 528, 7912, 2635, 11, 457, 411, 321, 920, 576, 411, 281, 5121, 490, 512, 3089], "temperature": 0.0, "avg_logprob": -0.21678408554622106, "compression_ratio": 1.5960784313725491, "no_speech_prob": 5.988841280668566e-07}, {"id": 148, "seek": 39236, "start": 406.40000000000003, "end": 407.76, "text": " generation.", "tokens": [5125, 13], "temperature": 0.0, "avg_logprob": -0.21678408554622106, "compression_ratio": 1.5960784313725491, "no_speech_prob": 5.988841280668566e-07}, {"id": 149, "seek": 39236, "start": 407.76, "end": 411.14, "text": " And what does that look like if you don't have like language support?", "tokens": [400, 437, 775, 300, 574, 411, 498, 291, 500, 380, 362, 411, 2856, 1406, 30], "temperature": 0.0, "avg_logprob": -0.21678408554622106, "compression_ratio": 1.5960784313725491, "no_speech_prob": 5.988841280668566e-07}, {"id": 150, "seek": 39236, "start": 411.14, "end": 416.6, "text": " So this is just a library and a little CLI tool to help you run stuff if you want, but", "tokens": [407, 341, 307, 445, 257, 6405, 293, 257, 707, 12855, 40, 2290, 281, 854, 291, 1190, 1507, 498, 291, 528, 11, 457], "temperature": 0.0, "avg_logprob": -0.21678408554622106, "compression_ratio": 1.5960784313725491, "no_speech_prob": 5.988841280668566e-07}, {"id": 151, "seek": 39236, "start": 416.6, "end": 418.90000000000003, "text": " these can be used sort of separately.", "tokens": [613, 393, 312, 1143, 1333, 295, 14759, 13], "temperature": 0.0, "avg_logprob": -0.21678408554622106, "compression_ratio": 1.5960784313725491, "no_speech_prob": 5.988841280668566e-07}, {"id": 152, "seek": 41890, "start": 418.9, "end": 427.28, "text": " And could you write a CodeGen library that was very intuitive and with, you know, that", "tokens": [400, 727, 291, 2464, 257, 15549, 26647, 6405, 300, 390, 588, 21769, 293, 365, 11, 291, 458, 11, 300], "temperature": 0.0, "avg_logprob": -0.22370016465493298, "compression_ratio": 1.7065637065637065, "no_speech_prob": 1.0511180335015524e-06}, {"id": 153, "seek": 41890, "start": 427.28, "end": 432.08, "text": " that was able to be sort of abstracted over that would allow a whole other class of tools", "tokens": [300, 390, 1075, 281, 312, 1333, 295, 12649, 292, 670, 300, 576, 2089, 257, 1379, 661, 1508, 295, 3873], "temperature": 0.0, "avg_logprob": -0.22370016465493298, "compression_ratio": 1.7065637065637065, "no_speech_prob": 1.0511180335015524e-06}, {"id": 154, "seek": 41890, "start": 432.08, "end": 435.62, "text": " for Elm that I sort of saw was kind of missing.", "tokens": [337, 2699, 76, 300, 286, 1333, 295, 1866, 390, 733, 295, 5361, 13], "temperature": 0.0, "avg_logprob": -0.22370016465493298, "compression_ratio": 1.7065637065637065, "no_speech_prob": 1.0511180335015524e-06}, {"id": 155, "seek": 41890, "start": 435.62, "end": 439.0, "text": " Or if they were implemented like CodeGen tools, it was sort of like everybody had their own", "tokens": [1610, 498, 436, 645, 12270, 411, 15549, 26647, 3873, 11, 309, 390, 1333, 295, 411, 2201, 632, 641, 1065], "temperature": 0.0, "avg_logprob": -0.22370016465493298, "compression_ratio": 1.7065637065637065, "no_speech_prob": 1.0511180335015524e-06}, {"id": 156, "seek": 41890, "start": 439.0, "end": 443.9, "text": " system and it was kind of harder.", "tokens": [1185, 293, 309, 390, 733, 295, 6081, 13], "temperature": 0.0, "avg_logprob": -0.22370016465493298, "compression_ratio": 1.7065637065637065, "no_speech_prob": 1.0511180335015524e-06}, {"id": 157, "seek": 41890, "start": 443.9, "end": 448.15999999999997, "text": " That ultimately, like the reason why I did that project was motivated because I had another", "tokens": [663, 6284, 11, 411, 264, 1778, 983, 286, 630, 300, 1716, 390, 14515, 570, 286, 632, 1071], "temperature": 0.0, "avg_logprob": -0.22370016465493298, "compression_ratio": 1.7065637065637065, "no_speech_prob": 1.0511180335015524e-06}, {"id": 158, "seek": 44816, "start": 448.16, "end": 450.84000000000003, "text": " project which is Elm GQL.", "tokens": [1716, 597, 307, 2699, 76, 460, 13695, 13], "temperature": 0.0, "avg_logprob": -0.2545779301570012, "compression_ratio": 1.6724137931034482, "no_speech_prob": 6.643265351158334e-06}, {"id": 159, "seek": 44816, "start": 450.84000000000003, "end": 458.32000000000005, "text": " And what Elm GQL was is we definitely wanted, we at Vendor, which is we use at Vendor just", "tokens": [400, 437, 2699, 76, 460, 13695, 390, 307, 321, 2138, 1415, 11, 321, 412, 691, 521, 284, 11, 597, 307, 321, 764, 412, 691, 521, 284, 445], "temperature": 0.0, "avg_logprob": -0.2545779301570012, "compression_ratio": 1.6724137931034482, "no_speech_prob": 6.643265351158334e-06}, {"id": 160, "seek": 44816, "start": 458.32000000000005, "end": 463.44000000000005, "text": " to pitch it a little bit, we have 600,000 lines of Elm code.", "tokens": [281, 7293, 309, 257, 707, 857, 11, 321, 362, 11849, 11, 1360, 3876, 295, 2699, 76, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2545779301570012, "compression_ratio": 1.6724137931034482, "no_speech_prob": 6.643265351158334e-06}, {"id": 161, "seek": 44816, "start": 463.44000000000005, "end": 468.08000000000004, "text": " And so the entire front end is Elm and we use a lot of GraphQL.", "tokens": [400, 370, 264, 2302, 1868, 917, 307, 2699, 76, 293, 321, 764, 257, 688, 295, 21884, 13695, 13], "temperature": 0.0, "avg_logprob": -0.2545779301570012, "compression_ratio": 1.6724137931034482, "no_speech_prob": 6.643265351158334e-06}, {"id": 162, "seek": 44816, "start": 468.08000000000004, "end": 472.48, "text": " And we were looking at tools to work with GraphQL.", "tokens": [400, 321, 645, 1237, 412, 3873, 281, 589, 365, 21884, 13695, 13], "temperature": 0.0, "avg_logprob": -0.2545779301570012, "compression_ratio": 1.6724137931034482, "no_speech_prob": 6.643265351158334e-06}, {"id": 163, "seek": 44816, "start": 472.48, "end": 476.96000000000004, "text": " And one of the biggest challenging things with writing a tool like that is the code", "tokens": [400, 472, 295, 264, 3880, 7595, 721, 365, 3579, 257, 2290, 411, 300, 307, 264, 3089], "temperature": 0.0, "avg_logprob": -0.2545779301570012, "compression_ratio": 1.6724137931034482, "no_speech_prob": 6.643265351158334e-06}, {"id": 164, "seek": 44816, "start": 476.96000000000004, "end": 478.04, "text": " generation.", "tokens": [5125, 13], "temperature": 0.0, "avg_logprob": -0.2545779301570012, "compression_ratio": 1.6724137931034482, "no_speech_prob": 6.643265351158334e-06}, {"id": 165, "seek": 47804, "start": 478.04, "end": 483.64000000000004, "text": " So if you try to do just some sort of string template thing, you'll quickly like kind of", "tokens": [407, 498, 291, 853, 281, 360, 445, 512, 1333, 295, 6798, 12379, 551, 11, 291, 603, 2661, 411, 733, 295], "temperature": 0.0, "avg_logprob": -0.2240492794491829, "compression_ratio": 1.5708812260536398, "no_speech_prob": 1.577897137394757e-06}, {"id": 166, "seek": 47804, "start": 483.64000000000004, "end": 486.08000000000004, "text": " run into some challenges.", "tokens": [1190, 666, 512, 4759, 13], "temperature": 0.0, "avg_logprob": -0.2240492794491829, "compression_ratio": 1.5708812260536398, "no_speech_prob": 1.577897137394757e-06}, {"id": 167, "seek": 47804, "start": 486.08000000000004, "end": 492.08000000000004, "text": " Like I know Dillon is probably intimately familiar with all of these challenges because he also", "tokens": [1743, 286, 458, 28160, 307, 1391, 560, 5401, 4963, 365, 439, 295, 613, 4759, 570, 415, 611], "temperature": 0.0, "avg_logprob": -0.2240492794491829, "compression_ratio": 1.5708812260536398, "no_speech_prob": 1.577897137394757e-06}, {"id": 168, "seek": 47804, "start": 492.08000000000004, "end": 494.76, "text": " has a GraphQL tool.", "tokens": [575, 257, 21884, 13695, 2290, 13], "temperature": 0.0, "avg_logprob": -0.2240492794491829, "compression_ratio": 1.5708812260536398, "no_speech_prob": 1.577897137394757e-06}, {"id": 169, "seek": 47804, "start": 494.76, "end": 500.44, "text": " And so yeah, so I wanted the Elm GQL to be out there and to pitch Elm GQL a little bit.", "tokens": [400, 370, 1338, 11, 370, 286, 1415, 264, 2699, 76, 460, 13695, 281, 312, 484, 456, 293, 281, 7293, 2699, 76, 460, 13695, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.2240492794491829, "compression_ratio": 1.5708812260536398, "no_speech_prob": 1.577897137394757e-06}, {"id": 170, "seek": 47804, "start": 500.44, "end": 506.76, "text": " It's a takes your GraphQL queries or mutations, parses them, checks them against your schema", "tokens": [467, 311, 257, 2516, 428, 21884, 13695, 24109, 420, 29243, 11, 21156, 279, 552, 11, 13834, 552, 1970, 428, 34078], "temperature": 0.0, "avg_logprob": -0.2240492794491829, "compression_ratio": 1.5708812260536398, "no_speech_prob": 1.577897137394757e-06}, {"id": 171, "seek": 50676, "start": 506.76, "end": 513.12, "text": " and generates some nice concise Elm code for you to use them or compose them together or", "tokens": [293, 23815, 512, 1481, 44882, 2699, 76, 3089, 337, 291, 281, 764, 552, 420, 35925, 552, 1214, 420], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 172, "seek": 50676, "start": 513.12, "end": 514.8, "text": " whatever.", "tokens": [2035, 13], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 173, "seek": 50676, "start": 514.8, "end": 520.96, "text": " And so yeah, those were the two main projects that were actually released.", "tokens": [400, 370, 1338, 11, 729, 645, 264, 732, 2135, 4455, 300, 645, 767, 4736, 13], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 174, "seek": 50676, "start": 520.96, "end": 521.96, "text": " They are out there.", "tokens": [814, 366, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 175, "seek": 50676, "start": 521.96, "end": 523.3199999999999, "text": " You can use them now.", "tokens": [509, 393, 764, 552, 586, 13], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 176, "seek": 50676, "start": 523.3199999999999, "end": 524.48, "text": " Go check them out.", "tokens": [1037, 1520, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 177, "seek": 50676, "start": 524.48, "end": 530.48, "text": " And the other thing which unifies the two kind of is I gave a talk at Strange Loop called", "tokens": [400, 264, 661, 551, 597, 517, 11221, 264, 732, 733, 295, 307, 286, 2729, 257, 751, 412, 29068, 45660, 1219], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 178, "seek": 50676, "start": 530.48, "end": 532.04, "text": " Code Gen with Types.", "tokens": [15549, 3632, 365, 5569, 5190, 13], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 179, "seek": 50676, "start": 532.04, "end": 536.48, "text": " And it was not only about the Code Gen tool, but it was about also how, what other things", "tokens": [400, 309, 390, 406, 787, 466, 264, 15549, 3632, 2290, 11, 457, 309, 390, 466, 611, 577, 11, 437, 661, 721], "temperature": 0.0, "avg_logprob": -0.2783088360802602, "compression_ratio": 1.6730769230769231, "no_speech_prob": 9.276058108298457e-07}, {"id": 180, "seek": 53648, "start": 536.48, "end": 537.96, "text": " you could do with it.", "tokens": [291, 727, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 181, "seek": 53648, "start": 537.96, "end": 538.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 182, "seek": 53648, "start": 538.96, "end": 540.9200000000001, "text": " That's my, that's my summary for the year.", "tokens": [663, 311, 452, 11, 300, 311, 452, 12691, 337, 264, 1064, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 183, "seek": 53648, "start": 540.9200000000001, "end": 542.48, "text": " Actually you're really good about it.", "tokens": [5135, 291, 434, 534, 665, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 184, "seek": 53648, "start": 542.48, "end": 543.48, "text": " I'm really happy.", "tokens": [286, 478, 534, 2055, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 185, "seek": 53648, "start": 543.48, "end": 545.48, "text": " It's been a big year for you, Matt.", "tokens": [467, 311, 668, 257, 955, 1064, 337, 291, 11, 7397, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 186, "seek": 53648, "start": 545.48, "end": 546.48, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 187, "seek": 53648, "start": 546.48, "end": 551.8000000000001, "text": " And I will also note I've been since, since we recorded our Elm Code Gen episode, I've", "tokens": [400, 286, 486, 611, 3637, 286, 600, 668, 1670, 11, 1670, 321, 8287, 527, 2699, 76, 15549, 3632, 3500, 11, 286, 600], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 188, "seek": 53648, "start": 551.8000000000001, "end": 554.96, "text": " been using it quite a bit and it's been a joy.", "tokens": [668, 1228, 309, 1596, 257, 857, 293, 309, 311, 668, 257, 6258, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 189, "seek": 53648, "start": 554.96, "end": 562.84, "text": " And I'm currently generating quite a lot of Elm pages V3 code in the V3 beta using Elm", "tokens": [400, 286, 478, 4362, 17746, 1596, 257, 688, 295, 2699, 76, 7183, 691, 18, 3089, 294, 264, 691, 18, 9861, 1228, 2699, 76], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 190, "seek": 53648, "start": 562.84, "end": 563.84, "text": " Code Gen.", "tokens": [15549, 3632, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 191, "seek": 53648, "start": 563.84, "end": 564.84, "text": " And it's, it's awesome.", "tokens": [400, 309, 311, 11, 309, 311, 3476, 13], "temperature": 0.0, "avg_logprob": -0.2795405827515514, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.737801475585002e-07}, {"id": 192, "seek": 56484, "start": 564.84, "end": 570.2, "text": " It's a, it's really better than Spring Template it in TypeScript was.", "tokens": [467, 311, 257, 11, 309, 311, 534, 1101, 813, 14013, 39563, 473, 309, 294, 15576, 14237, 390, 13], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 193, "seek": 56484, "start": 570.2, "end": 571.2, "text": " So thank you.", "tokens": [407, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 194, "seek": 56484, "start": 571.2, "end": 572.2, "text": " Well, thank you.", "tokens": [1042, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 195, "seek": 56484, "start": 572.2, "end": 574.96, "text": " I'm, I'm, I'm glad there've been a number.", "tokens": [286, 478, 11, 286, 478, 11, 286, 478, 5404, 456, 600, 668, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 196, "seek": 56484, "start": 574.96, "end": 579.4, "text": " My hope with Elm Code Gen was that, you know, it's a tool for tool makers, right?", "tokens": [1222, 1454, 365, 2699, 76, 15549, 3632, 390, 300, 11, 291, 458, 11, 309, 311, 257, 2290, 337, 2290, 19323, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 197, "seek": 56484, "start": 579.4, "end": 584.96, "text": " So the hope is that there would be sort of a, a small set of very useful tools that would", "tokens": [407, 264, 1454, 307, 300, 456, 576, 312, 1333, 295, 257, 11, 257, 1359, 992, 295, 588, 4420, 3873, 300, 576], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 198, "seek": 56484, "start": 584.96, "end": 586.24, "text": " kind of leverage it.", "tokens": [733, 295, 13982, 309, 13], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 199, "seek": 56484, "start": 586.24, "end": 587.6800000000001, "text": " So I could see Elm pages using that.", "tokens": [407, 286, 727, 536, 2699, 76, 7183, 1228, 300, 13], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 200, "seek": 56484, "start": 587.6800000000001, "end": 593.52, "text": " I could see something like Ryan Haskell-Glatz's Elmland potentially using it, obviously Elm", "tokens": [286, 727, 536, 746, 411, 9116, 8646, 43723, 12, 38, 75, 10300, 311, 2699, 76, 1661, 7263, 1228, 309, 11, 2745, 2699, 76], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 201, "seek": 56484, "start": 593.52, "end": 594.52, "text": " GQL.", "tokens": [460, 13695, 13], "temperature": 0.0, "avg_logprob": -0.268763292555841, "compression_ratio": 1.6319444444444444, "no_speech_prob": 9.131549632002134e-07}, {"id": 202, "seek": 59452, "start": 594.52, "end": 598.16, "text": " And honestly, projects that I'm hoping to do this coming year.", "tokens": [400, 6095, 11, 4455, 300, 286, 478, 7159, 281, 360, 341, 1348, 1064, 13], "temperature": 0.0, "avg_logprob": -0.21318715991395892, "compression_ratio": 1.722972972972973, "no_speech_prob": 8.714063710613118e-07}, {"id": 203, "seek": 59452, "start": 598.16, "end": 600.88, "text": " And this is why I was like, okay, I'm going to take time to do this Code Gen thing is", "tokens": [400, 341, 307, 983, 286, 390, 411, 11, 1392, 11, 286, 478, 516, 281, 747, 565, 281, 360, 341, 15549, 3632, 551, 307], "temperature": 0.0, "avg_logprob": -0.21318715991395892, "compression_ratio": 1.722972972972973, "no_speech_prob": 8.714063710613118e-07}, {"id": 204, "seek": 59452, "start": 600.88, "end": 605.6999999999999, "text": " because I'm like, there are a lot of things that I think I would like to generate specifically", "tokens": [570, 286, 478, 411, 11, 456, 366, 257, 688, 295, 721, 300, 286, 519, 286, 576, 411, 281, 8460, 4682], "temperature": 0.0, "avg_logprob": -0.21318715991395892, "compression_ratio": 1.722972972972973, "no_speech_prob": 8.714063710613118e-07}, {"id": 205, "seek": 59452, "start": 605.6999999999999, "end": 612.6, "text": " regarding like Elm UI stuff, like layout, CSS, whatever that I think would be very powerful.", "tokens": [8595, 411, 2699, 76, 15682, 1507, 11, 411, 13333, 11, 24387, 11, 2035, 300, 286, 519, 576, 312, 588, 4005, 13], "temperature": 0.0, "avg_logprob": -0.21318715991395892, "compression_ratio": 1.722972972972973, "no_speech_prob": 8.714063710613118e-07}, {"id": 206, "seek": 59452, "start": 612.6, "end": 616.28, "text": " And it's cool to see people sort of pop up and write their own little tool in Elm Code", "tokens": [400, 309, 311, 1627, 281, 536, 561, 1333, 295, 1665, 493, 293, 2464, 641, 1065, 707, 2290, 294, 2699, 76, 15549], "temperature": 0.0, "avg_logprob": -0.21318715991395892, "compression_ratio": 1.722972972972973, "no_speech_prob": 8.714063710613118e-07}, {"id": 207, "seek": 59452, "start": 616.28, "end": 619.3199999999999, "text": " Gen and generally have a pretty good experience.", "tokens": [3632, 293, 5101, 362, 257, 1238, 665, 1752, 13], "temperature": 0.0, "avg_logprob": -0.21318715991395892, "compression_ratio": 1.722972972972973, "no_speech_prob": 8.714063710613118e-07}, {"id": 208, "seek": 59452, "start": 619.3199999999999, "end": 621.5, "text": " Code generation is hard as a concept.", "tokens": [15549, 5125, 307, 1152, 382, 257, 3410, 13], "temperature": 0.0, "avg_logprob": -0.21318715991395892, "compression_ratio": 1.722972972972973, "no_speech_prob": 8.714063710613118e-07}, {"id": 209, "seek": 62150, "start": 621.5, "end": 626.2, "text": " Like it's like just going into arbitrary code generation is challenging.", "tokens": [1743, 309, 311, 411, 445, 516, 666, 23211, 3089, 5125, 307, 7595, 13], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 210, "seek": 62150, "start": 626.2, "end": 629.8, "text": " So seeing people jump in and, oh yeah, I wrote this little tool to generate this thing and", "tokens": [407, 2577, 561, 3012, 294, 293, 11, 1954, 1338, 11, 286, 4114, 341, 707, 2290, 281, 8460, 341, 551, 293], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 211, "seek": 62150, "start": 629.8, "end": 630.8, "text": " it took me 20 minutes.", "tokens": [309, 1890, 385, 945, 2077, 13], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 212, "seek": 62150, "start": 630.8, "end": 632.8, "text": " I'm like, oh my God, that's amazing.", "tokens": [286, 478, 411, 11, 1954, 452, 1265, 11, 300, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 213, "seek": 62150, "start": 632.8, "end": 634.76, "text": " Yeah, it really is.", "tokens": [865, 11, 309, 534, 307, 13], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 214, "seek": 62150, "start": 634.76, "end": 635.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 215, "seek": 62150, "start": 635.76, "end": 639.84, "text": " Having the right like set of primitives for, for generating code makes a huge difference.", "tokens": [10222, 264, 558, 411, 992, 295, 2886, 38970, 337, 11, 337, 17746, 3089, 1669, 257, 2603, 2649, 13], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 216, "seek": 62150, "start": 639.84, "end": 643.2, "text": " And it feels like so much more high level with Elm Code Gen.", "tokens": [400, 309, 3417, 411, 370, 709, 544, 1090, 1496, 365, 2699, 76, 15549, 3632, 13], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 217, "seek": 62150, "start": 643.2, "end": 648.52, "text": " Yeah, I've, yeah, I agree, but I can't like my opinion only kind of matters, right?", "tokens": [865, 11, 286, 600, 11, 1338, 11, 286, 3986, 11, 457, 286, 393, 380, 411, 452, 4800, 787, 733, 295, 7001, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.28748655319213867, "compression_ratio": 1.6241610738255035, "no_speech_prob": 5.539151857192337e-07}, {"id": 218, "seek": 64852, "start": 648.52, "end": 655.96, "text": " Yeah, I'm really excited to see it used in Elm Review rules as well.", "tokens": [865, 11, 286, 478, 534, 2919, 281, 536, 309, 1143, 294, 2699, 76, 19954, 4474, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.3531963420364092, "compression_ratio": 1.6200873362445414, "no_speech_prob": 3.632626999205968e-07}, {"id": 219, "seek": 64852, "start": 655.96, "end": 656.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3531963420364092, "compression_ratio": 1.6200873362445414, "no_speech_prob": 3.632626999205968e-07}, {"id": 220, "seek": 64852, "start": 656.96, "end": 658.64, "text": " I'm really curious to see how that looks.", "tokens": [286, 478, 534, 6369, 281, 536, 577, 300, 1542, 13], "temperature": 0.0, "avg_logprob": -0.3531963420364092, "compression_ratio": 1.6200873362445414, "no_speech_prob": 3.632626999205968e-07}, {"id": 221, "seek": 64852, "start": 658.64, "end": 660.16, "text": " That should be really cool.", "tokens": [663, 820, 312, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.3531963420364092, "compression_ratio": 1.6200873362445414, "no_speech_prob": 3.632626999205968e-07}, {"id": 222, "seek": 64852, "start": 660.16, "end": 664.0799999999999, "text": " Because you can do this review code, Jay, or is it, is there?", "tokens": [1436, 291, 393, 360, 341, 3131, 3089, 11, 11146, 11, 420, 307, 309, 11, 307, 456, 30], "temperature": 0.0, "avg_logprob": -0.3531963420364092, "compression_ratio": 1.6200873362445414, "no_speech_prob": 3.632626999205968e-07}, {"id": 223, "seek": 64852, "start": 664.0799999999999, "end": 669.5799999999999, "text": " So in Elm Review, you can have rules report errors and those errors provide a fix.", "tokens": [407, 294, 2699, 76, 19954, 11, 291, 393, 362, 4474, 2275, 13603, 293, 729, 13603, 2893, 257, 3191, 13], "temperature": 0.0, "avg_logprob": -0.3531963420364092, "compression_ratio": 1.6200873362445414, "no_speech_prob": 3.632626999205968e-07}, {"id": 224, "seek": 64852, "start": 669.5799999999999, "end": 676.0799999999999, "text": " And that fix is basically just a string, like insert a string, remove some texts.", "tokens": [400, 300, 3191, 307, 1936, 445, 257, 6798, 11, 411, 8969, 257, 6798, 11, 4159, 512, 15765, 13], "temperature": 0.0, "avg_logprob": -0.3531963420364092, "compression_ratio": 1.6200873362445414, "no_speech_prob": 3.632626999205968e-07}, {"id": 225, "seek": 67608, "start": 676.08, "end": 680.48, "text": " And as long as you can transform some Elm code to a string, which you can do with Elm", "tokens": [400, 382, 938, 382, 291, 393, 4088, 512, 2699, 76, 3089, 281, 257, 6798, 11, 597, 291, 393, 360, 365, 2699, 76], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 226, "seek": 67608, "start": 680.48, "end": 681.48, "text": " Code Gen, right?", "tokens": [15549, 3632, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 227, "seek": 67608, "start": 681.48, "end": 682.48, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 228, "seek": 67608, "start": 682.48, "end": 683.72, "text": " Because it's just a library.", "tokens": [1436, 309, 311, 445, 257, 6405, 13], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 229, "seek": 67608, "start": 683.72, "end": 685.88, "text": " You can use Elm Code Gen in Elm Review.", "tokens": [509, 393, 764, 2699, 76, 15549, 3632, 294, 2699, 76, 19954, 13], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 230, "seek": 67608, "start": 685.88, "end": 686.88, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 231, "seek": 67608, "start": 686.88, "end": 687.88, "text": " Very cool.", "tokens": [4372, 1627, 13], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 232, "seek": 67608, "start": 687.88, "end": 688.88, "text": " That's, that's great.", "tokens": [663, 311, 11, 300, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 233, "seek": 67608, "start": 688.88, "end": 697.08, "text": " And speaking of vendor, VENDR, the company, not the technique of vendoring packages, our", "tokens": [400, 4124, 295, 24321, 11, 691, 18910, 49, 11, 264, 2237, 11, 406, 264, 6532, 295, 371, 521, 3662, 17401, 11, 527], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 234, "seek": 67608, "start": 697.08, "end": 703.72, "text": " Elm at a billion dollar company, Elm Radio episode is like by far our most popular Elm", "tokens": [2699, 76, 412, 257, 5218, 7241, 2237, 11, 2699, 76, 17296, 3500, 307, 411, 538, 1400, 527, 881, 3743, 2699, 76], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 235, "seek": 67608, "start": 703.72, "end": 705.1600000000001, "text": " Radio episode we've done.", "tokens": [17296, 3500, 321, 600, 1096, 13], "temperature": 0.0, "avg_logprob": -0.25050726869052514, "compression_ratio": 1.6076923076923078, "no_speech_prob": 9.132498348662921e-07}, {"id": 236, "seek": 70516, "start": 705.16, "end": 712.52, "text": " So thanks again to Aaron White for being an amazing guest and for VENDR for having like", "tokens": [407, 3231, 797, 281, 14018, 5552, 337, 885, 364, 2243, 8341, 293, 337, 691, 18910, 49, 337, 1419, 411], "temperature": 0.0, "avg_logprob": -0.2662880502898118, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.4823453966528177e-06}, {"id": 237, "seek": 70516, "start": 712.52, "end": 720.0, "text": " an awesome story for, you know, social proof for using Elm in an awesome way and building", "tokens": [364, 3476, 1657, 337, 11, 291, 458, 11, 2093, 8177, 337, 1228, 2699, 76, 294, 364, 3476, 636, 293, 2390], "temperature": 0.0, "avg_logprob": -0.2662880502898118, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.4823453966528177e-06}, {"id": 238, "seek": 70516, "start": 720.0, "end": 721.52, "text": " a successful company off of it.", "tokens": [257, 4406, 2237, 766, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.2662880502898118, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.4823453966528177e-06}, {"id": 239, "seek": 70516, "start": 721.52, "end": 722.52, "text": " It's been amazing.", "tokens": [467, 311, 668, 2243, 13], "temperature": 0.0, "avg_logprob": -0.2662880502898118, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.4823453966528177e-06}, {"id": 240, "seek": 70516, "start": 722.52, "end": 728.64, "text": " It should be noted too, the Elm GQL library that I was able to write is like vendor inc", "tokens": [467, 820, 312, 12964, 886, 11, 264, 2699, 76, 460, 13695, 6405, 300, 286, 390, 1075, 281, 2464, 307, 411, 24321, 834], "temperature": 0.0, "avg_logprob": -0.2662880502898118, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.4823453966528177e-06}, {"id": 241, "seek": 70516, "start": 728.64, "end": 729.64, "text": " slash Elm GQL.", "tokens": [17330, 2699, 76, 460, 13695, 13], "temperature": 0.0, "avg_logprob": -0.2662880502898118, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.4823453966528177e-06}, {"id": 242, "seek": 70516, "start": 729.64, "end": 735.0, "text": " I did it basically entirely at VENDR and the Code Gen library as well, though it's under", "tokens": [286, 630, 309, 1936, 7696, 412, 691, 18910, 49, 293, 264, 15549, 3632, 6405, 382, 731, 11, 1673, 309, 311, 833], "temperature": 0.0, "avg_logprob": -0.2662880502898118, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.4823453966528177e-06}, {"id": 243, "seek": 73500, "start": 735.0, "end": 736.0, "text": " my name.", "tokens": [452, 1315, 13], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 244, "seek": 73500, "start": 736.0, "end": 741.4, "text": " So like the fact that they are, you know, pushing more resources to like give back to", "tokens": [407, 411, 264, 1186, 300, 436, 366, 11, 291, 458, 11, 7380, 544, 3593, 281, 411, 976, 646, 281], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 245, "seek": 73500, "start": 741.4, "end": 745.64, "text": " the community and stuff is, is one of the reasons why I'm very excited about working", "tokens": [264, 1768, 293, 1507, 307, 11, 307, 472, 295, 264, 4112, 983, 286, 478, 588, 2919, 466, 1364], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 246, "seek": 73500, "start": 745.64, "end": 747.04, "text": " at VENDR just in general.", "tokens": [412, 691, 18910, 49, 445, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 247, "seek": 73500, "start": 747.04, "end": 748.04, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 248, "seek": 73500, "start": 748.04, "end": 749.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 249, "seek": 73500, "start": 749.04, "end": 752.6, "text": " I think there's a lot of opportunity and I'm hoping that there'll be some more, some more", "tokens": [286, 519, 456, 311, 257, 688, 295, 2650, 293, 286, 478, 7159, 300, 456, 603, 312, 512, 544, 11, 512, 544], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 250, "seek": 73500, "start": 752.6, "end": 754.28, "text": " goodies in the future too.", "tokens": [44072, 294, 264, 2027, 886, 13], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 251, "seek": 73500, "start": 754.28, "end": 756.46, "text": " So yeah, that's huge.", "tokens": [407, 1338, 11, 300, 311, 2603, 13], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 252, "seek": 73500, "start": 756.46, "end": 761.72, "text": " And so for people who want to know more about Elm GQL and Elm Code Gen, we also made a Elm", "tokens": [400, 370, 337, 561, 567, 528, 281, 458, 544, 466, 2699, 76, 460, 13695, 293, 2699, 76, 15549, 3632, 11, 321, 611, 1027, 257, 2699, 76], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 253, "seek": 73500, "start": 761.72, "end": 764.56, "text": " Radio podcasts episodes with you, Matt.", "tokens": [17296, 24045, 9313, 365, 291, 11, 7397, 13], "temperature": 0.0, "avg_logprob": -0.2499499585893419, "compression_ratio": 1.5792880258899675, "no_speech_prob": 1.0844991038538865e-06}, {"id": 254, "seek": 76456, "start": 764.56, "end": 765.56, "text": " That's right.", "tokens": [663, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 255, "seek": 76456, "start": 765.56, "end": 766.56, "text": " I was there.", "tokens": [286, 390, 456, 13], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 256, "seek": 76456, "start": 766.56, "end": 772.68, "text": " And they were all like, in the span of five episodes, we did three about things that were", "tokens": [400, 436, 645, 439, 411, 11, 294, 264, 16174, 295, 1732, 9313, 11, 321, 630, 1045, 466, 721, 300, 645], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 257, "seek": 76456, "start": 772.68, "end": 777.88, "text": " done at VENDR, including the VENDR specific episode.", "tokens": [1096, 412, 691, 18910, 49, 11, 3009, 264, 691, 18910, 49, 2685, 3500, 13], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 258, "seek": 76456, "start": 777.88, "end": 780.16, "text": " So I wouldn't say it's a busy year.", "tokens": [407, 286, 2759, 380, 584, 309, 311, 257, 5856, 1064, 13], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 259, "seek": 76456, "start": 780.16, "end": 782.9599999999999, "text": " It was almost like a busy month for you.", "tokens": [467, 390, 1920, 411, 257, 5856, 1618, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 260, "seek": 76456, "start": 782.9599999999999, "end": 785.76, "text": " Two months.", "tokens": [4453, 2493, 13], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 261, "seek": 76456, "start": 785.76, "end": 787.2399999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 262, "seek": 76456, "start": 787.2399999999999, "end": 793.8399999999999, "text": " At least in terms of release cadences, because I'm guessing that the work has been lasting", "tokens": [1711, 1935, 294, 2115, 295, 4374, 12209, 2667, 11, 570, 286, 478, 17939, 300, 264, 589, 575, 668, 20714], "temperature": 0.0, "avg_logprob": -0.3488551344826957, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.1189169981662417e-06}, {"id": 263, "seek": 79384, "start": 793.84, "end": 795.4, "text": " longer than that.", "tokens": [2854, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 264, "seek": 79384, "start": 795.4, "end": 796.4, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 265, "seek": 79384, "start": 796.4, "end": 797.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 266, "seek": 79384, "start": 797.4, "end": 798.4, "text": " There's still a lot of stuff that I want to get out.", "tokens": [821, 311, 920, 257, 688, 295, 1507, 300, 286, 528, 281, 483, 484, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 267, "seek": 79384, "start": 798.4, "end": 802.6, "text": " There are a lot of projects that I'm trying to get into shape where I can ship them.", "tokens": [821, 366, 257, 688, 295, 4455, 300, 286, 478, 1382, 281, 483, 666, 3909, 689, 286, 393, 5374, 552, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 268, "seek": 79384, "start": 802.6, "end": 806.12, "text": " That's actually why Elm GQL and Elm Code Gen was, it was such a relief just because it's", "tokens": [663, 311, 767, 983, 2699, 76, 460, 13695, 293, 2699, 76, 15549, 3632, 390, 11, 309, 390, 1270, 257, 10915, 445, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 269, "seek": 79384, "start": 806.12, "end": 810.24, "text": " like, there's a lot of other stuff that I'm very excited to work on, but I can't do until", "tokens": [411, 11, 456, 311, 257, 688, 295, 661, 1507, 300, 286, 478, 588, 2919, 281, 589, 322, 11, 457, 286, 393, 380, 360, 1826], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 270, "seek": 79384, "start": 810.24, "end": 812.48, "text": " those are shipped and using them.", "tokens": [729, 366, 25312, 293, 1228, 552, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 271, "seek": 79384, "start": 812.48, "end": 818.0400000000001, "text": " And so I'm very relieved those projects are out and generally just work for people.", "tokens": [400, 370, 286, 478, 588, 27972, 729, 4455, 366, 484, 293, 5101, 445, 589, 337, 561, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 272, "seek": 79384, "start": 818.0400000000001, "end": 819.36, "text": " There's some improvements we want to bring.", "tokens": [821, 311, 512, 13797, 321, 528, 281, 1565, 13], "temperature": 0.0, "avg_logprob": -0.21405088490453258, "compression_ratio": 1.7372013651877134, "no_speech_prob": 1.2878027746410226e-06}, {"id": 273, "seek": 81936, "start": 819.36, "end": 826.36, "text": " I know Dillon and I spoke actually about some cool Code Gen stuff that he ran into to improve", "tokens": [286, 458, 28160, 293, 286, 7179, 767, 466, 512, 1627, 15549, 3632, 1507, 300, 415, 5872, 666, 281, 3470], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 274, "seek": 81936, "start": 826.36, "end": 827.36, "text": " the library.", "tokens": [264, 6405, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 275, "seek": 81936, "start": 827.36, "end": 830.84, "text": " And I'm excited to bring that out there, but it seems like these projects are out there.", "tokens": [400, 286, 478, 2919, 281, 1565, 300, 484, 456, 11, 457, 309, 2544, 411, 613, 4455, 366, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 276, "seek": 81936, "start": 830.84, "end": 833.12, "text": " People are using them and it's like, yeah, they work.", "tokens": [3432, 366, 1228, 552, 293, 309, 311, 411, 11, 1338, 11, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 277, "seek": 81936, "start": 833.12, "end": 834.12, "text": " Yeah, they work.", "tokens": [865, 11, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 278, "seek": 81936, "start": 834.12, "end": 835.12, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 279, "seek": 81936, "start": 835.12, "end": 838.0, "text": " And so I'm like, okay, cool.", "tokens": [400, 370, 286, 478, 411, 11, 1392, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 280, "seek": 81936, "start": 838.0, "end": 841.96, "text": " They're in maintenance mode or at least like back burner a little bit where I can jump", "tokens": [814, 434, 294, 11258, 4391, 420, 412, 1935, 411, 646, 36116, 257, 707, 857, 689, 286, 393, 3012], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 281, "seek": 81936, "start": 841.96, "end": 847.2, "text": " in on a weekend and improve them, but I can put my main focus to the future.", "tokens": [294, 322, 257, 6711, 293, 3470, 552, 11, 457, 286, 393, 829, 452, 2135, 1879, 281, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 282, "seek": 81936, "start": 847.2, "end": 848.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.26424188756231054, "compression_ratio": 1.6881720430107527, "no_speech_prob": 9.276226933252474e-07}, {"id": 283, "seek": 84820, "start": 848.2, "end": 854.12, "text": " It's such a weight off your shoulders when you get something out there and it's shipped.", "tokens": [467, 311, 1270, 257, 3364, 766, 428, 10245, 562, 291, 483, 746, 484, 456, 293, 309, 311, 25312, 13], "temperature": 0.0, "avg_logprob": -0.3021951484680176, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.2058036936177814e-07}, {"id": 284, "seek": 84820, "start": 854.12, "end": 856.36, "text": " You can always iterate, but it's shipped.", "tokens": [509, 393, 1009, 44497, 11, 457, 309, 311, 25312, 13], "temperature": 0.0, "avg_logprob": -0.3021951484680176, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.2058036936177814e-07}, {"id": 285, "seek": 84820, "start": 856.36, "end": 857.36, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3021951484680176, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.2058036936177814e-07}, {"id": 286, "seek": 84820, "start": 857.36, "end": 860.76, "text": " Jeroen, what have you shipped in 2022?", "tokens": [508, 2032, 268, 11, 437, 362, 291, 25312, 294, 20229, 30], "temperature": 0.0, "avg_logprob": -0.3021951484680176, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.2058036936177814e-07}, {"id": 287, "seek": 84820, "start": 860.76, "end": 862.6400000000001, "text": " What have you been working on this year?", "tokens": [708, 362, 291, 668, 1364, 322, 341, 1064, 30], "temperature": 0.0, "avg_logprob": -0.3021951484680176, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.2058036936177814e-07}, {"id": 288, "seek": 84820, "start": 862.6400000000001, "end": 867.76, "text": " Something I started last year, at the end of last year, but I only got to, well, not", "tokens": [6595, 286, 1409, 1036, 1064, 11, 412, 264, 917, 295, 1036, 1064, 11, 457, 286, 787, 658, 281, 11, 731, 11, 406], "temperature": 0.0, "avg_logprob": -0.3021951484680176, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.2058036936177814e-07}, {"id": 289, "seek": 84820, "start": 867.76, "end": 872.1600000000001, "text": " release because you will understand why it's not released.", "tokens": [4374, 570, 291, 486, 1223, 983, 309, 311, 406, 4736, 13], "temperature": 0.0, "avg_logprob": -0.3021951484680176, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.2058036936177814e-07}, {"id": 290, "seek": 87216, "start": 872.16, "end": 878.92, "text": " I made a proposal called till recursion module cons for Elm Optimized Level 2.", "tokens": [286, 1027, 257, 11494, 1219, 4288, 20560, 313, 10088, 1014, 337, 2699, 76, 35013, 1602, 16872, 568, 13], "temperature": 0.0, "avg_logprob": -0.2995471483395423, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.9443940857399866e-07}, {"id": 291, "seek": 87216, "start": 878.92, "end": 884.9599999999999, "text": " So that is basically like a change in the Elm language or the compilation of the Elm", "tokens": [407, 300, 307, 1936, 411, 257, 1319, 294, 264, 2699, 76, 2856, 420, 264, 40261, 295, 264, 2699, 76], "temperature": 0.0, "avg_logprob": -0.2995471483395423, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.9443940857399866e-07}, {"id": 292, "seek": 87216, "start": 884.9599999999999, "end": 890.8, "text": " language where some functions that are not till recursive, meaning not very optimized,", "tokens": [2856, 689, 512, 6828, 300, 366, 406, 4288, 20560, 488, 11, 3620, 406, 588, 26941, 11], "temperature": 0.0, "avg_logprob": -0.2995471483395423, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.9443940857399866e-07}, {"id": 293, "seek": 87216, "start": 890.8, "end": 898.9599999999999, "text": " which can crash the stack and are just slower, can become till optimized and much faster.", "tokens": [597, 393, 8252, 264, 8630, 293, 366, 445, 14009, 11, 393, 1813, 4288, 26941, 293, 709, 4663, 13], "temperature": 0.0, "avg_logprob": -0.2995471483395423, "compression_ratio": 1.6267942583732058, "no_speech_prob": 1.9443940857399866e-07}, {"id": 294, "seek": 89896, "start": 898.96, "end": 904.08, "text": " And that took a long time and I'm very excited because I think it works really well.", "tokens": [400, 300, 1890, 257, 938, 565, 293, 286, 478, 588, 2919, 570, 286, 519, 309, 1985, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 295, "seek": 89896, "start": 904.08, "end": 908.6, "text": " The reason why it's not released yet is because it's still in a pull request to Elm Optimized", "tokens": [440, 1778, 983, 309, 311, 406, 4736, 1939, 307, 570, 309, 311, 920, 294, 257, 2235, 5308, 281, 2699, 76, 35013, 1602], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 296, "seek": 89896, "start": 908.6, "end": 909.76, "text": " Level 2.", "tokens": [16872, 568, 13], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 297, "seek": 89896, "start": 909.76, "end": 912.8000000000001, "text": " So as soon as Matt, you will have some time.", "tokens": [407, 382, 2321, 382, 7397, 11, 291, 486, 362, 512, 565, 13], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 298, "seek": 89896, "start": 912.8000000000001, "end": 915.2, "text": " I knew it.", "tokens": [286, 2586, 309, 13], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 299, "seek": 89896, "start": 915.2, "end": 918.88, "text": " I knew this was coming.", "tokens": [286, 2586, 341, 390, 1348, 13], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 300, "seek": 89896, "start": 918.88, "end": 923.8000000000001, "text": " It's the whole point of this podcast really, just to have Jeroen to grant Matt on his time", "tokens": [467, 311, 264, 1379, 935, 295, 341, 7367, 534, 11, 445, 281, 362, 508, 2032, 268, 281, 6386, 7397, 322, 702, 565], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 301, "seek": 89896, "start": 923.8000000000001, "end": 924.8000000000001, "text": " allocation.", "tokens": [27599, 13], "temperature": 0.0, "avg_logprob": -0.3699466917249892, "compression_ratio": 1.5289256198347108, "no_speech_prob": 2.5215292680513812e-06}, {"id": 302, "seek": 92480, "start": 924.8, "end": 929.0799999999999, "text": " It's more of an intervention for my PR than a podcast really.", "tokens": [467, 311, 544, 295, 364, 13176, 337, 452, 11568, 813, 257, 7367, 534, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 303, "seek": 92480, "start": 929.0799999999999, "end": 934.4, "text": " That's what I'm hearing.", "tokens": [663, 311, 437, 286, 478, 4763, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 304, "seek": 92480, "start": 934.4, "end": 935.4, "text": " Did you say PR?", "tokens": [2589, 291, 584, 11568, 30], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 305, "seek": 92480, "start": 935.4, "end": 937.5999999999999, "text": " Did you mean PRs plural?", "tokens": [2589, 291, 914, 11568, 82, 25377, 30], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 306, "seek": 92480, "start": 937.5999999999999, "end": 939.12, "text": " Oh yeah.", "tokens": [876, 1338, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 307, "seek": 92480, "start": 939.12, "end": 940.12, "text": " No, sure.", "tokens": [883, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 308, "seek": 92480, "start": 940.12, "end": 944.9599999999999, "text": " Matt, this is actually not going live.", "tokens": [7397, 11, 341, 307, 767, 406, 516, 1621, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 309, "seek": 92480, "start": 944.9599999999999, "end": 946.76, "text": " We are just faking a podcast.", "tokens": [492, 366, 445, 283, 2456, 257, 7367, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 310, "seek": 92480, "start": 946.76, "end": 947.76, "text": " It's not even recorded.", "tokens": [467, 311, 406, 754, 8287, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 311, "seek": 92480, "start": 947.76, "end": 948.76, "text": " It's just, no, I get it.", "tokens": [467, 311, 445, 11, 572, 11, 286, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 312, "seek": 92480, "start": 948.76, "end": 949.76, "text": " Pressure you into-", "tokens": [6776, 540, 291, 666, 12], "temperature": 0.0, "avg_logprob": -0.4445669884775199, "compression_ratio": 1.4221105527638191, "no_speech_prob": 8.529370461474173e-06}, {"id": 313, "seek": 94976, "start": 949.76, "end": 955.16, "text": " Oddly enough, not the first time this has happened, but.", "tokens": [43630, 356, 1547, 11, 406, 264, 700, 565, 341, 575, 2011, 11, 457, 13], "temperature": 0.0, "avg_logprob": -0.3410269011051283, "compression_ratio": 1.6561264822134387, "no_speech_prob": 1.529241785647173e-06}, {"id": 314, "seek": 94976, "start": 955.16, "end": 957.3199999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3410269011051283, "compression_ratio": 1.6561264822134387, "no_speech_prob": 1.529241785647173e-06}, {"id": 315, "seek": 94976, "start": 957.3199999999999, "end": 962.08, "text": " So it's not released, but it was a very fun exploration and I'm very happy with the results.", "tokens": [407, 309, 311, 406, 4736, 11, 457, 309, 390, 257, 588, 1019, 16197, 293, 286, 478, 588, 2055, 365, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.3410269011051283, "compression_ratio": 1.6561264822134387, "no_speech_prob": 1.529241785647173e-06}, {"id": 316, "seek": 94976, "start": 962.08, "end": 968.4, "text": " Other than that, plenty of new releases for Elm Review, improving performance, improving", "tokens": [5358, 813, 300, 11, 7140, 295, 777, 16952, 337, 2699, 76, 19954, 11, 11470, 3389, 11, 11470], "temperature": 0.0, "avg_logprob": -0.3410269011051283, "compression_ratio": 1.6561264822134387, "no_speech_prob": 1.529241785647173e-06}, {"id": 317, "seek": 94976, "start": 968.4, "end": 973.16, "text": " things you can do with it, data that you can have and quality of life improvements.", "tokens": [721, 291, 393, 360, 365, 309, 11, 1412, 300, 291, 393, 362, 293, 3125, 295, 993, 13797, 13], "temperature": 0.0, "avg_logprob": -0.3410269011051283, "compression_ratio": 1.6561264822134387, "no_speech_prob": 1.529241785647173e-06}, {"id": 318, "seek": 94976, "start": 973.16, "end": 977.52, "text": " I think you're also underselling the performance aspect of this.", "tokens": [286, 519, 291, 434, 611, 833, 30427, 264, 3389, 4171, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.3410269011051283, "compression_ratio": 1.6561264822134387, "no_speech_prob": 1.529241785647173e-06}, {"id": 319, "seek": 94976, "start": 977.52, "end": 979.52, "text": " You had a major speed up.", "tokens": [509, 632, 257, 2563, 3073, 493, 13], "temperature": 0.0, "avg_logprob": -0.3410269011051283, "compression_ratio": 1.6561264822134387, "no_speech_prob": 1.529241785647173e-06}, {"id": 320, "seek": 97952, "start": 979.52, "end": 980.52, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 321, "seek": 97952, "start": 980.52, "end": 981.52, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 322, "seek": 97952, "start": 981.52, "end": 982.52, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 323, "seek": 97952, "start": 982.52, "end": 983.52, "text": " Especially for fixes.", "tokens": [8545, 337, 32539, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 324, "seek": 97952, "start": 983.52, "end": 984.52, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 325, "seek": 97952, "start": 984.52, "end": 988.3199999999999, "text": " So the main one that you're thinking of is making fixes a lot faster.", "tokens": [407, 264, 2135, 472, 300, 291, 434, 1953, 295, 307, 1455, 32539, 257, 688, 4663, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 326, "seek": 97952, "start": 988.3199999999999, "end": 996.04, "text": " Some cases it's improved 13 times and I'm very happy because now at Vendor, where you", "tokens": [2188, 3331, 309, 311, 9689, 3705, 1413, 293, 286, 478, 588, 2055, 570, 586, 412, 691, 521, 284, 11, 689, 291], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 327, "seek": 97952, "start": 996.04, "end": 1002.24, "text": " have a very large code base, you can finally start using it normally instead of your Zamboni", "tokens": [362, 257, 588, 2416, 3089, 3096, 11, 291, 393, 2721, 722, 1228, 309, 5646, 2602, 295, 428, 1176, 2173, 17049], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 328, "seek": 97952, "start": 1002.24, "end": 1004.88, "text": " thing that ran every weekend or something.", "tokens": [551, 300, 5872, 633, 6711, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 329, "seek": 97952, "start": 1004.88, "end": 1007.02, "text": " Which probably requires some context with people.", "tokens": [3013, 1391, 7029, 512, 4319, 365, 561, 13], "temperature": 0.0, "avg_logprob": -0.2291446044066242, "compression_ratio": 1.548, "no_speech_prob": 2.1355111812226824e-07}, {"id": 330, "seek": 100702, "start": 1007.02, "end": 1012.64, "text": " So at Vendor, we have something called the Zamboni, which we love the Zamboni.", "tokens": [407, 412, 691, 521, 284, 11, 321, 362, 746, 1219, 264, 1176, 2173, 17049, 11, 597, 321, 959, 264, 1176, 2173, 17049, 13], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 331, "seek": 100702, "start": 1012.64, "end": 1014.12, "text": " We don't need no Zamboni hate.", "tokens": [492, 500, 380, 643, 572, 1176, 2173, 17049, 4700, 13], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 332, "seek": 100702, "start": 1014.12, "end": 1018.92, "text": " The Zamboni runs every weekend and opens a PR.", "tokens": [440, 1176, 2173, 17049, 6676, 633, 6711, 293, 9870, 257, 11568, 13], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 333, "seek": 100702, "start": 1018.92, "end": 1025.56, "text": " In case you don't know what a Zamboni is, it's the little machine that re-does the surface", "tokens": [682, 1389, 291, 500, 380, 458, 437, 257, 1176, 2173, 17049, 307, 11, 309, 311, 264, 707, 3479, 300, 319, 12, 48409, 264, 3753], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 334, "seek": 100702, "start": 1025.56, "end": 1027.96, "text": " of the ice for an ice rink.", "tokens": [295, 264, 4435, 337, 364, 4435, 367, 475, 13], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 335, "seek": 100702, "start": 1027.96, "end": 1029.84, "text": " I wouldn't call it a little machine, but-", "tokens": [286, 2759, 380, 818, 309, 257, 707, 3479, 11, 457, 12], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 336, "seek": 100702, "start": 1029.84, "end": 1030.84, "text": " It's a big machine.", "tokens": [467, 311, 257, 955, 3479, 13], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 337, "seek": 100702, "start": 1030.84, "end": 1031.84, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 338, "seek": 100702, "start": 1031.84, "end": 1032.84, "text": " A big machine.", "tokens": [316, 955, 3479, 13], "temperature": 0.0, "avg_logprob": -0.24585055516771048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 8.186171385204943e-07}, {"id": 339, "seek": 103284, "start": 1032.84, "end": 1037.52, "text": " It's a big machine when there are people out there or else you might run them over.", "tokens": [467, 311, 257, 955, 3479, 562, 456, 366, 561, 484, 456, 420, 1646, 291, 1062, 1190, 552, 670, 13], "temperature": 0.0, "avg_logprob": -0.3022516197133287, "compression_ratio": 1.67578125, "no_speech_prob": 3.6326557051324926e-07}, {"id": 340, "seek": 103284, "start": 1037.52, "end": 1043.6799999999998, "text": " And mostly that was because we wanted rules that could be run that may take longer.", "tokens": [400, 5240, 300, 390, 570, 321, 1415, 4474, 300, 727, 312, 1190, 300, 815, 747, 2854, 13], "temperature": 0.0, "avg_logprob": -0.3022516197133287, "compression_ratio": 1.67578125, "no_speech_prob": 3.6326557051324926e-07}, {"id": 341, "seek": 103284, "start": 1043.6799999999998, "end": 1047.84, "text": " And yeah, it definitely cut down the time of the average run for that.", "tokens": [400, 1338, 11, 309, 2138, 1723, 760, 264, 565, 295, 264, 4274, 1190, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.3022516197133287, "compression_ratio": 1.67578125, "no_speech_prob": 3.6326557051324926e-07}, {"id": 342, "seek": 103284, "start": 1047.84, "end": 1054.08, "text": " We haven't totally turned on Elm Review as far as something that would run in our build", "tokens": [492, 2378, 380, 3879, 3574, 322, 2699, 76, 19954, 382, 1400, 382, 746, 300, 576, 1190, 294, 527, 1322], "temperature": 0.0, "avg_logprob": -0.3022516197133287, "compression_ratio": 1.67578125, "no_speech_prob": 3.6326557051324926e-07}, {"id": 343, "seek": 103284, "start": 1054.08, "end": 1057.04, "text": " process, like when we're editing stuff.", "tokens": [1399, 11, 411, 562, 321, 434, 10000, 1507, 13], "temperature": 0.0, "avg_logprob": -0.3022516197133287, "compression_ratio": 1.67578125, "no_speech_prob": 3.6326557051324926e-07}, {"id": 344, "seek": 103284, "start": 1057.04, "end": 1062.0, "text": " Build process meaning in dev, like when you're watching stuff.", "tokens": [11875, 1399, 3620, 294, 1905, 11, 411, 562, 291, 434, 1976, 1507, 13], "temperature": 0.0, "avg_logprob": -0.3022516197133287, "compression_ratio": 1.67578125, "no_speech_prob": 3.6326557051324926e-07}, {"id": 345, "seek": 106200, "start": 1062.0, "end": 1068.64, "text": " But I know there are a few motivated individuals at Vendor who are kind of looking into that.", "tokens": [583, 286, 458, 456, 366, 257, 1326, 14515, 5346, 412, 691, 521, 284, 567, 366, 733, 295, 1237, 666, 300, 13], "temperature": 0.0, "avg_logprob": -0.21695189747383925, "compression_ratio": 1.6713780918727916, "no_speech_prob": 1.760327677402529e-06}, {"id": 346, "seek": 106200, "start": 1068.64, "end": 1072.76, "text": " We did actually just write a new Elm Review rule that I need to actually review.", "tokens": [492, 630, 767, 445, 2464, 257, 777, 2699, 76, 19954, 4978, 300, 286, 643, 281, 767, 3131, 13], "temperature": 0.0, "avg_logprob": -0.21695189747383925, "compression_ratio": 1.6713780918727916, "no_speech_prob": 1.760327677402529e-06}, {"id": 347, "seek": 106200, "start": 1072.76, "end": 1074.36, "text": " So that's exciting.", "tokens": [407, 300, 311, 4670, 13], "temperature": 0.0, "avg_logprob": -0.21695189747383925, "compression_ratio": 1.6713780918727916, "no_speech_prob": 1.760327677402529e-06}, {"id": 348, "seek": 106200, "start": 1074.36, "end": 1080.72, "text": " We did one for message naming conventions, just because as we grow, we realized that", "tokens": [492, 630, 472, 337, 3636, 25290, 33520, 11, 445, 570, 382, 321, 1852, 11, 321, 5334, 300], "temperature": 0.0, "avg_logprob": -0.21695189747383925, "compression_ratio": 1.6713780918727916, "no_speech_prob": 1.760327677402529e-06}, {"id": 349, "seek": 106200, "start": 1080.72, "end": 1086.52, "text": " we were getting more Elm developers and that we had written a document for a while ago,", "tokens": [321, 645, 1242, 544, 2699, 76, 8849, 293, 300, 321, 632, 3720, 257, 4166, 337, 257, 1339, 2057, 11], "temperature": 0.0, "avg_logprob": -0.21695189747383925, "compression_ratio": 1.6713780918727916, "no_speech_prob": 1.760327677402529e-06}, {"id": 350, "seek": 106200, "start": 1086.52, "end": 1089.32, "text": " and it's not a big document and it's fairly clear.", "tokens": [293, 309, 311, 406, 257, 955, 4166, 293, 309, 311, 6457, 1850, 13], "temperature": 0.0, "avg_logprob": -0.21695189747383925, "compression_ratio": 1.6713780918727916, "no_speech_prob": 1.760327677402529e-06}, {"id": 351, "seek": 106200, "start": 1089.32, "end": 1091.62, "text": " People who see it are like, oh yeah, that makes sense.", "tokens": [3432, 567, 536, 309, 366, 411, 11, 1954, 1338, 11, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.21695189747383925, "compression_ratio": 1.6713780918727916, "no_speech_prob": 1.760327677402529e-06}, {"id": 352, "seek": 109162, "start": 1091.62, "end": 1093.2199999999998, "text": " But you have no way to discover that.", "tokens": [583, 291, 362, 572, 636, 281, 4411, 300, 13], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 353, "seek": 109162, "start": 1093.2199999999998, "end": 1098.6, "text": " So the Elm Review rule is honestly really good.", "tokens": [407, 264, 2699, 76, 19954, 4978, 307, 6095, 534, 665, 13], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 354, "seek": 109162, "start": 1098.6, "end": 1101.08, "text": " And Wolfgang put it together in like 15 minutes.", "tokens": [400, 16634, 19619, 829, 309, 1214, 294, 411, 2119, 2077, 13], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 355, "seek": 109162, "start": 1101.08, "end": 1102.36, "text": " It was great.", "tokens": [467, 390, 869, 13], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 356, "seek": 109162, "start": 1102.36, "end": 1103.36, "text": " Nice.", "tokens": [5490, 13], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 357, "seek": 109162, "start": 1103.36, "end": 1107.4399999999998, "text": " Another performance optimization that I'm working on right now is, so it's not released", "tokens": [3996, 3389, 19618, 300, 286, 478, 1364, 322, 558, 586, 307, 11, 370, 309, 311, 406, 4736], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 358, "seek": 109162, "start": 1107.4399999999998, "end": 1109.2399999999998, "text": " yet, but I am working on it.", "tokens": [1939, 11, 457, 286, 669, 1364, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 359, "seek": 109162, "start": 1109.2399999999998, "end": 1116.56, "text": " So it's still kind of as part of 2022 and maybe would be released soon, is adding caching", "tokens": [407, 309, 311, 920, 733, 295, 382, 644, 295, 20229, 293, 1310, 576, 312, 4736, 2321, 11, 307, 5127, 269, 2834], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 360, "seek": 109162, "start": 1116.56, "end": 1121.08, "text": " to Elm Review that would be persisted to the file system.", "tokens": [281, 2699, 76, 19954, 300, 576, 312, 13233, 292, 281, 264, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.26102483070502847, "compression_ratio": 1.5692883895131087, "no_speech_prob": 2.6015854928118642e-06}, {"id": 361, "seek": 112108, "start": 1121.08, "end": 1126.08, "text": " And so far it looks like on my work project, it speeds it up by two times.", "tokens": [400, 370, 1400, 309, 1542, 411, 322, 452, 589, 1716, 11, 309, 16411, 309, 493, 538, 732, 1413, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 362, "seek": 112108, "start": 1126.08, "end": 1128.22, "text": " So that's really nice.", "tokens": [407, 300, 311, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 363, "seek": 112108, "start": 1128.22, "end": 1130.1999999999998, "text": " What is it caching specifically?", "tokens": [708, 307, 309, 269, 2834, 4682, 30], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 364, "seek": 112108, "start": 1130.1999999999998, "end": 1134.32, "text": " Basically whenever you run Elm Review, it reports a lot of errors, right?", "tokens": [8537, 5699, 291, 1190, 2699, 76, 19954, 11, 309, 7122, 257, 688, 295, 13603, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 365, "seek": 112108, "start": 1134.32, "end": 1136.8999999999999, "text": " And those would be persisted to the file system.", "tokens": [400, 729, 576, 312, 13233, 292, 281, 264, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 366, "seek": 112108, "start": 1136.8999999999999, "end": 1143.24, "text": " So if you run it again and it noticed that a lot of these files haven't changed, then", "tokens": [407, 498, 291, 1190, 309, 797, 293, 309, 5694, 300, 257, 688, 295, 613, 7098, 2378, 380, 3105, 11, 550], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 367, "seek": 112108, "start": 1143.24, "end": 1145.84, "text": " it doesn't need to recompute a lot of things.", "tokens": [309, 1177, 380, 643, 281, 48000, 1169, 257, 688, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 368, "seek": 112108, "start": 1145.84, "end": 1146.84, "text": " Got it.", "tokens": [5803, 309, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 369, "seek": 112108, "start": 1146.84, "end": 1147.84, "text": " That makes total sense.", "tokens": [663, 1669, 3217, 2020, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 370, "seek": 112108, "start": 1147.84, "end": 1148.84, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 371, "seek": 112108, "start": 1148.84, "end": 1149.84, "text": " And that was quite tricky.", "tokens": [400, 300, 390, 1596, 12414, 13], "temperature": 0.0, "avg_logprob": -0.2152263034473766, "compression_ratio": 1.601423487544484, "no_speech_prob": 1.0845089946087683e-06}, {"id": 372, "seek": 114984, "start": 1149.84, "end": 1155.56, "text": " So probably if I don't end up releasing it this year, it's because I'm running a very", "tokens": [407, 1391, 498, 286, 500, 380, 917, 493, 16327, 309, 341, 1064, 11, 309, 311, 570, 286, 478, 2614, 257, 588], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 373, "seek": 114984, "start": 1155.56, "end": 1157.4399999999998, "text": " large blog post explaining it.", "tokens": [2416, 6968, 2183, 13468, 309, 13], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 374, "seek": 114984, "start": 1157.4399999999998, "end": 1158.4399999999998, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 375, "seek": 114984, "start": 1158.4399999999998, "end": 1161.8, "text": " Well, if you don't release it this year, we'll just hold another podcast intervention and", "tokens": [1042, 11, 498, 291, 500, 380, 4374, 309, 341, 1064, 11, 321, 603, 445, 1797, 1071, 7367, 13176, 293], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 376, "seek": 114984, "start": 1161.8, "end": 1162.8, "text": " follow up.", "tokens": [1524, 493, 13], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 377, "seek": 114984, "start": 1162.8, "end": 1163.8, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 378, "seek": 114984, "start": 1163.8, "end": 1169.56, "text": " Do you want to give the elevator pitch for extractors too?", "tokens": [1144, 291, 528, 281, 976, 264, 18782, 7293, 337, 8947, 830, 886, 30], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 379, "seek": 114984, "start": 1169.56, "end": 1172.36, "text": " I'm very excited about that feature.", "tokens": [286, 478, 588, 2919, 466, 300, 4111, 13], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 380, "seek": 114984, "start": 1172.36, "end": 1173.36, "text": " Oh yeah.", "tokens": [876, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3247059301002738, "compression_ratio": 1.4955752212389382, "no_speech_prob": 4.2469633854125277e-07}, {"id": 381, "seek": 117336, "start": 1173.36, "end": 1183.36, "text": " So there's now Elm Review extract, which is a new sub command or flag for Elm Review,", "tokens": [407, 456, 311, 586, 2699, 76, 19954, 8947, 11, 597, 307, 257, 777, 1422, 5622, 420, 7166, 337, 2699, 76, 19954, 11], "temperature": 0.0, "avg_logprob": -0.28113250732421874, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.051114963956934e-06}, {"id": 382, "seek": 117336, "start": 1183.36, "end": 1189.36, "text": " where basically you can write rules which collect data about your project.", "tokens": [689, 1936, 291, 393, 2464, 4474, 597, 2500, 1412, 466, 428, 1716, 13], "temperature": 0.0, "avg_logprob": -0.28113250732421874, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.051114963956934e-06}, {"id": 383, "seek": 117336, "start": 1189.36, "end": 1190.3999999999999, "text": " That's what they do.", "tokens": [663, 311, 437, 436, 360, 13], "temperature": 0.0, "avg_logprob": -0.28113250732421874, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.051114963956934e-06}, {"id": 384, "seek": 117336, "start": 1190.3999999999999, "end": 1192.52, "text": " And at the end, they report errors.", "tokens": [400, 412, 264, 917, 11, 436, 2275, 13603, 13], "temperature": 0.0, "avg_logprob": -0.28113250732421874, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.051114963956934e-06}, {"id": 385, "seek": 117336, "start": 1192.52, "end": 1196.84, "text": " The additional thing that you can now do is to take all the data that you've collected", "tokens": [440, 4497, 551, 300, 291, 393, 586, 360, 307, 281, 747, 439, 264, 1412, 300, 291, 600, 11087], "temperature": 0.0, "avg_logprob": -0.28113250732421874, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.051114963956934e-06}, {"id": 386, "seek": 119684, "start": 1196.84, "end": 1204.8, "text": " and transform into JSON that you can then pass to other projects, to scripts, or make", "tokens": [293, 4088, 666, 31828, 300, 291, 393, 550, 1320, 281, 661, 4455, 11, 281, 23294, 11, 420, 652], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 387, "seek": 119684, "start": 1204.8, "end": 1208.1999999999998, "text": " graphs or images or documentation from those.", "tokens": [24877, 420, 5267, 420, 14333, 490, 729, 13], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 388, "seek": 119684, "start": 1208.1999999999998, "end": 1214.12, "text": " So basically any data that you have in your Elm project, you can now collect and extract", "tokens": [407, 1936, 604, 1412, 300, 291, 362, 294, 428, 2699, 76, 1716, 11, 291, 393, 586, 2500, 293, 8947], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 389, "seek": 119684, "start": 1214.12, "end": 1216.4399999999998, "text": " and do whatever you want with it.", "tokens": [293, 360, 2035, 291, 528, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 390, "seek": 119684, "start": 1216.4399999999998, "end": 1218.12, "text": " And I think that's very powerful.", "tokens": [400, 286, 519, 300, 311, 588, 4005, 13], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 391, "seek": 119684, "start": 1218.12, "end": 1222.08, "text": " I don't know what people will do with it yet, but-", "tokens": [286, 500, 380, 458, 437, 561, 486, 360, 365, 309, 1939, 11, 457, 12], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 392, "seek": 119684, "start": 1222.08, "end": 1223.08, "text": " I'm curious.", "tokens": [286, 478, 6369, 13], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 393, "seek": 119684, "start": 1223.08, "end": 1224.08, "text": " I mean-", "tokens": [286, 914, 12], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 394, "seek": 119684, "start": 1224.08, "end": 1225.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23935475716224083, "compression_ratio": 1.5508474576271187, "no_speech_prob": 1.0676660622266354e-06}, {"id": 395, "seek": 122508, "start": 1225.08, "end": 1231.08, "text": " I mean, I guess you could make a metrics dashboard for your code base, right?", "tokens": [286, 914, 11, 286, 2041, 291, 727, 652, 257, 16367, 18342, 337, 428, 3089, 3096, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 396, "seek": 122508, "start": 1231.08, "end": 1232.08, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 397, "seek": 122508, "start": 1232.08, "end": 1233.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 398, "seek": 122508, "start": 1233.08, "end": 1236.48, "text": " I'd be curious to see what you'd want to put in there.", "tokens": [286, 1116, 312, 6369, 281, 536, 437, 291, 1116, 528, 281, 829, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 399, "seek": 122508, "start": 1236.48, "end": 1239.08, "text": " But yeah, and it wouldn't be that involved to actually do.", "tokens": [583, 1338, 11, 293, 309, 2759, 380, 312, 300, 3288, 281, 767, 360, 13], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 400, "seek": 122508, "start": 1239.08, "end": 1240.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 401, "seek": 122508, "start": 1240.08, "end": 1245.8799999999999, "text": " You just have something, like have a CI deployment or whatever, where you calculate this stuff,", "tokens": [509, 445, 362, 746, 11, 411, 362, 257, 37777, 19317, 420, 2035, 11, 689, 291, 8873, 341, 1507, 11], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 402, "seek": 122508, "start": 1245.8799999999999, "end": 1249.28, "text": " take out the JSON, put it to a little Elm app to display it.", "tokens": [747, 484, 264, 31828, 11, 829, 309, 281, 257, 707, 2699, 76, 724, 281, 4674, 309, 13], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 403, "seek": 122508, "start": 1249.28, "end": 1250.28, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3321212906021256, "compression_ratio": 1.5, "no_speech_prob": 2.025949470407795e-06}, {"id": 404, "seek": 125028, "start": 1250.28, "end": 1257.36, "text": " A few things that you can already do with it is like compute the import module graph,", "tokens": [316, 1326, 721, 300, 291, 393, 1217, 360, 365, 309, 307, 411, 14722, 264, 974, 10088, 4295, 11], "temperature": 0.0, "avg_logprob": -0.2842963933944702, "compression_ratio": 1.7716894977168949, "no_speech_prob": 1.933315161295468e-06}, {"id": 405, "seek": 125028, "start": 1257.36, "end": 1263.3999999999999, "text": " find a list of licenses that you have in your dependencies, small things like that.", "tokens": [915, 257, 1329, 295, 32821, 300, 291, 362, 294, 428, 36606, 11, 1359, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2842963933944702, "compression_ratio": 1.7716894977168949, "no_speech_prob": 1.933315161295468e-06}, {"id": 406, "seek": 125028, "start": 1263.3999999999999, "end": 1269.32, "text": " Other things that I can imagine is people can write a rule that extracts the list of", "tokens": [5358, 721, 300, 286, 393, 3811, 307, 561, 393, 2464, 257, 4978, 300, 8947, 82, 264, 1329, 295], "temperature": 0.0, "avg_logprob": -0.2842963933944702, "compression_ratio": 1.7716894977168949, "no_speech_prob": 1.933315161295468e-06}, {"id": 407, "seek": 125028, "start": 1269.32, "end": 1276.16, "text": " use CSS classes, and then they give it to a CSS pruner or something, things that remove", "tokens": [764, 24387, 5359, 11, 293, 550, 436, 976, 309, 281, 257, 24387, 582, 409, 260, 420, 746, 11, 721, 300, 4159], "temperature": 0.0, "avg_logprob": -0.2842963933944702, "compression_ratio": 1.7716894977168949, "no_speech_prob": 1.933315161295468e-06}, {"id": 408, "seek": 125028, "start": 1276.16, "end": 1279.48, "text": " all the unused CSS classes, things like that.", "tokens": [439, 264, 44383, 24387, 5359, 11, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2842963933944702, "compression_ratio": 1.7716894977168949, "no_speech_prob": 1.933315161295468e-06}, {"id": 409, "seek": 127948, "start": 1279.48, "end": 1283.68, "text": " It's a very generic feature.", "tokens": [467, 311, 257, 588, 19577, 4111, 13], "temperature": 0.0, "avg_logprob": -0.2673908233642578, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.812987986544613e-06}, {"id": 410, "seek": 127948, "start": 1283.68, "end": 1288.88, "text": " So how powerful it is, is related to what people do with it.", "tokens": [407, 577, 4005, 309, 307, 11, 307, 4077, 281, 437, 561, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.2673908233642578, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.812987986544613e-06}, {"id": 411, "seek": 127948, "start": 1288.88, "end": 1289.88, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2673908233642578, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.812987986544613e-06}, {"id": 412, "seek": 127948, "start": 1289.88, "end": 1290.88, "text": " So many possibilities.", "tokens": [407, 867, 12178, 13], "temperature": 0.0, "avg_logprob": -0.2673908233642578, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.812987986544613e-06}, {"id": 413, "seek": 127948, "start": 1290.88, "end": 1298.44, "text": " Somebody was asking something about optimizations for an Elm pages site, about pulling in translations", "tokens": [13463, 390, 3365, 746, 466, 5028, 14455, 337, 364, 2699, 76, 7183, 3621, 11, 466, 8407, 294, 37578], "temperature": 0.0, "avg_logprob": -0.2673908233642578, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.812987986544613e-06}, {"id": 414, "seek": 127948, "start": 1298.44, "end": 1300.16, "text": " for a given page.", "tokens": [337, 257, 2212, 3028, 13], "temperature": 0.0, "avg_logprob": -0.2673908233642578, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.812987986544613e-06}, {"id": 415, "seek": 127948, "start": 1300.16, "end": 1302.64, "text": " And I was thinking, like, wouldn't it be cool?", "tokens": [400, 286, 390, 1953, 11, 411, 11, 2759, 380, 309, 312, 1627, 30], "temperature": 0.0, "avg_logprob": -0.2673908233642578, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.812987986544613e-06}, {"id": 416, "seek": 130264, "start": 1302.64, "end": 1310.68, "text": " You've got this kind of JSON file of all these translations, and you have specific ones that", "tokens": [509, 600, 658, 341, 733, 295, 31828, 3991, 295, 439, 613, 37578, 11, 293, 291, 362, 2685, 2306, 300], "temperature": 0.0, "avg_logprob": -0.23083682770424702, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.181152633009333e-07}, {"id": 417, "seek": 130264, "start": 1310.68, "end": 1312.44, "text": " you need for specific pages.", "tokens": [291, 643, 337, 2685, 7183, 13], "temperature": 0.0, "avg_logprob": -0.23083682770424702, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.181152633009333e-07}, {"id": 418, "seek": 130264, "start": 1312.44, "end": 1317.68, "text": " And wouldn't it be cool if you just had a big data source in Elm pages of all the translations,", "tokens": [400, 2759, 380, 309, 312, 1627, 498, 291, 445, 632, 257, 955, 1412, 4009, 294, 2699, 76, 7183, 295, 439, 264, 37578, 11], "temperature": 0.0, "avg_logprob": -0.23083682770424702, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.181152633009333e-07}, {"id": 419, "seek": 130264, "start": 1317.68, "end": 1324.2800000000002, "text": " but then if you had an Elm review extractor that looked at which keys are used for a given", "tokens": [457, 550, 498, 291, 632, 364, 2699, 76, 3131, 8947, 284, 300, 2956, 412, 597, 9317, 366, 1143, 337, 257, 2212], "temperature": 0.0, "avg_logprob": -0.23083682770424702, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.181152633009333e-07}, {"id": 420, "seek": 130264, "start": 1324.2800000000002, "end": 1329.24, "text": " route and then massaged that data source to contain the subset.", "tokens": [7955, 293, 550, 2758, 2980, 300, 1412, 4009, 281, 5304, 264, 25993, 13], "temperature": 0.0, "avg_logprob": -0.23083682770424702, "compression_ratio": 1.6607142857142858, "no_speech_prob": 4.181152633009333e-07}, {"id": 421, "seek": 132924, "start": 1329.24, "end": 1335.4, "text": " So basically, as a developer, you're not paying the cost of micromanaging which translation", "tokens": [407, 1936, 11, 382, 257, 10754, 11, 291, 434, 406, 6229, 264, 2063, 295, 3123, 81, 4277, 3568, 597, 12853], "temperature": 0.0, "avg_logprob": -0.22179009837488975, "compression_ratio": 1.7570422535211268, "no_speech_prob": 3.5763125083576597e-07}, {"id": 422, "seek": 132924, "start": 1335.4, "end": 1339.92, "text": " keys you're pulling in, doing some builder pattern to pull in only the translations you", "tokens": [9317, 291, 434, 8407, 294, 11, 884, 512, 27377, 5102, 281, 2235, 294, 787, 264, 37578, 291], "temperature": 0.0, "avg_logprob": -0.22179009837488975, "compression_ratio": 1.7570422535211268, "no_speech_prob": 3.5763125083576597e-07}, {"id": 423, "seek": 132924, "start": 1339.92, "end": 1345.88, "text": " need, but then you can, under the hood, optimize it in your build step using this fancy rule.", "tokens": [643, 11, 457, 550, 291, 393, 11, 833, 264, 13376, 11, 19719, 309, 294, 428, 1322, 1823, 1228, 341, 10247, 4978, 13], "temperature": 0.0, "avg_logprob": -0.22179009837488975, "compression_ratio": 1.7570422535211268, "no_speech_prob": 3.5763125083576597e-07}, {"id": 424, "seek": 132924, "start": 1345.88, "end": 1349.56, "text": " So there's just so many things I could imagine people building with it.", "tokens": [407, 456, 311, 445, 370, 867, 721, 286, 727, 3811, 561, 2390, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.22179009837488975, "compression_ratio": 1.7570422535211268, "no_speech_prob": 3.5763125083576597e-07}, {"id": 425, "seek": 132924, "start": 1349.56, "end": 1351.56, "text": " It'll be really cool to see what people do with it.", "tokens": [467, 603, 312, 534, 1627, 281, 536, 437, 561, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.22179009837488975, "compression_ratio": 1.7570422535211268, "no_speech_prob": 3.5763125083576597e-07}, {"id": 426, "seek": 132924, "start": 1351.56, "end": 1352.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22179009837488975, "compression_ratio": 1.7570422535211268, "no_speech_prob": 3.5763125083576597e-07}, {"id": 427, "seek": 132924, "start": 1352.56, "end": 1357.92, "text": " If you want to start translating your application as well, you can just look at all the strings", "tokens": [759, 291, 528, 281, 722, 35030, 428, 3861, 382, 731, 11, 291, 393, 445, 574, 412, 439, 264, 13985], "temperature": 0.0, "avg_logprob": -0.22179009837488975, "compression_ratio": 1.7570422535211268, "no_speech_prob": 3.5763125083576597e-07}, {"id": 428, "seek": 135792, "start": 1357.92, "end": 1364.0800000000002, "text": " that are passed to HTML.txt, for instance, and then extract them and provide them to", "tokens": [300, 366, 4678, 281, 17995, 13, 83, 734, 11, 337, 5197, 11, 293, 550, 8947, 552, 293, 2893, 552, 281], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 429, "seek": 135792, "start": 1364.0800000000002, "end": 1365.24, "text": " your translators.", "tokens": [428, 5105, 3391, 13], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 430, "seek": 135792, "start": 1365.24, "end": 1368.68, "text": " And they can now translate all those keys.", "tokens": [400, 436, 393, 586, 13799, 439, 729, 9317, 13], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 431, "seek": 135792, "start": 1368.68, "end": 1369.68, "text": " Hmm.", "tokens": [8239, 13], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 432, "seek": 135792, "start": 1369.68, "end": 1370.68, "text": " Brilliant.", "tokens": [34007, 13], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 433, "seek": 135792, "start": 1370.68, "end": 1371.68, "text": " What about you, Dillon?", "tokens": [708, 466, 291, 11, 28160, 30], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 434, "seek": 135792, "start": 1371.68, "end": 1373.16, "text": " What have you released this year?", "tokens": [708, 362, 291, 4736, 341, 1064, 30], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 435, "seek": 135792, "start": 1373.16, "end": 1380.4, "text": " So I was gonna say before Jeroen says anything, because yes, it has been a year for me of", "tokens": [407, 286, 390, 799, 584, 949, 508, 2032, 268, 1619, 1340, 11, 570, 2086, 11, 309, 575, 668, 257, 1064, 337, 385, 295], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 436, "seek": 135792, "start": 1380.4, "end": 1386.8000000000002, "text": " a mega whale, not yet released stable Elm pages v3.", "tokens": [257, 17986, 25370, 11, 406, 1939, 4736, 8351, 2699, 76, 7183, 371, 18, 13], "temperature": 0.0, "avg_logprob": -0.3076554770781615, "compression_ratio": 1.5254237288135593, "no_speech_prob": 5.014685484638903e-06}, {"id": 437, "seek": 138680, "start": 1386.8, "end": 1389.08, "text": " I was not going to say anything.", "tokens": [286, 390, 406, 516, 281, 584, 1340, 13], "temperature": 0.0, "avg_logprob": -0.5841547476278769, "compression_ratio": 1.391304347826087, "no_speech_prob": 2.6836619326786604e-06}, {"id": 438, "seek": 138680, "start": 1389.08, "end": 1390.56, "text": " You were thinking.", "tokens": [509, 645, 1953, 13], "temperature": 0.0, "avg_logprob": -0.5841547476278769, "compression_ratio": 1.391304347826087, "no_speech_prob": 2.6836619326786604e-06}, {"id": 439, "seek": 138680, "start": 1390.56, "end": 1393.9199999999998, "text": " But I'm glad he has a PR open.", "tokens": [583, 286, 478, 5404, 415, 575, 257, 11568, 1269, 13], "temperature": 0.0, "avg_logprob": -0.5841547476278769, "compression_ratio": 1.391304347826087, "no_speech_prob": 2.6836619326786604e-06}, {"id": 440, "seek": 138680, "start": 1393.9199999999998, "end": 1394.9199999999998, "text": " I bet.", "tokens": [286, 778, 13], "temperature": 0.0, "avg_logprob": -0.5841547476278769, "compression_ratio": 1.391304347826087, "no_speech_prob": 2.6836619326786604e-06}, {"id": 441, "seek": 138680, "start": 1394.9199999999998, "end": 1395.9199999999998, "text": " Or two.", "tokens": [1610, 732, 13], "temperature": 0.0, "avg_logprob": -0.5841547476278769, "compression_ratio": 1.391304347826087, "no_speech_prob": 2.6836619326786604e-06}, {"id": 442, "seek": 138680, "start": 1395.9199999999998, "end": 1405.52, "text": " So, yes, I do have a mega release.", "tokens": [407, 11, 2086, 11, 286, 360, 362, 257, 17986, 4374, 13], "temperature": 0.0, "avg_logprob": -0.5841547476278769, "compression_ratio": 1.391304347826087, "no_speech_prob": 2.6836619326786604e-06}, {"id": 443, "seek": 138680, "start": 1405.52, "end": 1416.56, "text": " It has been a real joy and privilege to get to really dedicate most of the year to building", "tokens": [467, 575, 668, 257, 957, 6258, 293, 12122, 281, 483, 281, 534, 30718, 881, 295, 264, 1064, 281, 2390], "temperature": 0.0, "avg_logprob": -0.5841547476278769, "compression_ratio": 1.391304347826087, "no_speech_prob": 2.6836619326786604e-06}, {"id": 444, "seek": 141656, "start": 1416.56, "end": 1423.0, "text": " this and no holds barred building exactly what I want in this.", "tokens": [341, 293, 572, 9190, 2159, 986, 2390, 2293, 437, 286, 528, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 445, "seek": 141656, "start": 1423.0, "end": 1426.1599999999999, "text": " So I will say I did release the beta.", "tokens": [407, 286, 486, 584, 286, 630, 4374, 264, 9861, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 446, "seek": 141656, "start": 1426.1599999999999, "end": 1427.1599999999999, "text": " That's out there.", "tokens": [663, 311, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 447, "seek": 141656, "start": 1427.1599999999999, "end": 1428.44, "text": " And people can install and use it.", "tokens": [400, 561, 393, 3625, 293, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 448, "seek": 141656, "start": 1428.44, "end": 1429.44, "text": " And they are.", "tokens": [400, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 449, "seek": 141656, "start": 1429.44, "end": 1430.44, "text": " So I did release that.", "tokens": [407, 286, 630, 4374, 300, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 450, "seek": 141656, "start": 1430.44, "end": 1431.44, "text": " That's a release.", "tokens": [663, 311, 257, 4374, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 451, "seek": 141656, "start": 1431.44, "end": 1432.44, "text": " That's a release.", "tokens": [663, 311, 257, 4374, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 452, "seek": 141656, "start": 1432.44, "end": 1433.44, "text": " It's a beta release.", "tokens": [467, 311, 257, 9861, 4374, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 453, "seek": 141656, "start": 1433.44, "end": 1436.96, "text": " It's a pre-release.", "tokens": [467, 311, 257, 659, 12, 265, 1122, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 454, "seek": 141656, "start": 1436.96, "end": 1439.56, "text": " And so, yeah, that's been amazing.", "tokens": [400, 370, 11, 1338, 11, 300, 311, 668, 2243, 13], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 455, "seek": 141656, "start": 1439.56, "end": 1446.08, "text": " Mario, I had this on my radar of wouldn't it be cool if instead of this sort of optimized", "tokens": [9343, 11, 286, 632, 341, 322, 452, 16544, 295, 2759, 380, 309, 312, 1627, 498, 2602, 295, 341, 1333, 295, 26941], "temperature": 0.0, "avg_logprob": -0.27082833647727966, "compression_ratio": 1.75, "no_speech_prob": 1.0287680197507143e-05}, {"id": 456, "seek": 144608, "start": 1446.08, "end": 1452.4399999999998, "text": " decoder functionality that Elm pages v2 had, which was like one of the most complicated", "tokens": [979, 19866, 14980, 300, 2699, 76, 7183, 371, 17, 632, 11, 597, 390, 411, 472, 295, 264, 881, 6179], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 457, "seek": 144608, "start": 1452.4399999999998, "end": 1455.8, "text": " features I've ever built that I was super proud of.", "tokens": [4122, 286, 600, 1562, 3094, 300, 286, 390, 1687, 4570, 295, 13], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 458, "seek": 144608, "start": 1455.8, "end": 1460.12, "text": " And then Mario's like, you know, it's pretty easy to just use Lambda to get the automatic", "tokens": [400, 550, 9343, 311, 411, 11, 291, 458, 11, 309, 311, 1238, 1858, 281, 445, 764, 45691, 281, 483, 264, 12509], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 459, "seek": 144608, "start": 1460.12, "end": 1462.1599999999999, "text": " serialization from the compiler.", "tokens": [17436, 2144, 490, 264, 31958, 13], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 460, "seek": 144608, "start": 1462.1599999999999, "end": 1463.1599999999999, "text": " I'm like, really?", "tokens": [286, 478, 411, 11, 534, 30], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 461, "seek": 144608, "start": 1463.1599999999999, "end": 1467.96, "text": " It's like, yeah, you just like call these secret functions and it automatically serializes", "tokens": [467, 311, 411, 11, 1338, 11, 291, 445, 411, 818, 613, 4054, 6828, 293, 309, 6772, 17436, 5660], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 462, "seek": 144608, "start": 1467.96, "end": 1469.32, "text": " all your types.", "tokens": [439, 428, 3467, 13], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 463, "seek": 144608, "start": 1469.32, "end": 1475.6799999999998, "text": " And so, yeah, there goes all my like most brilliant things I've ever built out the window.", "tokens": [400, 370, 11, 1338, 11, 456, 1709, 439, 452, 411, 881, 10248, 721, 286, 600, 1562, 3094, 484, 264, 4910, 13], "temperature": 0.0, "avg_logprob": -0.22921287536621093, "compression_ratio": 1.7194244604316546, "no_speech_prob": 6.338555522233946e-06}, {"id": 464, "seek": 147568, "start": 1475.68, "end": 1481.5600000000002, "text": " And now it's like a much better user experience, much more optimized byte data that's sent", "tokens": [400, 586, 309, 311, 411, 257, 709, 1101, 4195, 1752, 11, 709, 544, 26941, 40846, 1412, 300, 311, 2279], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 465, "seek": 147568, "start": 1481.5600000000002, "end": 1482.5600000000002, "text": " over the wire.", "tokens": [670, 264, 6234, 13], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 466, "seek": 147568, "start": 1482.5600000000002, "end": 1484.28, "text": " So that's been amazing.", "tokens": [407, 300, 311, 668, 2243, 13], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 467, "seek": 147568, "start": 1484.28, "end": 1492.64, "text": " Stripping out all my meticulously crafted features and replacing it with just Lambda", "tokens": [20390, 3759, 484, 439, 452, 41566, 25038, 36213, 4122, 293, 19139, 309, 365, 445, 45691], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 468, "seek": 147568, "start": 1492.64, "end": 1493.64, "text": " serialization.", "tokens": [17436, 2144, 13], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 469, "seek": 147568, "start": 1493.64, "end": 1495.3600000000001, "text": " So that's been amazing.", "tokens": [407, 300, 311, 668, 2243, 13], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 470, "seek": 147568, "start": 1495.3600000000001, "end": 1502.44, "text": " I built a feature that I'm super excited about Elm pages script, which I think is so Matt", "tokens": [286, 3094, 257, 4111, 300, 286, 478, 1687, 2919, 466, 2699, 76, 7183, 5755, 11, 597, 286, 519, 307, 370, 7397], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 471, "seek": 147568, "start": 1502.44, "end": 1505.04, "text": " was talking about this Elm code.", "tokens": [390, 1417, 466, 341, 2699, 76, 3089, 13], "temperature": 0.0, "avg_logprob": -0.24028425216674804, "compression_ratio": 1.5932203389830508, "no_speech_prob": 1.8161365460400702e-06}, {"id": 472, "seek": 150504, "start": 1505.04, "end": 1509.84, "text": " What's the command in the CLI Elm code gen generate Elm code gen gen or run.", "tokens": [708, 311, 264, 5622, 294, 264, 12855, 40, 2699, 76, 3089, 1049, 8460, 2699, 76, 3089, 1049, 1049, 420, 1190, 13], "temperature": 0.0, "avg_logprob": -0.26080610133983473, "compression_ratio": 1.7707317073170732, "no_speech_prob": 4.116312197766092e-07}, {"id": 473, "seek": 150504, "start": 1509.84, "end": 1510.84, "text": " Okay, cool.", "tokens": [1033, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.26080610133983473, "compression_ratio": 1.7707317073170732, "no_speech_prob": 4.116312197766092e-07}, {"id": 474, "seek": 150504, "start": 1510.84, "end": 1512.32, "text": " I'm coaching run.", "tokens": [286, 478, 15818, 1190, 13], "temperature": 0.0, "avg_logprob": -0.26080610133983473, "compression_ratio": 1.7707317073170732, "no_speech_prob": 4.116312197766092e-07}, {"id": 475, "seek": 150504, "start": 1512.32, "end": 1518.48, "text": " So Elm pages run is actually something we talked about on the Elm code gen episode with", "tokens": [407, 2699, 76, 7183, 1190, 307, 767, 746, 321, 2825, 466, 322, 264, 2699, 76, 3089, 1049, 3500, 365], "temperature": 0.0, "avg_logprob": -0.26080610133983473, "compression_ratio": 1.7707317073170732, "no_speech_prob": 4.116312197766092e-07}, {"id": 476, "seek": 150504, "start": 1518.48, "end": 1519.48, "text": " you.", "tokens": [291, 13], "temperature": 0.0, "avg_logprob": -0.26080610133983473, "compression_ratio": 1.7707317073170732, "no_speech_prob": 4.116312197766092e-07}, {"id": 477, "seek": 150504, "start": 1519.48, "end": 1526.74, "text": " I was saying, I want to build Elm pages code gen where you can basically run like so Elm", "tokens": [286, 390, 1566, 11, 286, 528, 281, 1322, 2699, 76, 7183, 3089, 1049, 689, 291, 393, 1936, 1190, 411, 370, 2699, 76], "temperature": 0.0, "avg_logprob": -0.26080610133983473, "compression_ratio": 1.7707317073170732, "no_speech_prob": 4.116312197766092e-07}, {"id": 478, "seek": 150504, "start": 1526.74, "end": 1532.36, "text": " code gen run lets you run an Elm file where you can execute this Elm file.", "tokens": [3089, 1049, 1190, 6653, 291, 1190, 364, 2699, 76, 3991, 689, 291, 393, 14483, 341, 2699, 76, 3991, 13], "temperature": 0.0, "avg_logprob": -0.26080610133983473, "compression_ratio": 1.7707317073170732, "no_speech_prob": 4.116312197766092e-07}, {"id": 479, "seek": 153236, "start": 1532.36, "end": 1536.6, "text": " It runs Elm code gen and generate some files for you, which is great.", "tokens": [467, 6676, 2699, 76, 3089, 1049, 293, 8460, 512, 7098, 337, 291, 11, 597, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.21266251802444458, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.3454263953226473e-07}, {"id": 480, "seek": 153236, "start": 1536.6, "end": 1540.4799999999998, "text": " But then maybe you need to pull in an environment variable.", "tokens": [583, 550, 1310, 291, 643, 281, 2235, 294, 364, 2823, 7006, 13], "temperature": 0.0, "avg_logprob": -0.21266251802444458, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.3454263953226473e-07}, {"id": 481, "seek": 153236, "start": 1540.4799999999998, "end": 1546.9599999999998, "text": " Maybe you need to read some files and pass them in and Elm code gen run.", "tokens": [2704, 291, 643, 281, 1401, 512, 7098, 293, 1320, 552, 294, 293, 2699, 76, 3089, 1049, 1190, 13], "temperature": 0.0, "avg_logprob": -0.21266251802444458, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.3454263953226473e-07}, {"id": 482, "seek": 153236, "start": 1546.9599999999998, "end": 1552.36, "text": " You can do that technically, but you have to maybe like strew together a few piped commands", "tokens": [509, 393, 360, 300, 12120, 11, 457, 291, 362, 281, 1310, 411, 342, 2236, 1214, 257, 1326, 8489, 292, 16901], "temperature": 0.0, "avg_logprob": -0.21266251802444458, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.3454263953226473e-07}, {"id": 483, "seek": 153236, "start": 1552.36, "end": 1556.4799999999998, "text": " on the CLI to pass in that data as a flag.", "tokens": [322, 264, 12855, 40, 281, 1320, 294, 300, 1412, 382, 257, 7166, 13], "temperature": 0.0, "avg_logprob": -0.21266251802444458, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.3454263953226473e-07}, {"id": 484, "seek": 153236, "start": 1556.4799999999998, "end": 1557.4799999999998, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.21266251802444458, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.3454263953226473e-07}, {"id": 485, "seek": 155748, "start": 1557.48, "end": 1562.44, "text": " So that's basically how Elm code gen run, it's basically just a little TypeScript little", "tokens": [407, 300, 311, 1936, 577, 2699, 76, 3089, 1049, 1190, 11, 309, 311, 1936, 445, 257, 707, 15576, 14237, 707], "temperature": 0.0, "avg_logprob": -0.28078200022379557, "compression_ratio": 1.7470817120622568, "no_speech_prob": 3.611934289438068e-06}, {"id": 486, "seek": 155748, "start": 1562.44, "end": 1569.48, "text": " file that just sort of has a standard, you know, shape of an Elm app that it's expecting", "tokens": [3991, 300, 445, 1333, 295, 575, 257, 3832, 11, 291, 458, 11, 3909, 295, 364, 2699, 76, 724, 300, 309, 311, 9650], "temperature": 0.0, "avg_logprob": -0.28078200022379557, "compression_ratio": 1.7470817120622568, "no_speech_prob": 3.611934289438068e-06}, {"id": 487, "seek": 155748, "start": 1569.48, "end": 1573.3600000000001, "text": " and and it hands it some flags, some initial data.", "tokens": [293, 293, 309, 2377, 309, 512, 23265, 11, 512, 5883, 1412, 13], "temperature": 0.0, "avg_logprob": -0.28078200022379557, "compression_ratio": 1.7470817120622568, "no_speech_prob": 3.611934289438068e-06}, {"id": 488, "seek": 155748, "start": 1573.3600000000001, "end": 1579.16, "text": " And then it's expecting this Elm program to give sort of a list of files to generate.", "tokens": [400, 550, 309, 311, 9650, 341, 2699, 76, 1461, 281, 976, 1333, 295, 257, 1329, 295, 7098, 281, 8460, 13], "temperature": 0.0, "avg_logprob": -0.28078200022379557, "compression_ratio": 1.7470817120622568, "no_speech_prob": 3.611934289438068e-06}, {"id": 489, "seek": 155748, "start": 1579.16, "end": 1583.46, "text": " And there's a little bit of like logic as to where it would actually write those.", "tokens": [400, 456, 311, 257, 707, 857, 295, 411, 9952, 382, 281, 689, 309, 576, 767, 2464, 729, 13], "temperature": 0.0, "avg_logprob": -0.28078200022379557, "compression_ratio": 1.7470817120622568, "no_speech_prob": 3.611934289438068e-06}, {"id": 490, "seek": 155748, "start": 1583.46, "end": 1585.56, "text": " But essentially all it can do is write files.", "tokens": [583, 4476, 439, 309, 393, 360, 307, 2464, 7098, 13], "temperature": 0.0, "avg_logprob": -0.28078200022379557, "compression_ratio": 1.7470817120622568, "no_speech_prob": 3.611934289438068e-06}, {"id": 491, "seek": 155748, "start": 1585.56, "end": 1586.56, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28078200022379557, "compression_ratio": 1.7470817120622568, "no_speech_prob": 3.611934289438068e-06}, {"id": 492, "seek": 158656, "start": 1586.56, "end": 1594.12, "text": " And yeah, so you it's very bare bones as far as supporting arbitrary CLI stuff.", "tokens": [400, 1338, 11, 370, 291, 309, 311, 588, 6949, 10491, 382, 1400, 382, 7231, 23211, 12855, 40, 1507, 13], "temperature": 0.0, "avg_logprob": -0.2905398989186704, "compression_ratio": 1.5346938775510204, "no_speech_prob": 7.224371074698865e-07}, {"id": 493, "seek": 158656, "start": 1594.12, "end": 1598.52, "text": " So writing something more to basically gather more data or maybe, you know, like for an", "tokens": [407, 3579, 746, 544, 281, 1936, 5448, 544, 1412, 420, 1310, 11, 291, 458, 11, 411, 337, 364], "temperature": 0.0, "avg_logprob": -0.2905398989186704, "compression_ratio": 1.5346938775510204, "no_speech_prob": 7.224371074698865e-07}, {"id": 494, "seek": 158656, "start": 1598.52, "end": 1601.94, "text": " example, Elm GQL does not use Elm code gen run.", "tokens": [1365, 11, 2699, 76, 460, 13695, 775, 406, 764, 2699, 76, 3089, 1049, 1190, 13], "temperature": 0.0, "avg_logprob": -0.2905398989186704, "compression_ratio": 1.5346938775510204, "no_speech_prob": 7.224371074698865e-07}, {"id": 495, "seek": 158656, "start": 1601.94, "end": 1607.9199999999998, "text": " It has its own sort of custom made TypeScript harness that's made for Elm, the Elm GQL thing", "tokens": [467, 575, 1080, 1065, 1333, 295, 2375, 1027, 15576, 14237, 19700, 300, 311, 1027, 337, 2699, 76, 11, 264, 2699, 76, 460, 13695, 551], "temperature": 0.0, "avg_logprob": -0.2905398989186704, "compression_ratio": 1.5346938775510204, "no_speech_prob": 7.224371074698865e-07}, {"id": 496, "seek": 158656, "start": 1607.9199999999998, "end": 1611.8, "text": " itself that does handle stuff a little differently.", "tokens": [2564, 300, 775, 4813, 1507, 257, 707, 7614, 13], "temperature": 0.0, "avg_logprob": -0.2905398989186704, "compression_ratio": 1.5346938775510204, "no_speech_prob": 7.224371074698865e-07}, {"id": 497, "seek": 158656, "start": 1611.8, "end": 1612.8799999999999, "text": " Right, exactly.", "tokens": [1779, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.2905398989186704, "compression_ratio": 1.5346938775510204, "no_speech_prob": 7.224371074698865e-07}, {"id": 498, "seek": 161288, "start": 1612.88, "end": 1620.1200000000001, "text": " So I think it's natural like Elm code gen, it's like if you try to build everything under", "tokens": [407, 286, 519, 309, 311, 3303, 411, 2699, 76, 3089, 1049, 11, 309, 311, 411, 498, 291, 853, 281, 1322, 1203, 833], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 499, "seek": 161288, "start": 1620.1200000000001, "end": 1622.68, "text": " the sun, what if you want to pull an HTTP data?", "tokens": [264, 3295, 11, 437, 498, 291, 528, 281, 2235, 364, 33283, 1412, 30], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 500, "seek": 161288, "start": 1622.68, "end": 1624.24, "text": " What if you want to read environment variables?", "tokens": [708, 498, 291, 528, 281, 1401, 2823, 9102, 30], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 501, "seek": 161288, "start": 1624.24, "end": 1630.5600000000002, "text": " What if instead of writing files, you want to make an HTTP request to post something", "tokens": [708, 498, 2602, 295, 3579, 7098, 11, 291, 528, 281, 652, 364, 33283, 5308, 281, 2183, 746], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 502, "seek": 161288, "start": 1630.5600000000002, "end": 1636.3200000000002, "text": " to an API or print it in the console or like it's a slippery slope and eventually you've", "tokens": [281, 364, 9362, 420, 4482, 309, 294, 264, 11076, 420, 411, 309, 311, 257, 28100, 13525, 293, 4728, 291, 600], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 503, "seek": 161288, "start": 1636.3200000000002, "end": 1638.48, "text": " built like a general purpose scripting.", "tokens": [3094, 411, 257, 2674, 4334, 5755, 278, 13], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 504, "seek": 161288, "start": 1638.48, "end": 1639.8000000000002, "text": " Yeah, right, right.", "tokens": [865, 11, 558, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 505, "seek": 161288, "start": 1639.8000000000002, "end": 1640.8000000000002, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.2627228944197945, "compression_ratio": 1.6589147286821706, "no_speech_prob": 1.679710749158403e-06}, {"id": 506, "seek": 164080, "start": 1640.8, "end": 1650.52, "text": " So basically like the Elm pages engine is a sort of general purpose way to bind to Node.js.", "tokens": [407, 1936, 411, 264, 2699, 76, 7183, 2848, 307, 257, 1333, 295, 2674, 4334, 636, 281, 14786, 281, 38640, 13, 25530, 13], "temperature": 0.0, "avg_logprob": -0.2594861337694071, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.6644166218356986e-07}, {"id": 507, "seek": 164080, "start": 1650.52, "end": 1654.3999999999999, "text": " So already with Elm pages data sources, you can read files, you can write files, you can", "tokens": [407, 1217, 365, 2699, 76, 7183, 1412, 7139, 11, 291, 393, 1401, 7098, 11, 291, 393, 2464, 7098, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.2594861337694071, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.6644166218356986e-07}, {"id": 508, "seek": 164080, "start": 1654.3999999999999, "end": 1659.6, "text": " read JSON files, you can read, you know, mark markdown content, you can do all these different", "tokens": [1401, 31828, 7098, 11, 291, 393, 1401, 11, 291, 458, 11, 1491, 1491, 5093, 2701, 11, 291, 393, 360, 439, 613, 819], "temperature": 0.0, "avg_logprob": -0.2594861337694071, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.6644166218356986e-07}, {"id": 509, "seek": 164080, "start": 1659.6, "end": 1660.6, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.2594861337694071, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.6644166218356986e-07}, {"id": 510, "seek": 164080, "start": 1660.6, "end": 1665.08, "text": " But you can also create custom Node.js bindings with data source.port.", "tokens": [583, 291, 393, 611, 1884, 2375, 38640, 13, 25530, 14786, 1109, 365, 1412, 4009, 13, 2707, 13], "temperature": 0.0, "avg_logprob": -0.2594861337694071, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.6644166218356986e-07}, {"id": 511, "seek": 164080, "start": 1665.08, "end": 1670.72, "text": " So you can write files and you can build arbitrary bindings where you write a JSON encoder to", "tokens": [407, 291, 393, 2464, 7098, 293, 291, 393, 1322, 23211, 14786, 1109, 689, 291, 2464, 257, 31828, 2058, 19866, 281], "temperature": 0.0, "avg_logprob": -0.2594861337694071, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.6644166218356986e-07}, {"id": 512, "seek": 167072, "start": 1670.72, "end": 1674.32, "text": " send data into an async Node.js function.", "tokens": [2845, 1412, 666, 364, 382, 34015, 38640, 13, 25530, 2445, 13], "temperature": 0.0, "avg_logprob": -0.20400284432076118, "compression_ratio": 1.7828054298642535, "no_speech_prob": 1.4677286230835307e-07}, {"id": 513, "seek": 167072, "start": 1674.32, "end": 1679.8, "text": " And then you return JSON in that async function and you write an Elm decoder to decode that", "tokens": [400, 550, 291, 2736, 31828, 294, 300, 382, 34015, 2445, 293, 291, 2464, 364, 2699, 76, 979, 19866, 281, 979, 1429, 300], "temperature": 0.0, "avg_logprob": -0.20400284432076118, "compression_ratio": 1.7828054298642535, "no_speech_prob": 1.4677286230835307e-07}, {"id": 514, "seek": 167072, "start": 1679.8, "end": 1680.8, "text": " response.", "tokens": [4134, 13], "temperature": 0.0, "avg_logprob": -0.20400284432076118, "compression_ratio": 1.7828054298642535, "no_speech_prob": 1.4677286230835307e-07}, {"id": 515, "seek": 167072, "start": 1680.8, "end": 1688.0, "text": " So Elm pages script, you do the Elm pages run command and it will run a module in your", "tokens": [407, 2699, 76, 7183, 5755, 11, 291, 360, 264, 2699, 76, 7183, 1190, 5622, 293, 309, 486, 1190, 257, 10088, 294, 428], "temperature": 0.0, "avg_logprob": -0.20400284432076118, "compression_ratio": 1.7828054298642535, "no_speech_prob": 1.4677286230835307e-07}, {"id": 516, "seek": 167072, "start": 1688.0, "end": 1691.64, "text": " script folder and execute that.", "tokens": [5755, 10820, 293, 14483, 300, 13], "temperature": 0.0, "avg_logprob": -0.20400284432076118, "compression_ratio": 1.7828054298642535, "no_speech_prob": 1.4677286230835307e-07}, {"id": 517, "seek": 167072, "start": 1691.64, "end": 1694.04, "text": " And you can use that to scaffold things.", "tokens": [400, 291, 393, 764, 300, 281, 44094, 721, 13], "temperature": 0.0, "avg_logprob": -0.20400284432076118, "compression_ratio": 1.7828054298642535, "no_speech_prob": 1.4677286230835307e-07}, {"id": 518, "seek": 167072, "start": 1694.04, "end": 1700.56, "text": " So if you wanted to use Elm pages to scaffold a new route module for an Elm pages app, you", "tokens": [407, 498, 291, 1415, 281, 764, 2699, 76, 7183, 281, 44094, 257, 777, 7955, 10088, 337, 364, 2699, 76, 7183, 724, 11, 291], "temperature": 0.0, "avg_logprob": -0.20400284432076118, "compression_ratio": 1.7828054298642535, "no_speech_prob": 1.4677286230835307e-07}, {"id": 519, "seek": 170056, "start": 1700.56, "end": 1702.1599999999999, "text": " can do that using Elm CodeGen.", "tokens": [393, 360, 300, 1228, 2699, 76, 15549, 26647, 13], "temperature": 0.0, "avg_logprob": -0.2497600827898298, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.5056867823659559e-06}, {"id": 520, "seek": 170056, "start": 1702.1599999999999, "end": 1707.8, "text": " But you can also, like the sky's the limit because it's just an arbitrary scripting tool.", "tokens": [583, 291, 393, 611, 11, 411, 264, 5443, 311, 264, 4948, 570, 309, 311, 445, 364, 23211, 5755, 278, 2290, 13], "temperature": 0.0, "avg_logprob": -0.2497600827898298, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.5056867823659559e-06}, {"id": 521, "seek": 170056, "start": 1707.8, "end": 1714.44, "text": " So you can do, so all you do is you write a module that exposes a top level value called", "tokens": [407, 291, 393, 360, 11, 370, 439, 291, 360, 307, 291, 2464, 257, 10088, 300, 1278, 4201, 257, 1192, 1496, 2158, 1219], "temperature": 0.0, "avg_logprob": -0.2497600827898298, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.5056867823659559e-06}, {"id": 522, "seek": 170056, "start": 1714.44, "end": 1717.24, "text": " run and it's basically just a data source.", "tokens": [1190, 293, 309, 311, 1936, 445, 257, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.2497600827898298, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.5056867823659559e-06}, {"id": 523, "seek": 170056, "start": 1717.24, "end": 1723.76, "text": " So if you, so you can do like data source.http to get HTTP data.", "tokens": [407, 498, 291, 11, 370, 291, 393, 360, 411, 1412, 4009, 13, 357, 83, 79, 281, 483, 33283, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2497600827898298, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.5056867823659559e-06}, {"id": 524, "seek": 170056, "start": 1723.76, "end": 1728.22, "text": " If that HTTP data fails, then your script just fails, which is fine.", "tokens": [759, 300, 33283, 1412, 18199, 11, 550, 428, 5755, 445, 18199, 11, 597, 307, 2489, 13], "temperature": 0.0, "avg_logprob": -0.2497600827898298, "compression_ratio": 1.6637931034482758, "no_speech_prob": 1.5056867823659559e-06}, {"id": 525, "seek": 172822, "start": 1728.22, "end": 1732.34, "text": " Like your script just stops and says, oops, I couldn't hit that HTTP endpoint.", "tokens": [1743, 428, 5755, 445, 10094, 293, 1619, 11, 34166, 11, 286, 2809, 380, 2045, 300, 33283, 35795, 13], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 526, "seek": 172822, "start": 1732.34, "end": 1734.16, "text": " So I'm super excited about this.", "tokens": [407, 286, 478, 1687, 2919, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 527, "seek": 172822, "start": 1734.16, "end": 1737.0, "text": " You can pull in environment variables, you can do all these things and you can use it", "tokens": [509, 393, 2235, 294, 2823, 9102, 11, 291, 393, 360, 439, 613, 721, 293, 291, 393, 764, 309], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 528, "seek": 172822, "start": 1737.0, "end": 1738.0, "text": " with Elm CodeGen.", "tokens": [365, 2699, 76, 15549, 26647, 13], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 529, "seek": 172822, "start": 1738.0, "end": 1743.72, "text": " So I kind of like, I built this thing that I kind of somewhat jokingly talked about in", "tokens": [407, 286, 733, 295, 411, 11, 286, 3094, 341, 551, 300, 286, 733, 295, 8344, 17396, 356, 2825, 466, 294], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 530, "seek": 172822, "start": 1743.72, "end": 1747.56, "text": " our Elm CodeGen episode of Elm pages CodeGen.", "tokens": [527, 2699, 76, 15549, 26647, 3500, 295, 2699, 76, 7183, 15549, 26647, 13], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 531, "seek": 172822, "start": 1747.56, "end": 1752.22, "text": " Now we just need to get Elm review in there so we can do that extractors in a context", "tokens": [823, 321, 445, 643, 281, 483, 2699, 76, 3131, 294, 456, 370, 321, 393, 360, 300, 8947, 830, 294, 257, 4319], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 532, "seek": 172822, "start": 1752.22, "end": 1753.84, "text": " where we have data sources.", "tokens": [689, 321, 362, 1412, 7139, 13], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 533, "seek": 172822, "start": 1753.84, "end": 1756.44, "text": " So yeah, I'm very excited about that.", "tokens": [407, 1338, 11, 286, 478, 588, 2919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 534, "seek": 172822, "start": 1756.44, "end": 1758.1200000000001, "text": " Excited to see what people do with it.", "tokens": [9368, 1226, 281, 536, 437, 561, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.1956333737624319, "compression_ratio": 1.738709677419355, "no_speech_prob": 6.577813564945245e-07}, {"id": 535, "seek": 175812, "start": 1758.12, "end": 1764.1999999999998, "text": " Another thing that I shipped this year that I'm really excited about was OpenAI Whisper", "tokens": [3996, 551, 300, 286, 25312, 341, 1064, 300, 286, 478, 534, 2919, 466, 390, 7238, 48698, 41132, 610], "temperature": 0.0, "avg_logprob": -0.21818128924503505, "compression_ratio": 1.6704980842911878, "no_speech_prob": 6.276595172494126e-07}, {"id": 536, "seek": 175812, "start": 1764.1999999999998, "end": 1769.12, "text": " is this voice transcription service that works very well for Elm Radio.", "tokens": [307, 341, 3177, 35288, 2643, 300, 1985, 588, 731, 337, 2699, 76, 17296, 13], "temperature": 0.0, "avg_logprob": -0.21818128924503505, "compression_ratio": 1.6704980842911878, "no_speech_prob": 6.276595172494126e-07}, {"id": 537, "seek": 175812, "start": 1769.12, "end": 1774.36, "text": " I tried several paid and free transcription services and they were pretty bad.", "tokens": [286, 3031, 2940, 4835, 293, 1737, 35288, 3328, 293, 436, 645, 1238, 1578, 13], "temperature": 0.0, "avg_logprob": -0.21818128924503505, "compression_ratio": 1.6704980842911878, "no_speech_prob": 6.276595172494126e-07}, {"id": 538, "seek": 175812, "start": 1774.36, "end": 1778.56, "text": " Half a year ago, a year ago, even the paid ones, there were some things I'm like, I definitely", "tokens": [15917, 257, 1064, 2057, 11, 257, 1064, 2057, 11, 754, 264, 4835, 2306, 11, 456, 645, 512, 721, 286, 478, 411, 11, 286, 2138], "temperature": 0.0, "avg_logprob": -0.21818128924503505, "compression_ratio": 1.6704980842911878, "no_speech_prob": 6.276595172494126e-07}, {"id": 539, "seek": 175812, "start": 1778.56, "end": 1783.76, "text": " don't want to put that up in a public place because that's a very embarrassing typo in", "tokens": [500, 380, 528, 281, 829, 300, 493, 294, 257, 1908, 1081, 570, 300, 311, 257, 588, 17299, 2125, 78, 294], "temperature": 0.0, "avg_logprob": -0.21818128924503505, "compression_ratio": 1.6704980842911878, "no_speech_prob": 6.276595172494126e-07}, {"id": 540, "seek": 175812, "start": 1783.76, "end": 1785.28, "text": " the transcript.", "tokens": [264, 24444, 13], "temperature": 0.0, "avg_logprob": -0.21818128924503505, "compression_ratio": 1.6704980842911878, "no_speech_prob": 6.276595172494126e-07}, {"id": 541, "seek": 178528, "start": 1785.28, "end": 1792.12, "text": " And then I tried OpenAI Whisper, this recently released like MIT licensed transcription project.", "tokens": [400, 550, 286, 3031, 7238, 48698, 41132, 610, 11, 341, 3938, 4736, 411, 13100, 25225, 35288, 1716, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 542, "seek": 178528, "start": 1792.12, "end": 1793.12, "text": " And it's amazing.", "tokens": [400, 309, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 543, "seek": 178528, "start": 1793.12, "end": 1794.72, "text": " The quality is so good.", "tokens": [440, 3125, 307, 370, 665, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 544, "seek": 178528, "start": 1794.72, "end": 1802.8799999999999, "text": " So now Elm Radio, the ElmRadio.com website has transcripts powered by OpenAI Whisper.", "tokens": [407, 586, 2699, 76, 17296, 11, 264, 2699, 76, 48444, 1004, 13, 1112, 3144, 575, 24444, 82, 17786, 538, 7238, 48698, 41132, 610, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 545, "seek": 178528, "start": 1802.8799999999999, "end": 1803.8799999999999, "text": " They're great.", "tokens": [814, 434, 869, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 546, "seek": 178528, "start": 1803.8799999999999, "end": 1805.82, "text": " And you can, there's an audio player in there.", "tokens": [400, 291, 393, 11, 456, 311, 364, 6278, 4256, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 547, "seek": 178528, "start": 1805.82, "end": 1807.8, "text": " You can click to a timestamp.", "tokens": [509, 393, 2052, 281, 257, 49108, 1215, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 548, "seek": 178528, "start": 1807.8, "end": 1809.12, "text": " You can share that timestamp.", "tokens": [509, 393, 2073, 300, 49108, 1215, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 549, "seek": 178528, "start": 1809.12, "end": 1811.04, "text": " So pretty excited about that.", "tokens": [407, 1238, 2919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 550, "seek": 178528, "start": 1811.04, "end": 1812.04, "text": " That's really cool.", "tokens": [663, 311, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.19405664949335605, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.866906865823694e-07}, {"id": 551, "seek": 181204, "start": 1812.04, "end": 1815.72, "text": " I've been curious about the transcription stuff.", "tokens": [286, 600, 668, 6369, 466, 264, 35288, 1507, 13], "temperature": 0.0, "avg_logprob": -0.21913020982654816, "compression_ratio": 1.6506024096385543, "no_speech_prob": 8.31513148114027e-07}, {"id": 552, "seek": 181204, "start": 1815.72, "end": 1822.68, "text": " Does Whisper do, does it track or is it able to distinguish like who's talking?", "tokens": [4402, 41132, 610, 360, 11, 775, 309, 2837, 420, 307, 309, 1075, 281, 20206, 411, 567, 311, 1417, 30], "temperature": 0.0, "avg_logprob": -0.21913020982654816, "compression_ratio": 1.6506024096385543, "no_speech_prob": 8.31513148114027e-07}, {"id": 553, "seek": 181204, "start": 1822.68, "end": 1823.84, "text": " Not right now.", "tokens": [1726, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.21913020982654816, "compression_ratio": 1.6506024096385543, "no_speech_prob": 8.31513148114027e-07}, {"id": 554, "seek": 181204, "start": 1823.84, "end": 1829.36, "text": " There are some, there are some experiments that try to build in that functionality.", "tokens": [821, 366, 512, 11, 456, 366, 512, 12050, 300, 853, 281, 1322, 294, 300, 14980, 13], "temperature": 0.0, "avg_logprob": -0.21913020982654816, "compression_ratio": 1.6506024096385543, "no_speech_prob": 8.31513148114027e-07}, {"id": 555, "seek": 181204, "start": 1829.36, "end": 1833.2, "text": " So I think we could see, I mean, the fact that it's open source, I think we could see", "tokens": [407, 286, 519, 321, 727, 536, 11, 286, 914, 11, 264, 1186, 300, 309, 311, 1269, 4009, 11, 286, 519, 321, 727, 536], "temperature": 0.0, "avg_logprob": -0.21913020982654816, "compression_ratio": 1.6506024096385543, "no_speech_prob": 8.31513148114027e-07}, {"id": 556, "seek": 181204, "start": 1833.2, "end": 1835.84, "text": " some cool things emerging in the future for that.", "tokens": [512, 1627, 721, 14989, 294, 264, 2027, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.21913020982654816, "compression_ratio": 1.6506024096385543, "no_speech_prob": 8.31513148114027e-07}, {"id": 557, "seek": 181204, "start": 1835.84, "end": 1838.56, "text": " But right now there's no stable way to do that.", "tokens": [583, 558, 586, 456, 311, 572, 8351, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.21913020982654816, "compression_ratio": 1.6506024096385543, "no_speech_prob": 8.31513148114027e-07}, {"id": 558, "seek": 183856, "start": 1838.56, "end": 1843.0, "text": " I mean, this is evolving, just wait two weeks and it's there.", "tokens": [286, 914, 11, 341, 307, 21085, 11, 445, 1699, 732, 3259, 293, 309, 311, 456, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 559, "seek": 183856, "start": 1843.0, "end": 1844.0, "text": " Yeah, yeah, yeah, sure.", "tokens": [865, 11, 1338, 11, 1338, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 560, "seek": 183856, "start": 1844.0, "end": 1845.0, "text": " Yeah, I think so.", "tokens": [865, 11, 286, 519, 370, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 561, "seek": 183856, "start": 1845.0, "end": 1850.32, "text": " It's like for me, once that hits, I have an app idea that I don't want to talk about right", "tokens": [467, 311, 411, 337, 385, 11, 1564, 300, 8664, 11, 286, 362, 364, 724, 1558, 300, 286, 500, 380, 528, 281, 751, 466, 558], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 562, "seek": 183856, "start": 1850.32, "end": 1851.32, "text": " now.", "tokens": [586, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 563, "seek": 183856, "start": 1851.32, "end": 1852.32, "text": " Ooh, intriguing.", "tokens": [7951, 11, 32503, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 564, "seek": 183856, "start": 1852.32, "end": 1853.32, "text": " We'll save that for 2023.", "tokens": [492, 603, 3155, 300, 337, 44377, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 565, "seek": 183856, "start": 1853.32, "end": 1854.32, "text": " Yeah, yeah, we will.", "tokens": [865, 11, 1338, 11, 321, 486, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 566, "seek": 183856, "start": 1854.32, "end": 1855.32, "text": " It'll be good.", "tokens": [467, 603, 312, 665, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 567, "seek": 183856, "start": 1855.32, "end": 1856.32, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 568, "seek": 183856, "start": 1856.32, "end": 1858.32, "text": " See, it's so good.", "tokens": [3008, 11, 309, 311, 370, 665, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 569, "seek": 183856, "start": 1858.32, "end": 1864.8799999999999, "text": " And it, little details in your transcripts that a human would catch, but most transcription", "tokens": [400, 309, 11, 707, 4365, 294, 428, 24444, 82, 300, 257, 1952, 576, 3745, 11, 457, 881, 35288], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 570, "seek": 183856, "start": 1864.8799999999999, "end": 1866.9199999999998, "text": " services won't, it nails.", "tokens": [3328, 1582, 380, 11, 309, 15394, 13], "temperature": 0.0, "avg_logprob": -0.3577966622427, "compression_ratio": 1.6130268199233717, "no_speech_prob": 4.381802227726439e-07}, {"id": 571, "seek": 186692, "start": 1866.92, "end": 1873.96, "text": " For example, if you have a false start in a sentence, it removes the false start and", "tokens": [1171, 1365, 11, 498, 291, 362, 257, 7908, 722, 294, 257, 8174, 11, 309, 30445, 264, 7908, 722, 293], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 572, "seek": 186692, "start": 1873.96, "end": 1877.4, "text": " removes duplicate words and things like that and just figures it out.", "tokens": [30445, 23976, 2283, 293, 721, 411, 300, 293, 445, 9624, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 573, "seek": 186692, "start": 1877.4, "end": 1878.6000000000001, "text": " It's pretty brilliant.", "tokens": [467, 311, 1238, 10248, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 574, "seek": 186692, "start": 1878.6000000000001, "end": 1879.6000000000001, "text": " So interesting.", "tokens": [407, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 575, "seek": 186692, "start": 1879.6000000000001, "end": 1882.0, "text": " That's been, that's been a lot of fun to build.", "tokens": [663, 311, 668, 11, 300, 311, 668, 257, 688, 295, 1019, 281, 1322, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 576, "seek": 186692, "start": 1882.0, "end": 1883.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 577, "seek": 186692, "start": 1883.0, "end": 1884.0, "text": " I'll have to play with that.", "tokens": [286, 603, 362, 281, 862, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 578, "seek": 186692, "start": 1884.0, "end": 1885.0, "text": " That's great.", "tokens": [663, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 579, "seek": 186692, "start": 1885.0, "end": 1886.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 580, "seek": 186692, "start": 1886.0, "end": 1887.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 581, "seek": 186692, "start": 1887.0, "end": 1893.1200000000001, "text": " Well, did anybody have any, any trends that they noticed happening in Elm in 2022?", "tokens": [1042, 11, 630, 4472, 362, 604, 11, 604, 13892, 300, 436, 5694, 2737, 294, 2699, 76, 294, 20229, 30], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 582, "seek": 186692, "start": 1893.1200000000001, "end": 1896.3600000000001, "text": " Any, any cool projects that they saw other people shipping?", "tokens": [2639, 11, 604, 1627, 4455, 300, 436, 1866, 661, 561, 14122, 30], "temperature": 0.0, "avg_logprob": -0.22733733837421125, "compression_ratio": 1.618705035971223, "no_speech_prob": 9.27608539313951e-07}, {"id": 583, "seek": 189636, "start": 1896.36, "end": 1900.52, "text": " Any trends in the way people are building things in 2022 in Elm?", "tokens": [2639, 13892, 294, 264, 636, 561, 366, 2390, 721, 294, 20229, 294, 2699, 76, 30], "temperature": 0.0, "avg_logprob": -0.24736013250835873, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.5779347677380429e-06}, {"id": 584, "seek": 189636, "start": 1900.52, "end": 1905.9199999999998, "text": " I think if we, if we took a sample size of the Elm developers on this call, I'd say a", "tokens": [286, 519, 498, 321, 11, 498, 321, 1890, 257, 6889, 2744, 295, 264, 2699, 76, 8849, 322, 341, 818, 11, 286, 1116, 584, 257], "temperature": 0.0, "avg_logprob": -0.24736013250835873, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.5779347677380429e-06}, {"id": 585, "seek": 189636, "start": 1905.9199999999998, "end": 1909.76, "text": " hundred percent of Elm developers are making Elm tooling.", "tokens": [3262, 3043, 295, 2699, 76, 8849, 366, 1455, 2699, 76, 46593, 13], "temperature": 0.0, "avg_logprob": -0.24736013250835873, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.5779347677380429e-06}, {"id": 586, "seek": 189636, "start": 1909.76, "end": 1914.9199999999998, "text": " That seems to be, I mean, if that's not the trend, then I'm not sure what is.", "tokens": [663, 2544, 281, 312, 11, 286, 914, 11, 498, 300, 311, 406, 264, 6028, 11, 550, 286, 478, 406, 988, 437, 307, 13], "temperature": 0.0, "avg_logprob": -0.24736013250835873, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.5779347677380429e-06}, {"id": 587, "seek": 189636, "start": 1914.9199999999998, "end": 1915.9199999999998, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.24736013250835873, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.5779347677380429e-06}, {"id": 588, "seek": 189636, "start": 1915.9199999999998, "end": 1923.9599999999998, "text": " But yeah, no, I, I, I'm pretty, I'm pretty bullish on, on Elm tools, expanding the Elm", "tokens": [583, 1338, 11, 572, 11, 286, 11, 286, 11, 286, 478, 1238, 11, 286, 478, 1238, 38692, 322, 11, 322, 2699, 76, 3873, 11, 14702, 264, 2699, 76], "temperature": 0.0, "avg_logprob": -0.24736013250835873, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.5779347677380429e-06}, {"id": 589, "seek": 192396, "start": 1923.96, "end": 1927.72, "text": " language surface area, which I think is what has kind of been happening.", "tokens": [2856, 3753, 1859, 11, 597, 286, 519, 307, 437, 575, 733, 295, 668, 2737, 13], "temperature": 0.0, "avg_logprob": -0.2493069839477539, "compression_ratio": 1.6577777777777778, "no_speech_prob": 3.7259806049405597e-06}, {"id": 590, "seek": 192396, "start": 1927.72, "end": 1933.08, "text": " And I'm not, I'm not sure it's like, I mean, I think it's, it feels to me like it's becoming", "tokens": [400, 286, 478, 406, 11, 286, 478, 406, 988, 309, 311, 411, 11, 286, 914, 11, 286, 519, 309, 311, 11, 309, 3417, 281, 385, 411, 309, 311, 5617], "temperature": 0.0, "avg_logprob": -0.2493069839477539, "compression_ratio": 1.6577777777777778, "no_speech_prob": 3.7259806049405597e-06}, {"id": 591, "seek": 192396, "start": 1933.08, "end": 1939.48, "text": " increasingly obvious that that is a really cool way to go and we can get so much out", "tokens": [12980, 6322, 300, 300, 307, 257, 534, 1627, 636, 281, 352, 293, 321, 393, 483, 370, 709, 484], "temperature": 0.0, "avg_logprob": -0.2493069839477539, "compression_ratio": 1.6577777777777778, "no_speech_prob": 3.7259806049405597e-06}, {"id": 592, "seek": 192396, "start": 1939.48, "end": 1940.48, "text": " of the Elm language.", "tokens": [295, 264, 2699, 76, 2856, 13], "temperature": 0.0, "avg_logprob": -0.2493069839477539, "compression_ratio": 1.6577777777777778, "no_speech_prob": 3.7259806049405597e-06}, {"id": 593, "seek": 192396, "start": 1940.48, "end": 1947.1200000000001, "text": " I wonder if maybe it wasn't so obvious originally as a community, maybe we kind of thought originally", "tokens": [286, 2441, 498, 1310, 309, 2067, 380, 370, 6322, 7993, 382, 257, 1768, 11, 1310, 321, 733, 295, 1194, 7993], "temperature": 0.0, "avg_logprob": -0.2493069839477539, "compression_ratio": 1.6577777777777778, "no_speech_prob": 3.7259806049405597e-06}, {"id": 594, "seek": 194712, "start": 1947.12, "end": 1955.1599999999999, "text": " like, oh, it will, you know, Elm will evolve in really awesome ways once Evan does XYZ.", "tokens": [411, 11, 1954, 11, 309, 486, 11, 291, 458, 11, 2699, 76, 486, 16693, 294, 534, 3476, 2098, 1564, 22613, 775, 48826, 57, 13], "temperature": 0.0, "avg_logprob": -0.28825909033753816, "compression_ratio": 1.643884892086331, "no_speech_prob": 4.0293002712132875e-06}, {"id": 595, "seek": 194712, "start": 1955.1599999999999, "end": 1958.4799999999998, "text": " And I'm not sure I, I don't feel that as much anymore.", "tokens": [400, 286, 478, 406, 988, 286, 11, 286, 500, 380, 841, 300, 382, 709, 3602, 13], "temperature": 0.0, "avg_logprob": -0.28825909033753816, "compression_ratio": 1.643884892086331, "no_speech_prob": 4.0293002712132875e-06}, {"id": 596, "seek": 194712, "start": 1958.4799999999998, "end": 1963.1599999999999, "text": " Like I think this year I've seen so many people kind of run with awesome tools that don't", "tokens": [1743, 286, 519, 341, 1064, 286, 600, 1612, 370, 867, 561, 733, 295, 1190, 365, 3476, 3873, 300, 500, 380], "temperature": 0.0, "avg_logprob": -0.28825909033753816, "compression_ratio": 1.643884892086331, "no_speech_prob": 4.0293002712132875e-06}, {"id": 597, "seek": 194712, "start": 1963.1599999999999, "end": 1966.4799999999998, "text": " conflict with the Elm language, but to kind of empower it.", "tokens": [6596, 365, 264, 2699, 76, 2856, 11, 457, 281, 733, 295, 11071, 309, 13], "temperature": 0.0, "avg_logprob": -0.28825909033753816, "compression_ratio": 1.643884892086331, "no_speech_prob": 4.0293002712132875e-06}, {"id": 598, "seek": 194712, "start": 1966.4799999999998, "end": 1968.52, "text": " I'm like, those really, really lovely primitives.", "tokens": [286, 478, 411, 11, 729, 534, 11, 534, 7496, 2886, 38970, 13], "temperature": 0.0, "avg_logprob": -0.28825909033753816, "compression_ratio": 1.643884892086331, "no_speech_prob": 4.0293002712132875e-06}, {"id": 599, "seek": 194712, "start": 1968.52, "end": 1974.28, "text": " I feel like it's been a year of discovering the joy of Elm's primitives in, in, in other", "tokens": [286, 841, 411, 309, 311, 668, 257, 1064, 295, 24773, 264, 6258, 295, 2699, 76, 311, 2886, 38970, 294, 11, 294, 11, 294, 661], "temperature": 0.0, "avg_logprob": -0.28825909033753816, "compression_ratio": 1.643884892086331, "no_speech_prob": 4.0293002712132875e-06}, {"id": 600, "seek": 194712, "start": 1974.28, "end": 1976.3999999999999, "text": " contexts, at least for me.", "tokens": [30628, 11, 412, 1935, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.28825909033753816, "compression_ratio": 1.643884892086331, "no_speech_prob": 4.0293002712132875e-06}, {"id": 601, "seek": 197640, "start": 1976.4, "end": 1977.4, "text": " Yeah, totally.", "tokens": [865, 11, 3879, 13], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 602, "seek": 197640, "start": 1977.4, "end": 1981.96, "text": " I mean, a decent number of my tools, or at least two of them that I can think of have", "tokens": [286, 914, 11, 257, 8681, 1230, 295, 452, 3873, 11, 420, 412, 1935, 732, 295, 552, 300, 286, 393, 519, 295, 362], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 603, "seek": 197640, "start": 1981.96, "end": 1988.0, "text": " that sort of could have been a, I'm irritated that Elm doesn't have this conversation, but", "tokens": [300, 1333, 295, 727, 362, 668, 257, 11, 286, 478, 43650, 300, 2699, 76, 1177, 380, 362, 341, 3761, 11, 457], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 604, "seek": 197640, "start": 1988.0, "end": 1990.1200000000001, "text": " turned into a like, oh, I made this project.", "tokens": [3574, 666, 257, 411, 11, 1954, 11, 286, 1027, 341, 1716, 13], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 605, "seek": 197640, "start": 1990.1200000000001, "end": 1991.1200000000001, "text": " It just works.", "tokens": [467, 445, 1985, 13], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 606, "seek": 197640, "start": 1991.1200000000001, "end": 1992.1200000000001, "text": " You can have this benefit.", "tokens": [509, 393, 362, 341, 5121, 13], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 607, "seek": 197640, "start": 1992.1200000000001, "end": 1996.76, "text": " So the two, two tools that talking about is Elm Optimize Level 2, which was actually,", "tokens": [407, 264, 732, 11, 732, 3873, 300, 1417, 466, 307, 2699, 76, 35013, 1125, 16872, 568, 11, 597, 390, 767, 11], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 608, "seek": 197640, "start": 1996.76, "end": 1998.1000000000001, "text": " I believe last year.", "tokens": [286, 1697, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 609, "seek": 197640, "start": 1998.1000000000001, "end": 2005.24, "text": " So that will optimize Elm generated JS a bit further and actually get some surprising results.", "tokens": [407, 300, 486, 19719, 2699, 76, 10833, 33063, 257, 857, 3052, 293, 767, 483, 512, 8830, 3542, 13], "temperature": 0.0, "avg_logprob": -0.22306868187466958, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4058526832959615e-06}, {"id": 610, "seek": 200524, "start": 2005.24, "end": 2011.32, "text": " This is the project that Yerun has something like 48 open PRs for.", "tokens": [639, 307, 264, 1716, 300, 398, 260, 409, 575, 746, 411, 11174, 1269, 11568, 82, 337, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 611, "seek": 200524, "start": 2011.32, "end": 2013.08, "text": " And then there's the second one, which is Elm Code Gen.", "tokens": [400, 550, 456, 311, 264, 1150, 472, 11, 597, 307, 2699, 76, 15549, 3632, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 612, "seek": 200524, "start": 2013.08, "end": 2015.96, "text": " Again, you'd be like, Elm doesn't have macros.", "tokens": [3764, 11, 291, 1116, 312, 411, 11, 2699, 76, 1177, 380, 362, 7912, 2635, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 613, "seek": 200524, "start": 2015.96, "end": 2016.96, "text": " Elm doesn't have a thing.", "tokens": [2699, 76, 1177, 380, 362, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 614, "seek": 200524, "start": 2016.96, "end": 2021.76, "text": " It's like, oh, we could just have a library and like a little bit of node JS and like,", "tokens": [467, 311, 411, 11, 1954, 11, 321, 727, 445, 362, 257, 6405, 293, 411, 257, 707, 857, 295, 9984, 33063, 293, 411, 11], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 615, "seek": 200524, "start": 2021.76, "end": 2023.2, "text": " oh, we have a thing.", "tokens": [1954, 11, 321, 362, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 616, "seek": 200524, "start": 2023.2, "end": 2024.44, "text": " Actually turns out to be pretty nice.", "tokens": [5135, 4523, 484, 281, 312, 1238, 1481, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 617, "seek": 200524, "start": 2024.44, "end": 2025.44, "text": " Okay, cool.", "tokens": [1033, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 618, "seek": 200524, "start": 2025.44, "end": 2027.0, "text": " We don't need a language change.", "tokens": [492, 500, 380, 643, 257, 2856, 1319, 13], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 619, "seek": 200524, "start": 2027.0, "end": 2032.64, "text": " In fact, we might even, well, you never say never, like, you know, who knows, but, but", "tokens": [682, 1186, 11, 321, 1062, 754, 11, 731, 11, 291, 1128, 584, 1128, 11, 411, 11, 291, 458, 11, 567, 3255, 11, 457, 11, 457], "temperature": 0.0, "avg_logprob": -0.29502615175749125, "compression_ratio": 1.6596491228070176, "no_speech_prob": 1.1910797184100375e-06}, {"id": 620, "seek": 203264, "start": 2032.64, "end": 2036.8000000000002, "text": " it's not obvious that having something like Code Gen built into the language would be", "tokens": [309, 311, 406, 6322, 300, 1419, 746, 411, 15549, 3632, 3094, 666, 264, 2856, 576, 312], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 621, "seek": 203264, "start": 2036.8000000000002, "end": 2037.8000000000002, "text": " the way to go.", "tokens": [264, 636, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 622, "seek": 203264, "start": 2037.8000000000002, "end": 2038.8000000000002, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 623, "seek": 203264, "start": 2038.8000000000002, "end": 2039.8000000000002, "text": " But yeah.", "tokens": [583, 1338, 13], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 624, "seek": 203264, "start": 2039.8000000000002, "end": 2043.6000000000001, "text": " I've felt that way about this Elm pages script thing too.", "tokens": [286, 600, 2762, 300, 636, 466, 341, 2699, 76, 7183, 5755, 551, 886, 13], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 625, "seek": 203264, "start": 2043.6000000000001, "end": 2048.84, "text": " I mean, to me, that's just a more clear illustration of this more general thing, which is like", "tokens": [286, 914, 11, 281, 385, 11, 300, 311, 445, 257, 544, 1850, 22645, 295, 341, 544, 2674, 551, 11, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 626, "seek": 203264, "start": 2048.84, "end": 2056.6400000000003, "text": " Elm is a really good target for frameworks because you can define these effects as data", "tokens": [2699, 76, 307, 257, 534, 665, 3779, 337, 29834, 570, 291, 393, 6964, 613, 5065, 382, 1412], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 627, "seek": 203264, "start": 2056.6400000000003, "end": 2061.6, "text": " and then execute them however you want and swap out optimizations under the hood.", "tokens": [293, 550, 14483, 552, 4461, 291, 528, 293, 18135, 484, 5028, 14455, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.22865501336291827, "compression_ratio": 1.602189781021898, "no_speech_prob": 4.247011133884371e-07}, {"id": 628, "seek": 206160, "start": 2061.6, "end": 2068.72, "text": " So it's very good for, for building frameworks and, and like, you don't need to wait for", "tokens": [407, 309, 311, 588, 665, 337, 11, 337, 2390, 29834, 293, 11, 293, 411, 11, 291, 500, 380, 643, 281, 1699, 337], "temperature": 0.0, "avg_logprob": -0.21779908452715194, "compression_ratio": 1.7456140350877194, "no_speech_prob": 1.275184047244693e-07}, {"id": 629, "seek": 206160, "start": 2068.72, "end": 2076.2, "text": " Elm to have some binding to node JS because well, like if, if you have effects as data,", "tokens": [2699, 76, 281, 362, 512, 17359, 281, 9984, 33063, 570, 731, 11, 411, 498, 11, 498, 291, 362, 5065, 382, 1412, 11], "temperature": 0.0, "avg_logprob": -0.21779908452715194, "compression_ratio": 1.7456140350877194, "no_speech_prob": 1.275184047244693e-07}, {"id": 630, "seek": 206160, "start": 2076.2, "end": 2083.24, "text": " then you say, here's this, here's this type that represents an effect to do in node JS.", "tokens": [550, 291, 584, 11, 510, 311, 341, 11, 510, 311, 341, 2010, 300, 8855, 364, 1802, 281, 360, 294, 9984, 33063, 13], "temperature": 0.0, "avg_logprob": -0.21779908452715194, "compression_ratio": 1.7456140350877194, "no_speech_prob": 1.275184047244693e-07}, {"id": 631, "seek": 206160, "start": 2083.24, "end": 2084.24, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.21779908452715194, "compression_ratio": 1.7456140350877194, "no_speech_prob": 1.275184047244693e-07}, {"id": 632, "seek": 206160, "start": 2084.24, "end": 2085.6, "text": " You don't need Elm to build it.", "tokens": [509, 500, 380, 643, 2699, 76, 281, 1322, 309, 13], "temperature": 0.0, "avg_logprob": -0.21779908452715194, "compression_ratio": 1.7456140350877194, "no_speech_prob": 1.275184047244693e-07}, {"id": 633, "seek": 206160, "start": 2085.6, "end": 2089.16, "text": " You just need to build some glue that does some code generation and has a framework to", "tokens": [509, 445, 643, 281, 1322, 512, 8998, 300, 775, 512, 3089, 5125, 293, 575, 257, 8388, 281], "temperature": 0.0, "avg_logprob": -0.21779908452715194, "compression_ratio": 1.7456140350877194, "no_speech_prob": 1.275184047244693e-07}, {"id": 634, "seek": 208916, "start": 2089.16, "end": 2091.64, "text": " abstract that away from the user.", "tokens": [12649, 300, 1314, 490, 264, 4195, 13], "temperature": 0.0, "avg_logprob": -0.2395358657836914, "compression_ratio": 1.7069767441860466, "no_speech_prob": 2.0061524708125944e-07}, {"id": 635, "seek": 208916, "start": 2091.64, "end": 2095.72, "text": " But Elm is a language is completely capable of that.", "tokens": [583, 2699, 76, 307, 257, 2856, 307, 2584, 8189, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.2395358657836914, "compression_ratio": 1.7069767441860466, "no_speech_prob": 2.0061524708125944e-07}, {"id": 636, "seek": 208916, "start": 2095.72, "end": 2103.56, "text": " Just like JavaScript as a language is capable of interfacing with C++ bindings through node", "tokens": [1449, 411, 15778, 382, 257, 2856, 307, 8189, 295, 14510, 5615, 365, 383, 25472, 14786, 1109, 807, 9984], "temperature": 0.0, "avg_logprob": -0.2395358657836914, "compression_ratio": 1.7069767441860466, "no_speech_prob": 2.0061524708125944e-07}, {"id": 637, "seek": 208916, "start": 2103.56, "end": 2104.56, "text": " JS.", "tokens": [33063, 13], "temperature": 0.0, "avg_logprob": -0.2395358657836914, "compression_ratio": 1.7069767441860466, "no_speech_prob": 2.0061524708125944e-07}, {"id": 638, "seek": 208916, "start": 2104.56, "end": 2107.04, "text": " You don't need the JavaScript language.", "tokens": [509, 500, 380, 643, 264, 15778, 2856, 13], "temperature": 0.0, "avg_logprob": -0.2395358657836914, "compression_ratio": 1.7069767441860466, "no_speech_prob": 2.0061524708125944e-07}, {"id": 639, "seek": 208916, "start": 2107.04, "end": 2112.98, "text": " You don't need V8 to have bindings to executing C++ code.", "tokens": [509, 500, 380, 643, 691, 23, 281, 362, 14786, 1109, 281, 32368, 383, 25472, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2395358657836914, "compression_ratio": 1.7069767441860466, "no_speech_prob": 2.0061524708125944e-07}, {"id": 640, "seek": 208916, "start": 2112.98, "end": 2118.44, "text": " You just need a JavaScript language that has a way to, you know, run promises and, uh,", "tokens": [509, 445, 643, 257, 15778, 2856, 300, 575, 257, 636, 281, 11, 291, 458, 11, 1190, 16403, 293, 11, 2232, 11], "temperature": 0.0, "avg_logprob": -0.2395358657836914, "compression_ratio": 1.7069767441860466, "no_speech_prob": 2.0061524708125944e-07}, {"id": 641, "seek": 211844, "start": 2118.44, "end": 2119.44, "text": " and hook into that.", "tokens": [293, 6328, 666, 300, 13], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 642, "seek": 211844, "start": 2119.44, "end": 2125.2000000000003, "text": " So yeah, Elm, I like Mario's phrase that he's used in the past of Elm in unusual places.", "tokens": [407, 1338, 11, 2699, 76, 11, 286, 411, 9343, 311, 9535, 300, 415, 311, 1143, 294, 264, 1791, 295, 2699, 76, 294, 10901, 3190, 13], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 643, "seek": 211844, "start": 2125.2000000000003, "end": 2126.96, "text": " And I'm, I'm also very bullish on this.", "tokens": [400, 286, 478, 11, 286, 478, 611, 588, 38692, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 644, "seek": 211844, "start": 2126.96, "end": 2131.4, "text": " I think it's, I think we're, we're really seeing that come to fruition and I think there's", "tokens": [286, 519, 309, 311, 11, 286, 519, 321, 434, 11, 321, 434, 534, 2577, 300, 808, 281, 48738, 293, 286, 519, 456, 311], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 645, "seek": 211844, "start": 2131.4, "end": 2132.4, "text": " going to be more of it.", "tokens": [516, 281, 312, 544, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 646, "seek": 211844, "start": 2132.4, "end": 2137.84, "text": " I think that for a long time, Elm developers were a bit scared of what would change in", "tokens": [286, 519, 300, 337, 257, 938, 565, 11, 2699, 76, 8849, 645, 257, 857, 5338, 295, 437, 576, 1319, 294], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 647, "seek": 211844, "start": 2137.84, "end": 2139.4, "text": " Elm 0.20.", "tokens": [2699, 76, 1958, 13, 2009, 13], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 648, "seek": 211844, "start": 2139.4, "end": 2145.4, "text": " And as you said, Mark, Emmet, that we're waiting for Evan to do something.", "tokens": [400, 382, 291, 848, 11, 3934, 11, 28237, 302, 11, 300, 321, 434, 3806, 337, 22613, 281, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.23600934533511891, "compression_ratio": 1.6171003717472119, "no_speech_prob": 1.8447457250658772e-06}, {"id": 649, "seek": 214540, "start": 2145.4, "end": 2149.88, "text": " And now we're all realizing, well, actually we, we are seeing a lot of tools being released", "tokens": [400, 586, 321, 434, 439, 16734, 11, 731, 11, 767, 321, 11, 321, 366, 2577, 257, 688, 295, 3873, 885, 4736], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 650, "seek": 214540, "start": 2149.88, "end": 2152.32, "text": " and they're very cool and they're very powerful.", "tokens": [293, 436, 434, 588, 1627, 293, 436, 434, 588, 4005, 13], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 651, "seek": 214540, "start": 2152.32, "end": 2155.76, "text": " And we are actually have the freedom to do so.", "tokens": [400, 321, 366, 767, 362, 264, 5645, 281, 360, 370, 13], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 652, "seek": 214540, "start": 2155.76, "end": 2161.0, "text": " And we have the needs and we have, we have all those benefits from Elm, which allows", "tokens": [400, 321, 362, 264, 2203, 293, 321, 362, 11, 321, 362, 439, 729, 5311, 490, 2699, 76, 11, 597, 4045], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 653, "seek": 214540, "start": 2161.0, "end": 2163.08, "text": " us to make great tools.", "tokens": [505, 281, 652, 869, 3873, 13], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 654, "seek": 214540, "start": 2163.08, "end": 2164.88, "text": " So let's do it.", "tokens": [407, 718, 311, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 655, "seek": 214540, "start": 2164.88, "end": 2169.2400000000002, "text": " And yeah, that's what I'm noticing is that we are making more and more of these tools", "tokens": [400, 1338, 11, 300, 311, 437, 286, 478, 21814, 307, 300, 321, 366, 1455, 544, 293, 544, 295, 613, 3873], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 656, "seek": 214540, "start": 2169.2400000000002, "end": 2173.28, "text": " and the quality is amazing.", "tokens": [293, 264, 3125, 307, 2243, 13], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 657, "seek": 214540, "start": 2173.28, "end": 2174.64, "text": " Every time almost.", "tokens": [2048, 565, 1920, 13], "temperature": 0.0, "avg_logprob": -0.2206590576171875, "compression_ratio": 1.78714859437751, "no_speech_prob": 3.9276733332371805e-07}, {"id": 658, "seek": 217464, "start": 2174.64, "end": 2181.0, "text": " One of them that I'm thinking of is Elm CodeGen, obviously, but also Elm Watch from Simon Leider.", "tokens": [1485, 295, 552, 300, 286, 478, 1953, 295, 307, 2699, 76, 15549, 26647, 11, 2745, 11, 457, 611, 2699, 76, 7277, 490, 13193, 1456, 1438, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 659, "seek": 217464, "start": 2181.0, "end": 2182.0, "text": " Yes, totally.", "tokens": [1079, 11, 3879, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 660, "seek": 217464, "start": 2182.0, "end": 2183.7999999999997, "text": " That one is amazing.", "tokens": [663, 472, 307, 2243, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 661, "seek": 217464, "start": 2183.7999999999997, "end": 2184.7999999999997, "text": " Really fast.", "tokens": [4083, 2370, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 662, "seek": 217464, "start": 2184.7999999999997, "end": 2191.0, "text": " And just because it's using Elm's primitives under the, or it's making use of all the promises", "tokens": [400, 445, 570, 309, 311, 1228, 2699, 76, 311, 2886, 38970, 833, 264, 11, 420, 309, 311, 1455, 764, 295, 439, 264, 16403], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 663, "seek": 217464, "start": 2191.0, "end": 2194.08, "text": " that Elm makes and to make it really fast.", "tokens": [300, 2699, 76, 1669, 293, 281, 652, 309, 534, 2370, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 664, "seek": 217464, "start": 2194.08, "end": 2195.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 665, "seek": 217464, "start": 2195.08, "end": 2197.3199999999997, "text": " This is something that I want to see more and more of.", "tokens": [639, 307, 746, 300, 286, 528, 281, 536, 544, 293, 544, 295, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 666, "seek": 217464, "start": 2197.3199999999997, "end": 2198.3599999999997, "text": " I think we will.", "tokens": [286, 519, 321, 486, 13], "temperature": 0.0, "avg_logprob": -0.3025118282863072, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.3686341137363343e-06}, {"id": 667, "seek": 219836, "start": 2198.36, "end": 2204.88, "text": " Another trend that I saw happening in 2022 is that I feel like we're seeing more and", "tokens": [3996, 6028, 300, 286, 1866, 2737, 294, 20229, 307, 300, 286, 841, 411, 321, 434, 2577, 544, 293], "temperature": 0.0, "avg_logprob": -0.24581375472042538, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.186057129933033e-07}, {"id": 668, "seek": 219836, "start": 2204.88, "end": 2207.96, "text": " more tools to make applications.", "tokens": [544, 3873, 281, 652, 5821, 13], "temperature": 0.0, "avg_logprob": -0.24581375472042538, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.186057129933033e-07}, {"id": 669, "seek": 219836, "start": 2207.96, "end": 2216.84, "text": " Like we've had Elm SPA, we have Elm Pages, Elm Static, and Elm Lend is also new, which", "tokens": [1743, 321, 600, 632, 2699, 76, 8420, 32, 11, 321, 362, 2699, 76, 430, 1660, 11, 2699, 76, 745, 2399, 11, 293, 2699, 76, 441, 521, 307, 611, 777, 11, 597], "temperature": 0.0, "avg_logprob": -0.24581375472042538, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.186057129933033e-07}, {"id": 670, "seek": 219836, "start": 2216.84, "end": 2219.8, "text": " hasn't been released.", "tokens": [6132, 380, 668, 4736, 13], "temperature": 0.0, "avg_logprob": -0.24581375472042538, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.186057129933033e-07}, {"id": 671, "seek": 219836, "start": 2219.8, "end": 2222.04, "text": " It's still a beta, I think, at the moment.", "tokens": [467, 311, 920, 257, 9861, 11, 286, 519, 11, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.24581375472042538, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.186057129933033e-07}, {"id": 672, "seek": 219836, "start": 2222.04, "end": 2227.52, "text": " And I feel like we're seeing more and more of these and it's starting to be like, huh,", "tokens": [400, 286, 841, 411, 321, 434, 2577, 544, 293, 544, 295, 613, 293, 309, 311, 2891, 281, 312, 411, 11, 7020, 11], "temperature": 0.0, "avg_logprob": -0.24581375472042538, "compression_ratio": 1.679245283018868, "no_speech_prob": 8.186057129933033e-07}, {"id": 673, "seek": 222752, "start": 2227.52, "end": 2231.6, "text": " we need to make choices when we start an Elm application right now.", "tokens": [321, 643, 281, 652, 7994, 562, 321, 722, 364, 2699, 76, 3861, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 674, "seek": 222752, "start": 2231.6, "end": 2238.4, "text": " And that worries me a little bit, but a little bit of computation is also quite nice and", "tokens": [400, 300, 16340, 385, 257, 707, 857, 11, 457, 257, 707, 857, 295, 24903, 307, 611, 1596, 1481, 293], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 675, "seek": 222752, "start": 2238.4, "end": 2239.4, "text": " useful.", "tokens": [4420, 13], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 676, "seek": 222752, "start": 2239.4, "end": 2240.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 677, "seek": 222752, "start": 2240.4, "end": 2241.4, "text": " And LambdaRay, of course.", "tokens": [400, 45691, 35737, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 678, "seek": 222752, "start": 2241.4, "end": 2242.4, "text": " And LambdaRay, obviously.", "tokens": [400, 45691, 35737, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 679, "seek": 222752, "start": 2242.4, "end": 2243.4, "text": " LambdaRay is underlying, right?", "tokens": [45691, 35737, 307, 14217, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 680, "seek": 222752, "start": 2243.4, "end": 2249.6, "text": " I'm not going to, I'm going to make this problem worse by releasing LambdaRay Elm Pages and", "tokens": [286, 478, 406, 516, 281, 11, 286, 478, 516, 281, 652, 341, 1154, 5324, 538, 16327, 45691, 35737, 2699, 76, 430, 1660, 293], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 681, "seek": 222752, "start": 2249.6, "end": 2253.7599999999998, "text": " LambdaRay Elm SPA or LambdaRay Elm Land.", "tokens": [45691, 35737, 2699, 76, 8420, 32, 420, 45691, 35737, 2699, 76, 6607, 13], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 682, "seek": 222752, "start": 2253.7599999999998, "end": 2254.7599999999998, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3165108955512613, "compression_ratio": 1.708695652173913, "no_speech_prob": 3.393101906112861e-06}, {"id": 683, "seek": 225476, "start": 2254.76, "end": 2260.6800000000003, "text": " But it's like a nice type safe battle royale, you know?", "tokens": [583, 309, 311, 411, 257, 1481, 2010, 3273, 4635, 36364, 1220, 11, 291, 458, 30], "temperature": 0.0, "avg_logprob": -0.3167448811147405, "compression_ratio": 1.3673469387755102, "no_speech_prob": 6.375290126925393e-07}, {"id": 684, "seek": 225476, "start": 2260.6800000000003, "end": 2263.88, "text": " Like whichever one wins is going to be lovely.", "tokens": [1743, 24123, 472, 10641, 307, 516, 281, 312, 7496, 13], "temperature": 0.0, "avg_logprob": -0.3167448811147405, "compression_ratio": 1.3673469387755102, "no_speech_prob": 6.375290126925393e-07}, {"id": 685, "seek": 225476, "start": 2263.88, "end": 2267.5600000000004, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3167448811147405, "compression_ratio": 1.3673469387755102, "no_speech_prob": 6.375290126925393e-07}, {"id": 686, "seek": 225476, "start": 2267.5600000000004, "end": 2271.1200000000003, "text": " And similarly, I feel like we're seeing more and more UI frameworks as well.", "tokens": [400, 14138, 11, 286, 841, 411, 321, 434, 2577, 544, 293, 544, 15682, 29834, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.3167448811147405, "compression_ratio": 1.3673469387755102, "no_speech_prob": 6.375290126925393e-07}, {"id": 687, "seek": 225476, "start": 2271.1200000000003, "end": 2278.6400000000003, "text": " We've got ElmUI, ElmCSS, ElmTailwind, ElmHTML, all those variants with context.", "tokens": [492, 600, 658, 2699, 76, 46324, 11, 2699, 76, 34, 21929, 11, 2699, 76, 51, 864, 12199, 11, 2699, 76, 39, 51, 12683, 11, 439, 729, 21669, 365, 4319, 13], "temperature": 0.0, "avg_logprob": -0.3167448811147405, "compression_ratio": 1.3673469387755102, "no_speech_prob": 6.375290126925393e-07}, {"id": 688, "seek": 227864, "start": 2278.64, "end": 2284.8799999999997, "text": " We've got accessibility packages and more, George Boris also came on the show to talk", "tokens": [492, 600, 658, 15002, 17401, 293, 544, 11, 7136, 27158, 611, 1361, 322, 264, 855, 281, 751], "temperature": 0.0, "avg_logprob": -0.2614779525928283, "compression_ratio": 1.502439024390244, "no_speech_prob": 4.965140192325634e-07}, {"id": 689, "seek": 227864, "start": 2284.8799999999997, "end": 2286.96, "text": " about one of those, I think.", "tokens": [466, 472, 295, 729, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.2614779525928283, "compression_ratio": 1.502439024390244, "no_speech_prob": 4.965140192325634e-07}, {"id": 690, "seek": 227864, "start": 2286.96, "end": 2287.96, "text": " Elmbook.", "tokens": [2699, 76, 2939, 13], "temperature": 0.0, "avg_logprob": -0.2614779525928283, "compression_ratio": 1.502439024390244, "no_speech_prob": 4.965140192325634e-07}, {"id": 691, "seek": 227864, "start": 2287.96, "end": 2288.96, "text": " Elmbook.", "tokens": [2699, 76, 2939, 13], "temperature": 0.0, "avg_logprob": -0.2614779525928283, "compression_ratio": 1.502439024390244, "no_speech_prob": 4.965140192325634e-07}, {"id": 692, "seek": 227864, "start": 2288.96, "end": 2297.8399999999997, "text": " And he also released recently, but not, he didn't come on the show for this, ElmWidgets.", "tokens": [400, 415, 611, 4736, 3938, 11, 457, 406, 11, 415, 994, 380, 808, 322, 264, 855, 337, 341, 11, 2699, 76, 54, 327, 16284, 13], "temperature": 0.0, "avg_logprob": -0.2614779525928283, "compression_ratio": 1.502439024390244, "no_speech_prob": 4.965140192325634e-07}, {"id": 693, "seek": 227864, "start": 2297.8399999999997, "end": 2305.2799999999997, "text": " So there's a lot of these UI frameworks and components collections, which is both nice", "tokens": [407, 456, 311, 257, 688, 295, 613, 15682, 29834, 293, 6677, 16641, 11, 597, 307, 1293, 1481], "temperature": 0.0, "avg_logprob": -0.2614779525928283, "compression_ratio": 1.502439024390244, "no_speech_prob": 4.965140192325634e-07}, {"id": 694, "seek": 230528, "start": 2305.28, "end": 2311.52, "text": " and also like, again, like there's a little bit too much choice in my opinion, but that", "tokens": [293, 611, 411, 11, 797, 11, 411, 456, 311, 257, 707, 857, 886, 709, 3922, 294, 452, 4800, 11, 457, 300], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 695, "seek": 230528, "start": 2311.52, "end": 2314.44, "text": " can be very useful for adoption at least, I think.", "tokens": [393, 312, 588, 4420, 337, 19215, 412, 1935, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 696, "seek": 230528, "start": 2314.44, "end": 2317.36, "text": " And to get quick, quick started on a project.", "tokens": [400, 281, 483, 1702, 11, 1702, 1409, 322, 257, 1716, 13], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 697, "seek": 230528, "start": 2317.36, "end": 2318.36, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 698, "seek": 230528, "start": 2318.36, "end": 2322.0, "text": " And that leads into, I'm not sure if we want to transition to the future, but definitely", "tokens": [400, 300, 6689, 666, 11, 286, 478, 406, 988, 498, 321, 528, 281, 6034, 281, 264, 2027, 11, 457, 2138], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 699, "seek": 230528, "start": 2322.0, "end": 2324.4, "text": " leans into one of the projects I'm excited about.", "tokens": [476, 599, 666, 472, 295, 264, 4455, 286, 478, 2919, 466, 13], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 700, "seek": 230528, "start": 2324.4, "end": 2325.4, "text": " Go for it.", "tokens": [1037, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 701, "seek": 230528, "start": 2325.4, "end": 2326.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 702, "seek": 230528, "start": 2326.4, "end": 2333.1600000000003, "text": " So one of the motivations for Elm CodeGen just in general was, had to do, so there's", "tokens": [407, 472, 295, 264, 39034, 337, 2699, 76, 15549, 26647, 445, 294, 2674, 390, 11, 632, 281, 360, 11, 370, 456, 311], "temperature": 0.0, "avg_logprob": -0.2781355418856182, "compression_ratio": 1.6450381679389312, "no_speech_prob": 6.893509976180212e-07}, {"id": 703, "seek": 233316, "start": 2333.16, "end": 2337.46, "text": " the Elm UI library, which you may or may not be familiar with, but essentially is a way", "tokens": [264, 2699, 76, 15682, 6405, 11, 597, 291, 815, 420, 815, 406, 312, 4963, 365, 11, 457, 4476, 307, 257, 636], "temperature": 0.0, "avg_logprob": -0.23675159015486727, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.08323318829207e-07}, {"id": 704, "seek": 233316, "start": 2337.46, "end": 2344.24, "text": " to kind of write your layout and it turns into CSS, but I wanted a language or a library", "tokens": [281, 733, 295, 2464, 428, 13333, 293, 309, 4523, 666, 24387, 11, 457, 286, 1415, 257, 2856, 420, 257, 6405], "temperature": 0.0, "avg_logprob": -0.23675159015486727, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.08323318829207e-07}, {"id": 705, "seek": 233316, "start": 2344.24, "end": 2352.92, "text": " where the layout itself was tighter than CSS, where there were fewer basically situations", "tokens": [689, 264, 13333, 2564, 390, 30443, 813, 24387, 11, 689, 456, 645, 13366, 1936, 6851], "temperature": 0.0, "avg_logprob": -0.23675159015486727, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.08323318829207e-07}, {"id": 706, "seek": 233316, "start": 2352.92, "end": 2355.6, "text": " where you just had no idea what was going on.", "tokens": [689, 291, 445, 632, 572, 1558, 437, 390, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.23675159015486727, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.08323318829207e-07}, {"id": 707, "seek": 233316, "start": 2355.6, "end": 2358.44, "text": " I wanted to make it as type safe as possible, essentially.", "tokens": [286, 1415, 281, 652, 309, 382, 2010, 3273, 382, 1944, 11, 4476, 13], "temperature": 0.0, "avg_logprob": -0.23675159015486727, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.08323318829207e-07}, {"id": 708, "seek": 233316, "start": 2358.44, "end": 2362.7999999999997, "text": " Like another way to think about this is if you could look at the code and know what it", "tokens": [1743, 1071, 636, 281, 519, 466, 341, 307, 498, 291, 727, 574, 412, 264, 3089, 293, 458, 437, 309], "temperature": 0.0, "avg_logprob": -0.23675159015486727, "compression_ratio": 1.6962962962962962, "no_speech_prob": 6.08323318829207e-07}, {"id": 709, "seek": 236280, "start": 2362.8, "end": 2365.1600000000003, "text": " looked like, know what your layout was.", "tokens": [2956, 411, 11, 458, 437, 428, 13333, 390, 13], "temperature": 0.0, "avg_logprob": -0.21649418936835396, "compression_ratio": 1.6184738955823292, "no_speech_prob": 1.7603249489184236e-06}, {"id": 710, "seek": 236280, "start": 2365.1600000000003, "end": 2366.6400000000003, "text": " For me, that's just insanely valuable.", "tokens": [1171, 385, 11, 300, 311, 445, 40965, 8263, 13], "temperature": 0.0, "avg_logprob": -0.21649418936835396, "compression_ratio": 1.6184738955823292, "no_speech_prob": 1.7603249489184236e-06}, {"id": 711, "seek": 236280, "start": 2366.6400000000003, "end": 2370.1200000000003, "text": " If you're able to read the code and just in your mind, you're able to compose what is", "tokens": [759, 291, 434, 1075, 281, 1401, 264, 3089, 293, 445, 294, 428, 1575, 11, 291, 434, 1075, 281, 35925, 437, 307], "temperature": 0.0, "avg_logprob": -0.21649418936835396, "compression_ratio": 1.6184738955823292, "no_speech_prob": 1.7603249489184236e-06}, {"id": 712, "seek": 236280, "start": 2370.1200000000003, "end": 2371.36, "text": " actually visible.", "tokens": [767, 8974, 13], "temperature": 0.0, "avg_logprob": -0.21649418936835396, "compression_ratio": 1.6184738955823292, "no_speech_prob": 1.7603249489184236e-06}, {"id": 713, "seek": 236280, "start": 2371.36, "end": 2376.2000000000003, "text": " That's just incredibly valuable as far as speed of development.", "tokens": [663, 311, 445, 6252, 8263, 382, 1400, 382, 3073, 295, 3250, 13], "temperature": 0.0, "avg_logprob": -0.21649418936835396, "compression_ratio": 1.6184738955823292, "no_speech_prob": 1.7603249489184236e-06}, {"id": 714, "seek": 236280, "start": 2376.2000000000003, "end": 2382.36, "text": " And I think that resonated with the community for sure, but it's missing a piece because", "tokens": [400, 286, 519, 300, 47957, 365, 264, 1768, 337, 988, 11, 457, 309, 311, 5361, 257, 2522, 570], "temperature": 0.0, "avg_logprob": -0.21649418936835396, "compression_ratio": 1.6184738955823292, "no_speech_prob": 1.7603249489184236e-06}, {"id": 715, "seek": 236280, "start": 2382.36, "end": 2387.32, "text": " I was like, okay, why don't people use Elm UI or why wouldn't they?", "tokens": [286, 390, 411, 11, 1392, 11, 983, 500, 380, 561, 764, 2699, 76, 15682, 420, 983, 2759, 380, 436, 30], "temperature": 0.0, "avg_logprob": -0.21649418936835396, "compression_ratio": 1.6184738955823292, "no_speech_prob": 1.7603249489184236e-06}, {"id": 716, "seek": 238732, "start": 2387.32, "end": 2395.56, "text": " I think there's a very valid argument of they want a design system and there's no design", "tokens": [286, 519, 456, 311, 257, 588, 7363, 6770, 295, 436, 528, 257, 1715, 1185, 293, 456, 311, 572, 1715], "temperature": 0.0, "avg_logprob": -0.24695865238938375, "compression_ratio": 1.6738197424892705, "no_speech_prob": 2.309059539129521e-07}, {"id": 717, "seek": 238732, "start": 2395.56, "end": 2397.0800000000004, "text": " system built into Elm UI.", "tokens": [1185, 3094, 666, 2699, 76, 15682, 13], "temperature": 0.0, "avg_logprob": -0.24695865238938375, "compression_ratio": 1.6738197424892705, "no_speech_prob": 2.309059539129521e-07}, {"id": 718, "seek": 238732, "start": 2397.0800000000004, "end": 2400.2400000000002, "text": " Elm UI is sort of a replacement almost for CSS, right?", "tokens": [2699, 76, 15682, 307, 1333, 295, 257, 14419, 1920, 337, 24387, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24695865238938375, "compression_ratio": 1.6738197424892705, "no_speech_prob": 2.309059539129521e-07}, {"id": 719, "seek": 238732, "start": 2400.2400000000002, "end": 2405.1600000000003, "text": " Where it's like you can think of style this way, but there's no design system on top of", "tokens": [2305, 309, 311, 411, 291, 393, 519, 295, 3758, 341, 636, 11, 457, 456, 311, 572, 1715, 1185, 322, 1192, 295], "temperature": 0.0, "avg_logprob": -0.24695865238938375, "compression_ratio": 1.6738197424892705, "no_speech_prob": 2.309059539129521e-07}, {"id": 720, "seek": 238732, "start": 2405.1600000000003, "end": 2406.1600000000003, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.24695865238938375, "compression_ratio": 1.6738197424892705, "no_speech_prob": 2.309059539129521e-07}, {"id": 721, "seek": 238732, "start": 2406.1600000000003, "end": 2407.1600000000003, "text": " There's no set of presets.", "tokens": [821, 311, 572, 992, 295, 41865, 13], "temperature": 0.0, "avg_logprob": -0.24695865238938375, "compression_ratio": 1.6738197424892705, "no_speech_prob": 2.309059539129521e-07}, {"id": 722, "seek": 238732, "start": 2407.1600000000003, "end": 2412.56, "text": " And I've thought a lot and had a lot of experience at vendor building as we've gone through different", "tokens": [400, 286, 600, 1194, 257, 688, 293, 632, 257, 688, 295, 1752, 412, 24321, 2390, 382, 321, 600, 2780, 807, 819], "temperature": 0.0, "avg_logprob": -0.24695865238938375, "compression_ratio": 1.6738197424892705, "no_speech_prob": 2.309059539129521e-07}, {"id": 723, "seek": 241256, "start": 2412.56, "end": 2418.64, "text": " versions of design systems at vendor and what is a good design system look like with a system", "tokens": [9606, 295, 1715, 3652, 412, 24321, 293, 437, 307, 257, 665, 1715, 1185, 574, 411, 365, 257, 1185], "temperature": 0.0, "avg_logprob": -0.27292597068930574, "compression_ratio": 1.7377049180327868, "no_speech_prob": 4.381803933029005e-07}, {"id": 724, "seek": 241256, "start": 2418.64, "end": 2421.12, "text": " that is roughly the shape of Elm UI.", "tokens": [300, 307, 9810, 264, 3909, 295, 2699, 76, 15682, 13], "temperature": 0.0, "avg_logprob": -0.27292597068930574, "compression_ratio": 1.7377049180327868, "no_speech_prob": 4.381803933029005e-07}, {"id": 725, "seek": 241256, "start": 2421.12, "end": 2427.84, "text": " It doesn't have to be Elm UI, but if it makes some fundamental choices that are similar.", "tokens": [467, 1177, 380, 362, 281, 312, 2699, 76, 15682, 11, 457, 498, 309, 1669, 512, 8088, 7994, 300, 366, 2531, 13], "temperature": 0.0, "avg_logprob": -0.27292597068930574, "compression_ratio": 1.7377049180327868, "no_speech_prob": 4.381803933029005e-07}, {"id": 726, "seek": 241256, "start": 2427.84, "end": 2433.12, "text": " And I was thinking like, okay, well I want a language for design, your design system,", "tokens": [400, 286, 390, 1953, 411, 11, 1392, 11, 731, 286, 528, 257, 2856, 337, 1715, 11, 428, 1715, 1185, 11], "temperature": 0.0, "avg_logprob": -0.27292597068930574, "compression_ratio": 1.7377049180327868, "no_speech_prob": 4.381803933029005e-07}, {"id": 727, "seek": 241256, "start": 2433.12, "end": 2434.12, "text": " like a higher level language.", "tokens": [411, 257, 2946, 1496, 2856, 13], "temperature": 0.0, "avg_logprob": -0.27292597068930574, "compression_ratio": 1.7377049180327868, "no_speech_prob": 4.381803933029005e-07}, {"id": 728, "seek": 241256, "start": 2434.12, "end": 2441.12, "text": " And I think the challenge initially is that implementing that language in Elm code, like", "tokens": [400, 286, 519, 264, 3430, 9105, 307, 300, 18114, 300, 2856, 294, 2699, 76, 3089, 11, 411], "temperature": 0.0, "avg_logprob": -0.27292597068930574, "compression_ratio": 1.7377049180327868, "no_speech_prob": 4.381803933029005e-07}, {"id": 729, "seek": 244112, "start": 2441.12, "end": 2445.52, "text": " that you're writing by hand is challenging for a number of reasons because you have to", "tokens": [300, 291, 434, 3579, 538, 1011, 307, 7595, 337, 257, 1230, 295, 4112, 570, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 730, "seek": 244112, "start": 2445.52, "end": 2448.6, "text": " basically, you have to be disciplined about what you're writing.", "tokens": [1936, 11, 291, 362, 281, 312, 40061, 466, 437, 291, 434, 3579, 13], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 731, "seek": 244112, "start": 2448.6, "end": 2453.08, "text": " Where it's like, okay, here's my, you gather a set of values that work well together, but", "tokens": [2305, 309, 311, 411, 11, 1392, 11, 510, 311, 452, 11, 291, 5448, 257, 992, 295, 4190, 300, 589, 731, 1214, 11, 457], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 732, "seek": 244112, "start": 2453.08, "end": 2455.88, "text": " you have to make all these like little tiny decisions that are kind of dumb.", "tokens": [291, 362, 281, 652, 439, 613, 411, 707, 5870, 5327, 300, 366, 733, 295, 10316, 13], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 733, "seek": 244112, "start": 2455.88, "end": 2459.3599999999997, "text": " It's kind of like, where do I put my colors?", "tokens": [467, 311, 733, 295, 411, 11, 689, 360, 286, 829, 452, 4577, 30], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 734, "seek": 244112, "start": 2459.3599999999997, "end": 2462.96, "text": " Like how do I even organize my UI code?", "tokens": [1743, 577, 360, 286, 754, 13859, 452, 15682, 3089, 30], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 735, "seek": 244112, "start": 2462.96, "end": 2464.04, "text": " I would like some widgets.", "tokens": [286, 576, 411, 512, 43355, 13], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 736, "seek": 244112, "start": 2464.04, "end": 2468.56, "text": " And even when I'm like looking at this design system, I would like to even be able to like", "tokens": [400, 754, 562, 286, 478, 411, 1237, 412, 341, 1715, 1185, 11, 286, 576, 411, 281, 754, 312, 1075, 281, 411], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 737, "seek": 244112, "start": 2468.56, "end": 2471.08, "text": " have a few widgets already made for me.", "tokens": [362, 257, 1326, 43355, 1217, 1027, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.23806378648087784, "compression_ratio": 1.7641509433962264, "no_speech_prob": 1.1544502740434837e-06}, {"id": 738, "seek": 247108, "start": 2471.08, "end": 2474.84, "text": " Nobody is going to create a button or module.", "tokens": [9297, 307, 516, 281, 1884, 257, 2960, 420, 10088, 13], "temperature": 0.0, "avg_logprob": -0.2413180295158835, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.5056851907502278e-06}, {"id": 739, "seek": 247108, "start": 2474.84, "end": 2479.88, "text": " Everyone's going to create like probably a number of very common UI elements.", "tokens": [5198, 311, 516, 281, 1884, 411, 1391, 257, 1230, 295, 588, 2689, 15682, 4959, 13], "temperature": 0.0, "avg_logprob": -0.2413180295158835, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.5056851907502278e-06}, {"id": 740, "seek": 247108, "start": 2479.88, "end": 2486.08, "text": " So I made Elm Code Gen with the idea, and this is basically still being formed, but", "tokens": [407, 286, 1027, 2699, 76, 15549, 3632, 365, 264, 1558, 11, 293, 341, 307, 1936, 920, 885, 8693, 11, 457], "temperature": 0.0, "avg_logprob": -0.2413180295158835, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.5056851907502278e-06}, {"id": 741, "seek": 247108, "start": 2486.08, "end": 2491.7599999999998, "text": " I think the project is one that I want to get out because I want to use it this year", "tokens": [286, 519, 264, 1716, 307, 472, 300, 286, 528, 281, 483, 484, 570, 286, 528, 281, 764, 309, 341, 1064], "temperature": 0.0, "avg_logprob": -0.2413180295158835, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.5056851907502278e-06}, {"id": 742, "seek": 247108, "start": 2491.7599999999998, "end": 2496.4, "text": " is that you'd be able to sort of define what your design, like high level primitives for", "tokens": [307, 300, 291, 1116, 312, 1075, 281, 1333, 295, 6964, 437, 428, 1715, 11, 411, 1090, 1496, 2886, 38970, 337], "temperature": 0.0, "avg_logprob": -0.2413180295158835, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.5056851907502278e-06}, {"id": 743, "seek": 247108, "start": 2496.4, "end": 2497.4, "text": " your design.", "tokens": [428, 1715, 13], "temperature": 0.0, "avg_logprob": -0.2413180295158835, "compression_ratio": 1.6147540983606556, "no_speech_prob": 1.5056851907502278e-06}, {"id": 744, "seek": 249740, "start": 2497.4, "end": 2501.96, "text": " And it would generate sort of a bunch of Elm Code, including possibly a handful of kind", "tokens": [400, 309, 576, 8460, 1333, 295, 257, 3840, 295, 2699, 76, 15549, 11, 3009, 6264, 257, 16458, 295, 733], "temperature": 0.0, "avg_logprob": -0.20375658254154394, "compression_ratio": 1.698961937716263, "no_speech_prob": 1.2482556712711812e-06}, {"id": 745, "seek": 249740, "start": 2501.96, "end": 2504.28, "text": " of widgets that everybody's going to use.", "tokens": [295, 43355, 300, 2201, 311, 516, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.20375658254154394, "compression_ratio": 1.698961937716263, "no_speech_prob": 1.2482556712711812e-06}, {"id": 746, "seek": 249740, "start": 2504.28, "end": 2508.84, "text": " And it would just be in like a really nice shape already and kind of ready to use.", "tokens": [400, 309, 576, 445, 312, 294, 411, 257, 534, 1481, 3909, 1217, 293, 733, 295, 1919, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.20375658254154394, "compression_ratio": 1.698961937716263, "no_speech_prob": 1.2482556712711812e-06}, {"id": 747, "seek": 249740, "start": 2508.84, "end": 2514.4, "text": " And because the experience I want, I think about who the ideal customer for Elm is as", "tokens": [400, 570, 264, 1752, 286, 528, 11, 286, 519, 466, 567, 264, 7157, 5474, 337, 2699, 76, 307, 382], "temperature": 0.0, "avg_logprob": -0.20375658254154394, "compression_ratio": 1.698961937716263, "no_speech_prob": 1.2482556712711812e-06}, {"id": 748, "seek": 249740, "start": 2514.4, "end": 2515.56, "text": " a language.", "tokens": [257, 2856, 13], "temperature": 0.0, "avg_logprob": -0.20375658254154394, "compression_ratio": 1.698961937716263, "no_speech_prob": 1.2482556712711812e-06}, {"id": 749, "seek": 249740, "start": 2515.56, "end": 2520.32, "text": " And one thing that I think about is somebody who doesn't want to deal with kind of the", "tokens": [400, 472, 551, 300, 286, 519, 466, 307, 2618, 567, 1177, 380, 528, 281, 2028, 365, 733, 295, 264], "temperature": 0.0, "avg_logprob": -0.20375658254154394, "compression_ratio": 1.698961937716263, "no_speech_prob": 1.2482556712711812e-06}, {"id": 750, "seek": 249740, "start": 2520.32, "end": 2527.1600000000003, "text": " mess of JavaScript and like the always iterating, you know, massive cognitive overhead of the", "tokens": [2082, 295, 15778, 293, 411, 264, 1009, 17138, 990, 11, 291, 458, 11, 5994, 15605, 19922, 295, 264], "temperature": 0.0, "avg_logprob": -0.20375658254154394, "compression_ratio": 1.698961937716263, "no_speech_prob": 1.2482556712711812e-06}, {"id": 751, "seek": 252716, "start": 2527.16, "end": 2528.3599999999997, "text": " ecosystem of JavaScript.", "tokens": [11311, 295, 15778, 13], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 752, "seek": 252716, "start": 2528.3599999999997, "end": 2532.2, "text": " They want something simple and they want to get something working very quickly.", "tokens": [814, 528, 746, 2199, 293, 436, 528, 281, 483, 746, 1364, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 753, "seek": 252716, "start": 2532.2, "end": 2534.56, "text": " I think there's a big challenge there.", "tokens": [286, 519, 456, 311, 257, 955, 3430, 456, 13], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 754, "seek": 252716, "start": 2534.56, "end": 2538.48, "text": " If they don't have a widget library, they can boot up very quickly.", "tokens": [759, 436, 500, 380, 362, 257, 34047, 6405, 11, 436, 393, 11450, 493, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 755, "seek": 252716, "start": 2538.48, "end": 2539.52, "text": " It's actually very hard.", "tokens": [467, 311, 767, 588, 1152, 13], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 756, "seek": 252716, "start": 2539.52, "end": 2543.68, "text": " It's actually like almost like a stopper where it's like someone's going to choose React", "tokens": [467, 311, 767, 411, 1920, 411, 257, 1590, 610, 689, 309, 311, 411, 1580, 311, 516, 281, 2826, 30644], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 757, "seek": 252716, "start": 2543.68, "end": 2544.68, "text": " because guess what?", "tokens": [570, 2041, 437, 30], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 758, "seek": 252716, "start": 2544.68, "end": 2548.2799999999997, "text": " You can just pull down a design system easily.", "tokens": [509, 393, 445, 2235, 760, 257, 1715, 1185, 3612, 13], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 759, "seek": 252716, "start": 2548.2799999999997, "end": 2553.12, "text": " And even though you may conceptually really love the idea of purity and everything, like", "tokens": [400, 754, 1673, 291, 815, 3410, 671, 534, 959, 264, 1558, 295, 34382, 293, 1203, 11, 411], "temperature": 0.0, "avg_logprob": -0.222576413835798, "compression_ratio": 1.7240143369175627, "no_speech_prob": 4.029362116853008e-06}, {"id": 760, "seek": 255312, "start": 2553.12, "end": 2558.4, "text": " you really, your main focus may be on a backend implementation of something and you just need", "tokens": [291, 534, 11, 428, 2135, 1879, 815, 312, 322, 257, 38087, 11420, 295, 746, 293, 291, 445, 643], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 761, "seek": 255312, "start": 2558.4, "end": 2559.68, "text": " a UI for it.", "tokens": [257, 15682, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 762, "seek": 255312, "start": 2559.68, "end": 2561.92, "text": " Anyway, so that's what I want to build.", "tokens": [5684, 11, 370, 300, 311, 437, 286, 528, 281, 1322, 13], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 763, "seek": 255312, "start": 2561.92, "end": 2563.24, "text": " I've been thinking about it for a long time.", "tokens": [286, 600, 668, 1953, 466, 309, 337, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 764, "seek": 255312, "start": 2563.24, "end": 2564.72, "text": " I think I know what it looks like.", "tokens": [286, 519, 286, 458, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 765, "seek": 255312, "start": 2564.72, "end": 2567.6, "text": " Yeah, it's very exciting.", "tokens": [865, 11, 309, 311, 588, 4670, 13], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 766, "seek": 255312, "start": 2567.6, "end": 2572.48, "text": " And there are precedents out there for kind of design system palettes like Tailwind being", "tokens": [400, 456, 366, 16969, 791, 484, 456, 337, 733, 295, 1715, 1185, 3984, 16049, 411, 46074, 12199, 885], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 767, "seek": 255312, "start": 2572.48, "end": 2574.4, "text": " the obvious one.", "tokens": [264, 6322, 472, 13], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 768, "seek": 255312, "start": 2574.4, "end": 2576.2, "text": " That's not that complicated.", "tokens": [663, 311, 406, 300, 6179, 13], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 769, "seek": 255312, "start": 2576.2, "end": 2579.08, "text": " It's like, what colors do you have to work with?", "tokens": [467, 311, 411, 11, 437, 4577, 360, 291, 362, 281, 589, 365, 30], "temperature": 0.0, "avg_logprob": -0.28411917532643965, "compression_ratio": 1.6007326007326008, "no_speech_prob": 7.112422508726013e-07}, {"id": 770, "seek": 257908, "start": 2579.08, "end": 2584.84, "text": " What spacing units do you have to work with and generate some permutations of things?", "tokens": [708, 27739, 6815, 360, 291, 362, 281, 589, 365, 293, 8460, 512, 4784, 325, 763, 295, 721, 30], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 771, "seek": 257908, "start": 2584.84, "end": 2585.84, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 772, "seek": 257908, "start": 2585.84, "end": 2590.3199999999997, "text": " I think the actual complexity of the project won't be, I think it'll be in the design decisions.", "tokens": [286, 519, 264, 3539, 14024, 295, 264, 1716, 1582, 380, 312, 11, 286, 519, 309, 603, 312, 294, 264, 1715, 5327, 13], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 773, "seek": 257908, "start": 2590.3199999999997, "end": 2593.24, "text": " So like, but not in the actual implementation.", "tokens": [407, 411, 11, 457, 406, 294, 264, 3539, 11420, 13], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 774, "seek": 257908, "start": 2593.24, "end": 2598.12, "text": " I think there are things to know about for each of those concepts that if you're aware", "tokens": [286, 519, 456, 366, 721, 281, 458, 466, 337, 1184, 295, 729, 10392, 300, 498, 291, 434, 3650], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 775, "seek": 257908, "start": 2598.12, "end": 2599.52, "text": " of them, it makes things a lot easier.", "tokens": [295, 552, 11, 309, 1669, 721, 257, 688, 3571, 13], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 776, "seek": 257908, "start": 2599.52, "end": 2600.6, "text": " If you're not, then you're not.", "tokens": [759, 291, 434, 406, 11, 550, 291, 434, 406, 13], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 777, "seek": 257908, "start": 2600.6, "end": 2604.48, "text": " So an example being you're choosing a color palette and this is if you're not getting", "tokens": [407, 364, 1365, 885, 291, 434, 10875, 257, 2017, 15851, 293, 341, 307, 498, 291, 434, 406, 1242], "temperature": 0.0, "avg_logprob": -0.20982527354406932, "compression_ratio": 1.7518248175182483, "no_speech_prob": 1.2098573733965168e-06}, {"id": 778, "seek": 260448, "start": 2604.48, "end": 2611.2, "text": " it from a designer who would have, hopefully, they would just know that have this on lockdown.", "tokens": [309, 490, 257, 11795, 567, 576, 362, 11, 4696, 11, 436, 576, 445, 458, 300, 362, 341, 322, 17267, 13], "temperature": 0.0, "avg_logprob": -0.24559288694147477, "compression_ratio": 1.7611336032388665, "no_speech_prob": 1.3081715906082536e-06}, {"id": 779, "seek": 260448, "start": 2611.2, "end": 2616.34, "text": " But if you're designing a color palette yourself, if you don't know about color spaces that", "tokens": [583, 498, 291, 434, 14685, 257, 2017, 15851, 1803, 11, 498, 291, 500, 380, 458, 466, 2017, 7673, 300], "temperature": 0.0, "avg_logprob": -0.24559288694147477, "compression_ratio": 1.7611336032388665, "no_speech_prob": 1.3081715906082536e-06}, {"id": 780, "seek": 260448, "start": 2616.34, "end": 2620.92, "text": " make it much easier to develop a palette, so this would be something based on what's", "tokens": [652, 309, 709, 3571, 281, 1499, 257, 15851, 11, 370, 341, 576, 312, 746, 2361, 322, 437, 311], "temperature": 0.0, "avg_logprob": -0.24559288694147477, "compression_ratio": 1.7611336032388665, "no_speech_prob": 1.3081715906082536e-06}, {"id": 781, "seek": 260448, "start": 2620.92, "end": 2626.32, "text": " called Lab, L-A-B, then you're going to have a much harder time with colors.", "tokens": [1219, 10137, 11, 441, 12, 32, 12, 33, 11, 550, 291, 434, 516, 281, 362, 257, 709, 6081, 565, 365, 4577, 13], "temperature": 0.0, "avg_logprob": -0.24559288694147477, "compression_ratio": 1.7611336032388665, "no_speech_prob": 1.3081715906082536e-06}, {"id": 782, "seek": 260448, "start": 2626.32, "end": 2632.2, "text": " If you don't know about it, well, if you do know about it, you'd probably start to use", "tokens": [759, 291, 500, 380, 458, 466, 309, 11, 731, 11, 498, 291, 360, 458, 466, 309, 11, 291, 1116, 1391, 722, 281, 764], "temperature": 0.0, "avg_logprob": -0.24559288694147477, "compression_ratio": 1.7611336032388665, "no_speech_prob": 1.3081715906082536e-06}, {"id": 783, "seek": 263220, "start": 2632.2, "end": 2634.7999999999997, "text": " it, but even then it can be a little confusing.", "tokens": [309, 11, 457, 754, 550, 309, 393, 312, 257, 707, 13181, 13], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 784, "seek": 263220, "start": 2634.7999999999997, "end": 2639.72, "text": " So an example of what Lab does that just so I'm not like throwing out terms and people", "tokens": [407, 364, 1365, 295, 437, 10137, 775, 300, 445, 370, 286, 478, 406, 411, 10238, 484, 2115, 293, 561], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 785, "seek": 263220, "start": 2639.72, "end": 2640.9199999999996, "text": " are like, what the heck?", "tokens": [366, 411, 11, 437, 264, 12872, 30], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 786, "seek": 263220, "start": 2640.9199999999996, "end": 2645.24, "text": " You may be familiar with defining a color where it says like HSL, which is like hue,", "tokens": [509, 815, 312, 4963, 365, 17827, 257, 2017, 689, 309, 1619, 411, 34194, 43, 11, 597, 307, 411, 24967, 11], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 787, "seek": 263220, "start": 2645.24, "end": 2647.08, "text": " saturation, and lightness, right?", "tokens": [27090, 11, 293, 1442, 1287, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 788, "seek": 263220, "start": 2647.08, "end": 2651.4399999999996, "text": " The lightness parameter is actually, it's a lie.", "tokens": [440, 1442, 1287, 13075, 307, 767, 11, 309, 311, 257, 4544, 13], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 789, "seek": 263220, "start": 2651.4399999999996, "end": 2653.24, "text": " It's not lightness.", "tokens": [467, 311, 406, 1442, 1287, 13], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 790, "seek": 263220, "start": 2653.24, "end": 2660.3399999999997, "text": " Two colors that have the same lightness will be vastly different perceptual brightnesses.", "tokens": [4453, 4577, 300, 362, 264, 912, 1442, 1287, 486, 312, 41426, 819, 43276, 901, 21367, 279, 13], "temperature": 0.0, "avg_logprob": -0.22789536804712118, "compression_ratio": 1.6305970149253732, "no_speech_prob": 1.8162013475375716e-06}, {"id": 791, "seek": 266034, "start": 2660.34, "end": 2665.08, "text": " And what this means is that when you're developing a color palette, it means that the color space,", "tokens": [400, 437, 341, 1355, 307, 300, 562, 291, 434, 6416, 257, 2017, 15851, 11, 309, 1355, 300, 264, 2017, 1901, 11], "temperature": 0.0, "avg_logprob": -0.22795933011978392, "compression_ratio": 1.8159722222222223, "no_speech_prob": 2.156802338504349e-06}, {"id": 792, "seek": 266034, "start": 2665.08, "end": 2670.1200000000003, "text": " the numbers you're using to define your colors, things that are numerically near each other", "tokens": [264, 3547, 291, 434, 1228, 281, 6964, 428, 4577, 11, 721, 300, 366, 7866, 984, 2651, 1184, 661], "temperature": 0.0, "avg_logprob": -0.22795933011978392, "compression_ratio": 1.8159722222222223, "no_speech_prob": 2.156802338504349e-06}, {"id": 793, "seek": 266034, "start": 2670.1200000000003, "end": 2672.6000000000004, "text": " are not perceptually near each other.", "tokens": [366, 406, 43276, 671, 2651, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.22795933011978392, "compression_ratio": 1.8159722222222223, "no_speech_prob": 2.156802338504349e-06}, {"id": 794, "seek": 266034, "start": 2672.6000000000004, "end": 2676.44, "text": " And that means it's very frustrating to actually come to a higher level abstraction for anything.", "tokens": [400, 300, 1355, 309, 311, 588, 16522, 281, 767, 808, 281, 257, 2946, 1496, 37765, 337, 1340, 13], "temperature": 0.0, "avg_logprob": -0.22795933011978392, "compression_ratio": 1.8159722222222223, "no_speech_prob": 2.156802338504349e-06}, {"id": 795, "seek": 266034, "start": 2676.44, "end": 2679.2000000000003, "text": " I think there, so it's like, okay, well, that's really interesting.", "tokens": [286, 519, 456, 11, 370, 309, 311, 411, 11, 1392, 11, 731, 11, 300, 311, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22795933011978392, "compression_ratio": 1.8159722222222223, "no_speech_prob": 2.156802338504349e-06}, {"id": 796, "seek": 266034, "start": 2679.2000000000003, "end": 2684.34, "text": " Like one color space, again, that ultimately, because this is all mathematics, turns into", "tokens": [1743, 472, 2017, 1901, 11, 797, 11, 300, 6284, 11, 570, 341, 307, 439, 18666, 11, 4523, 666], "temperature": 0.0, "avg_logprob": -0.22795933011978392, "compression_ratio": 1.8159722222222223, "no_speech_prob": 2.156802338504349e-06}, {"id": 797, "seek": 266034, "start": 2684.34, "end": 2688.58, "text": " something called Lab is called HSL UV.", "tokens": [746, 1219, 10137, 307, 1219, 34194, 43, 17887, 13], "temperature": 0.0, "avg_logprob": -0.22795933011978392, "compression_ratio": 1.8159722222222223, "no_speech_prob": 2.156802338504349e-06}, {"id": 798, "seek": 268858, "start": 2688.58, "end": 2693.0, "text": " And that's where they basically adjust these things so that they are sort of numerical.", "tokens": [400, 300, 311, 689, 436, 1936, 4369, 613, 721, 370, 300, 436, 366, 1333, 295, 29054, 13], "temperature": 0.0, "avg_logprob": -0.2484782507660192, "compression_ratio": 1.730909090909091, "no_speech_prob": 1.267926222681126e-06}, {"id": 799, "seek": 268858, "start": 2693.0, "end": 2695.64, "text": " Numerical closeness means perceptual closeness.", "tokens": [426, 15583, 804, 2611, 15264, 1355, 43276, 901, 2611, 15264, 13], "temperature": 0.0, "avg_logprob": -0.2484782507660192, "compression_ratio": 1.730909090909091, "no_speech_prob": 1.267926222681126e-06}, {"id": 800, "seek": 268858, "start": 2695.64, "end": 2699.22, "text": " But anyway, having insights like that, which are awesome.", "tokens": [583, 4033, 11, 1419, 14310, 411, 300, 11, 597, 366, 3476, 13], "temperature": 0.0, "avg_logprob": -0.2484782507660192, "compression_ratio": 1.730909090909091, "no_speech_prob": 1.267926222681126e-06}, {"id": 801, "seek": 268858, "start": 2699.22, "end": 2705.88, "text": " I think there are also insights around typography, around spacing, around visual weight of things,", "tokens": [286, 519, 456, 366, 611, 14310, 926, 2125, 5820, 11, 926, 27739, 11, 926, 5056, 3364, 295, 721, 11], "temperature": 0.0, "avg_logprob": -0.2484782507660192, "compression_ratio": 1.730909090909091, "no_speech_prob": 1.267926222681126e-06}, {"id": 802, "seek": 268858, "start": 2705.88, "end": 2711.84, "text": " gradients, being able to construct nice gradients is a subtle thing that you could go spend", "tokens": [2771, 2448, 11, 885, 1075, 281, 7690, 1481, 2771, 2448, 307, 257, 13743, 551, 300, 291, 727, 352, 3496], "temperature": 0.0, "avg_logprob": -0.2484782507660192, "compression_ratio": 1.730909090909091, "no_speech_prob": 1.267926222681126e-06}, {"id": 803, "seek": 268858, "start": 2711.84, "end": 2716.88, "text": " a few weeks learning, maybe not a few weeks, but definitely like surprisingly more involved", "tokens": [257, 1326, 3259, 2539, 11, 1310, 406, 257, 1326, 3259, 11, 457, 2138, 411, 17600, 544, 3288], "temperature": 0.0, "avg_logprob": -0.2484782507660192, "compression_ratio": 1.730909090909091, "no_speech_prob": 1.267926222681126e-06}, {"id": 804, "seek": 271688, "start": 2716.88, "end": 2719.1600000000003, "text": " than it is immediately obvious.", "tokens": [813, 309, 307, 4258, 6322, 13], "temperature": 0.0, "avg_logprob": -0.22817648502818325, "compression_ratio": 1.6865671641791045, "no_speech_prob": 1.7880602172226645e-06}, {"id": 805, "seek": 271688, "start": 2719.1600000000003, "end": 2723.88, "text": " So like gathering all that design knowledge into one code base that kind of knows what", "tokens": [407, 411, 13519, 439, 300, 1715, 3601, 666, 472, 3089, 3096, 300, 733, 295, 3255, 437], "temperature": 0.0, "avg_logprob": -0.22817648502818325, "compression_ratio": 1.6865671641791045, "no_speech_prob": 1.7880602172226645e-06}, {"id": 806, "seek": 271688, "start": 2723.88, "end": 2726.84, "text": " the mental model should be, I think will be super valuable.", "tokens": [264, 4973, 2316, 820, 312, 11, 286, 519, 486, 312, 1687, 8263, 13], "temperature": 0.0, "avg_logprob": -0.22817648502818325, "compression_ratio": 1.6865671641791045, "no_speech_prob": 1.7880602172226645e-06}, {"id": 807, "seek": 271688, "start": 2726.84, "end": 2727.84, "text": " That's amazing.", "tokens": [663, 311, 2243, 13], "temperature": 0.0, "avg_logprob": -0.22817648502818325, "compression_ratio": 1.6865671641791045, "no_speech_prob": 1.7880602172226645e-06}, {"id": 808, "seek": 271688, "start": 2727.84, "end": 2734.08, "text": " I just want to say like, this is like one of my favorite things about Elm and the Elm", "tokens": [286, 445, 528, 281, 584, 411, 11, 341, 307, 411, 472, 295, 452, 2954, 721, 466, 2699, 76, 293, 264, 2699, 76], "temperature": 0.0, "avg_logprob": -0.22817648502818325, "compression_ratio": 1.6865671641791045, "no_speech_prob": 1.7880602172226645e-06}, {"id": 809, "seek": 271688, "start": 2734.08, "end": 2741.36, "text": " community and the Elm ecosystem, I think is this like, not settling for the status quo", "tokens": [1768, 293, 264, 2699, 76, 11311, 11, 286, 519, 307, 341, 411, 11, 406, 33841, 337, 264, 6558, 28425], "temperature": 0.0, "avg_logprob": -0.22817648502818325, "compression_ratio": 1.6865671641791045, "no_speech_prob": 1.7880602172226645e-06}, {"id": 810, "seek": 271688, "start": 2741.36, "end": 2746.36, "text": " and saying like, yeah, there are palettes out there for creating a design system and", "tokens": [293, 1566, 411, 11, 1338, 11, 456, 366, 3984, 16049, 484, 456, 337, 4084, 257, 1715, 1185, 293], "temperature": 0.0, "avg_logprob": -0.22817648502818325, "compression_ratio": 1.6865671641791045, "no_speech_prob": 1.7880602172226645e-06}, {"id": 811, "seek": 274636, "start": 2746.36, "end": 2747.36, "text": " laying out these colors.", "tokens": [14903, 484, 613, 4577, 13], "temperature": 0.0, "avg_logprob": -0.21517080224078636, "compression_ratio": 1.7675276752767528, "no_speech_prob": 2.12336112781486e-06}, {"id": 812, "seek": 274636, "start": 2747.36, "end": 2751.32, "text": " It's like, well, what if we think this through from first principles?", "tokens": [467, 311, 411, 11, 731, 11, 437, 498, 321, 519, 341, 807, 490, 700, 9156, 30], "temperature": 0.0, "avg_logprob": -0.21517080224078636, "compression_ratio": 1.7675276752767528, "no_speech_prob": 2.12336112781486e-06}, {"id": 813, "seek": 274636, "start": 2751.32, "end": 2752.32, "text": " Where would we arrive?", "tokens": [2305, 576, 321, 8881, 30], "temperature": 0.0, "avg_logprob": -0.21517080224078636, "compression_ratio": 1.7675276752767528, "no_speech_prob": 2.12336112781486e-06}, {"id": 814, "seek": 274636, "start": 2752.32, "end": 2758.6, "text": " And I think like, I think Evan has really modeled and embodied that and that has shaped", "tokens": [400, 286, 519, 411, 11, 286, 519, 22613, 575, 534, 37140, 293, 42046, 300, 293, 300, 575, 13475], "temperature": 0.0, "avg_logprob": -0.21517080224078636, "compression_ratio": 1.7675276752767528, "no_speech_prob": 2.12336112781486e-06}, {"id": 815, "seek": 274636, "start": 2758.6, "end": 2763.44, "text": " the Elm community and attracted a lot of people to the Elm community who care about that.", "tokens": [264, 2699, 76, 1768, 293, 15912, 257, 688, 295, 561, 281, 264, 2699, 76, 1768, 567, 1127, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.21517080224078636, "compression_ratio": 1.7675276752767528, "no_speech_prob": 2.12336112781486e-06}, {"id": 816, "seek": 274636, "start": 2763.44, "end": 2771.44, "text": " I think Elm's data types and purity have created tools to help people do that and made that", "tokens": [286, 519, 2699, 76, 311, 1412, 3467, 293, 34382, 362, 2942, 3873, 281, 854, 561, 360, 300, 293, 1027, 300], "temperature": 0.0, "avg_logprob": -0.21517080224078636, "compression_ratio": 1.7675276752767528, "no_speech_prob": 2.12336112781486e-06}, {"id": 817, "seek": 274636, "start": 2771.44, "end": 2775.4, "text": " the path of least resistance to make impossible states impossible and make you really think", "tokens": [264, 3100, 295, 1935, 7335, 281, 652, 6243, 4368, 6243, 293, 652, 291, 534, 519], "temperature": 0.0, "avg_logprob": -0.21517080224078636, "compression_ratio": 1.7675276752767528, "no_speech_prob": 2.12336112781486e-06}, {"id": 818, "seek": 277540, "start": 2775.4, "end": 2777.32, "text": " through the options.", "tokens": [807, 264, 3956, 13], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 819, "seek": 277540, "start": 2777.32, "end": 2782.1600000000003, "text": " And like Matt, I know you've personally inspired me the way you take that approach.", "tokens": [400, 411, 7397, 11, 286, 458, 291, 600, 5665, 7547, 385, 264, 636, 291, 747, 300, 3109, 13], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 820, "seek": 277540, "start": 2782.1600000000003, "end": 2789.32, "text": " And I like, that's what I love about Elm and the Elm community is that we take these principles", "tokens": [400, 286, 411, 11, 300, 311, 437, 286, 959, 466, 2699, 76, 293, 264, 2699, 76, 1768, 307, 300, 321, 747, 613, 9156], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 821, "seek": 277540, "start": 2789.32, "end": 2793.26, "text": " and we say, yeah, we're not going to settle for like, people have figured out a pretty", "tokens": [293, 321, 584, 11, 1338, 11, 321, 434, 406, 516, 281, 11852, 337, 411, 11, 561, 362, 8932, 484, 257, 1238], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 822, "seek": 277540, "start": 2793.26, "end": 2794.26, "text": " okay way to do this.", "tokens": [1392, 636, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 823, "seek": 277540, "start": 2794.26, "end": 2797.86, "text": " We're like, let's think this through from the ground up from first principles and find", "tokens": [492, 434, 411, 11, 718, 311, 519, 341, 807, 490, 264, 2727, 493, 490, 700, 9156, 293, 915], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 824, "seek": 277540, "start": 2797.86, "end": 2799.48, "text": " a really great way to approach this.", "tokens": [257, 534, 869, 636, 281, 3109, 341, 13], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 825, "seek": 277540, "start": 2799.48, "end": 2801.6800000000003, "text": " So very exciting stuff.", "tokens": [407, 588, 4670, 1507, 13], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 826, "seek": 277540, "start": 2801.6800000000003, "end": 2803.4, "text": " Shall we go around the room and talk about plans?", "tokens": [12128, 321, 352, 926, 264, 1808, 293, 751, 466, 5482, 30], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 827, "seek": 277540, "start": 2803.4, "end": 2804.4, "text": " Is that exciting?", "tokens": [1119, 300, 4670, 30], "temperature": 0.0, "avg_logprob": -0.26336792537144255, "compression_ratio": 1.776271186440678, "no_speech_prob": 2.225251137133455e-06}, {"id": 828, "seek": 280440, "start": 2804.4, "end": 2805.4, "text": " Is that a thing?", "tokens": [1119, 300, 257, 551, 30], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 829, "seek": 280440, "start": 2805.4, "end": 2806.4, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 830, "seek": 280440, "start": 2806.4, "end": 2807.4, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 831, "seek": 280440, "start": 2807.4, "end": 2808.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 832, "seek": 280440, "start": 2808.4, "end": 2809.4, "text": " Who's got plans?", "tokens": [2102, 311, 658, 5482, 30], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 833, "seek": 280440, "start": 2809.4, "end": 2815.32, "text": " Any, anything you say here, you are going to be held to legally.", "tokens": [2639, 11, 1340, 291, 584, 510, 11, 291, 366, 516, 281, 312, 5167, 281, 21106, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 834, "seek": 280440, "start": 2815.32, "end": 2816.8, "text": " There may be interventions.", "tokens": [821, 815, 312, 20924, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 835, "seek": 280440, "start": 2816.8, "end": 2819.28, "text": " There may be interventions.", "tokens": [821, 815, 312, 20924, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 836, "seek": 280440, "start": 2819.28, "end": 2822.2000000000003, "text": " Legally and also like socially slash emotionally.", "tokens": [7470, 379, 293, 611, 411, 21397, 17330, 17991, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 837, "seek": 280440, "start": 2822.2000000000003, "end": 2824.12, "text": " Basically all of them.", "tokens": [8537, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 838, "seek": 280440, "start": 2824.12, "end": 2828.0, "text": " Any PRs that are mentioned here must be reviewed.", "tokens": [2639, 11568, 82, 300, 366, 2835, 510, 1633, 312, 18429, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 839, "seek": 280440, "start": 2828.0, "end": 2829.0, "text": " Or they will be closed.", "tokens": [1610, 436, 486, 312, 5395, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 840, "seek": 280440, "start": 2829.0, "end": 2830.0, "text": " Tell recursion monitor to close.", "tokens": [5115, 20560, 313, 6002, 281, 1998, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 841, "seek": 280440, "start": 2830.0, "end": 2833.92, "text": " I was going to say any PR that's mentioned here, I'm just going to go close.", "tokens": [286, 390, 516, 281, 584, 604, 11568, 300, 311, 2835, 510, 11, 286, 478, 445, 516, 281, 352, 1998, 13], "temperature": 0.0, "avg_logprob": -0.3893852233886719, "compression_ratio": 1.8073770491803278, "no_speech_prob": 7.071136678860057e-06}, {"id": 842, "seek": 283392, "start": 2833.92, "end": 2837.4, "text": " It's been reviewed.", "tokens": [467, 311, 668, 18429, 13], "temperature": 0.0, "avg_logprob": -0.36775068803267047, "compression_ratio": 1.4264705882352942, "no_speech_prob": 2.282495188410394e-05}, {"id": 843, "seek": 283392, "start": 2837.4, "end": 2844.12, "text": " Well, I don't make any PRs for Lambda because it's all closed.", "tokens": [1042, 11, 286, 500, 380, 652, 604, 11568, 82, 337, 45691, 570, 309, 311, 439, 5395, 13], "temperature": 0.0, "avg_logprob": -0.36775068803267047, "compression_ratio": 1.4264705882352942, "no_speech_prob": 2.282495188410394e-05}, {"id": 844, "seek": 283392, "start": 2844.12, "end": 2848.2400000000002, "text": " So yeah, I guess I'll dive in.", "tokens": [407, 1338, 11, 286, 2041, 286, 603, 9192, 294, 13], "temperature": 0.0, "avg_logprob": -0.36775068803267047, "compression_ratio": 1.4264705882352942, "no_speech_prob": 2.282495188410394e-05}, {"id": 845, "seek": 283392, "start": 2848.2400000000002, "end": 2856.48, "text": " I am excited for my year long hustling towards a Lambda version 1.1.", "tokens": [286, 669, 2919, 337, 452, 1064, 938, 25822, 1688, 3030, 257, 45691, 371, 433, 313, 502, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.36775068803267047, "compression_ratio": 1.4264705882352942, "no_speech_prob": 2.282495188410394e-05}, {"id": 846, "seek": 283392, "start": 2856.48, "end": 2860.28, "text": " Funnily enough that the feature I'm most excited about isn't actually one that I primarily", "tokens": [11166, 77, 953, 1547, 300, 264, 4111, 286, 478, 881, 2919, 466, 1943, 380, 767, 472, 300, 286, 10029], "temperature": 0.0, "avg_logprob": -0.36775068803267047, "compression_ratio": 1.4264705882352942, "no_speech_prob": 2.282495188410394e-05}, {"id": 847, "seek": 283392, "start": 2860.28, "end": 2861.28, "text": " worked on anyway.", "tokens": [2732, 322, 4033, 13], "temperature": 0.0, "avg_logprob": -0.36775068803267047, "compression_ratio": 1.4264705882352942, "no_speech_prob": 2.282495188410394e-05}, {"id": 848, "seek": 286128, "start": 2861.28, "end": 2864.2000000000003, "text": " So that's also kind of cool.", "tokens": [407, 300, 311, 611, 733, 295, 1627, 13], "temperature": 0.0, "avg_logprob": -0.25302061541327114, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.637186066247523e-06}, {"id": 849, "seek": 286128, "start": 2864.2000000000003, "end": 2870.88, "text": " So what I've been mostly working on is evergreen migration auto generation.", "tokens": [407, 437, 286, 600, 668, 5240, 1364, 322, 307, 1562, 27399, 17011, 8399, 5125, 13], "temperature": 0.0, "avg_logprob": -0.25302061541327114, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.637186066247523e-06}, {"id": 850, "seek": 286128, "start": 2870.88, "end": 2877.44, "text": " So for those that don't know in Lambda the migrations and deployments are type safe,", "tokens": [407, 337, 729, 300, 500, 380, 458, 294, 45691, 264, 6186, 12154, 293, 7274, 1117, 366, 2010, 3273, 11], "temperature": 0.0, "avg_logprob": -0.25302061541327114, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.637186066247523e-06}, {"id": 851, "seek": 286128, "start": 2877.44, "end": 2879.84, "text": " or at least my goal is for them to be 100% type safe.", "tokens": [420, 412, 1935, 452, 3387, 307, 337, 552, 281, 312, 2319, 4, 2010, 3273, 13], "temperature": 0.0, "avg_logprob": -0.25302061541327114, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.637186066247523e-06}, {"id": 852, "seek": 286128, "start": 2879.84, "end": 2884.52, "text": " So if it compiles locally, you know it's going to deploy and you know it's going to migrate", "tokens": [407, 498, 309, 715, 4680, 16143, 11, 291, 458, 309, 311, 516, 281, 7274, 293, 291, 458, 309, 311, 516, 281, 31821], "temperature": 0.0, "avg_logprob": -0.25302061541327114, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.637186066247523e-06}, {"id": 853, "seek": 286128, "start": 2884.52, "end": 2886.76, "text": " and everything's going to be lovely.", "tokens": [293, 1203, 311, 516, 281, 312, 7496, 13], "temperature": 0.0, "avg_logprob": -0.25302061541327114, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.637186066247523e-06}, {"id": 854, "seek": 286128, "start": 2886.76, "end": 2889.84, "text": " And so one of the problems of that introduced is Elm.", "tokens": [400, 370, 472, 295, 264, 2740, 295, 300, 7268, 307, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.25302061541327114, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.637186066247523e-06}, {"id": 855, "seek": 288984, "start": 2889.84, "end": 2896.6800000000003, "text": " So what Elm does have is it has equality between records that are equally defined, right?", "tokens": [407, 437, 2699, 76, 775, 362, 307, 309, 575, 14949, 1296, 7724, 300, 366, 12309, 7642, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2496689910888672, "compression_ratio": 1.8197879858657244, "no_speech_prob": 2.1567768726526992e-06}, {"id": 856, "seek": 288984, "start": 2896.6800000000003, "end": 2901.08, "text": " So if you have a record that has first name and last name as strings, and then somewhere", "tokens": [407, 498, 291, 362, 257, 2136, 300, 575, 700, 1315, 293, 1036, 1315, 382, 13985, 11, 293, 550, 4079], "temperature": 0.0, "avg_logprob": -0.2496689910888672, "compression_ratio": 1.8197879858657244, "no_speech_prob": 2.1567768726526992e-06}, {"id": 857, "seek": 288984, "start": 2901.08, "end": 2905.32, "text": " else in your project, you've defined identically the same record, and then you compare those", "tokens": [1646, 294, 428, 1716, 11, 291, 600, 7642, 2473, 984, 264, 912, 2136, 11, 293, 550, 291, 6794, 729], "temperature": 0.0, "avg_logprob": -0.2496689910888672, "compression_ratio": 1.8197879858657244, "no_speech_prob": 2.1567768726526992e-06}, {"id": 858, "seek": 288984, "start": 2905.32, "end": 2907.44, "text": " somewhere, they will be equal.", "tokens": [4079, 11, 436, 486, 312, 2681, 13], "temperature": 0.0, "avg_logprob": -0.2496689910888672, "compression_ratio": 1.8197879858657244, "no_speech_prob": 2.1567768726526992e-06}, {"id": 859, "seek": 288984, "start": 2907.44, "end": 2912.52, "text": " So there's kind of like the underlying reason for that is that there are really no records,", "tokens": [407, 456, 311, 733, 295, 411, 264, 14217, 1778, 337, 300, 307, 300, 456, 366, 534, 572, 7724, 11], "temperature": 0.0, "avg_logprob": -0.2496689910888672, "compression_ratio": 1.8197879858657244, "no_speech_prob": 2.1567768726526992e-06}, {"id": 860, "seek": 288984, "start": 2912.52, "end": 2916.76, "text": " all records are anonymous and you only have kind of type aliases for records.", "tokens": [439, 7724, 366, 24932, 293, 291, 787, 362, 733, 295, 2010, 10198, 1957, 337, 7724, 13], "temperature": 0.0, "avg_logprob": -0.2496689910888672, "compression_ratio": 1.8197879858657244, "no_speech_prob": 2.1567768726526992e-06}, {"id": 861, "seek": 288984, "start": 2916.76, "end": 2919.02, "text": " Whereas for custom types, that's not true.", "tokens": [13813, 337, 2375, 3467, 11, 300, 311, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.2496689910888672, "compression_ratio": 1.8197879858657244, "no_speech_prob": 2.1567768726526992e-06}, {"id": 862, "seek": 291902, "start": 2919.02, "end": 2923.16, "text": " So a custom type is not an alias, a custom type is kind of like a concrete primitive,", "tokens": [407, 257, 2375, 2010, 307, 406, 364, 419, 4609, 11, 257, 2375, 2010, 307, 733, 295, 411, 257, 9859, 28540, 11], "temperature": 0.0, "avg_logprob": -0.2534168243408203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 5.338120445230743e-06}, {"id": 863, "seek": 291902, "start": 2923.16, "end": 2924.32, "text": " if you will.", "tokens": [498, 291, 486, 13], "temperature": 0.0, "avg_logprob": -0.2534168243408203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 5.338120445230743e-06}, {"id": 864, "seek": 291902, "start": 2924.32, "end": 2929.92, "text": " So if you create a custom type that has, you know, a custom type of ice cream flavors that", "tokens": [407, 498, 291, 1884, 257, 2375, 2010, 300, 575, 11, 291, 458, 11, 257, 2375, 2010, 295, 4435, 4689, 16303, 300], "temperature": 0.0, "avg_logprob": -0.2534168243408203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 5.338120445230743e-06}, {"id": 865, "seek": 291902, "start": 2929.92, "end": 2934.16, "text": " has chocolate, vanilla and strawberry, and then in another file somewhere else, you create", "tokens": [575, 6215, 11, 17528, 293, 20440, 11, 293, 550, 294, 1071, 3991, 4079, 1646, 11, 291, 1884], "temperature": 0.0, "avg_logprob": -0.2534168243408203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 5.338120445230743e-06}, {"id": 866, "seek": 291902, "start": 2934.16, "end": 2938.48, "text": " exactly the same type with the same name and the same variance, creating two values of", "tokens": [2293, 264, 912, 2010, 365, 264, 912, 1315, 293, 264, 912, 21977, 11, 4084, 732, 4190, 295], "temperature": 0.0, "avg_logprob": -0.2534168243408203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 5.338120445230743e-06}, {"id": 867, "seek": 291902, "start": 2938.48, "end": 2942.84, "text": " those two different types and then comparing them, they won't, well, it'll be a type error,", "tokens": [729, 732, 819, 3467, 293, 550, 15763, 552, 11, 436, 1582, 380, 11, 731, 11, 309, 603, 312, 257, 2010, 6713, 11], "temperature": 0.0, "avg_logprob": -0.2534168243408203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 5.338120445230743e-06}, {"id": 868, "seek": 291902, "start": 2942.84, "end": 2945.16, "text": " you can't even compare them.", "tokens": [291, 393, 380, 754, 6794, 552, 13], "temperature": 0.0, "avg_logprob": -0.2534168243408203, "compression_ratio": 1.8988326848249026, "no_speech_prob": 5.338120445230743e-06}, {"id": 869, "seek": 294516, "start": 2945.16, "end": 2950.44, "text": " So the annoying thing about trying to apply the evergreen kind of philosophy to your apps", "tokens": [407, 264, 11304, 551, 466, 1382, 281, 3079, 264, 1562, 27399, 733, 295, 10675, 281, 428, 7733], "temperature": 0.0, "avg_logprob": -0.2545106532210011, "compression_ratio": 1.7383512544802868, "no_speech_prob": 1.3081704537398764e-06}, {"id": 870, "seek": 294516, "start": 2950.44, "end": 2957.68, "text": " is that basically between two different versions of your apps, we do snapshots to keep hold", "tokens": [307, 300, 1936, 1296, 732, 819, 9606, 295, 428, 7733, 11, 321, 360, 19206, 27495, 281, 1066, 1797], "temperature": 0.0, "avg_logprob": -0.2545106532210011, "compression_ratio": 1.7383512544802868, "no_speech_prob": 1.3081704537398764e-06}, {"id": 871, "seek": 294516, "start": 2957.68, "end": 2962.24, "text": " of what all your types were so that Lambdaera can know, well, you know, you've changed these", "tokens": [295, 437, 439, 428, 3467, 645, 370, 300, 45691, 1663, 393, 458, 11, 731, 11, 291, 458, 11, 291, 600, 3105, 613], "temperature": 0.0, "avg_logprob": -0.2545106532210011, "compression_ratio": 1.7383512544802868, "no_speech_prob": 1.3081704537398764e-06}, {"id": 872, "seek": 294516, "start": 2962.24, "end": 2964.3999999999996, "text": " types and this is exactly what's changing.", "tokens": [3467, 293, 341, 307, 2293, 437, 311, 4473, 13], "temperature": 0.0, "avg_logprob": -0.2545106532210011, "compression_ratio": 1.7383512544802868, "no_speech_prob": 1.3081704537398764e-06}, {"id": 873, "seek": 294516, "start": 2964.3999999999996, "end": 2969.68, "text": " And the frustrating thing has been that users kind of have to write out, even if you haven't", "tokens": [400, 264, 16522, 551, 575, 668, 300, 5022, 733, 295, 362, 281, 2464, 484, 11, 754, 498, 291, 2378, 380], "temperature": 0.0, "avg_logprob": -0.2545106532210011, "compression_ratio": 1.7383512544802868, "no_speech_prob": 1.3081704537398764e-06}, {"id": 874, "seek": 294516, "start": 2969.68, "end": 2972.3599999999997, "text": " changed a custom type, technically, according to Elm, it's changed, right?", "tokens": [3105, 257, 2375, 2010, 11, 12120, 11, 4650, 281, 2699, 76, 11, 309, 311, 3105, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2545106532210011, "compression_ratio": 1.7383512544802868, "no_speech_prob": 1.3081704537398764e-06}, {"id": 875, "seek": 297236, "start": 2972.36, "end": 2977.56, "text": " So you have to write this really big migration function to go, okay, every single old variant", "tokens": [407, 291, 362, 281, 2464, 341, 534, 955, 17011, 2445, 281, 352, 11, 1392, 11, 633, 2167, 1331, 17501], "temperature": 0.0, "avg_logprob": -0.21568630305865338, "compression_ratio": 1.791044776119403, "no_speech_prob": 3.089437996095512e-06}, {"id": 876, "seek": 297236, "start": 2977.56, "end": 2981.2000000000003, "text": " maps to every single new variant of this thing that, you know, hasn't changed.", "tokens": [11317, 281, 633, 2167, 777, 17501, 295, 341, 551, 300, 11, 291, 458, 11, 6132, 380, 3105, 13], "temperature": 0.0, "avg_logprob": -0.21568630305865338, "compression_ratio": 1.791044776119403, "no_speech_prob": 3.089437996095512e-06}, {"id": 877, "seek": 297236, "start": 2981.2000000000003, "end": 2982.6400000000003, "text": " And it's a bit annoying.", "tokens": [400, 309, 311, 257, 857, 11304, 13], "temperature": 0.0, "avg_logprob": -0.21568630305865338, "compression_ratio": 1.791044776119403, "no_speech_prob": 3.089437996095512e-06}, {"id": 878, "seek": 297236, "start": 2982.6400000000003, "end": 2986.52, "text": " So yeah, it's one of those things where I'm like, I feel slightly annoyed because it's", "tokens": [407, 1338, 11, 309, 311, 472, 295, 729, 721, 689, 286, 478, 411, 11, 286, 841, 4748, 25921, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.21568630305865338, "compression_ratio": 1.791044776119403, "no_speech_prob": 3.089437996095512e-06}, {"id": 879, "seek": 297236, "start": 2986.52, "end": 2991.92, "text": " like, it's not a feature that's delightful in the sense of like, hey, look at this new", "tokens": [411, 11, 309, 311, 406, 257, 4111, 300, 311, 35194, 294, 264, 2020, 295, 411, 11, 4177, 11, 574, 412, 341, 777], "temperature": 0.0, "avg_logprob": -0.21568630305865338, "compression_ratio": 1.791044776119403, "no_speech_prob": 3.089437996095512e-06}, {"id": 880, "seek": 297236, "start": 2991.92, "end": 2992.92, "text": " thing you've got.", "tokens": [551, 291, 600, 658, 13], "temperature": 0.0, "avg_logprob": -0.21568630305865338, "compression_ratio": 1.791044776119403, "no_speech_prob": 3.089437996095512e-06}, {"id": 881, "seek": 297236, "start": 2992.92, "end": 2998.7400000000002, "text": " And it's more like a, hey, look at this thing I fixed that was annoying and imposed by me.", "tokens": [400, 309, 311, 544, 411, 257, 11, 4177, 11, 574, 412, 341, 551, 286, 6806, 300, 390, 11304, 293, 26491, 538, 385, 13], "temperature": 0.0, "avg_logprob": -0.21568630305865338, "compression_ratio": 1.791044776119403, "no_speech_prob": 3.089437996095512e-06}, {"id": 882, "seek": 299874, "start": 2998.74, "end": 3003.16, "text": " So it has been not a huge amount of joy in that because I'm like, oh, it was kind of", "tokens": [407, 309, 575, 668, 406, 257, 2603, 2372, 295, 6258, 294, 300, 570, 286, 478, 411, 11, 1954, 11, 309, 390, 733, 295], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 883, "seek": 299874, "start": 3003.16, "end": 3004.64, "text": " just bad to start off with.", "tokens": [445, 1578, 281, 722, 766, 365, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 884, "seek": 299874, "start": 3004.64, "end": 3007.08, "text": " And I've had to figure out a way to patch it.", "tokens": [400, 286, 600, 632, 281, 2573, 484, 257, 636, 281, 9972, 309, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 885, "seek": 299874, "start": 3007.08, "end": 3009.08, "text": " So it'll save people a lot of time.", "tokens": [407, 309, 603, 3155, 561, 257, 688, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 886, "seek": 299874, "start": 3009.08, "end": 3011.56, "text": " I think 99% of migrations will now get written for you.", "tokens": [286, 519, 11803, 4, 295, 6186, 12154, 486, 586, 483, 3720, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 887, "seek": 299874, "start": 3011.56, "end": 3012.8799999999997, "text": " So I'm really excited about that.", "tokens": [407, 286, 478, 534, 2919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 888, "seek": 299874, "start": 3012.8799999999997, "end": 3015.9599999999996, "text": " But most people would just probably, I'm assuming just be like, oh, great.", "tokens": [583, 881, 561, 576, 445, 1391, 11, 286, 478, 11926, 445, 312, 411, 11, 1954, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 889, "seek": 299874, "start": 3015.9599999999996, "end": 3018.7599999999998, "text": " Now I don't have to do that annoying thing that you made me do.", "tokens": [823, 286, 500, 380, 362, 281, 360, 300, 11304, 551, 300, 291, 1027, 385, 360, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 890, "seek": 299874, "start": 3018.7599999999998, "end": 3021.6, "text": " And it's how it maybe should have been always.", "tokens": [400, 309, 311, 577, 309, 1310, 820, 362, 668, 1009, 13], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 891, "seek": 299874, "start": 3021.6, "end": 3025.3199999999997, "text": " But the second part of it, and there's a whole slew of features in there, but the one I'm", "tokens": [583, 264, 1150, 644, 295, 309, 11, 293, 456, 311, 257, 1379, 2426, 86, 295, 4122, 294, 456, 11, 457, 264, 472, 286, 478], "temperature": 0.0, "avg_logprob": -0.2435962493161121, "compression_ratio": 1.6918429003021147, "no_speech_prob": 2.3186250473372638e-05}, {"id": 892, "seek": 302532, "start": 3025.32, "end": 3030.44, "text": " super, super excited about, and I think even non-Lambdera enthusiasts will be excited about", "tokens": [1687, 11, 1687, 2919, 466, 11, 293, 286, 519, 754, 2107, 12, 43, 2173, 67, 1663, 45873, 486, 312, 2919, 466], "temperature": 0.0, "avg_logprob": -0.29109161778500203, "compression_ratio": 1.7945619335347431, "no_speech_prob": 7.410739726765314e-06}, {"id": 893, "seek": 302532, "start": 3030.44, "end": 3036.1200000000003, "text": " this is Martin Stewart came up with an implementation of an idea that I'd kind of thought and toyed", "tokens": [341, 307, 9184, 25951, 1361, 493, 365, 364, 11420, 295, 364, 1558, 300, 286, 1116, 733, 295, 1194, 293, 12058, 292], "temperature": 0.0, "avg_logprob": -0.29109161778500203, "compression_ratio": 1.7945619335347431, "no_speech_prob": 7.410739726765314e-06}, {"id": 894, "seek": 302532, "start": 3036.1200000000003, "end": 3038.6400000000003, "text": " with, but he really kind of nailed it down.", "tokens": [365, 11, 457, 415, 534, 733, 295, 30790, 309, 760, 13], "temperature": 0.0, "avg_logprob": -0.29109161778500203, "compression_ratio": 1.7945619335347431, "no_speech_prob": 7.410739726765314e-06}, {"id": 895, "seek": 302532, "start": 3038.6400000000003, "end": 3042.7200000000003, "text": " So basically in the next version of Lambdera, what you'll be able to do is you boot up Lambdera", "tokens": [407, 1936, 294, 264, 958, 3037, 295, 441, 2173, 67, 1663, 11, 437, 291, 603, 312, 1075, 281, 360, 307, 291, 11450, 493, 441, 2173, 67, 1663], "temperature": 0.0, "avg_logprob": -0.29109161778500203, "compression_ratio": 1.7945619335347431, "no_speech_prob": 7.410739726765314e-06}, {"id": 896, "seek": 302532, "start": 3042.7200000000003, "end": 3047.1200000000003, "text": " Live, you browse around anywhere in your project, you want to edit a particular piece of the", "tokens": [10385, 11, 291, 31442, 926, 4992, 294, 428, 1716, 11, 291, 528, 281, 8129, 257, 1729, 2522, 295, 264], "temperature": 0.0, "avg_logprob": -0.29109161778500203, "compression_ratio": 1.7945619335347431, "no_speech_prob": 7.410739726765314e-06}, {"id": 897, "seek": 302532, "start": 3047.1200000000003, "end": 3051.48, "text": " UI, you've navigated somewhere deep or you've navigated into some complex state and you'll", "tokens": [15682, 11, 291, 600, 7407, 770, 4079, 2452, 420, 291, 600, 7407, 770, 666, 512, 3997, 1785, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.29109161778500203, "compression_ratio": 1.7945619335347431, "no_speech_prob": 7.410739726765314e-06}, {"id": 898, "seek": 302532, "start": 3051.48, "end": 3054.82, "text": " notice a borders off where there's something's wrong and you want to go there.", "tokens": [3449, 257, 16287, 766, 689, 456, 311, 746, 311, 2085, 293, 291, 528, 281, 352, 456, 13], "temperature": 0.0, "avg_logprob": -0.29109161778500203, "compression_ratio": 1.7945619335347431, "no_speech_prob": 7.410739726765314e-06}, {"id": 899, "seek": 305482, "start": 3054.82, "end": 3059.7200000000003, "text": " And I think now the experience, at least for me, is I kind of jump into my editor and if", "tokens": [400, 286, 519, 586, 264, 1752, 11, 412, 1935, 337, 385, 11, 307, 286, 733, 295, 3012, 666, 452, 9839, 293, 498], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 900, "seek": 305482, "start": 3059.7200000000003, "end": 3064.0, "text": " I'm lucky, I'll kind of guess the name of the identifier that I'm kind of globally searching", "tokens": [286, 478, 6356, 11, 286, 603, 733, 295, 2041, 264, 1315, 295, 264, 45690, 300, 286, 478, 733, 295, 18958, 10808], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 901, "seek": 305482, "start": 3064.0, "end": 3065.96, "text": " for and maybe I'll find it.", "tokens": [337, 293, 1310, 286, 603, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 902, "seek": 305482, "start": 3065.96, "end": 3069.28, "text": " Or maybe on the page, there's a bit of text and you're like, oh, that looks like a bit", "tokens": [1610, 1310, 322, 264, 3028, 11, 456, 311, 257, 857, 295, 2487, 293, 291, 434, 411, 11, 1954, 11, 300, 1542, 411, 257, 857], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 903, "seek": 305482, "start": 3069.28, "end": 3070.28, "text": " of unique text.", "tokens": [295, 3845, 2487, 13], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 904, "seek": 305482, "start": 3070.28, "end": 3073.56, "text": " Maybe I'll just copy that text and search for that and maybe I'll find the element.", "tokens": [2704, 286, 603, 445, 5055, 300, 2487, 293, 3164, 337, 300, 293, 1310, 286, 603, 915, 264, 4478, 13], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 905, "seek": 305482, "start": 3073.56, "end": 3077.52, "text": " But a lot of the time, especially as a project I've not worked on for ages, you just, you", "tokens": [583, 257, 688, 295, 264, 565, 11, 2318, 382, 257, 1716, 286, 600, 406, 2732, 322, 337, 12357, 11, 291, 445, 11, 291], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 906, "seek": 305482, "start": 3077.52, "end": 3079.0, "text": " can't remember where stuff is, right?", "tokens": [393, 380, 1604, 689, 1507, 307, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 907, "seek": 305482, "start": 3079.0, "end": 3081.5800000000004, "text": " And everything's slightly, it's just a little bit of friction.", "tokens": [400, 1203, 311, 4748, 11, 309, 311, 445, 257, 707, 857, 295, 17710, 13], "temperature": 0.0, "avg_logprob": -0.2563625661338248, "compression_ratio": 1.8061538461538462, "no_speech_prob": 2.48232845478924e-06}, {"id": 908, "seek": 308158, "start": 3081.58, "end": 3085.0, "text": " And so now instead of doing the thing that you were thinking like, oh, I need to do that,", "tokens": [400, 370, 586, 2602, 295, 884, 264, 551, 300, 291, 645, 1953, 411, 11, 1954, 11, 286, 643, 281, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.21283568105389994, "compression_ratio": 1.7174721189591078, "no_speech_prob": 3.2377104162151227e-06}, {"id": 909, "seek": 308158, "start": 3085.0, "end": 3089.16, "text": " you've had to switch modes, switch gears entirely to where's the thing.", "tokens": [291, 600, 632, 281, 3679, 14068, 11, 3679, 20915, 7696, 281, 689, 311, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.21283568105389994, "compression_ratio": 1.7174721189591078, "no_speech_prob": 3.2377104162151227e-06}, {"id": 910, "seek": 308158, "start": 3089.16, "end": 3091.36, "text": " And then when you find the thing, you're like, what was I doing again?", "tokens": [400, 550, 562, 291, 915, 264, 551, 11, 291, 434, 411, 11, 437, 390, 286, 884, 797, 30], "temperature": 0.0, "avg_logprob": -0.21283568105389994, "compression_ratio": 1.7174721189591078, "no_speech_prob": 3.2377104162151227e-06}, {"id": 911, "seek": 308158, "start": 3091.36, "end": 3096.04, "text": " And it's just a bit of mental kind of impediment.", "tokens": [400, 309, 311, 445, 257, 857, 295, 4973, 733, 295, 22584, 2328, 13], "temperature": 0.0, "avg_logprob": -0.21283568105389994, "compression_ratio": 1.7174721189591078, "no_speech_prob": 3.2377104162151227e-06}, {"id": 912, "seek": 308158, "start": 3096.04, "end": 3099.7999999999997, "text": " So yeah, now with the feature that Martin's worked on and we've kind of worked together", "tokens": [407, 1338, 11, 586, 365, 264, 4111, 300, 9184, 311, 2732, 322, 293, 321, 600, 733, 295, 2732, 1214], "temperature": 0.0, "avg_logprob": -0.21283568105389994, "compression_ratio": 1.7174721189591078, "no_speech_prob": 3.2377104162151227e-06}, {"id": 913, "seek": 308158, "start": 3099.7999999999997, "end": 3105.96, "text": " to integrate into the compiler, you can hover over any UI element, hit a hotkey, it'll give", "tokens": [281, 13365, 666, 264, 31958, 11, 291, 393, 20076, 670, 604, 15682, 4478, 11, 2045, 257, 2368, 4119, 11, 309, 603, 976], "temperature": 0.0, "avg_logprob": -0.21283568105389994, "compression_ratio": 1.7174721189591078, "no_speech_prob": 3.2377104162151227e-06}, {"id": 914, "seek": 310596, "start": 3105.96, "end": 3112.44, "text": " you a dropdown of the kind of tree of functions that were involved in generating that particular", "tokens": [291, 257, 47599, 295, 264, 733, 295, 4230, 295, 6828, 300, 645, 3288, 294, 17746, 300, 1729], "temperature": 0.0, "avg_logprob": -0.23316005267928133, "compression_ratio": 1.7233201581027668, "no_speech_prob": 4.860380613536108e-06}, {"id": 915, "seek": 310596, "start": 3112.44, "end": 3113.7200000000003, "text": " piece of UI.", "tokens": [2522, 295, 15682, 13], "temperature": 0.0, "avg_logprob": -0.23316005267928133, "compression_ratio": 1.7233201581027668, "no_speech_prob": 4.860380613536108e-06}, {"id": 916, "seek": 310596, "start": 3113.7200000000003, "end": 3119.08, "text": " And if you click on any of them, it'll try and find your current editor and open up like", "tokens": [400, 498, 291, 2052, 322, 604, 295, 552, 11, 309, 603, 853, 293, 915, 428, 2190, 9839, 293, 1269, 493, 411], "temperature": 0.0, "avg_logprob": -0.23316005267928133, "compression_ratio": 1.7233201581027668, "no_speech_prob": 4.860380613536108e-06}, {"id": 917, "seek": 310596, "start": 3119.08, "end": 3125.02, "text": " directly that line and like the column and the row of the exact expression that is generating", "tokens": [3838, 300, 1622, 293, 411, 264, 7738, 293, 264, 5386, 295, 264, 1900, 6114, 300, 307, 17746], "temperature": 0.0, "avg_logprob": -0.23316005267928133, "compression_ratio": 1.7233201581027668, "no_speech_prob": 4.860380613536108e-06}, {"id": 918, "seek": 310596, "start": 3125.02, "end": 3126.02, "text": " that UI element.", "tokens": [300, 15682, 4478, 13], "temperature": 0.0, "avg_logprob": -0.23316005267928133, "compression_ratio": 1.7233201581027668, "no_speech_prob": 4.860380613536108e-06}, {"id": 919, "seek": 310596, "start": 3126.02, "end": 3130.08, "text": " So it's kind of, I'm calling it UI source maps or like live UI source maps.", "tokens": [407, 309, 311, 733, 295, 11, 286, 478, 5141, 309, 15682, 4009, 11317, 420, 411, 1621, 15682, 4009, 11317, 13], "temperature": 0.0, "avg_logprob": -0.23316005267928133, "compression_ratio": 1.7233201581027668, "no_speech_prob": 4.860380613536108e-06}, {"id": 920, "seek": 310596, "start": 3130.08, "end": 3134.66, "text": " So I'm, yeah, I'm phenomenally excited about this.", "tokens": [407, 286, 478, 11, 1338, 11, 286, 478, 9388, 379, 2919, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.23316005267928133, "compression_ratio": 1.7233201581027668, "no_speech_prob": 4.860380613536108e-06}, {"id": 921, "seek": 313466, "start": 3134.66, "end": 3138.52, "text": " And yeah, the moment that I started using it, I was like, how have I not had this?", "tokens": [400, 1338, 11, 264, 1623, 300, 286, 1409, 1228, 309, 11, 286, 390, 411, 11, 577, 362, 286, 406, 632, 341, 30], "temperature": 0.0, "avg_logprob": -0.23495339590405662, "compression_ratio": 1.6460481099656357, "no_speech_prob": 4.637778602045728e-06}, {"id": 922, "seek": 313466, "start": 3138.52, "end": 3140.92, "text": " Like now I can't not have it.", "tokens": [1743, 586, 286, 393, 380, 406, 362, 309, 13], "temperature": 0.0, "avg_logprob": -0.23495339590405662, "compression_ratio": 1.6460481099656357, "no_speech_prob": 4.637778602045728e-06}, {"id": 923, "seek": 313466, "start": 3140.92, "end": 3145.2, "text": " And the exciting thing for other Elm users will be because one of my goals of the Lambdaera", "tokens": [400, 264, 4670, 551, 337, 661, 2699, 76, 5022, 486, 312, 570, 472, 295, 452, 5493, 295, 264, 45691, 1663], "temperature": 0.0, "avg_logprob": -0.23495339590405662, "compression_ratio": 1.6460481099656357, "no_speech_prob": 4.637778602045728e-06}, {"id": 924, "seek": 313466, "start": 3145.2, "end": 3147.8799999999997, "text": " compiler is for it to be backwards compatible.", "tokens": [31958, 307, 337, 309, 281, 312, 12204, 18218, 13], "temperature": 0.0, "avg_logprob": -0.23495339590405662, "compression_ratio": 1.6460481099656357, "no_speech_prob": 4.637778602045728e-06}, {"id": 925, "seek": 313466, "start": 3147.8799999999997, "end": 3152.42, "text": " You should be able to, well, we have to figure this out how that'll integrate into other", "tokens": [509, 820, 312, 1075, 281, 11, 731, 11, 321, 362, 281, 2573, 341, 484, 577, 300, 603, 13365, 666, 661], "temperature": 0.0, "avg_logprob": -0.23495339590405662, "compression_ratio": 1.6460481099656357, "no_speech_prob": 4.637778602045728e-06}, {"id": 926, "seek": 313466, "start": 3152.42, "end": 3159.12, "text": " pipelines, but there should be a pathway for using this feature on just a regular Elm project,", "tokens": [40168, 11, 457, 456, 820, 312, 257, 18590, 337, 1228, 341, 4111, 322, 445, 257, 3890, 2699, 76, 1716, 11], "temperature": 0.0, "avg_logprob": -0.23495339590405662, "compression_ratio": 1.6460481099656357, "no_speech_prob": 4.637778602045728e-06}, {"id": 927, "seek": 313466, "start": 3159.12, "end": 3160.92, "text": " which I think would be really, really cool.", "tokens": [597, 286, 519, 576, 312, 534, 11, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.23495339590405662, "compression_ratio": 1.6460481099656357, "no_speech_prob": 4.637778602045728e-06}, {"id": 928, "seek": 316092, "start": 3160.92, "end": 3165.6800000000003, "text": " But I think that as a whole release thing, I'm excited about that, but I think that segue", "tokens": [583, 286, 519, 300, 382, 257, 1379, 4374, 551, 11, 286, 478, 2919, 466, 300, 11, 457, 286, 519, 300, 33850], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 929, "seek": 316092, "start": 3165.6800000000003, "end": 3169.56, "text": " is I'm going to just throw the baton over to Matt for the other project that we've been", "tokens": [307, 286, 478, 516, 281, 445, 3507, 264, 7362, 266, 670, 281, 7397, 337, 264, 661, 1716, 300, 321, 600, 668], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 930, "seek": 316092, "start": 3169.56, "end": 3174.08, "text": " kind of on and off collaborating for this whole year, because yeah, this feature, I", "tokens": [733, 295, 322, 293, 766, 30188, 337, 341, 1379, 1064, 11, 570, 1338, 11, 341, 4111, 11, 286], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 931, "seek": 316092, "start": 3174.08, "end": 3176.6, "text": " want to end up in that tool as well.", "tokens": [528, 281, 917, 493, 294, 300, 2290, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 932, "seek": 316092, "start": 3176.6, "end": 3177.96, "text": " Yeah, a thousand percent.", "tokens": [865, 11, 257, 4714, 3043, 13], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 933, "seek": 316092, "start": 3177.96, "end": 3183.88, "text": " So this is something I initially sort of announced at Strangeloop for my talk, Code Gen with", "tokens": [407, 341, 307, 746, 286, 9105, 1333, 295, 7548, 412, 8251, 656, 10590, 404, 337, 452, 751, 11, 15549, 3632, 365], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 934, "seek": 316092, "start": 3183.88, "end": 3184.88, "text": " Types.", "tokens": [5569, 5190, 13], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 935, "seek": 316092, "start": 3184.88, "end": 3187.6, "text": " And you'll see a little demo at the end of something that's possible.", "tokens": [400, 291, 603, 536, 257, 707, 10723, 412, 264, 917, 295, 746, 300, 311, 1944, 13], "temperature": 0.0, "avg_logprob": -0.29270522274188143, "compression_ratio": 1.6860068259385665, "no_speech_prob": 2.8572928840731038e-06}, {"id": 936, "seek": 318760, "start": 3187.6, "end": 3193.2, "text": " If you go look for that talk and watch it, it's at the end, so you'll have to stay till", "tokens": [759, 291, 352, 574, 337, 300, 751, 293, 1159, 309, 11, 309, 311, 412, 264, 917, 11, 370, 291, 603, 362, 281, 1754, 4288], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 937, "seek": 318760, "start": 3193.2, "end": 3194.52, "text": " the end.", "tokens": [264, 917, 13], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 938, "seek": 318760, "start": 3194.52, "end": 3196.92, "text": " So this project is called ElmDev.", "tokens": [407, 341, 1716, 307, 1219, 2699, 76, 11089, 85, 13], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 939, "seek": 318760, "start": 3196.92, "end": 3199.2, "text": " There's a placeholder website at elm.dev.", "tokens": [821, 311, 257, 1081, 20480, 3144, 412, 806, 76, 13, 40343, 13], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 940, "seek": 318760, "start": 3199.2, "end": 3202.36, "text": " I actually got that domain, which I'm so happy about.", "tokens": [286, 767, 658, 300, 9274, 11, 597, 286, 478, 370, 2055, 466, 13], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 941, "seek": 318760, "start": 3202.36, "end": 3210.1, "text": " What ElmDev is, is again, talking about how Dillon was like, well, in Elm, we sort of try", "tokens": [708, 2699, 76, 11089, 85, 307, 11, 307, 797, 11, 1417, 466, 577, 28160, 390, 411, 11, 731, 11, 294, 2699, 76, 11, 321, 1333, 295, 853], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 942, "seek": 318760, "start": 3210.1, "end": 3212.7599999999998, "text": " to rethink things maybe from principles, first principles.", "tokens": [281, 34595, 721, 1310, 490, 9156, 11, 700, 9156, 13], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 943, "seek": 318760, "start": 3212.7599999999998, "end": 3216.6, "text": " Not that that's always the way, but like, it can be very nice.", "tokens": [1726, 300, 300, 311, 1009, 264, 636, 11, 457, 411, 11, 309, 393, 312, 588, 1481, 13], "temperature": 0.0, "avg_logprob": -0.23085561439172544, "compression_ratio": 1.6066176470588236, "no_speech_prob": 5.337891252565896e-06}, {"id": 944, "seek": 321660, "start": 3216.6, "end": 3224.44, "text": " So ElmDev is basically support for editor tooling based on a version of the Elm compiler.", "tokens": [407, 2699, 76, 11089, 85, 307, 1936, 1406, 337, 9839, 46593, 2361, 322, 257, 3037, 295, 264, 2699, 76, 31958, 13], "temperature": 0.0, "avg_logprob": -0.2262063962276851, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.0903726181131788e-06}, {"id": 945, "seek": 321660, "start": 3224.44, "end": 3228.7599999999998, "text": " So this is something that Mario and I have been talking about and have been working on", "tokens": [407, 341, 307, 746, 300, 9343, 293, 286, 362, 668, 1417, 466, 293, 362, 668, 1364, 322], "temperature": 0.0, "avg_logprob": -0.2262063962276851, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.0903726181131788e-06}, {"id": 946, "seek": 321660, "start": 3228.7599999999998, "end": 3230.52, "text": " for actually a decent amount of time.", "tokens": [337, 767, 257, 8681, 2372, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.2262063962276851, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.0903726181131788e-06}, {"id": 947, "seek": 321660, "start": 3230.52, "end": 3234.72, "text": " The idea is that, you know, the compiler actually knows a lot about your project.", "tokens": [440, 1558, 307, 300, 11, 291, 458, 11, 264, 31958, 767, 3255, 257, 688, 466, 428, 1716, 13], "temperature": 0.0, "avg_logprob": -0.2262063962276851, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.0903726181131788e-06}, {"id": 948, "seek": 321660, "start": 3234.72, "end": 3239.04, "text": " And a lot of that information could be surfaced in really interesting ways.", "tokens": [400, 257, 688, 295, 300, 1589, 727, 312, 9684, 3839, 294, 534, 1880, 2098, 13], "temperature": 0.0, "avg_logprob": -0.2262063962276851, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.0903726181131788e-06}, {"id": 949, "seek": 321660, "start": 3239.04, "end": 3241.64, "text": " Like yeah, you could have a mapping to like a view function.", "tokens": [1743, 1338, 11, 291, 727, 362, 257, 18350, 281, 411, 257, 1910, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2262063962276851, "compression_ratio": 1.6463878326996197, "no_speech_prob": 2.0903726181131788e-06}, {"id": 950, "seek": 324164, "start": 3241.64, "end": 3248.6, "text": " You could have like, I think actually navigation of your code base in different ways than files.", "tokens": [509, 727, 362, 411, 11, 286, 519, 767, 17346, 295, 428, 3089, 3096, 294, 819, 2098, 813, 7098, 13], "temperature": 0.0, "avg_logprob": -0.22649971835584526, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.8447510683472501e-06}, {"id": 951, "seek": 324164, "start": 3248.6, "end": 3252.9, "text": " You could have like information from the compiler about your code.", "tokens": [509, 727, 362, 411, 1589, 490, 264, 31958, 466, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22649971835584526, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.8447510683472501e-06}, {"id": 952, "seek": 324164, "start": 3252.9, "end": 3257.44, "text": " So this would be like essentially like bringing the static analysis that the compiler already", "tokens": [407, 341, 576, 312, 411, 4476, 411, 5062, 264, 13437, 5215, 300, 264, 31958, 1217], "temperature": 0.0, "avg_logprob": -0.22649971835584526, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.8447510683472501e-06}, {"id": 953, "seek": 324164, "start": 3257.44, "end": 3260.44, "text": " does and surfacing that when you can.", "tokens": [775, 293, 9684, 5615, 300, 562, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.22649971835584526, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.8447510683472501e-06}, {"id": 954, "seek": 324164, "start": 3260.44, "end": 3265.8399999999997, "text": " And I was just very excited because it seemed like there were so many interesting avenues", "tokens": [400, 286, 390, 445, 588, 2919, 570, 309, 6576, 411, 456, 645, 370, 867, 1880, 43039], "temperature": 0.0, "avg_logprob": -0.22649971835584526, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.8447510683472501e-06}, {"id": 955, "seek": 326584, "start": 3265.84, "end": 3271.6000000000004, "text": " for editing that an Elm would be like ideal for a lot of this because of just how it's", "tokens": [337, 10000, 300, 364, 2699, 76, 576, 312, 411, 7157, 337, 257, 688, 295, 341, 570, 295, 445, 577, 309, 311], "temperature": 0.0, "avg_logprob": -0.21761343771951241, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.7330033870166517e-06}, {"id": 956, "seek": 326584, "start": 3271.6000000000004, "end": 3277.84, "text": " sort of put together that we'd be able, that I wanted to explore personally.", "tokens": [1333, 295, 829, 1214, 300, 321, 1116, 312, 1075, 11, 300, 286, 1415, 281, 6839, 5665, 13], "temperature": 0.0, "avg_logprob": -0.21761343771951241, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.7330033870166517e-06}, {"id": 957, "seek": 326584, "start": 3277.84, "end": 3282.92, "text": " And the main blocker seemed to be that, you know, there's this sort of this concept of", "tokens": [400, 264, 2135, 3461, 260, 6576, 281, 312, 300, 11, 291, 458, 11, 456, 311, 341, 1333, 295, 341, 3410, 295], "temperature": 0.0, "avg_logprob": -0.21761343771951241, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.7330033870166517e-06}, {"id": 958, "seek": 326584, "start": 3282.92, "end": 3287.96, "text": " that and Mario, let me know if I'm getting ahead of myself, but like that, you know,", "tokens": [300, 293, 9343, 11, 718, 385, 458, 498, 286, 478, 1242, 2286, 295, 2059, 11, 457, 411, 300, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.21761343771951241, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.7330033870166517e-06}, {"id": 959, "seek": 326584, "start": 3287.96, "end": 3293.48, "text": " a batch compiler, a compiler that is meant to run, you know, you fire it and then it", "tokens": [257, 15245, 31958, 11, 257, 31958, 300, 307, 4140, 281, 1190, 11, 291, 458, 11, 291, 2610, 309, 293, 550, 309], "temperature": 0.0, "avg_logprob": -0.21761343771951241, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.7330033870166517e-06}, {"id": 960, "seek": 329348, "start": 3293.48, "end": 3298.08, "text": " finishes is just going to be too slow for editor tooling.", "tokens": [23615, 307, 445, 516, 281, 312, 886, 2964, 337, 9839, 46593, 13], "temperature": 0.0, "avg_logprob": -0.21730326804794184, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.0844998996617505e-06}, {"id": 961, "seek": 329348, "start": 3298.08, "end": 3304.16, "text": " And of course, if you're Rust or if you're Haskell, this is probably true just because,", "tokens": [400, 295, 1164, 11, 498, 291, 434, 34952, 420, 498, 291, 434, 8646, 43723, 11, 341, 307, 1391, 2074, 445, 570, 11], "temperature": 0.0, "avg_logprob": -0.21730326804794184, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.0844998996617505e-06}, {"id": 962, "seek": 329348, "start": 3304.16, "end": 3307.96, "text": " but your language is vastly more complicated than Elms in a lot of ways.", "tokens": [457, 428, 2856, 307, 41426, 544, 6179, 813, 2699, 2592, 294, 257, 688, 295, 2098, 13], "temperature": 0.0, "avg_logprob": -0.21730326804794184, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.0844998996617505e-06}, {"id": 963, "seek": 329348, "start": 3307.96, "end": 3312.5, "text": " And also Evans put a lot of effort into making the Elm compiler efficient.", "tokens": [400, 611, 30055, 829, 257, 688, 295, 4630, 666, 1455, 264, 2699, 76, 31958, 7148, 13], "temperature": 0.0, "avg_logprob": -0.21730326804794184, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.0844998996617505e-06}, {"id": 964, "seek": 329348, "start": 3312.5, "end": 3314.46, "text": " It is very efficient.", "tokens": [467, 307, 588, 7148, 13], "temperature": 0.0, "avg_logprob": -0.21730326804794184, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.0844998996617505e-06}, {"id": 965, "seek": 329348, "start": 3314.46, "end": 3318.48, "text": " So what we've been working on is with our version of Elm dev, our version of the Elm", "tokens": [407, 437, 321, 600, 668, 1364, 322, 307, 365, 527, 3037, 295, 2699, 76, 1905, 11, 527, 3037, 295, 264, 2699, 76], "temperature": 0.0, "avg_logprob": -0.21730326804794184, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.0844998996617505e-06}, {"id": 966, "seek": 329348, "start": 3318.48, "end": 3322.68, "text": " compiler is what couldn't we cache in memory?", "tokens": [31958, 307, 437, 2809, 380, 321, 19459, 294, 4675, 30], "temperature": 0.0, "avg_logprob": -0.21730326804794184, "compression_ratio": 1.702290076335878, "no_speech_prob": 1.0844998996617505e-06}, {"id": 967, "seek": 332268, "start": 3322.68, "end": 3326.2999999999997, "text": " Like can this be a running server that we talked to and can we cache some of these things", "tokens": [1743, 393, 341, 312, 257, 2614, 7154, 300, 321, 2825, 281, 293, 393, 321, 19459, 512, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.24552655687519148, "compression_ratio": 1.59375, "no_speech_prob": 1.553422862343723e-06}, {"id": 968, "seek": 332268, "start": 3326.2999999999997, "end": 3327.4199999999996, "text": " to make it fast?", "tokens": [281, 652, 309, 2370, 30], "temperature": 0.0, "avg_logprob": -0.24552655687519148, "compression_ratio": 1.59375, "no_speech_prob": 1.553422862343723e-06}, {"id": 969, "seek": 332268, "start": 3327.4199999999996, "end": 3332.8799999999997, "text": " And like, I, we think that that's, that's true from what we're seeing.", "tokens": [400, 411, 11, 286, 11, 321, 519, 300, 300, 311, 11, 300, 311, 2074, 490, 437, 321, 434, 2577, 13], "temperature": 0.0, "avg_logprob": -0.24552655687519148, "compression_ratio": 1.59375, "no_speech_prob": 1.553422862343723e-06}, {"id": 970, "seek": 332268, "start": 3332.8799999999997, "end": 3338.08, "text": " We can't probably talk to specifics on that, but, and there's still a lot of work to be", "tokens": [492, 393, 380, 1391, 751, 281, 28454, 322, 300, 11, 457, 11, 293, 456, 311, 920, 257, 688, 295, 589, 281, 312], "temperature": 0.0, "avg_logprob": -0.24552655687519148, "compression_ratio": 1.59375, "no_speech_prob": 1.553422862343723e-06}, {"id": 971, "seek": 332268, "start": 3338.08, "end": 3341.9199999999996, "text": " done, but very excited for what this opens up.", "tokens": [1096, 11, 457, 588, 2919, 337, 437, 341, 9870, 493, 13], "temperature": 0.0, "avg_logprob": -0.24552655687519148, "compression_ratio": 1.59375, "no_speech_prob": 1.553422862343723e-06}, {"id": 972, "seek": 332268, "start": 3341.9199999999996, "end": 3345.64, "text": " And this is to give a little bit of context.", "tokens": [400, 341, 307, 281, 976, 257, 707, 857, 295, 4319, 13], "temperature": 0.0, "avg_logprob": -0.24552655687519148, "compression_ratio": 1.59375, "no_speech_prob": 1.553422862343723e-06}, {"id": 973, "seek": 334564, "start": 3345.64, "end": 3354.48, "text": " When we had Simon Lidell on for the Elmwatch tool, he kind of hinted at this idea of like,", "tokens": [1133, 321, 632, 13193, 441, 482, 285, 322, 337, 264, 2699, 76, 15219, 2290, 11, 415, 733, 295, 12075, 292, 412, 341, 1558, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.2630612136971237, "compression_ratio": 1.6022727272727273, "no_speech_prob": 5.714078383789456e-07}, {"id": 974, "seek": 334564, "start": 3354.48, "end": 3361.96, "text": " okay, I can make the feedback cycle faster with this nice Elm live dev server environment", "tokens": [1392, 11, 286, 393, 652, 264, 5824, 6586, 4663, 365, 341, 1481, 2699, 76, 1621, 1905, 7154, 2823], "temperature": 0.0, "avg_logprob": -0.2630612136971237, "compression_ratio": 1.6022727272727273, "no_speech_prob": 5.714078383789456e-07}, {"id": 975, "seek": 334564, "start": 3361.96, "end": 3364.92, "text": " that, you know, compiles things very quickly.", "tokens": [300, 11, 291, 458, 11, 715, 4680, 721, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.2630612136971237, "compression_ratio": 1.6022727272727273, "no_speech_prob": 5.714078383789456e-07}, {"id": 976, "seek": 334564, "start": 3364.92, "end": 3369.16, "text": " But if I, like he was getting hooked on making it faster and he's like, well, if I really", "tokens": [583, 498, 286, 11, 411, 415, 390, 1242, 20410, 322, 1455, 309, 4663, 293, 415, 311, 411, 11, 731, 11, 498, 286, 534], "temperature": 0.0, "avg_logprob": -0.2630612136971237, "compression_ratio": 1.6022727272727273, "no_speech_prob": 5.714078383789456e-07}, {"id": 977, "seek": 334564, "start": 3369.16, "end": 3372.56, "text": " wanted to make it faster, I would have it in memory.", "tokens": [1415, 281, 652, 309, 4663, 11, 286, 576, 362, 309, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.2630612136971237, "compression_ratio": 1.6022727272727273, "no_speech_prob": 5.714078383789456e-07}, {"id": 978, "seek": 334564, "start": 3372.56, "end": 3375.6, "text": " And yeah, that's basically what you're talking about.", "tokens": [400, 1338, 11, 300, 311, 1936, 437, 291, 434, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.2630612136971237, "compression_ratio": 1.6022727272727273, "no_speech_prob": 5.714078383789456e-07}, {"id": 979, "seek": 337560, "start": 3375.6, "end": 3376.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 980, "seek": 337560, "start": 3376.6, "end": 3377.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 981, "seek": 337560, "start": 3377.6, "end": 3378.6, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 982, "seek": 337560, "start": 3378.6, "end": 3388.16, "text": " I find it funny that I've been working very hard to make ElmReviews cache that is in memory,", "tokens": [286, 915, 309, 4074, 300, 286, 600, 668, 1364, 588, 1152, 281, 652, 2699, 76, 8524, 1759, 82, 19459, 300, 307, 294, 4675, 11], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 983, "seek": 337560, "start": 3388.16, "end": 3391.88, "text": " put it on the file system and you've been trying to do the opposite.", "tokens": [829, 309, 322, 264, 3991, 1185, 293, 291, 600, 668, 1382, 281, 360, 264, 6182, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 984, "seek": 337560, "start": 3391.88, "end": 3392.88, "text": " The opposite.", "tokens": [440, 6182, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 985, "seek": 337560, "start": 3392.88, "end": 3393.88, "text": " That's right.", "tokens": [663, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 986, "seek": 337560, "start": 3393.88, "end": 3394.88, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 987, "seek": 337560, "start": 3394.88, "end": 3395.88, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 988, "seek": 337560, "start": 3395.88, "end": 3399.6, "text": " So one, one anecdote there, and like Matt said, like there's some, a lot of this work", "tokens": [407, 472, 11, 472, 49845, 456, 11, 293, 411, 7397, 848, 11, 411, 456, 311, 512, 11, 257, 688, 295, 341, 589], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 989, "seek": 337560, "start": 3399.6, "end": 3401.08, "text": " is, I mean, it's looking pretty promising.", "tokens": [307, 11, 286, 914, 11, 309, 311, 1237, 1238, 20257, 13], "temperature": 0.0, "avg_logprob": -0.24432319507264255, "compression_ratio": 1.5482456140350878, "no_speech_prob": 9.079168194148224e-06}, {"id": 990, "seek": 340108, "start": 3401.08, "end": 3407.16, "text": " I think Matt's currently been playing around with the kind of memory model work that I'm", "tokens": [286, 519, 7397, 311, 4362, 668, 2433, 926, 365, 264, 733, 295, 4675, 2316, 589, 300, 286, 478], "temperature": 0.0, "avg_logprob": -0.2959502507182001, "compression_ratio": 1.759493670886076, "no_speech_prob": 4.156741852057166e-06}, {"id": 991, "seek": 340108, "start": 3407.16, "end": 3409.52, "text": " desperately trying to get back to, to finish off.", "tokens": [23726, 1382, 281, 483, 646, 281, 11, 281, 2413, 766, 13], "temperature": 0.0, "avg_logprob": -0.2959502507182001, "compression_ratio": 1.759493670886076, "no_speech_prob": 4.156741852057166e-06}, {"id": 992, "seek": 340108, "start": 3409.52, "end": 3416.6, "text": " But yeah, one anecdote that I found both really funny and very kind of encouraging, continuing", "tokens": [583, 1338, 11, 472, 49845, 300, 286, 1352, 1293, 534, 4074, 293, 588, 733, 295, 14580, 11, 9289], "temperature": 0.0, "avg_logprob": -0.2959502507182001, "compression_ratio": 1.759493670886076, "no_speech_prob": 4.156741852057166e-06}, {"id": 993, "seek": 340108, "start": 3416.6, "end": 3421.7999999999997, "text": " to pursue this as I spent an entire weekend, kind of, I spent a weekend finishing off,", "tokens": [281, 12392, 341, 382, 286, 4418, 364, 2302, 6711, 11, 733, 295, 11, 286, 4418, 257, 6711, 12693, 766, 11], "temperature": 0.0, "avg_logprob": -0.2959502507182001, "compression_ratio": 1.759493670886076, "no_speech_prob": 4.156741852057166e-06}, {"id": 994, "seek": 340108, "start": 3421.7999999999997, "end": 3427.3199999999997, "text": " finishing off basically this kind of a chunk of this in-memory caching feature, caching", "tokens": [12693, 766, 1936, 341, 733, 295, 257, 16635, 295, 341, 294, 12, 17886, 827, 269, 2834, 4111, 11, 269, 2834], "temperature": 0.0, "avg_logprob": -0.2959502507182001, "compression_ratio": 1.759493670886076, "no_speech_prob": 4.156741852057166e-06}, {"id": 995, "seek": 340108, "start": 3427.3199999999997, "end": 3428.3199999999997, "text": " feature.", "tokens": [4111, 13], "temperature": 0.0, "avg_logprob": -0.2959502507182001, "compression_ratio": 1.759493670886076, "no_speech_prob": 4.156741852057166e-06}, {"id": 996, "seek": 342832, "start": 3428.32, "end": 3432.8, "text": " And I was getting really excited to be like, okay, I'm going to now it's, you know, it's,", "tokens": [400, 286, 390, 1242, 534, 2919, 281, 312, 411, 11, 1392, 11, 286, 478, 516, 281, 586, 309, 311, 11, 291, 458, 11, 309, 311, 11], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 997, "seek": 342832, "start": 3432.8, "end": 3434.1200000000003, "text": " it's, it's working, it's type checking.", "tokens": [309, 311, 11, 309, 311, 1364, 11, 309, 311, 2010, 8568, 13], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 998, "seek": 342832, "start": 3434.1200000000003, "end": 3435.36, "text": " I'm going to do some tests.", "tokens": [286, 478, 516, 281, 360, 512, 6921, 13], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 999, "seek": 342832, "start": 3435.36, "end": 3438.96, "text": " And I was really excited to send some results to Matt.", "tokens": [400, 286, 390, 534, 2919, 281, 2845, 512, 3542, 281, 7397, 13], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 1000, "seek": 342832, "start": 3438.96, "end": 3443.92, "text": " And I started, I started putting together like all these tests and you know, I was having,", "tokens": [400, 286, 1409, 11, 286, 1409, 3372, 1214, 411, 439, 613, 6921, 293, 291, 458, 11, 286, 390, 1419, 11], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 1001, "seek": 342832, "start": 3443.92, "end": 3446.1200000000003, "text": " this isn't in like a Haskell development mode, right?", "tokens": [341, 1943, 380, 294, 411, 257, 8646, 43723, 3250, 4391, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 1002, "seek": 342832, "start": 3446.1200000000003, "end": 3450.56, "text": " So it was slower than the normal Elm compiles would be anyway, but I was kind of seeing", "tokens": [407, 309, 390, 14009, 813, 264, 2710, 2699, 76, 715, 4680, 576, 312, 4033, 11, 457, 286, 390, 733, 295, 2577], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 1003, "seek": 342832, "start": 3450.56, "end": 3457.04, "text": " like 200, 300 millisecond compiles without caching like the normal ones.", "tokens": [411, 2331, 11, 6641, 27940, 18882, 715, 4680, 1553, 269, 2834, 411, 264, 2710, 2306, 13], "temperature": 0.0, "avg_logprob": -0.27294349670410156, "compression_ratio": 1.8111888111888113, "no_speech_prob": 2.8128795293014264e-06}, {"id": 1004, "seek": 345704, "start": 3457.04, "end": 3466.0, "text": " And then on my, my memory work, for some reason it was giving me like 600, 700 milliseconds.", "tokens": [400, 550, 322, 452, 11, 452, 4675, 589, 11, 337, 512, 1778, 309, 390, 2902, 385, 411, 11849, 11, 15204, 34184, 13], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1005, "seek": 345704, "start": 3466.0, "end": 3467.48, "text": " And I'm, I'm scratching my head.", "tokens": [400, 286, 478, 11, 286, 478, 29699, 452, 1378, 13], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1006, "seek": 345704, "start": 3467.48, "end": 3469.6, "text": " I'm going like, it doesn't, it doesn't make sense.", "tokens": [286, 478, 516, 411, 11, 309, 1177, 380, 11, 309, 1177, 380, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1007, "seek": 345704, "start": 3469.6, "end": 3470.6, "text": " I'm like, what if I deoptimize?", "tokens": [286, 478, 411, 11, 437, 498, 286, 368, 5747, 43890, 30], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1008, "seek": 345704, "start": 3470.6, "end": 3474.72, "text": " And I was digging, digging, I spent hours, I think I spent the full, I think I spent", "tokens": [400, 286, 390, 17343, 11, 17343, 11, 286, 4418, 2496, 11, 286, 519, 286, 4418, 264, 1577, 11, 286, 519, 286, 4418], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1009, "seek": 345704, "start": 3474.72, "end": 3479.3, "text": " the full of my Sunday kind of, kind of torpedoed into this.", "tokens": [264, 1577, 295, 452, 7776, 733, 295, 11, 733, 295, 46764, 292, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1010, "seek": 345704, "start": 3479.3, "end": 3484.08, "text": " And eventually I got a, something, something, something crashed at some point and I couldn't", "tokens": [400, 4728, 286, 658, 257, 11, 746, 11, 746, 11, 746, 24190, 412, 512, 935, 293, 286, 2809, 380], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1011, "seek": 345704, "start": 3484.08, "end": 3485.08, "text": " figure it out.", "tokens": [2573, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.26324886434218464, "compression_ratio": 1.7595419847328244, "no_speech_prob": 1.9947131022490794e-06}, {"id": 1012, "seek": 348508, "start": 3485.08, "end": 3489.84, "text": " I was looking at, at, at the code and I had this really, I mean, it's all entirely my", "tokens": [286, 390, 1237, 412, 11, 412, 11, 412, 264, 3089, 293, 286, 632, 341, 534, 11, 286, 914, 11, 309, 311, 439, 7696, 452], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1013, "seek": 348508, "start": 3489.84, "end": 3490.84, "text": " fault.", "tokens": [7441, 13], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1014, "seek": 348508, "start": 3490.84, "end": 3494.92, "text": " I had this really, really janky code where I was using some, some Haskell, a low level", "tokens": [286, 632, 341, 534, 11, 534, 361, 657, 88, 3089, 689, 286, 390, 1228, 512, 11, 512, 8646, 43723, 11, 257, 2295, 1496], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1015, "seek": 348508, "start": 3494.92, "end": 3497.0, "text": " library to do timing parsing.", "tokens": [6405, 281, 360, 10822, 21156, 278, 13], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1016, "seek": 348508, "start": 3497.0, "end": 3498.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1017, "seek": 348508, "start": 3498.0, "end": 3503.44, "text": " And I had assumed in my little string parser of this timing result, things would come back", "tokens": [400, 286, 632, 15895, 294, 452, 707, 6798, 21156, 260, 295, 341, 10822, 1874, 11, 721, 576, 808, 646], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1018, "seek": 348508, "start": 3503.44, "end": 3505.84, "text": " to me in seconds or milliseconds.", "tokens": [281, 385, 294, 3949, 420, 34184, 13], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1019, "seek": 348508, "start": 3505.84, "end": 3507.72, "text": " And I had this, this parsing crash.", "tokens": [400, 286, 632, 341, 11, 341, 21156, 278, 8252, 13], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1020, "seek": 348508, "start": 3507.72, "end": 3511.68, "text": " And then I look into it and I realized actually I hadn't handled the case where things are", "tokens": [400, 550, 286, 574, 666, 309, 293, 286, 5334, 767, 286, 8782, 380, 18033, 264, 1389, 689, 721, 366], "temperature": 0.0, "avg_logprob": -0.2475583929764597, "compression_ratio": 1.8, "no_speech_prob": 1.6027515812311321e-06}, {"id": 1021, "seek": 351168, "start": 3511.68, "end": 3518.2, "text": " coming back in nanoseconds, but what was actually happening is that the memory cache compile", "tokens": [1348, 646, 294, 14067, 541, 28750, 11, 457, 437, 390, 767, 2737, 307, 300, 264, 4675, 269, 326, 675, 31413], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1022, "seek": 351168, "start": 3518.2, "end": 3523.56, "text": " was coming back in like 600 to 700, 800 nanoseconds, not milliseconds.", "tokens": [390, 1348, 646, 294, 411, 11849, 281, 15204, 11, 13083, 14067, 541, 28750, 11, 406, 34184, 13], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1023, "seek": 351168, "start": 3523.56, "end": 3525.8399999999997, "text": " And so actually everything was fine and there wasn't a problem.", "tokens": [400, 370, 767, 1203, 390, 2489, 293, 456, 2067, 380, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1024, "seek": 351168, "start": 3525.8399999999997, "end": 3527.96, "text": " And I was just going around in circles for a day.", "tokens": [400, 286, 390, 445, 516, 926, 294, 13040, 337, 257, 786, 13], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1025, "seek": 351168, "start": 3527.96, "end": 3530.24, "text": " So it's so good.", "tokens": [407, 309, 311, 370, 665, 13], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1026, "seek": 351168, "start": 3530.24, "end": 3533.04, "text": " Very, very excited when I was able to share that with Matt.", "tokens": [4372, 11, 588, 2919, 562, 286, 390, 1075, 281, 2073, 300, 365, 7397, 13], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1027, "seek": 351168, "start": 3533.04, "end": 3534.3599999999997, "text": " I literally remember that day.", "tokens": [286, 3736, 1604, 300, 786, 13], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1028, "seek": 351168, "start": 3534.3599999999997, "end": 3536.8799999999997, "text": " It was, it was, yeah, it was awesome.", "tokens": [467, 390, 11, 309, 390, 11, 1338, 11, 309, 390, 3476, 13], "temperature": 0.0, "avg_logprob": -0.3266709214549954, "compression_ratio": 1.6588235294117648, "no_speech_prob": 8.579191330682079e-07}, {"id": 1029, "seek": 353688, "start": 3536.88, "end": 3543.0, "text": " I, yeah, and what we're using to sort of validate this is we actually are using the vendor code", "tokens": [286, 11, 1338, 11, 293, 437, 321, 434, 1228, 281, 1333, 295, 29562, 341, 307, 321, 767, 366, 1228, 264, 24321, 3089], "temperature": 0.0, "avg_logprob": -0.22583082802275306, "compression_ratio": 1.6284584980237153, "no_speech_prob": 1.8266115375809022e-07}, {"id": 1030, "seek": 353688, "start": 3543.0, "end": 3546.4, "text": " base, which again, 600,000 lines of code.", "tokens": [3096, 11, 597, 797, 11, 11849, 11, 1360, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22583082802275306, "compression_ratio": 1.6284584980237153, "no_speech_prob": 1.8266115375809022e-07}, {"id": 1031, "seek": 353688, "start": 3546.4, "end": 3549.36, "text": " So it's pretty big.", "tokens": [407, 309, 311, 1238, 955, 13], "temperature": 0.0, "avg_logprob": -0.22583082802275306, "compression_ratio": 1.6284584980237153, "no_speech_prob": 1.8266115375809022e-07}, {"id": 1032, "seek": 353688, "start": 3549.36, "end": 3553.7200000000003, "text": " You know, there may be bigger code bases, but it's definitely one of the top, you know,", "tokens": [509, 458, 11, 456, 815, 312, 3801, 3089, 17949, 11, 457, 309, 311, 2138, 472, 295, 264, 1192, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.22583082802275306, "compression_ratio": 1.6284584980237153, "no_speech_prob": 1.8266115375809022e-07}, {"id": 1033, "seek": 353688, "start": 3553.7200000000003, "end": 3557.08, "text": " whatever, five, maybe of things that are out there.", "tokens": [2035, 11, 1732, 11, 1310, 295, 721, 300, 366, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.22583082802275306, "compression_ratio": 1.6284584980237153, "no_speech_prob": 1.8266115375809022e-07}, {"id": 1034, "seek": 353688, "start": 3557.08, "end": 3560.84, "text": " So if we can get it where it can in the editor, it feels like instantaneous feedback, which", "tokens": [407, 498, 321, 393, 483, 309, 689, 309, 393, 294, 264, 9839, 11, 309, 3417, 411, 45596, 5824, 11, 597], "temperature": 0.0, "avg_logprob": -0.22583082802275306, "compression_ratio": 1.6284584980237153, "no_speech_prob": 1.8266115375809022e-07}, {"id": 1035, "seek": 353688, "start": 3560.84, "end": 3563.0, "text": " I, I feel is possible.", "tokens": [286, 11, 286, 841, 307, 1944, 13], "temperature": 0.0, "avg_logprob": -0.22583082802275306, "compression_ratio": 1.6284584980237153, "no_speech_prob": 1.8266115375809022e-07}, {"id": 1036, "seek": 356300, "start": 3563.0, "end": 3568.72, "text": " And I, I have experienced actually with some of this stuff because when the stuff is cached", "tokens": [400, 286, 11, 286, 362, 6751, 767, 365, 512, 295, 341, 1507, 570, 562, 264, 1507, 307, 269, 15095], "temperature": 0.0, "avg_logprob": -0.2551528696428266, "compression_ratio": 1.6177606177606179, "no_speech_prob": 4.246912226335553e-07}, {"id": 1037, "seek": 356300, "start": 3568.72, "end": 3571.92, "text": " and you haven't changed anything, you're just navigating around.", "tokens": [293, 291, 2378, 380, 3105, 1340, 11, 291, 434, 445, 32054, 926, 13], "temperature": 0.0, "avg_logprob": -0.2551528696428266, "compression_ratio": 1.6177606177606179, "no_speech_prob": 4.246912226335553e-07}, {"id": 1038, "seek": 356300, "start": 3571.92, "end": 3577.88, "text": " Like it should be nearly instantaneous, like, you know, a millisecond or two to do some", "tokens": [1743, 309, 820, 312, 6217, 45596, 11, 411, 11, 291, 458, 11, 257, 27940, 18882, 420, 732, 281, 360, 512], "temperature": 0.0, "avg_logprob": -0.2551528696428266, "compression_ratio": 1.6177606177606179, "no_speech_prob": 4.246912226335553e-07}, {"id": 1039, "seek": 356300, "start": 3577.88, "end": 3578.88, "text": " of these things.", "tokens": [295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.2551528696428266, "compression_ratio": 1.6177606177606179, "no_speech_prob": 4.246912226335553e-07}, {"id": 1040, "seek": 356300, "start": 3578.88, "end": 3584.12, "text": " So the tricky part is when like, okay, you made an edit now, can we get stuff?", "tokens": [407, 264, 12414, 644, 307, 562, 411, 11, 1392, 11, 291, 1027, 364, 8129, 586, 11, 393, 321, 483, 1507, 30], "temperature": 0.0, "avg_logprob": -0.2551528696428266, "compression_ratio": 1.6177606177606179, "no_speech_prob": 4.246912226335553e-07}, {"id": 1041, "seek": 356300, "start": 3584.12, "end": 3586.34, "text": " But yeah, cannot wait.", "tokens": [583, 1338, 11, 2644, 1699, 13], "temperature": 0.0, "avg_logprob": -0.2551528696428266, "compression_ratio": 1.6177606177606179, "no_speech_prob": 4.246912226335553e-07}, {"id": 1042, "seek": 356300, "start": 3586.34, "end": 3588.52, "text": " That's going to, that's going to be this year for sure.", "tokens": [663, 311, 516, 281, 11, 300, 311, 516, 281, 312, 341, 1064, 337, 988, 13], "temperature": 0.0, "avg_logprob": -0.2551528696428266, "compression_ratio": 1.6177606177606179, "no_speech_prob": 4.246912226335553e-07}, {"id": 1043, "seek": 358852, "start": 3588.52, "end": 3593.96, "text": " Because it's one of the projects I'm super, super excited about this year, 2022.", "tokens": [1436, 309, 311, 472, 295, 264, 4455, 286, 478, 1687, 11, 1687, 2919, 466, 341, 1064, 11, 20229, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1044, "seek": 358852, "start": 3593.96, "end": 3594.96, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1045, "seek": 358852, "start": 3594.96, "end": 3596.8, "text": " This upcoming year.", "tokens": [639, 11500, 1064, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1046, "seek": 358852, "start": 3596.8, "end": 3597.8, "text": " That's right.", "tokens": [663, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1047, "seek": 358852, "start": 3597.8, "end": 3600.84, "text": " I'm closing your PRs right now.", "tokens": [286, 478, 10377, 428, 11568, 82, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1048, "seek": 358852, "start": 3600.84, "end": 3603.4, "text": " I have the window open.", "tokens": [286, 362, 264, 4910, 1269, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1049, "seek": 358852, "start": 3603.4, "end": 3606.4, "text": " I'm leaving an emoji.", "tokens": [286, 478, 5012, 364, 31595, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1050, "seek": 358852, "start": 3606.4, "end": 3611.52, "text": " Another middle finger one.", "tokens": [3996, 2808, 5984, 472, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1051, "seek": 358852, "start": 3611.52, "end": 3613.12, "text": " So this is all very exciting.", "tokens": [407, 341, 307, 439, 588, 4670, 13], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1052, "seek": 358852, "start": 3613.12, "end": 3618.5, "text": " I'm not sure whether you guys want to get into, you're talking about like speed and", "tokens": [286, 478, 406, 988, 1968, 291, 1074, 528, 281, 483, 666, 11, 291, 434, 1417, 466, 411, 3073, 293], "temperature": 0.0, "avg_logprob": -0.3876989420177867, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.3687364318902837e-06}, {"id": 1053, "seek": 361850, "start": 3618.5, "end": 3619.88, "text": " the feedback cycle.", "tokens": [264, 5824, 6586, 13], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1054, "seek": 361850, "start": 3619.88, "end": 3625.32, "text": " I'm not sure if you, how much you want to get into things beyond that of like getting", "tokens": [286, 478, 406, 988, 498, 291, 11, 577, 709, 291, 528, 281, 483, 666, 721, 4399, 300, 295, 411, 1242], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1055, "seek": 361850, "start": 3625.32, "end": 3632.32, "text": " more rich information about Elm code here, but just wanted to give you the opportunity", "tokens": [544, 4593, 1589, 466, 2699, 76, 3089, 510, 11, 457, 445, 1415, 281, 976, 291, 264, 2650], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1056, "seek": 361850, "start": 3632.32, "end": 3633.32, "text": " if you want to talk about it.", "tokens": [498, 291, 528, 281, 751, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1057, "seek": 361850, "start": 3633.32, "end": 3635.2, "text": " Yeah, I can kind of talk to this a little bit.", "tokens": [865, 11, 286, 393, 733, 295, 751, 281, 341, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1058, "seek": 361850, "start": 3635.2, "end": 3640.24, "text": " It's kind of been aware, like Mario's been working on the caching strategy and I've been", "tokens": [467, 311, 733, 295, 668, 3650, 11, 411, 9343, 311, 668, 1364, 322, 264, 269, 2834, 5206, 293, 286, 600, 668], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1059, "seek": 361850, "start": 3640.24, "end": 3645.76, "text": " working on iterating on a specific plugin for VS code to sort of try out these editing", "tokens": [1364, 322, 17138, 990, 322, 257, 2685, 23407, 337, 25091, 3089, 281, 1333, 295, 853, 484, 613, 10000], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1060, "seek": 361850, "start": 3645.76, "end": 3647.72, "text": " ideas that we sort of have.", "tokens": [3487, 300, 321, 1333, 295, 362, 13], "temperature": 0.0, "avg_logprob": -0.24785559517996653, "compression_ratio": 1.7075812274368232, "no_speech_prob": 9.721444484966923e-07}, {"id": 1061, "seek": 364772, "start": 3647.72, "end": 3653.08, "text": " It's a, there's the basic premise, which is that the compiler can allow us to get information", "tokens": [467, 311, 257, 11, 456, 311, 264, 3875, 22045, 11, 597, 307, 300, 264, 31958, 393, 2089, 505, 281, 483, 1589], "temperature": 0.0, "avg_logprob": -0.2225209382864145, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.8447402680976666e-06}, {"id": 1062, "seek": 364772, "start": 3653.08, "end": 3658.12, "text": " that is otherwise hard to compute, or you basically just end up into a situation where", "tokens": [300, 307, 5911, 1152, 281, 14722, 11, 420, 291, 1936, 445, 917, 493, 666, 257, 2590, 689], "temperature": 0.0, "avg_logprob": -0.2225209382864145, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.8447402680976666e-06}, {"id": 1063, "seek": 364772, "start": 3658.12, "end": 3662.48, "text": " you're literally recreating the compiler in some other language, which is a pain.", "tokens": [291, 434, 3736, 850, 44613, 264, 31958, 294, 512, 661, 2856, 11, 597, 307, 257, 1822, 13], "temperature": 0.0, "avg_logprob": -0.2225209382864145, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.8447402680976666e-06}, {"id": 1064, "seek": 364772, "start": 3662.48, "end": 3663.48, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2225209382864145, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.8447402680976666e-06}, {"id": 1065, "seek": 364772, "start": 3663.48, "end": 3668.56, "text": " So it's like, okay, we have this information.", "tokens": [407, 309, 311, 411, 11, 1392, 11, 321, 362, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.2225209382864145, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.8447402680976666e-06}, {"id": 1066, "seek": 364772, "start": 3668.56, "end": 3672.9199999999996, "text": " So we have this information and also just the, you know, at vendor, I spent a lot of", "tokens": [407, 321, 362, 341, 1589, 293, 611, 445, 264, 11, 291, 458, 11, 412, 24321, 11, 286, 4418, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.2225209382864145, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.8447402680976666e-06}, {"id": 1067, "seek": 367292, "start": 3672.92, "end": 3678.36, "text": " time thinking about product and how to develop a product that's actually useful and, you", "tokens": [565, 1953, 466, 1674, 293, 577, 281, 1499, 257, 1674, 300, 311, 767, 4420, 293, 11, 291], "temperature": 0.0, "avg_logprob": -0.21983718872070312, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.3687280190642923e-06}, {"id": 1068, "seek": 367292, "start": 3678.36, "end": 3680.64, "text": " know, solves like for the user and everything.", "tokens": [458, 11, 39890, 411, 337, 264, 4195, 293, 1203, 13], "temperature": 0.0, "avg_logprob": -0.21983718872070312, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.3687280190642923e-06}, {"id": 1069, "seek": 367292, "start": 3680.64, "end": 3687.78, "text": " So we don't, it, what I am not pitching is a specific solution with this, but I am looking", "tokens": [407, 321, 500, 380, 11, 309, 11, 437, 286, 669, 406, 37499, 307, 257, 2685, 3827, 365, 341, 11, 457, 286, 669, 1237], "temperature": 0.0, "avg_logprob": -0.21983718872070312, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.3687280190642923e-06}, {"id": 1070, "seek": 367292, "start": 3687.78, "end": 3692.4, "text": " very hard at what do people actually do when they edit.", "tokens": [588, 1152, 412, 437, 360, 561, 767, 360, 562, 436, 8129, 13], "temperature": 0.0, "avg_logprob": -0.21983718872070312, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.3687280190642923e-06}, {"id": 1071, "seek": 367292, "start": 3692.4, "end": 3698.08, "text": " And it's not obvious to me that our default modality of editing, and when I say default,", "tokens": [400, 309, 311, 406, 6322, 281, 385, 300, 527, 7576, 1072, 1860, 295, 10000, 11, 293, 562, 286, 584, 7576, 11], "temperature": 0.0, "avg_logprob": -0.21983718872070312, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.3687280190642923e-06}, {"id": 1072, "seek": 369808, "start": 3698.08, "end": 3703.24, "text": " I mean something like the standard VS code editing experience where you have a lot of", "tokens": [286, 914, 746, 411, 264, 3832, 25091, 3089, 10000, 1752, 689, 291, 362, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.25789333979288737, "compression_ratio": 1.812, "no_speech_prob": 5.368707434172393e-07}, {"id": 1073, "seek": 369808, "start": 3703.24, "end": 3709.7999999999997, "text": " information in hover cards or, you know, even like the, the language server, like we're", "tokens": [1589, 294, 20076, 5632, 420, 11, 291, 458, 11, 754, 411, 264, 11, 264, 2856, 7154, 11, 411, 321, 434], "temperature": 0.0, "avg_logprob": -0.25789333979288737, "compression_ratio": 1.812, "no_speech_prob": 5.368707434172393e-07}, {"id": 1074, "seek": 369808, "start": 3709.7999999999997, "end": 3714.68, "text": " going to flash red with all these intermediate compilation steps.", "tokens": [516, 281, 7319, 2182, 365, 439, 613, 19376, 40261, 4439, 13], "temperature": 0.0, "avg_logprob": -0.25789333979288737, "compression_ratio": 1.812, "no_speech_prob": 5.368707434172393e-07}, {"id": 1075, "seek": 369808, "start": 3714.68, "end": 3720.52, "text": " Like it's not like that doesn't feel like the, that doesn't feel like the ideal.", "tokens": [1743, 309, 311, 406, 411, 300, 1177, 380, 841, 411, 264, 11, 300, 1177, 380, 841, 411, 264, 7157, 13], "temperature": 0.0, "avg_logprob": -0.25789333979288737, "compression_ratio": 1.812, "no_speech_prob": 5.368707434172393e-07}, {"id": 1076, "seek": 369808, "start": 3720.52, "end": 3721.52, "text": " It doesn't to me.", "tokens": [467, 1177, 380, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.25789333979288737, "compression_ratio": 1.812, "no_speech_prob": 5.368707434172393e-07}, {"id": 1077, "seek": 369808, "start": 3721.52, "end": 3722.84, "text": " I think it's really cool.", "tokens": [286, 519, 309, 311, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.25789333979288737, "compression_ratio": 1.812, "no_speech_prob": 5.368707434172393e-07}, {"id": 1078, "seek": 369808, "start": 3722.84, "end": 3726.2799999999997, "text": " And there's, there's a lot of, there's some stuff there, but there's a lot of stuff just", "tokens": [400, 456, 311, 11, 456, 311, 257, 688, 295, 11, 456, 311, 512, 1507, 456, 11, 457, 456, 311, 257, 688, 295, 1507, 445], "temperature": 0.0, "avg_logprob": -0.25789333979288737, "compression_ratio": 1.812, "no_speech_prob": 5.368707434172393e-07}, {"id": 1079, "seek": 372628, "start": 3726.28, "end": 3730.96, "text": " to learn of what information do you want at what time or what are you doing?", "tokens": [281, 1466, 295, 437, 1589, 360, 291, 528, 412, 437, 565, 420, 437, 366, 291, 884, 30], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1080, "seek": 372628, "start": 3730.96, "end": 3732.5600000000004, "text": " You know, what are you trying to accomplish?", "tokens": [509, 458, 11, 437, 366, 291, 1382, 281, 9021, 30], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1081, "seek": 372628, "start": 3732.5600000000004, "end": 3737.2000000000003, "text": " We spend, I think way more time or I spend a lot more time in my code.", "tokens": [492, 3496, 11, 286, 519, 636, 544, 565, 420, 286, 3496, 257, 688, 544, 565, 294, 452, 3089, 13], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1082, "seek": 372628, "start": 3737.2000000000003, "end": 3739.96, "text": " It's not about reading code necessarily.", "tokens": [467, 311, 406, 466, 3760, 3089, 4725, 13], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1083, "seek": 372628, "start": 3739.96, "end": 3744.86, "text": " It's literally about navigating and gathering enough information context.", "tokens": [467, 311, 3736, 466, 32054, 293, 13519, 1547, 1589, 4319, 13], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1084, "seek": 372628, "start": 3744.86, "end": 3746.88, "text": " So it's, you don't read it from front to back.", "tokens": [407, 309, 311, 11, 291, 500, 380, 1401, 309, 490, 1868, 281, 646, 13], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1085, "seek": 372628, "start": 3746.88, "end": 3753.2000000000003, "text": " Like you read a book, you know, you, you, you go around, you gather clues of, of what", "tokens": [1743, 291, 1401, 257, 1446, 11, 291, 458, 11, 291, 11, 291, 11, 291, 352, 926, 11, 291, 5448, 20936, 295, 11, 295, 437], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1086, "seek": 372628, "start": 3753.2000000000003, "end": 3755.2000000000003, "text": " you might, you gather references, right?", "tokens": [291, 1062, 11, 291, 5448, 15400, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19582491654616135, "compression_ratio": 1.794776119402985, "no_speech_prob": 2.7264593427389627e-06}, {"id": 1087, "seek": 375520, "start": 3755.2, "end": 3760.7599999999998, "text": " You set your table with various things from all over the code base, and then you get a", "tokens": [509, 992, 428, 3199, 365, 3683, 721, 490, 439, 670, 264, 3089, 3096, 11, 293, 550, 291, 483, 257], "temperature": 0.0, "avg_logprob": -0.20664564471378505, "compression_ratio": 1.7049808429118773, "no_speech_prob": 1.3287593674249365e-06}, {"id": 1088, "seek": 375520, "start": 3760.7599999999998, "end": 3762.8799999999997, "text": " piece of clarity and you do something.", "tokens": [2522, 295, 16992, 293, 291, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.20664564471378505, "compression_ratio": 1.7049808429118773, "no_speech_prob": 1.3287593674249365e-06}, {"id": 1089, "seek": 375520, "start": 3762.8799999999997, "end": 3766.3999999999996, "text": " So there's not too many, I mean, there are some tools, but there aren't too many tools", "tokens": [407, 456, 311, 406, 886, 867, 11, 286, 914, 11, 456, 366, 512, 3873, 11, 457, 456, 3212, 380, 886, 867, 3873], "temperature": 0.0, "avg_logprob": -0.20664564471378505, "compression_ratio": 1.7049808429118773, "no_speech_prob": 1.3287593674249365e-06}, {"id": 1090, "seek": 375520, "start": 3766.3999999999996, "end": 3773.06, "text": " that I think look at that concept at that primary interaction, which feels massively", "tokens": [300, 286, 519, 574, 412, 300, 3410, 412, 300, 6194, 9285, 11, 597, 3417, 29379], "temperature": 0.0, "avg_logprob": -0.20664564471378505, "compression_ratio": 1.7049808429118773, "no_speech_prob": 1.3287593674249365e-06}, {"id": 1091, "seek": 375520, "start": 3773.06, "end": 3777.54, "text": " common and actually try to solve for it beyond just like, we're going to kind of give you", "tokens": [2689, 293, 767, 853, 281, 5039, 337, 309, 4399, 445, 411, 11, 321, 434, 516, 281, 733, 295, 976, 291], "temperature": 0.0, "avg_logprob": -0.20664564471378505, "compression_ratio": 1.7049808429118773, "no_speech_prob": 1.3287593674249365e-06}, {"id": 1092, "seek": 375520, "start": 3777.54, "end": 3781.2, "text": " everything at every point and hopefully something sticks.", "tokens": [1203, 412, 633, 935, 293, 4696, 746, 12518, 13], "temperature": 0.0, "avg_logprob": -0.20664564471378505, "compression_ratio": 1.7049808429118773, "no_speech_prob": 1.3287593674249365e-06}, {"id": 1093, "seek": 378120, "start": 3781.2, "end": 3786.2799999999997, "text": " So I have some really cool experiments that I cannot wait to share, like once we have", "tokens": [407, 286, 362, 512, 534, 1627, 12050, 300, 286, 2644, 1699, 281, 2073, 11, 411, 1564, 321, 362], "temperature": 0.0, "avg_logprob": -0.28251857142294606, "compression_ratio": 1.56, "no_speech_prob": 1.3925143775850302e-06}, {"id": 1094, "seek": 378120, "start": 3786.2799999999997, "end": 3792.64, "text": " something going, but it's sort of in the stage of like, okay, I think I have a good idea", "tokens": [746, 516, 11, 457, 309, 311, 1333, 295, 294, 264, 3233, 295, 411, 11, 1392, 11, 286, 519, 286, 362, 257, 665, 1558], "temperature": 0.0, "avg_logprob": -0.28251857142294606, "compression_ratio": 1.56, "no_speech_prob": 1.3925143775850302e-06}, {"id": 1095, "seek": 378120, "start": 3792.64, "end": 3794.48, "text": " what the problem is.", "tokens": [437, 264, 1154, 307, 13], "temperature": 0.0, "avg_logprob": -0.28251857142294606, "compression_ratio": 1.56, "no_speech_prob": 1.3925143775850302e-06}, {"id": 1096, "seek": 378120, "start": 3794.48, "end": 3799.3199999999997, "text": " Let's start trying out some ideas that address those things directly and hopefully it'll", "tokens": [961, 311, 722, 1382, 484, 512, 3487, 300, 2985, 729, 721, 3838, 293, 4696, 309, 603], "temperature": 0.0, "avg_logprob": -0.28251857142294606, "compression_ratio": 1.56, "no_speech_prob": 1.3925143775850302e-06}, {"id": 1097, "seek": 378120, "start": 3799.3199999999997, "end": 3801.9199999999996, "text": " be successful, but you know, who knows?", "tokens": [312, 4406, 11, 457, 291, 458, 11, 567, 3255, 30], "temperature": 0.0, "avg_logprob": -0.28251857142294606, "compression_ratio": 1.56, "no_speech_prob": 1.3925143775850302e-06}, {"id": 1098, "seek": 378120, "start": 3801.9199999999996, "end": 3804.08, "text": " It's very exciting for me.", "tokens": [467, 311, 588, 4670, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.28251857142294606, "compression_ratio": 1.56, "no_speech_prob": 1.3925143775850302e-06}, {"id": 1099, "seek": 380408, "start": 3804.08, "end": 3811.7599999999998, "text": " It's every time I see these exciting things being done with AI code assist tools, like", "tokens": [467, 311, 633, 565, 286, 536, 613, 4670, 721, 885, 1096, 365, 7318, 3089, 4255, 3873, 11, 411], "temperature": 0.0, "avg_logprob": -0.26587800418629365, "compression_ratio": 1.440217391304348, "no_speech_prob": 1.0844901225937065e-06}, {"id": 1100, "seek": 380408, "start": 3811.7599999999998, "end": 3819.2799999999997, "text": " GitHub copilot or people using GPT chat to solve coding problems and, and stack overflow", "tokens": [23331, 2971, 31516, 420, 561, 1228, 26039, 51, 5081, 281, 5039, 17720, 2740, 293, 11, 293, 8630, 37772], "temperature": 0.0, "avg_logprob": -0.26587800418629365, "compression_ratio": 1.440217391304348, "no_speech_prob": 1.0844901225937065e-06}, {"id": 1101, "seek": 380408, "start": 3819.2799999999997, "end": 3827.08, "text": " banning GPT chat answers in, you know, I'm like, ah, this feels like the wrong direction.", "tokens": [5643, 773, 26039, 51, 5081, 6338, 294, 11, 291, 458, 11, 286, 478, 411, 11, 3716, 11, 341, 3417, 411, 264, 2085, 3513, 13], "temperature": 0.0, "avg_logprob": -0.26587800418629365, "compression_ratio": 1.440217391304348, "no_speech_prob": 1.0844901225937065e-06}, {"id": 1102, "seek": 382708, "start": 3827.08, "end": 3835.08, "text": " And like, I would love as a, you know, pure FP community to be like, here's another path", "tokens": [400, 411, 11, 286, 576, 959, 382, 257, 11, 291, 458, 11, 6075, 36655, 1768, 281, 312, 411, 11, 510, 311, 1071, 3100], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1103, "seek": 382708, "start": 3835.08, "end": 3838.56, "text": " we could go down and it's actually really awesome.", "tokens": [321, 727, 352, 760, 293, 309, 311, 767, 534, 3476, 13], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1104, "seek": 382708, "start": 3838.56, "end": 3840.2, "text": " Let us show you how cool it can be.", "tokens": [961, 505, 855, 291, 577, 1627, 309, 393, 312, 13], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1105, "seek": 382708, "start": 3840.2, "end": 3841.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1106, "seek": 382708, "start": 3841.2, "end": 3842.2, "text": " I'm really curious.", "tokens": [286, 478, 534, 6369, 13], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1107, "seek": 382708, "start": 3842.2, "end": 3845.0, "text": " I mean, I think AI stuff, it's not going to be like a fad.", "tokens": [286, 914, 11, 286, 519, 7318, 1507, 11, 309, 311, 406, 516, 281, 312, 411, 257, 283, 345, 13], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1108, "seek": 382708, "start": 3845.0, "end": 3848.24, "text": " It's going to be just like progressively moving forward.", "tokens": [467, 311, 516, 281, 312, 445, 411, 46667, 2684, 2128, 13], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1109, "seek": 382708, "start": 3848.24, "end": 3852.3199999999997, "text": " I think one of the challenges we're going to have just in general is not that right", "tokens": [286, 519, 472, 295, 264, 4759, 321, 434, 516, 281, 362, 445, 294, 2674, 307, 406, 300, 558], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1110, "seek": 382708, "start": 3852.3199999999997, "end": 3853.88, "text": " now there's a bifurcation, right?", "tokens": [586, 456, 311, 257, 272, 351, 374, 46252, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1111, "seek": 382708, "start": 3853.88, "end": 3856.84, "text": " There's like, okay, here's something that was totally like human developed and here's", "tokens": [821, 311, 411, 11, 1392, 11, 510, 311, 746, 300, 390, 3879, 411, 1952, 4743, 293, 510, 311], "temperature": 0.0, "avg_logprob": -0.21145402272542319, "compression_ratio": 1.7194719471947195, "no_speech_prob": 2.3823469064154779e-07}, {"id": 1112, "seek": 385684, "start": 3856.84, "end": 3861.08, "text": " something that was totally suggested by an AI, but like, there's going, there are going", "tokens": [746, 300, 390, 3879, 10945, 538, 364, 7318, 11, 457, 411, 11, 456, 311, 516, 11, 456, 366, 516], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1113, "seek": 385684, "start": 3861.08, "end": 3866.1600000000003, "text": " to be hybrid tools that are going to be more nuanced on both sides, but who knows what", "tokens": [281, 312, 13051, 3873, 300, 366, 516, 281, 312, 544, 45115, 322, 1293, 4881, 11, 457, 567, 3255, 437], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1114, "seek": 385684, "start": 3866.1600000000003, "end": 3867.76, "text": " that looks like?", "tokens": [300, 1542, 411, 30], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1115, "seek": 385684, "start": 3867.76, "end": 3868.76, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1116, "seek": 385684, "start": 3868.76, "end": 3869.76, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1117, "seek": 385684, "start": 3869.76, "end": 3872.04, "text": " And like type directed, right?", "tokens": [400, 411, 2010, 12898, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1118, "seek": 385684, "start": 3872.04, "end": 3875.2000000000003, "text": " Like, okay, we can, we can calculate the types already.", "tokens": [1743, 11, 1392, 11, 321, 393, 11, 321, 393, 8873, 264, 3467, 1217, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1119, "seek": 385684, "start": 3875.2000000000003, "end": 3877.88, "text": " Type directed AI intervention of some sort.", "tokens": [15576, 12898, 7318, 13176, 295, 512, 1333, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1120, "seek": 385684, "start": 3877.88, "end": 3879.88, "text": " That's I don't know.", "tokens": [663, 311, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1121, "seek": 385684, "start": 3879.88, "end": 3881.32, "text": " It could be a thing.", "tokens": [467, 727, 312, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1122, "seek": 385684, "start": 3881.32, "end": 3882.32, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1123, "seek": 385684, "start": 3882.32, "end": 3885.7200000000003, "text": " And it should be like computers are good at understanding constraints and they have them.", "tokens": [400, 309, 820, 312, 411, 10807, 366, 665, 412, 3701, 18491, 293, 436, 362, 552, 13], "temperature": 0.0, "avg_logprob": -0.2956770405624852, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276674753280531e-07}, {"id": 1124, "seek": 388572, "start": 3885.72, "end": 3890.04, "text": " So let them do smart AI things within those constraints.", "tokens": [407, 718, 552, 360, 4069, 7318, 721, 1951, 729, 18491, 13], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1125, "seek": 388572, "start": 3890.04, "end": 3891.04, "text": " Yeah, definitely.", "tokens": [865, 11, 2138, 13], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1126, "seek": 388572, "start": 3891.04, "end": 3894.48, "text": " I'm pretty, I'm pretty convinced that it's not going to be, that it's not going to be", "tokens": [286, 478, 1238, 11, 286, 478, 1238, 12561, 300, 309, 311, 406, 516, 281, 312, 11, 300, 309, 311, 406, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1127, "seek": 388572, "start": 3894.48, "end": 3899.7599999999998, "text": " AI long-term applied to directly spitting out code for us.", "tokens": [7318, 938, 12, 7039, 6456, 281, 3838, 637, 2414, 484, 3089, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1128, "seek": 388572, "start": 3899.7599999999998, "end": 3904.7999999999997, "text": " It feels like there's going to be a better, a better human interface there, or just, just", "tokens": [467, 3417, 411, 456, 311, 516, 281, 312, 257, 1101, 11, 257, 1101, 1952, 9226, 456, 11, 420, 445, 11, 445], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1129, "seek": 388572, "start": 3904.7999999999997, "end": 3906.7599999999998, "text": " a better interface in general.", "tokens": [257, 1101, 9226, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1130, "seek": 388572, "start": 3906.7599999999998, "end": 3911.3599999999997, "text": " And then maybe AI could target that interface or humans could direct AI to target that interface", "tokens": [400, 550, 1310, 7318, 727, 3779, 300, 9226, 420, 6255, 727, 2047, 7318, 281, 3779, 300, 9226], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1131, "seek": 388572, "start": 3911.3599999999997, "end": 3912.3599999999997, "text": " much better.", "tokens": [709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.2506074497842381, "compression_ratio": 1.8907563025210083, "no_speech_prob": 3.6119536162004806e-06}, {"id": 1132, "seek": 391236, "start": 3912.36, "end": 3917.2400000000002, "text": " Like I think the imagery, imagery is a pretty good view of how that goes, right?", "tokens": [1743, 286, 519, 264, 24340, 11, 24340, 307, 257, 1238, 665, 1910, 295, 577, 300, 1709, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1133, "seek": 391236, "start": 3917.2400000000002, "end": 3920.2000000000003, "text": " Like there's a, there's a, there's a visual target.", "tokens": [1743, 456, 311, 257, 11, 456, 311, 257, 11, 456, 311, 257, 5056, 3779, 13], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1134, "seek": 391236, "start": 3920.2000000000003, "end": 3921.56, "text": " It's a kind of softer visual target.", "tokens": [467, 311, 257, 733, 295, 23119, 5056, 3779, 13], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1135, "seek": 391236, "start": 3921.56, "end": 3927.0, "text": " Whereas I think programming right now is very, very precise and very, in a large amount of", "tokens": [13813, 286, 519, 9410, 558, 586, 307, 588, 11, 588, 13600, 293, 588, 11, 294, 257, 2416, 2372, 295], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1136, "seek": 391236, "start": 3927.0, "end": 3928.28, "text": " really pernickety ways.", "tokens": [534, 680, 77, 618, 2210, 2098, 13], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1137, "seek": 391236, "start": 3928.28, "end": 3931.4, "text": " And I think Elm, like Elm is a really great example of that, right?", "tokens": [400, 286, 519, 2699, 76, 11, 411, 2699, 76, 307, 257, 534, 869, 1365, 295, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1138, "seek": 391236, "start": 3931.4, "end": 3935.1200000000003, "text": " Like it's like a tighter interface where we go, you know what, let's use more of what", "tokens": [1743, 309, 311, 411, 257, 30443, 9226, 689, 321, 352, 11, 291, 458, 437, 11, 718, 311, 764, 544, 295, 437], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1139, "seek": 391236, "start": 3935.1200000000003, "end": 3936.96, "text": " the computer is really, really good at.", "tokens": [264, 3820, 307, 534, 11, 534, 665, 412, 13], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1140, "seek": 391236, "start": 3936.96, "end": 3941.1600000000003, "text": " Like let's narrow in on that or let the computer do more of that stuff rather than us, you", "tokens": [1743, 718, 311, 9432, 294, 322, 300, 420, 718, 264, 3820, 360, 544, 295, 300, 1507, 2831, 813, 505, 11, 291], "temperature": 0.0, "avg_logprob": -0.2679030460381658, "compression_ratio": 1.9222972972972974, "no_speech_prob": 8.579164614275214e-07}, {"id": 1141, "seek": 394116, "start": 3941.16, "end": 3944.68, "text": " know, having those kinds of foot guns lying around all over the place.", "tokens": [458, 11, 1419, 729, 3685, 295, 2671, 10153, 8493, 926, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1142, "seek": 394116, "start": 3944.68, "end": 3946.3199999999997, "text": " So yeah, we'll see.", "tokens": [407, 1338, 11, 321, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1143, "seek": 394116, "start": 3946.3199999999997, "end": 3952.52, "text": " AI in Elm.dev for version two now, I think.", "tokens": [7318, 294, 2699, 76, 13, 40343, 337, 3037, 732, 586, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1144, "seek": 394116, "start": 3952.52, "end": 3953.52, "text": " I like that.", "tokens": [286, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1145, "seek": 394116, "start": 3953.52, "end": 3954.6, "text": " I like that take, Mario.", "tokens": [286, 411, 300, 747, 11, 9343, 13], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1146, "seek": 394116, "start": 3954.6, "end": 3960.6, "text": " I can definitely imagine like AI tools to help assist you in generating like sample", "tokens": [286, 393, 2138, 3811, 411, 7318, 3873, 281, 854, 4255, 291, 294, 17746, 411, 6889], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1147, "seek": 394116, "start": 3960.6, "end": 3961.6, "text": " inputs and outputs.", "tokens": [15743, 293, 23930, 13], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1148, "seek": 394116, "start": 3961.6, "end": 3966.48, "text": " Cause it sees the pattern and it's like, well, okay, should I help you fill out your test", "tokens": [10865, 309, 8194, 264, 5102, 293, 309, 311, 411, 11, 731, 11, 1392, 11, 820, 286, 854, 291, 2836, 484, 428, 1500], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1149, "seek": 394116, "start": 3966.48, "end": 3967.48, "text": " suite?", "tokens": [14205, 30], "temperature": 0.0, "avg_logprob": -0.3101299285888672, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.247312841063831e-05}, {"id": 1150, "seek": 396748, "start": 3967.48, "end": 3975.92, "text": " And then it could understand how it's tweaking of the code or generation of code influences", "tokens": [400, 550, 309, 727, 1223, 577, 309, 311, 6986, 2456, 295, 264, 3089, 420, 5125, 295, 3089, 21222], "temperature": 0.0, "avg_logprob": -0.24250396929289164, "compression_ratio": 1.811023622047244, "no_speech_prob": 1.6893230281311844e-07}, {"id": 1151, "seek": 396748, "start": 3975.92, "end": 3980.0, "text": " the outputs as it runs that test suite and have that as part of its feedback loop.", "tokens": [264, 23930, 382, 309, 6676, 300, 1500, 14205, 293, 362, 300, 382, 644, 295, 1080, 5824, 6367, 13], "temperature": 0.0, "avg_logprob": -0.24250396929289164, "compression_ratio": 1.811023622047244, "no_speech_prob": 1.6893230281311844e-07}, {"id": 1152, "seek": 396748, "start": 3980.0, "end": 3983.8, "text": " It could understand the constraints of the type system it's working in.", "tokens": [467, 727, 1223, 264, 18491, 295, 264, 2010, 1185, 309, 311, 1364, 294, 13], "temperature": 0.0, "avg_logprob": -0.24250396929289164, "compression_ratio": 1.811023622047244, "no_speech_prob": 1.6893230281311844e-07}, {"id": 1153, "seek": 396748, "start": 3983.8, "end": 3989.16, "text": " And then it could iterate on that and start making those tests green and then show you,", "tokens": [400, 550, 309, 727, 44497, 322, 300, 293, 722, 1455, 729, 6921, 3092, 293, 550, 855, 291, 11], "temperature": 0.0, "avg_logprob": -0.24250396929289164, "compression_ratio": 1.811023622047244, "no_speech_prob": 1.6893230281311844e-07}, {"id": 1154, "seek": 396748, "start": 3989.16, "end": 3991.56, "text": " okay, here are five different ways to make this green.", "tokens": [1392, 11, 510, 366, 1732, 819, 2098, 281, 652, 341, 3092, 13], "temperature": 0.0, "avg_logprob": -0.24250396929289164, "compression_ratio": 1.811023622047244, "no_speech_prob": 1.6893230281311844e-07}, {"id": 1155, "seek": 396748, "start": 3991.56, "end": 3993.3, "text": " Like that's what I want.", "tokens": [1743, 300, 311, 437, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.24250396929289164, "compression_ratio": 1.811023622047244, "no_speech_prob": 1.6893230281311844e-07}, {"id": 1156, "seek": 396748, "start": 3993.3, "end": 3995.72, "text": " Those are the type of AI assists that I want.", "tokens": [3950, 366, 264, 2010, 295, 7318, 49416, 300, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.24250396929289164, "compression_ratio": 1.811023622047244, "no_speech_prob": 1.6893230281311844e-07}, {"id": 1157, "seek": 399572, "start": 3995.72, "end": 3997.68, "text": " Help us help you machines.", "tokens": [10773, 505, 854, 291, 8379, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1158, "seek": 399572, "start": 3997.68, "end": 4000.7599999999998, "text": " We're here for you.", "tokens": [492, 434, 510, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1159, "seek": 399572, "start": 4000.7599999999998, "end": 4001.7599999999998, "text": " Always with tests, right?", "tokens": [11270, 365, 6921, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1160, "seek": 399572, "start": 4001.7599999999998, "end": 4002.7599999999998, "text": " That's true.", "tokens": [663, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1161, "seek": 399572, "start": 4002.7599999999998, "end": 4006.68, "text": " It always goes back to types and tests.", "tokens": [467, 1009, 1709, 646, 281, 3467, 293, 6921, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1162, "seek": 399572, "start": 4006.68, "end": 4008.68, "text": " That's a moral of the story.", "tokens": [663, 311, 257, 9723, 295, 264, 1657, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1163, "seek": 399572, "start": 4008.68, "end": 4014.04, "text": " It's only going to work for you because you're the only one who writes tests first.", "tokens": [467, 311, 787, 516, 281, 589, 337, 291, 570, 291, 434, 264, 787, 472, 567, 13657, 6921, 700, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1164, "seek": 399572, "start": 4014.04, "end": 4015.04, "text": " In the typed world.", "tokens": [682, 264, 33941, 1002, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1165, "seek": 399572, "start": 4015.04, "end": 4018.12, "text": " Or like you could maybe take that further.", "tokens": [1610, 411, 291, 727, 1310, 747, 300, 3052, 13], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1166, "seek": 399572, "start": 4018.12, "end": 4022.9199999999996, "text": " You basically have a code and then have the AI ask you questions like, were you intending", "tokens": [509, 1936, 362, 257, 3089, 293, 550, 362, 264, 7318, 1029, 291, 1651, 411, 11, 645, 291, 560, 2029], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1167, "seek": 399572, "start": 4022.9199999999996, "end": 4024.7999999999997, "text": " for this list to be reversed afterwards?", "tokens": [337, 341, 1329, 281, 312, 30563, 10543, 30], "temperature": 0.0, "avg_logprob": -0.3470537449286236, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.307519023110217e-07}, {"id": 1168, "seek": 402480, "start": 4024.8, "end": 4025.8, "text": " That's interesting.", "tokens": [663, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1169, "seek": 402480, "start": 4025.8, "end": 4026.8, "text": " I like that.", "tokens": [286, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1170, "seek": 402480, "start": 4026.8, "end": 4030.2000000000003, "text": " Because you may not have noticed it, but it definitely is.", "tokens": [1436, 291, 815, 406, 362, 5694, 309, 11, 457, 309, 2138, 307, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1171, "seek": 402480, "start": 4030.2000000000003, "end": 4032.96, "text": " You could get the AI to review PRs for you.", "tokens": [509, 727, 483, 264, 7318, 281, 3131, 11568, 82, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1172, "seek": 402480, "start": 4032.96, "end": 4035.8, "text": " And then you basically just confirm properties, right?", "tokens": [400, 550, 291, 1936, 445, 9064, 7221, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1173, "seek": 402480, "start": 4035.8, "end": 4040.28, "text": " So like you say, yeah, this list is reversed when you call this function.", "tokens": [407, 411, 291, 584, 11, 1338, 11, 341, 1329, 307, 30563, 562, 291, 818, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1174, "seek": 402480, "start": 4040.28, "end": 4041.28, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1175, "seek": 402480, "start": 4041.28, "end": 4044.92, "text": " Jeroen, do you have any plans for 2023?", "tokens": [508, 2032, 268, 11, 360, 291, 362, 604, 5482, 337, 44377, 30], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1176, "seek": 402480, "start": 4044.92, "end": 4045.92, "text": " Any goals?", "tokens": [2639, 5493, 30], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1177, "seek": 402480, "start": 4045.92, "end": 4046.92, "text": " Any hopes?", "tokens": [2639, 13681, 30], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1178, "seek": 402480, "start": 4046.92, "end": 4048.92, "text": " I think I'm going to work on that.", "tokens": [286, 519, 286, 478, 516, 281, 589, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1179, "seek": 402480, "start": 4048.92, "end": 4049.92, "text": " It's not just the one hope.", "tokens": [467, 311, 406, 445, 264, 472, 1454, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1180, "seek": 402480, "start": 4049.92, "end": 4050.92, "text": " It's just that one PR.", "tokens": [467, 311, 445, 300, 472, 11568, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1181, "seek": 402480, "start": 4050.92, "end": 4051.92, "text": " That would be pretty cool.", "tokens": [663, 576, 312, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.3099432059213625, "compression_ratio": 1.5815602836879432, "no_speech_prob": 6.375513521561516e-07}, {"id": 1182, "seek": 405192, "start": 4051.92, "end": 4059.2000000000003, "text": " And then hopefully it gets backtracked to the Elm compiler.", "tokens": [400, 550, 4696, 309, 2170, 646, 19466, 292, 281, 264, 2699, 76, 31958, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1183, "seek": 405192, "start": 4059.2000000000003, "end": 4063.6, "text": " Because otherwise it's not very useful in practice.", "tokens": [1436, 5911, 309, 311, 406, 588, 4420, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1184, "seek": 405192, "start": 4063.6, "end": 4066.0, "text": " I think I'm going to work on Elm review.", "tokens": [286, 519, 286, 478, 516, 281, 589, 322, 2699, 76, 3131, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1185, "seek": 405192, "start": 4066.0, "end": 4067.0, "text": " Probably.", "tokens": [9210, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1186, "seek": 405192, "start": 4067.0, "end": 4069.12, "text": " Surprising, yeah.", "tokens": [6732, 26203, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1187, "seek": 405192, "start": 4069.12, "end": 4073.28, "text": " The one thing that I would really like to get in there is to add type information.", "tokens": [440, 472, 551, 300, 286, 576, 534, 411, 281, 483, 294, 456, 307, 281, 909, 2010, 1589, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1188, "seek": 405192, "start": 4073.28, "end": 4077.6800000000003, "text": " But for that I need to re-implant the Elm compiler.", "tokens": [583, 337, 300, 286, 643, 281, 319, 12, 332, 13067, 264, 2699, 76, 31958, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1189, "seek": 405192, "start": 4077.6800000000003, "end": 4080.2000000000003, "text": " To ask Martin Janicek to do that for me.", "tokens": [1407, 1029, 9184, 4956, 573, 74, 281, 360, 300, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.3399074122590839, "compression_ratio": 1.5411255411255411, "no_speech_prob": 4.313832562274911e-07}, {"id": 1190, "seek": 408020, "start": 4080.2, "end": 4086.24, "text": " So if you're listening, do it.", "tokens": [407, 498, 291, 434, 4764, 11, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1191, "seek": 408020, "start": 4086.24, "end": 4087.24, "text": " That would be amazing.", "tokens": [663, 576, 312, 2243, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1192, "seek": 408020, "start": 4087.24, "end": 4091.24, "text": " That would open up for so many possibilities.", "tokens": [663, 576, 1269, 493, 337, 370, 867, 12178, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1193, "seek": 408020, "start": 4091.24, "end": 4092.24, "text": " So that would be cool.", "tokens": [407, 300, 576, 312, 1627, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1194, "seek": 408020, "start": 4092.24, "end": 4093.64, "text": " Yeah, maybe I'll...", "tokens": [865, 11, 1310, 286, 603, 485], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1195, "seek": 408020, "start": 4093.64, "end": 4096.08, "text": " I'm really thinking about performance at the moment.", "tokens": [286, 478, 534, 1953, 466, 3389, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1196, "seek": 408020, "start": 4096.08, "end": 4099.72, "text": " So maybe I'll do a few more pull requests to Elm Optimizer 2.", "tokens": [407, 1310, 286, 603, 360, 257, 1326, 544, 2235, 12475, 281, 2699, 76, 35013, 6545, 568, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1197, "seek": 408020, "start": 4099.72, "end": 4101.04, "text": " Who knows?", "tokens": [2102, 3255, 30], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1198, "seek": 408020, "start": 4101.04, "end": 4105.08, "text": " Maybe we can look at those next year.", "tokens": [2704, 321, 393, 574, 412, 729, 958, 1064, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1199, "seek": 408020, "start": 4105.08, "end": 4107.8, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3639555561299227, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.844721049972577e-06}, {"id": 1200, "seek": 410780, "start": 4107.8, "end": 4112.56, "text": " Then what I'm probably going to do with Elm review is just add more information to the", "tokens": [1396, 437, 286, 478, 1391, 516, 281, 360, 365, 2699, 76, 3131, 307, 445, 909, 544, 1589, 281, 264], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1201, "seek": 410780, "start": 4112.56, "end": 4113.56, "text": " project.", "tokens": [1716, 13], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1202, "seek": 410780, "start": 4113.56, "end": 4120.72, "text": " Like being able to collect arbitrary files, like CSS files or arbitrary JSON, your translation", "tokens": [1743, 885, 1075, 281, 2500, 23211, 7098, 11, 411, 24387, 7098, 420, 23211, 31828, 11, 428, 12853], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1203, "seek": 410780, "start": 4120.72, "end": 4122.56, "text": " files, whatever.", "tokens": [7098, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1204, "seek": 410780, "start": 4122.56, "end": 4129.08, "text": " Because the more information you give to the linter, the better it becomes.", "tokens": [1436, 264, 544, 1589, 291, 976, 281, 264, 287, 5106, 11, 264, 1101, 309, 3643, 13], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1205, "seek": 410780, "start": 4129.08, "end": 4132.6, "text": " That's what I figured out when I was preparing talks this year.", "tokens": [663, 311, 437, 286, 8932, 484, 562, 286, 390, 10075, 6686, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1206, "seek": 410780, "start": 4132.6, "end": 4133.6, "text": " And then yeah.", "tokens": [400, 550, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1207, "seek": 410780, "start": 4133.6, "end": 4136.6, "text": " Because false positives is a lot of information.", "tokens": [1436, 7908, 35127, 307, 257, 688, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1208, "seek": 410780, "start": 4136.6, "end": 4137.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3180255355121933, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.555542889444041e-06}, {"id": 1209, "seek": 413760, "start": 4137.6, "end": 4144.84, "text": " And maybe to make it be able to do and maybe allow it to do more things like create files,", "tokens": [400, 1310, 281, 652, 309, 312, 1075, 281, 360, 293, 1310, 2089, 309, 281, 360, 544, 721, 411, 1884, 7098, 11], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1210, "seek": 413760, "start": 4144.84, "end": 4149.56, "text": " delete files, basically do something like Elm Code Gen in a way.", "tokens": [12097, 7098, 11, 1936, 360, 746, 411, 2699, 76, 15549, 3632, 294, 257, 636, 13], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1211, "seek": 413760, "start": 4149.56, "end": 4151.6, "text": " Elm Pages Code Gen review.", "tokens": [2699, 76, 430, 1660, 15549, 3632, 3131, 13], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1212, "seek": 413760, "start": 4151.6, "end": 4154.280000000001, "text": " Yeah, absolutely.", "tokens": [865, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1213, "seek": 413760, "start": 4154.280000000001, "end": 4155.280000000001, "text": " Maybe without the pages.", "tokens": [2704, 1553, 264, 7183, 13], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1214, "seek": 413760, "start": 4155.280000000001, "end": 4159.52, "text": " Matt, let's do something just you and me.", "tokens": [7397, 11, 718, 311, 360, 746, 445, 291, 293, 385, 13], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1215, "seek": 413760, "start": 4159.52, "end": 4161.0, "text": " It's always Elm Pages.", "tokens": [467, 311, 1009, 2699, 76, 430, 1660, 13], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1216, "seek": 413760, "start": 4161.0, "end": 4165.8, "text": " Why is it always me who's the third wheel?", "tokens": [1545, 307, 309, 1009, 385, 567, 311, 264, 2636, 5589, 30], "temperature": 0.0, "avg_logprob": -0.43547069436252706, "compression_ratio": 1.6567164179104477, "no_speech_prob": 2.5359886990372615e-07}, {"id": 1217, "seek": 416580, "start": 4165.8, "end": 4167.84, "text": " Okay, hear me out here.", "tokens": [1033, 11, 1568, 385, 484, 510, 13], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1218, "seek": 416580, "start": 4167.84, "end": 4171.0, "text": " Elm Pages Code Gen review Lambdaera.", "tokens": [2699, 76, 430, 1660, 15549, 3632, 3131, 45691, 1663, 13], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1219, "seek": 416580, "start": 4171.0, "end": 4172.0, "text": " Boom.", "tokens": [15523, 13], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1220, "seek": 416580, "start": 4172.0, "end": 4174.6, "text": " Now you're just trying to include Mario.", "tokens": [823, 291, 434, 445, 1382, 281, 4090, 9343, 13], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1221, "seek": 416580, "start": 4174.6, "end": 4178.68, "text": " No, actually I am very excited about the possibility.", "tokens": [883, 11, 767, 286, 669, 588, 2919, 466, 264, 7959, 13], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1222, "seek": 416580, "start": 4178.68, "end": 4186.24, "text": " Well already Elm Pages v3 beta uses Lambdaera, but other way around could be really cool", "tokens": [1042, 1217, 2699, 76, 430, 1660, 371, 18, 9861, 4960, 45691, 1663, 11, 457, 661, 636, 926, 727, 312, 534, 1627], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1223, "seek": 416580, "start": 4186.24, "end": 4187.24, "text": " too.", "tokens": [886, 13], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1224, "seek": 416580, "start": 4187.24, "end": 4190.4400000000005, "text": " And we've definitely discussed possibilities in that area.", "tokens": [400, 321, 600, 2138, 7152, 12178, 294, 300, 1859, 13], "temperature": 0.0, "avg_logprob": -0.3937212202284071, "compression_ratio": 1.4537037037037037, "no_speech_prob": 6.048795967217302e-06}, {"id": 1225, "seek": 419044, "start": 4190.44, "end": 4197.5599999999995, "text": " 2025 all tools converge into one mega tool.", "tokens": [39209, 439, 3873, 41881, 666, 472, 17986, 2290, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1226, "seek": 419044, "start": 4197.5599999999995, "end": 4198.5599999999995, "text": " That's what I'm predicting.", "tokens": [663, 311, 437, 286, 478, 32884, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1227, "seek": 419044, "start": 4198.5599999999995, "end": 4199.5599999999995, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1228, "seek": 419044, "start": 4199.5599999999995, "end": 4203.96, "text": " Elm Pages is already using Elm Code Gen, Lambdaera and Elm Review.", "tokens": [2699, 76, 430, 1660, 307, 1217, 1228, 2699, 76, 15549, 3632, 11, 45691, 1663, 293, 2699, 76, 19954, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1229, "seek": 419044, "start": 4203.96, "end": 4204.96, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1230, "seek": 419044, "start": 4204.96, "end": 4205.96, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1231, "seek": 419044, "start": 4205.96, "end": 4206.96, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1232, "seek": 419044, "start": 4206.96, "end": 4212.04, "text": " Yeah, it uses Elm Review for several things, including helping do dead code elimination", "tokens": [865, 11, 309, 4960, 2699, 76, 19954, 337, 2940, 721, 11, 3009, 4315, 360, 3116, 3089, 29224], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1233, "seek": 419044, "start": 4212.04, "end": 4213.28, "text": " for the data sources.", "tokens": [337, 264, 1412, 7139, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1234, "seek": 419044, "start": 4213.28, "end": 4216.759999999999, "text": " So they're stripped out of your client side bundle, which is very exciting.", "tokens": [407, 436, 434, 33221, 484, 295, 428, 6423, 1252, 24438, 11, 597, 307, 588, 4670, 13], "temperature": 0.0, "avg_logprob": -0.32581597108107346, "compression_ratio": 1.453061224489796, "no_speech_prob": 5.682258688466391e-06}, {"id": 1235, "seek": 421676, "start": 4216.76, "end": 4221.360000000001, "text": " So second hot take there, Elm Pages does a hostile reverse takeover.", "tokens": [407, 1150, 2368, 747, 456, 11, 2699, 76, 430, 1660, 775, 257, 27312, 9943, 747, 3570, 13], "temperature": 0.0, "avg_logprob": -0.29535301782751594, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.2376296985603403e-06}, {"id": 1236, "seek": 421676, "start": 4221.360000000001, "end": 4228.280000000001, "text": " This is all the other tools to be subservient to Elm Pages.", "tokens": [639, 307, 439, 264, 661, 3873, 281, 312, 2090, 1978, 1196, 281, 2699, 76, 430, 1660, 13], "temperature": 0.0, "avg_logprob": -0.29535301782751594, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.2376296985603403e-06}, {"id": 1237, "seek": 421676, "start": 4228.280000000001, "end": 4233.820000000001, "text": " I will neither confirm nor deny that.", "tokens": [286, 486, 9662, 9064, 6051, 15744, 300, 13], "temperature": 0.0, "avg_logprob": -0.29535301782751594, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.2376296985603403e-06}, {"id": 1238, "seek": 421676, "start": 4233.820000000001, "end": 4235.4800000000005, "text": " Anything else on your radar Jeroen?", "tokens": [11998, 1646, 322, 428, 16544, 508, 2032, 268, 30], "temperature": 0.0, "avg_logprob": -0.29535301782751594, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.2376296985603403e-06}, {"id": 1239, "seek": 421676, "start": 4235.4800000000005, "end": 4237.0, "text": " For now that's it.", "tokens": [1171, 586, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.29535301782751594, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.2376296985603403e-06}, {"id": 1240, "seek": 421676, "start": 4237.0, "end": 4242.4400000000005, "text": " But like most things I will go through with whatever I feel like I'm interested in.", "tokens": [583, 411, 881, 721, 286, 486, 352, 807, 365, 2035, 286, 841, 411, 286, 478, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.29535301782751594, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.2376296985603403e-06}, {"id": 1241, "seek": 421676, "start": 4242.4400000000005, "end": 4243.4400000000005, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.29535301782751594, "compression_ratio": 1.492822966507177, "no_speech_prob": 3.2376296985603403e-06}, {"id": 1242, "seek": 424344, "start": 4243.44, "end": 4247.36, "text": " If I'm interested in performance, I will look at that if I'm interested in fixing security", "tokens": [759, 286, 478, 3102, 294, 3389, 11, 286, 486, 574, 412, 300, 498, 286, 478, 3102, 294, 19442, 3825], "temperature": 0.0, "avg_logprob": -0.24010662993123708, "compression_ratio": 1.7870722433460076, "no_speech_prob": 1.4366275991051225e-06}, {"id": 1243, "seek": 424344, "start": 4247.36, "end": 4250.12, "text": " issues, I will do that again.", "tokens": [2663, 11, 286, 486, 360, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.24010662993123708, "compression_ratio": 1.7870722433460076, "no_speech_prob": 1.4366275991051225e-06}, {"id": 1244, "seek": 424344, "start": 4250.12, "end": 4254.48, "text": " If I'm interested in making Elm Review much faster, that's what I'll focus on.", "tokens": [759, 286, 478, 3102, 294, 1455, 2699, 76, 19954, 709, 4663, 11, 300, 311, 437, 286, 603, 1879, 322, 13], "temperature": 0.0, "avg_logprob": -0.24010662993123708, "compression_ratio": 1.7870722433460076, "no_speech_prob": 1.4366275991051225e-06}, {"id": 1245, "seek": 424344, "start": 4254.48, "end": 4259.799999999999, "text": " And it's probably going to be related to performance or Elm Review in some way.", "tokens": [400, 309, 311, 1391, 516, 281, 312, 4077, 281, 3389, 420, 2699, 76, 19954, 294, 512, 636, 13], "temperature": 0.0, "avg_logprob": -0.24010662993123708, "compression_ratio": 1.7870722433460076, "no_speech_prob": 1.4366275991051225e-06}, {"id": 1246, "seek": 424344, "start": 4259.799999999999, "end": 4265.28, "text": " And all the while I will keep trying to convince the JavaScript community that they're wrong", "tokens": [400, 439, 264, 1339, 286, 486, 1066, 1382, 281, 13447, 264, 15778, 1768, 300, 436, 434, 2085], "temperature": 0.0, "avg_logprob": -0.24010662993123708, "compression_ratio": 1.7870722433460076, "no_speech_prob": 1.4366275991051225e-06}, {"id": 1247, "seek": 424344, "start": 4265.28, "end": 4269.719999999999, "text": " and that they should do things like we do it in the Elm community.", "tokens": [293, 300, 436, 820, 360, 721, 411, 321, 360, 309, 294, 264, 2699, 76, 1768, 13], "temperature": 0.0, "avg_logprob": -0.24010662993123708, "compression_ratio": 1.7870722433460076, "no_speech_prob": 1.4366275991051225e-06}, {"id": 1248, "seek": 424344, "start": 4269.719999999999, "end": 4273.4, "text": " But not very hopeful for that.", "tokens": [583, 406, 588, 20531, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.24010662993123708, "compression_ratio": 1.7870722433460076, "no_speech_prob": 1.4366275991051225e-06}, {"id": 1249, "seek": 427340, "start": 4273.4, "end": 4278.16, "text": " The best way to convince someone is to tell them they're wrong, by the way.", "tokens": [440, 1151, 636, 281, 13447, 1580, 307, 281, 980, 552, 436, 434, 2085, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.24401357650756836, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521388068998931e-06}, {"id": 1250, "seek": 427340, "start": 4278.16, "end": 4280.92, "text": " I know, I know.", "tokens": [286, 458, 11, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.24401357650756836, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521388068998931e-06}, {"id": 1251, "seek": 427340, "start": 4280.92, "end": 4285.839999999999, "text": " In all seriousness, I do think actually a lot of the things we've talked about here", "tokens": [682, 439, 44880, 11, 286, 360, 519, 767, 257, 688, 295, 264, 721, 321, 600, 2825, 466, 510], "temperature": 0.0, "avg_logprob": -0.24401357650756836, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521388068998931e-06}, {"id": 1252, "seek": 427340, "start": 4285.839999999999, "end": 4291.839999999999, "text": " do reduce the barrier to entry for people coming into Elm and give a better on-ramp.", "tokens": [360, 5407, 264, 13357, 281, 8729, 337, 561, 1348, 666, 2699, 76, 293, 976, 257, 1101, 322, 12, 81, 1215, 13], "temperature": 0.0, "avg_logprob": -0.24401357650756836, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521388068998931e-06}, {"id": 1253, "seek": 427340, "start": 4291.839999999999, "end": 4298.219999999999, "text": " Because I think we all know that once you're up the on-ramp of Elm, you fall in love with", "tokens": [1436, 286, 519, 321, 439, 458, 300, 1564, 291, 434, 493, 264, 322, 12, 81, 1215, 295, 2699, 76, 11, 291, 2100, 294, 959, 365], "temperature": 0.0, "avg_logprob": -0.24401357650756836, "compression_ratio": 1.5695067264573992, "no_speech_prob": 2.521388068998931e-06}, {"id": 1254, "seek": 429822, "start": 4298.22, "end": 4304.0, "text": " all these things, all the things that seem like just a pain, decoders and not having", "tokens": [439, 613, 721, 11, 439, 264, 721, 300, 1643, 411, 445, 257, 1822, 11, 979, 378, 433, 293, 406, 1419], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1255, "seek": 429822, "start": 4304.0, "end": 4309.52, "text": " a bajillion NPM dependencies and things like that seem like a burden and how am I going", "tokens": [257, 23589, 11836, 426, 18819, 36606, 293, 721, 411, 300, 1643, 411, 257, 12578, 293, 577, 669, 286, 516], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1256, "seek": 429822, "start": 4309.52, "end": 4310.52, "text": " to manage?", "tokens": [281, 3067, 30], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1257, "seek": 429822, "start": 4310.52, "end": 4313.76, "text": " But then you're like, actually, packages that do exist are amazing.", "tokens": [583, 550, 291, 434, 411, 11, 767, 11, 17401, 300, 360, 2514, 366, 2243, 13], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1258, "seek": 429822, "start": 4313.76, "end": 4318.2, "text": " There are all these things that I can very easily do without a package.", "tokens": [821, 366, 439, 613, 721, 300, 286, 393, 588, 3612, 360, 1553, 257, 7372, 13], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1259, "seek": 429822, "start": 4318.2, "end": 4324.360000000001, "text": " It's so easy to manage my dependencies and it's so type safe to write these decoders", "tokens": [467, 311, 370, 1858, 281, 3067, 452, 36606, 293, 309, 311, 370, 2010, 3273, 281, 2464, 613, 979, 378, 433], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1260, "seek": 429822, "start": 4324.360000000001, "end": 4325.360000000001, "text": " in this way.", "tokens": [294, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1261, "seek": 429822, "start": 4325.360000000001, "end": 4327.4400000000005, "text": " You fall in love with it, but you need a good on-ramp.", "tokens": [509, 2100, 294, 959, 365, 309, 11, 457, 291, 643, 257, 665, 322, 12, 81, 1215, 13], "temperature": 0.0, "avg_logprob": -0.20827079365271647, "compression_ratio": 1.762962962962963, "no_speech_prob": 4.092739800398704e-06}, {"id": 1262, "seek": 432744, "start": 4327.44, "end": 4331.879999999999, "text": " So I think we're talking about a lot of things here that do ease that on-ramp.", "tokens": [407, 286, 519, 321, 434, 1417, 466, 257, 688, 295, 721, 510, 300, 360, 12708, 300, 322, 12, 81, 1215, 13], "temperature": 0.0, "avg_logprob": -0.2097357432047526, "compression_ratio": 1.6125, "no_speech_prob": 2.1233142888377188e-06}, {"id": 1263, "seek": 432744, "start": 4331.879999999999, "end": 4338.2, "text": " I'm definitely excited about in 2023, first of all, getting the Elmpages v3 stable release", "tokens": [286, 478, 2138, 2919, 466, 294, 44377, 11, 700, 295, 439, 11, 1242, 264, 2699, 76, 79, 1660, 371, 18, 8351, 4374], "temperature": 0.0, "avg_logprob": -0.2097357432047526, "compression_ratio": 1.6125, "no_speech_prob": 2.1233142888377188e-06}, {"id": 1264, "seek": 432744, "start": 4338.2, "end": 4342.28, "text": " is going to be a huge load off my back.", "tokens": [307, 516, 281, 312, 257, 2603, 3677, 766, 452, 646, 13], "temperature": 0.0, "avg_logprob": -0.2097357432047526, "compression_ratio": 1.6125, "no_speech_prob": 2.1233142888377188e-06}, {"id": 1265, "seek": 432744, "start": 4342.28, "end": 4349.12, "text": " I can't wait to finally be done with it, but also just have created this thing that I have", "tokens": [286, 393, 380, 1699, 281, 2721, 312, 1096, 365, 309, 11, 457, 611, 445, 362, 2942, 341, 551, 300, 286, 362], "temperature": 0.0, "avg_logprob": -0.2097357432047526, "compression_ratio": 1.6125, "no_speech_prob": 2.1233142888377188e-06}, {"id": 1266, "seek": 432744, "start": 4349.12, "end": 4355.36, "text": " really wanted to create for a long time for server-side rendering, dynamic server-side", "tokens": [534, 1415, 281, 1884, 337, 257, 938, 565, 337, 7154, 12, 1812, 22407, 11, 8546, 7154, 12, 1812], "temperature": 0.0, "avg_logprob": -0.2097357432047526, "compression_ratio": 1.6125, "no_speech_prob": 2.1233142888377188e-06}, {"id": 1267, "seek": 435536, "start": 4355.36, "end": 4358.839999999999, "text": " applications, a la Remix and Next.js.", "tokens": [5821, 11, 257, 635, 4080, 970, 293, 3087, 13, 25530, 13], "temperature": 0.0, "avg_logprob": -0.22049800769702807, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.5056364190968452e-06}, {"id": 1268, "seek": 435536, "start": 4358.839999999999, "end": 4362.639999999999, "text": " Very excited for there to be an answer to the question of how do you do that kind of", "tokens": [4372, 2919, 337, 456, 281, 312, 364, 1867, 281, 264, 1168, 295, 577, 360, 291, 360, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.22049800769702807, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.5056364190968452e-06}, {"id": 1269, "seek": 435536, "start": 4362.639999999999, "end": 4364.08, "text": " thing in Elm.", "tokens": [551, 294, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.22049800769702807, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.5056364190968452e-06}, {"id": 1270, "seek": 435536, "start": 4364.08, "end": 4370.599999999999, "text": " And I'm just, yeah, I think in 2023, I'm really excited to have that out there and do fun", "tokens": [400, 286, 478, 445, 11, 1338, 11, 286, 519, 294, 44377, 11, 286, 478, 534, 2919, 281, 362, 300, 484, 456, 293, 360, 1019], "temperature": 0.0, "avg_logprob": -0.22049800769702807, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.5056364190968452e-06}, {"id": 1271, "seek": 435536, "start": 4370.599999999999, "end": 4376.839999999999, "text": " things with it and to continue to try to answer this question of how do we make great experiences", "tokens": [721, 365, 309, 293, 281, 2354, 281, 853, 281, 1867, 341, 1168, 295, 577, 360, 321, 652, 869, 5235], "temperature": 0.0, "avg_logprob": -0.22049800769702807, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.5056364190968452e-06}, {"id": 1272, "seek": 435536, "start": 4376.839999999999, "end": 4381.48, "text": " for the web with Elm and how do we equip people with the tools to do that?", "tokens": [337, 264, 3670, 365, 2699, 76, 293, 577, 360, 321, 5037, 561, 365, 264, 3873, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.22049800769702807, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.5056364190968452e-06}, {"id": 1273, "seek": 438148, "start": 4381.48, "end": 4391.36, "text": " And so, yeah, I'm also, I've been doing some pairing with Philip Kruger on Tailwind for", "tokens": [400, 370, 11, 1338, 11, 286, 478, 611, 11, 286, 600, 668, 884, 512, 32735, 365, 21144, 6332, 44953, 322, 46074, 12199, 337], "temperature": 0.0, "avg_logprob": -0.25233398543463814, "compression_ratio": 1.5080213903743316, "no_speech_prob": 7.496298337628104e-08}, {"id": 1274, "seek": 438148, "start": 4391.36, "end": 4399.48, "text": " some Tailwind v3 changes for Elm Tailwind modules to take all of these permutations", "tokens": [512, 46074, 12199, 371, 18, 2962, 337, 2699, 76, 46074, 12199, 16679, 281, 747, 439, 295, 613, 4784, 325, 763], "temperature": 0.0, "avg_logprob": -0.25233398543463814, "compression_ratio": 1.5080213903743316, "no_speech_prob": 7.496298337628104e-08}, {"id": 1275, "seek": 438148, "start": 4399.48, "end": 4407.08, "text": " of different Tailwind variants that get generated into a bajillion generated functions and parameterize", "tokens": [295, 819, 46074, 12199, 21669, 300, 483, 10833, 666, 257, 23589, 11836, 10833, 6828, 293, 13075, 1125], "temperature": 0.0, "avg_logprob": -0.25233398543463814, "compression_ratio": 1.5080213903743316, "no_speech_prob": 7.496298337628104e-08}, {"id": 1276, "seek": 438148, "start": 4407.08, "end": 4408.08, "text": " color.", "tokens": [2017, 13], "temperature": 0.0, "avg_logprob": -0.25233398543463814, "compression_ratio": 1.5080213903743316, "no_speech_prob": 7.496298337628104e-08}, {"id": 1277, "seek": 440808, "start": 4408.08, "end": 4416.72, "text": " So instead of bg red 500, bg red 600, bg red 700, you have bg with color and then theme", "tokens": [407, 2602, 295, 272, 70, 2182, 5923, 11, 272, 70, 2182, 11849, 11, 272, 70, 2182, 15204, 11, 291, 362, 272, 70, 365, 2017, 293, 550, 6314], "temperature": 0.0, "avg_logprob": -0.22601513582117416, "compression_ratio": 1.52020202020202, "no_speech_prob": 3.689847005716729e-07}, {"id": 1278, "seek": 440808, "start": 4416.72, "end": 4420.0, "text": " dot red 600, for example.", "tokens": [5893, 2182, 11849, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.22601513582117416, "compression_ratio": 1.52020202020202, "no_speech_prob": 3.689847005716729e-07}, {"id": 1279, "seek": 440808, "start": 4420.0, "end": 4427.6, "text": " So that's another thing I'm excited about, shipping and getting a reduced period entry", "tokens": [407, 300, 311, 1071, 551, 286, 478, 2919, 466, 11, 14122, 293, 1242, 257, 9212, 2896, 8729], "temperature": 0.0, "avg_logprob": -0.22601513582117416, "compression_ratio": 1.52020202020202, "no_speech_prob": 3.689847005716729e-07}, {"id": 1280, "seek": 440808, "start": 4427.6, "end": 4434.8, "text": " for people who, you know, I think that having great first principles tools like Elm UI is", "tokens": [337, 561, 567, 11, 291, 458, 11, 286, 519, 300, 1419, 869, 700, 9156, 3873, 411, 2699, 76, 15682, 307], "temperature": 0.0, "avg_logprob": -0.22601513582117416, "compression_ratio": 1.52020202020202, "no_speech_prob": 3.689847005716729e-07}, {"id": 1281, "seek": 440808, "start": 4434.8, "end": 4435.8, "text": " important.", "tokens": [1021, 13], "temperature": 0.0, "avg_logprob": -0.22601513582117416, "compression_ratio": 1.52020202020202, "no_speech_prob": 3.689847005716729e-07}, {"id": 1282, "seek": 443580, "start": 4435.8, "end": 4440.400000000001, "text": " And I think people who say, well, we're on Tailwind, how do I do that in Elm is also", "tokens": [400, 286, 519, 561, 567, 584, 11, 731, 11, 321, 434, 322, 46074, 12199, 11, 577, 360, 286, 360, 300, 294, 2699, 76, 307, 611], "temperature": 0.0, "avg_logprob": -0.21970488420173304, "compression_ratio": 1.7056603773584906, "no_speech_prob": 1.4225548738977523e-07}, {"id": 1283, "seek": 443580, "start": 4440.400000000001, "end": 4441.52, "text": " super important.", "tokens": [1687, 1021, 13], "temperature": 0.0, "avg_logprob": -0.21970488420173304, "compression_ratio": 1.7056603773584906, "no_speech_prob": 1.4225548738977523e-07}, {"id": 1284, "seek": 443580, "start": 4441.52, "end": 4448.12, "text": " So I'm really excited to, like, we can have best in class ways to do that where it's like,", "tokens": [407, 286, 478, 534, 2919, 281, 11, 411, 11, 321, 393, 362, 1151, 294, 1508, 2098, 281, 360, 300, 689, 309, 311, 411, 11], "temperature": 0.0, "avg_logprob": -0.21970488420173304, "compression_ratio": 1.7056603773584906, "no_speech_prob": 1.4225548738977523e-07}, {"id": 1285, "seek": 443580, "start": 4448.12, "end": 4452.08, "text": " okay, here's how you do Tailwind and react, but check out how you can do it in Elm.", "tokens": [1392, 11, 510, 311, 577, 291, 360, 46074, 12199, 293, 4515, 11, 457, 1520, 484, 577, 291, 393, 360, 309, 294, 2699, 76, 13], "temperature": 0.0, "avg_logprob": -0.21970488420173304, "compression_ratio": 1.7056603773584906, "no_speech_prob": 1.4225548738977523e-07}, {"id": 1286, "seek": 443580, "start": 4452.08, "end": 4457.4400000000005, "text": " Check out how type safe it is and how high level it is, but you still are using Tailwind.", "tokens": [6881, 484, 577, 2010, 3273, 309, 307, 293, 577, 1090, 1496, 309, 307, 11, 457, 291, 920, 366, 1228, 46074, 12199, 13], "temperature": 0.0, "avg_logprob": -0.21970488420173304, "compression_ratio": 1.7056603773584906, "no_speech_prob": 1.4225548738977523e-07}, {"id": 1287, "seek": 443580, "start": 4457.4400000000005, "end": 4464.96, "text": " So I'm super bullish on that and yeah, excited to see what's in store for us in 2023.", "tokens": [407, 286, 478, 1687, 38692, 322, 300, 293, 1338, 11, 2919, 281, 536, 437, 311, 294, 3531, 337, 505, 294, 44377, 13], "temperature": 0.0, "avg_logprob": -0.21970488420173304, "compression_ratio": 1.7056603773584906, "no_speech_prob": 1.4225548738977523e-07}, {"id": 1288, "seek": 446496, "start": 4464.96, "end": 4465.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1289, "seek": 446496, "start": 4465.96, "end": 4472.68, "text": " I also want a website for Elm review because Matt's been making a few websites for his", "tokens": [286, 611, 528, 257, 3144, 337, 2699, 76, 3131, 570, 7397, 311, 668, 1455, 257, 1326, 12891, 337, 702], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1290, "seek": 446496, "start": 4472.68, "end": 4473.84, "text": " Elm projects.", "tokens": [2699, 76, 4455, 13], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1291, "seek": 446496, "start": 4473.84, "end": 4480.52, "text": " This year is going to be the year of Matt writing so many websites.", "tokens": [639, 1064, 307, 516, 281, 312, 264, 1064, 295, 7397, 3579, 370, 867, 12891, 13], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1292, "seek": 446496, "start": 4480.52, "end": 4483.56, "text": " And I'm just going to fork one of them and make Elm review.", "tokens": [400, 286, 478, 445, 516, 281, 17716, 472, 295, 552, 293, 652, 2699, 76, 3131, 13], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1293, "seek": 446496, "start": 4483.56, "end": 4484.56, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1294, "seek": 446496, "start": 4484.56, "end": 4485.88, "text": " Yeah, I need to write it.", "tokens": [865, 11, 286, 643, 281, 2464, 309, 13], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1295, "seek": 446496, "start": 4485.88, "end": 4492.72, "text": " You can do that right now, but they're very, it's like one page, but ultimately, I mean,", "tokens": [509, 393, 360, 300, 558, 586, 11, 457, 436, 434, 588, 11, 309, 311, 411, 472, 3028, 11, 457, 6284, 11, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1296, "seek": 446496, "start": 4492.72, "end": 4493.92, "text": " yeah, there's a sequence here.", "tokens": [1338, 11, 456, 311, 257, 8310, 510, 13], "temperature": 0.0, "avg_logprob": -0.27275182433047535, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.989228384351009e-07}, {"id": 1297, "seek": 449392, "start": 4493.92, "end": 4495.24, "text": " It's like design system generator.", "tokens": [467, 311, 411, 1715, 1185, 19265, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1298, "seek": 449392, "start": 4495.24, "end": 4496.24, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1299, "seek": 449392, "start": 4496.24, "end": 4497.24, "text": " I get that.", "tokens": [286, 483, 300, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1300, "seek": 449392, "start": 4497.24, "end": 4499.8, "text": " And then I'm able to use that to generate the stuff for the pages.", "tokens": [400, 550, 286, 478, 1075, 281, 764, 300, 281, 8460, 264, 1507, 337, 264, 7183, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1301, "seek": 449392, "start": 4499.8, "end": 4504.96, "text": " And then, yeah, no, I got at least four websites that I want to make this year.", "tokens": [400, 550, 11, 1338, 11, 572, 11, 286, 658, 412, 1935, 1451, 12891, 300, 286, 528, 281, 652, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1302, "seek": 449392, "start": 4504.96, "end": 4506.32, "text": " We've got ElmUI.com.", "tokens": [492, 600, 658, 2699, 76, 46324, 13, 1112, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1303, "seek": 449392, "start": 4506.32, "end": 4507.32, "text": " We got ElmGQL.com.", "tokens": [492, 658, 2699, 76, 38, 13695, 13, 1112, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1304, "seek": 449392, "start": 4507.32, "end": 4508.8, "text": " We got ElmCodeGen.com.", "tokens": [492, 658, 2699, 76, 34, 1429, 26647, 13, 1112, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1305, "seek": 449392, "start": 4508.8, "end": 4510.24, "text": " We got Elm.dev.", "tokens": [492, 658, 2699, 76, 13, 40343, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1306, "seek": 449392, "start": 4510.24, "end": 4513.6, "text": " I think I want to launch my own blog at some point.", "tokens": [286, 519, 286, 528, 281, 4025, 452, 1065, 6968, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1307, "seek": 449392, "start": 4513.6, "end": 4515.32, "text": " We'll see.", "tokens": [492, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1308, "seek": 449392, "start": 4515.32, "end": 4518.04, "text": " Don't each of those have to have their own blog as well?", "tokens": [1468, 380, 1184, 295, 729, 362, 281, 362, 641, 1065, 6968, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1309, "seek": 449392, "start": 4518.04, "end": 4519.04, "text": " Probably.", "tokens": [9210, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1310, "seek": 449392, "start": 4519.04, "end": 4520.04, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1311, "seek": 449392, "start": 4520.04, "end": 4523.28, "text": " Wait, your blog has a blog?", "tokens": [3802, 11, 428, 6968, 575, 257, 6968, 30], "temperature": 0.0, "avg_logprob": -0.3091773748397827, "compression_ratio": 1.6934865900383143, "no_speech_prob": 1.7330439732177183e-06}, {"id": 1312, "seek": 452328, "start": 4523.28, "end": 4524.28, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1313, "seek": 452328, "start": 4524.28, "end": 4526.32, "text": " He blogging about the blog.", "tokens": [634, 6968, 3249, 466, 264, 6968, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1314, "seek": 452328, "start": 4526.32, "end": 4529.12, "text": " It's the meta blog.", "tokens": [467, 311, 264, 19616, 6968, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1315, "seek": 452328, "start": 4529.12, "end": 4533.5199999999995, "text": " Well, I can't wait for all of these wonderful projects.", "tokens": [1042, 11, 286, 393, 380, 1699, 337, 439, 295, 613, 3715, 4455, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1316, "seek": 452328, "start": 4533.5199999999995, "end": 4537.36, "text": " And thank you again for coming on to ring in the new year with us.", "tokens": [400, 1309, 291, 797, 337, 1348, 322, 281, 4875, 294, 264, 777, 1064, 365, 505, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1317, "seek": 452328, "start": 4537.36, "end": 4543.719999999999, "text": " Mario, Matt, thank you so much for coming on the show and happy 2023, everybody.", "tokens": [9343, 11, 7397, 11, 1309, 291, 370, 709, 337, 1348, 322, 264, 855, 293, 2055, 44377, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1318, "seek": 452328, "start": 4543.719999999999, "end": 4544.719999999999, "text": " Thanks for having us.", "tokens": [2561, 337, 1419, 505, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1319, "seek": 452328, "start": 4544.719999999999, "end": 4545.719999999999, "text": " This is great.", "tokens": [639, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1320, "seek": 452328, "start": 4545.719999999999, "end": 4546.719999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1321, "seek": 452328, "start": 4546.719999999999, "end": 4547.719999999999, "text": " Happy 2023.", "tokens": [8277, 44377, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1322, "seek": 452328, "start": 4547.719999999999, "end": 4548.719999999999, "text": " And Jeroen, until next year.", "tokens": [400, 508, 2032, 268, 11, 1826, 958, 1064, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1323, "seek": 452328, "start": 4548.719999999999, "end": 4549.719999999999, "text": " Until next year.", "tokens": [9088, 958, 1064, 13], "temperature": 0.0, "avg_logprob": -0.33508192259689856, "compression_ratio": 1.584070796460177, "no_speech_prob": 4.5657043301616795e-06}, {"id": 1324, "seek": 454972, "start": 4549.72, "end": 4553.4800000000005, "text": " Happy 2021.", "tokens": [50364, 8277, 7201, 13, 50552], "temperature": 1.0, "avg_logprob": -1.910089333852132, "compression_ratio": 0.5789473684210527, "no_speech_prob": 3.456369449850172e-05}], "language": "en"}